52558: /* -*- Mode: C++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*-
52558:  * vim: set ts=4 sw=4 et tw=99:
52558:  *
52558:  * ***** BEGIN LICENSE BLOCK *****
52558:  * Version: MPL 1.1/GPL 2.0/LGPL 2.1
52558:  *
52558:  * The contents of this file are subject to the Mozilla Public License Version
52558:  * 1.1 (the "License"); you may not use this file except in compliance with
52558:  * the License. You may obtain a copy of the License at
52558:  * http://www.mozilla.org/MPL/
52558:  *
52558:  * Software distributed under the License is distributed on an "AS IS" basis,
52558:  * WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
52558:  * for the specific language governing rights and limitations under the
52558:  * License.
52558:  *
52558:  * The Original Code is Mozilla SpiderMonkey JavaScript 1.9 code, released
52558:  * May 28, 2008.
52558:  *
52558:  * The Initial Developer of the Original Code is
52558:  *   Brendan Eich <brendan@mozilla.org>
52558:  *
52558:  * Contributor(s):
52558:  *   David Anderson <danderson@mozilla.com>
52558:  *   David Mandelin <dmandelin@mozilla.com>
57718:  *   Jan de Mooij <jandemooij@gmail.com>
52558:  *
52558:  * Alternatively, the contents of this file may be used under the terms of
52558:  * either of the GNU General Public License Version 2 or later (the "GPL"),
52558:  * or the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
52558:  * in which case the provisions of the GPL or the LGPL are applicable instead
52558:  * of those above. If you wish to allow use of your version of this file only
52558:  * under the terms of either the GPL or the LGPL, and not to allow others to
52558:  * use your version of this file under the terms of the MPL, indicate your
52558:  * decision by deleting the provisions above and replace them with the notice
52558:  * and other provisions required by the GPL or the LGPL. If you do not delete
52558:  * the provisions above, a recipient may use your version of this file under
52558:  * the terms of any one of the MPL, the GPL or the LGPL.
52558:  *
52558:  * ***** END LICENSE BLOCK ***** */
53270: 
52558: #include "MethodJIT.h"
52651: #include "jsnum.h"
52686: #include "jsbool.h"
59882: #include "jsemit.h"
52737: #include "jsiter.h"
52558: #include "Compiler.h"
52611: #include "StubCalls.h"
52826: #include "MonoIC.h"
53270: #include "PolyIC.h"
60597: #include "ICChecker.h"
53168: #include "Retcon.h"
52611: #include "assembler/jit/ExecutableAllocator.h"
52826: #include "assembler/assembler/LinkBuffer.h"
52618: #include "FrameState-inl.h"
55746: #include "jsobjinlines.h"
52668: #include "jsscriptinlines.h"
68952: #include "InlineFrameAssembler.h"
54707: #include "jscompartment.h"
54840: #include "jsobjinlines.h"
54855: #include "jsopcodeinlines.h"
60534: #include "jshotloop.h"
52560: 
52560: #include "jsautooplen.h"
77415: #include "jstypedarrayinlines.h"
52558: 
52558: using namespace js;
52558: using namespace js::mjit;
56462: #if defined(JS_POLYIC) || defined(JS_MONOIC)
53270: using namespace js::mjit::ic;
53270: #endif
76194: using namespace js::analyze;
52558: 
56037: #define RETURN_IF_OOM(retval)                                   \
56037:     JS_BEGIN_MACRO                                              \
61233:         if (oomInVector || masm.oom() || stubcc.masm.oom())     \
56037:             return retval;                                      \
56037:     JS_END_MACRO
53168: 
52558: #if defined(JS_METHODJIT_SPEW)
52558: static const char *OpcodeNames[] = {
52558: # define OPDEF(op,val,name,token,length,nuses,ndefs,prec,format) #name,
52558: # include "jsopcode.tbl"
52558: # undef OPDEF
52558: };
52558: #endif
52558: 
76194: /*
76194:  * Number of times a script must be called or had a backedge before we try to
76194:  * inline its calls.
76194:  */
76194: static const size_t USES_BEFORE_INLINING = 10000;
76194: 
76194: mjit::Compiler::Compiler(JSContext *cx, JSScript *outerScript, bool isConstructing)
55463:   : BaseCompiler(cx),
76194:     outerScript(outerScript),
76194:     isConstructing(isConstructing),
76194:     ssa(cx, outerScript),
76194:     globalObj(outerScript->hasGlobal() ? outerScript->global() : NULL),
76194:     globalSlots(globalObj ? globalObj->getRawSlots() : NULL),
76194:     frame(cx, *thisFromCtor(), masm, stubcc),
76194:     a(NULL), outer(NULL), script(NULL), PC(NULL), loop(NULL),
76194:     inlineFrames(CompilerAllocPolicy(cx, *thisFromCtor())),
57679:     branchPatches(CompilerAllocPolicy(cx, *thisFromCtor())),
53119: #if defined JS_MONOIC
62386:     getGlobalNames(CompilerAllocPolicy(cx, *thisFromCtor())),
62386:     setGlobalNames(CompilerAllocPolicy(cx, *thisFromCtor())),
68952:     callICs(CompilerAllocPolicy(cx, *thisFromCtor())),
57679:     equalityICs(CompilerAllocPolicy(cx, *thisFromCtor())),
57679:     traceICs(CompilerAllocPolicy(cx, *thisFromCtor())),
53119: #endif
53119: #if defined JS_POLYIC
57679:     pics(CompilerAllocPolicy(cx, *thisFromCtor())),
57679:     getElemICs(CompilerAllocPolicy(cx, *thisFromCtor())),
57679:     setElemICs(CompilerAllocPolicy(cx, *thisFromCtor())),
53119: #endif
68952:     callPatches(CompilerAllocPolicy(cx, *thisFromCtor())),
57679:     callSites(CompilerAllocPolicy(cx, *thisFromCtor())),
57679:     doubleList(CompilerAllocPolicy(cx, *thisFromCtor())),
77457:     fixedIntToDoubleEntries(CompilerAllocPolicy(cx, *thisFromCtor())),
77457:     fixedDoubleToAnyEntries(CompilerAllocPolicy(cx, *thisFromCtor())),
59979:     jumpTables(CompilerAllocPolicy(cx, *thisFromCtor())),
59979:     jumpTableOffsets(CompilerAllocPolicy(cx, *thisFromCtor())),
76194:     loopEntries(CompilerAllocPolicy(cx, *thisFromCtor())),
78389:     rootedObjects(CompilerAllocPolicy(cx, *thisFromCtor())),
76194:     stubcc(cx, *thisFromCtor(), frame),
75489:     debugMode_(cx->compartment->debugMode()),
53136: #if defined JS_TRACER
57718:     addTraceHints(cx->traceJitEnabled),
76194: #else
76194:     addTraceHints(false),
53136: #endif
76194:     inlining_(false),
76194:     hasGlobalReallocation(false),
57718:     oomInVector(false),
78457:     overflowICSpace(false),
77892:     gcNumber(cx->runtime->gcNumber),
77407:     applyTricks(NoApplyTricks),
77407:     pcLengths(NULL)
74457: {
76194:     /* :FIXME: bug 637856 disabling traceJit if inference is enabled */
76194:     if (cx->typeInferenceEnabled())
76194:         addTraceHints = false;
76194: 
76194:     /* Once a script starts getting really hot we will inline calls in it. */
76194:     if (!debugMode() && cx->typeInferenceEnabled() && globalObj &&
77659:         (outerScript->getUseCount() >= USES_BEFORE_INLINING ||
76194:          cx->hasRunOption(JSOPTION_METHODJIT_ALWAYS))) {
76194:         inlining_ = true;
76194:     }
52558: }
52558: 
55503: CompileStatus
55503: mjit::Compiler::compile()
55503: {
76194:     JS_ASSERT_IF(isConstructing, !outerScript->jitCtor);
76194:     JS_ASSERT_IF(!isConstructing, !outerScript->jitNormal);
76194: 
76194:     JITScript **jit = isConstructing ? &outerScript->jitCtor : &outerScript->jitNormal;
55503:     void **checkAddr = isConstructing
76194:                        ? &outerScript->jitArityCheckCtor
76194:                        : &outerScript->jitArityCheckNormal;
55503: 
55503:     CompileStatus status = performCompilation(jit);
55503:     if (status == Compile_Okay) {
55503:         // Global scripts don't have an arity check entry. That's okay, we
55503:         // just need a pointer so the VM can quickly decide whether this
55503:         // method can be JIT'd or not. Global scripts cannot be IC'd, since
55503:         // they have no functions, so there is no danger.
55503:         *checkAddr = (*jit)->arityCheckEntry
55503:                      ? (*jit)->arityCheckEntry
55503:                      : (*jit)->invokeEntry;
76194:     } else if (status != Compile_Retry) {
76194:         *checkAddr = JS_UNJITTABLE_SCRIPT;
77391:         if (outerScript->hasFunction) {
77361:             outerScript->uninlineable = true;
77391:             types::MarkTypeObjectFlags(cx, outerScript->function(),
76194:                                        types::OBJECT_FLAG_UNINLINEABLE);
76194:         }
76194:     }
76194: 
76194:     return status;
76194: }
76194: 
76194: CompileStatus
76194: mjit::Compiler::checkAnalysis(JSScript *script)
76194: {
77630:     if (script->hasClearedGlobal()) {
77630:         JaegerSpew(JSpew_Abort, "script has a cleared global\n");
77630:         return Compile_Abort;
77630:     }
77630: 
77884:     if (!script->ensureRanAnalysis(cx))
77391:         return Compile_Error;
76194:     if (cx->typeInferenceEnabled() && !script->ensureRanInference(cx))
76194:         return Compile_Error;
77391: 
77391:     ScriptAnalysis *analysis = script->analysis();
76194:     if (analysis->failed()) {
76194:         JaegerSpew(JSpew_Abort, "couldn't analyze bytecode; probably switchX or OOM\n");
76194:         return Compile_Abort;
76194:     }
76194: 
76194:     return Compile_Okay;
76194: }
76194: 
76194: CompileStatus
76194: mjit::Compiler::addInlineFrame(JSScript *script, uint32 depth,
76194:                                uint32 parent, jsbytecode *parentpc)
76194: {
76194:     JS_ASSERT(inlining());
76194: 
76194:     CompileStatus status = checkAnalysis(script);
76194:     if (status != Compile_Okay)
76194:         return status;
76194: 
76194:     if (!ssa.addInlineFrame(script, depth, parent, parentpc))
76194:         return Compile_Error;
76194: 
76194:     uint32 index = ssa.iterFrame(ssa.numFrames() - 1).index;
76194:     return scanInlineCalls(index, depth);
76194: }
76194: 
76194: CompileStatus
76194: mjit::Compiler::scanInlineCalls(uint32 index, uint32 depth)
76194: {
76194:     /* Maximum number of calls we will inline at the same site. */
76194:     static const uint32 INLINE_SITE_LIMIT = 5;
76194: 
76194:     JS_ASSERT(inlining() && globalObj);
76194: 
76194:     /* Not inlining yet from 'new' scripts. */
76194:     if (isConstructing)
76194:         return Compile_Okay;
76194: 
76194:     JSScript *script = ssa.getFrame(index).script;
77391:     ScriptAnalysis *analysis = script->analysis();
76194: 
76194:     /* Don't inline from functions which could have a non-global scope object. */
76194:     if (!script->hasGlobal() ||
76194:         script->global() != globalObj ||
77391:         (script->hasFunction && script->function()->getParent() != globalObj) ||
77391:         (script->hasFunction && script->function()->isHeavyweight()) ||
76194:         script->isActiveEval) {
76194:         return Compile_Okay;
76194:     }
76194: 
76194:     uint32 nextOffset = 0;
76194:     while (nextOffset < script->length) {
76194:         uint32 offset = nextOffset;
76194:         jsbytecode *pc = script->code + offset;
77434:         nextOffset = offset + analyze::GetBytecodeLength(pc);
76194: 
76194:         Bytecode *code = analysis->maybeCode(pc);
76194:         if (!code)
76194:             continue;
76194: 
76194:         /* :XXX: Not yet inlining 'new' calls. */
76194:         if (JSOp(*pc) != JSOP_CALL)
76194:             continue;
76194: 
76194:         /* Not inlining at monitored call sites or those with type barriers. */
78194:         if (code->monitoredTypes || code->monitoredTypesReturn || analysis->typeBarriers(cx, pc) != NULL)
76194:             continue;
76194: 
76194:         uint32 argc = GET_ARGC(pc);
76194:         types::TypeSet *calleeTypes = analysis->poppedTypes(pc, argc + 1);
76194: 
76194:         if (calleeTypes->getKnownTypeTag(cx) != JSVAL_TYPE_OBJECT)
76194:             continue;
76194: 
76194:         if (calleeTypes->getObjectCount() >= INLINE_SITE_LIMIT)
76194:             continue;
76194: 
76194:         /*
76194:          * Compute the maximum height we can grow the stack for inlined frames.
76194:          * We always reserve space for loop temporaries, for an extra stack
76194:          * frame pushed when making a call from the deepest inlined frame, and
76194:          * for the temporary slot used by type barriers.
76194:          */
76194:         uint32 stackLimit = outerScript->nslots + StackSpace::STACK_JIT_EXTRA
76194:             - VALUES_PER_STACK_FRAME - FrameState::TEMPORARY_LIMIT - 1;
76194: 
76194:         /* Compute the depth of any frames inlined at this site. */
76194:         uint32 nextDepth = depth + VALUES_PER_STACK_FRAME + script->nfixed + code->stackDepth;
76194: 
76194:         /*
76194:          * Scan each of the possible callees for other conditions precluding
76194:          * inlining. We only inline at a call site if all callees are inlineable.
76194:          */
76194:         unsigned count = calleeTypes->getObjectCount();
76194:         bool okay = true;
76194:         for (unsigned i = 0; i < count; i++) {
77353:             if (calleeTypes->getTypeObject(i) != NULL) {
76194:                 okay = false;
76194:                 break;
76194:             }
76194: 
77353:             JSObject *obj = calleeTypes->getSingleObject(i);
77353:             if (!obj)
77353:                 continue;
77353: 
77353:             if (!obj->isFunction()) {
77353:                 okay = false;
77353:                 break;
77353:             }
77353: 
77353:             JSFunction *fun = obj->getFunctionPrivate();
76194:             if (!fun->isInterpreted()) {
76194:                 okay = false;
76194:                 break;
76194:             }
76194:             JSScript *script = fun->script();
76194: 
76194:             /*
78729:              * Don't inline calls to scripts which haven't been analyzed.
78729:              * We need to analyze the inlined scripts to compile them, and
78729:              * doing so can change type information we have queried already
78729:              * in making inlining decisions.
78729:              */
78729:             if (!script->hasAnalysis() || !script->analysis()->ranInference()) {
78729:                 okay = false;
78729:                 break;
78729:             }
78729: 
78729:             /*
76194:              * The outer and inner scripts must have the same scope. This only
76194:              * allows us to inline calls between non-inner functions. Also
76194:              * check for consistent strictness between the functions.
76194:              */
76194:             if (!globalObj ||
76194:                 fun->getParent() != globalObj ||
76194:                 outerScript->strictModeCode != script->strictModeCode) {
76194:                 okay = false;
76194:                 break;
76194:             }
76194: 
76194:             /* We can't cope with inlining recursive functions yet. */
76194:             uint32 nindex = index;
76194:             while (nindex != CrossScriptSSA::INVALID_FRAME) {
76194:                 if (ssa.getFrame(nindex).script == script)
76194:                     okay = false;
76194:                 nindex = ssa.getFrame(nindex).parent;
76194:             }
76194:             if (!okay)
76194:                 break;
76194: 
76194:             /* Watch for excessively deep nesting of inlined frames. */
76194:             if (nextDepth + script->nslots >= stackLimit) {
76194:                 okay = false;
76194:                 break;
76194:             }
76194: 
77884:             if (!script->types || !script->types->hasScope()) {
77884:                 okay = false;
77884:                 break;
77884:             }
77884: 
76194:             CompileStatus status = checkAnalysis(script);
76194:             if (status != Compile_Okay)
76194:                 return status;
76194: 
77391:             if (!script->analysis()->inlineable(argc)) {
76194:                 okay = false;
76194:                 break;
76194:             }
76194: 
77353:             if (types::TypeSet::HasObjectFlags(cx, fun->getType(cx),
76194:                                                types::OBJECT_FLAG_UNINLINEABLE)) {
76194:                 okay = false;
76194:                 break;
76194:             }
76194: 
76194:             /*
76194:              * Don't inline scripts which use 'this' if it is possible they
76194:              * could be called with a 'this' value requiring wrapping. During
76194:              * inlining we do not want to modify frame entries belonging to the
76194:              * caller.
76194:              */
77391:             if (script->analysis()->usesThisValue() &&
77391:                 types::TypeScript::ThisTypes(script)->getKnownTypeTag(cx) != JSVAL_TYPE_OBJECT) {
76194:                 okay = false;
76194:                 break;
76194:             }
76194:         }
76194:         if (!okay)
76194:             continue;
76194: 
76194:         calleeTypes->addFreeze(cx);
76194: 
76194:         /*
76194:          * Add the inline frames to the cross script SSA. We will pick these
76194:          * back up when compiling the call site.
76194:          */
76194:         for (unsigned i = 0; i < count; i++) {
77353:             JSObject *obj = calleeTypes->getSingleObject(i);
77353:             if (!obj)
76194:                 continue;
76194: 
77353:             JSFunction *fun = obj->getFunctionPrivate();
76194:             JSScript *script = fun->script();
76194: 
76194:             CompileStatus status = addInlineFrame(script, nextDepth, index, pc);
76194:             if (status != Compile_Okay)
76194:                 return status;
76194:         }
76194:     }
76194: 
76194:     return Compile_Okay;
76194: }
76194: 
76194: CompileStatus
76194: mjit::Compiler::pushActiveFrame(JSScript *script, uint32 argc)
76194: {
76194:     ActiveFrame *newa = cx->new_<ActiveFrame>(cx);
76194:     if (!newa)
76194:         return Compile_Error;
76194: 
76194:     newa->parent = a;
76194:     if (a)
76194:         newa->parentPC = PC;
76194:     newa->script = script;
80222:     newa->mainCodeStart = masm.size();
80222:     newa->stubCodeStart = stubcc.size();
76194: 
76194:     if (outer) {
76194:         newa->inlineIndex = uint32(inlineFrames.length());
76194:         inlineFrames.append(newa);
55503:     } else {
76194:         newa->inlineIndex = CrossScriptSSA::OUTER_FRAME;
76194:         outer = newa;
76194:     }
76194:     JS_ASSERT(ssa.getFrame(newa->inlineIndex).script == script);
76194: 
80222:     newa->inlinePCOffset = ssa.frameLength(newa->inlineIndex);
80222: 
77391:     ScriptAnalysis *newAnalysis = script->analysis();
76194: 
76194: #ifdef JS_METHODJIT_SPEW
76194:     if (cx->typeInferenceEnabled() && IsJaegerSpewChannelActive(JSpew_Regalloc)) {
77391:         unsigned nargs = script->hasFunction ? script->function()->nargs : 0;
76194:         for (unsigned i = 0; i < nargs; i++) {
76194:             uint32 slot = ArgSlot(i);
76194:             if (!newAnalysis->slotEscapes(slot)) {
76194:                 JaegerSpew(JSpew_Regalloc, "Argument %u:", i);
76194:                 newAnalysis->liveness(slot).print();
76194:             }
76194:         }
76194:         for (unsigned i = 0; i < script->nfixed; i++) {
76194:             uint32 slot = LocalSlot(script, i);
76194:             if (!newAnalysis->slotEscapes(slot)) {
76194:                 JaegerSpew(JSpew_Regalloc, "Local %u:", i);
76194:                 newAnalysis->liveness(slot).print();
76194:             }
76194:         }
76194:     }
76194: #endif
76194: 
76194:     if (!frame.pushActiveFrame(script, argc)) {
76194:         js_ReportOutOfMemory(cx);
76194:         return Compile_Error;
76194:     }
76194: 
76194:     newa->jumpMap = (Label *)cx->malloc_(sizeof(Label) * script->length);
76194:     if (!newa->jumpMap) {
76194:         js_ReportOutOfMemory(cx);
76194:         return Compile_Error;
76194:     }
76194: #ifdef DEBUG
76194:     for (uint32 i = 0; i < script->length; i++)
76194:         newa->jumpMap[i] = Label();
76194: #endif
76194: 
76194:     if (cx->typeInferenceEnabled()) {
76194:         CompileStatus status = prepareInferenceTypes(script, newa);
76194:         if (status != Compile_Okay)
55503:             return status;
55503:     }
55503: 
76194:     this->script = script;
76194:     this->analysis = newAnalysis;
76194:     this->PC = script->code;
76194:     this->a = newa;
76194: 
76194:     return Compile_Okay;
76194: }
76194: 
76194: void
76194: mjit::Compiler::popActiveFrame()
76194: {
76194:     JS_ASSERT(a->parent);
76194:     this->PC = a->parentPC;
76194:     this->a = a->parent;
76194:     this->script = a->script;
77391:     this->analysis = this->script->analysis();
76194: 
76194:     frame.popActiveFrame();
76194: }
76194: 
52558: #define CHECK_STATUS(expr)                                           \
52558:     JS_BEGIN_MACRO                                                   \
52558:         CompileStatus status_ = (expr);                              \
61233:         if (status_ != Compile_Okay) {                               \
61233:             if (oomInVector || masm.oom() || stubcc.masm.oom())      \
61233:                 js_ReportOutOfMemory(cx);                            \
52558:             return status_;                                          \
61233:         }                                                            \
52558:     JS_END_MACRO
52558: 
52558: CompileStatus
55503: mjit::Compiler::performCompilation(JITScript **jitp)
52558: {
52558:     JaegerSpew(JSpew_Scripts, "compiling script (file \"%s\") (line \"%d\") (length \"%d\")\n",
76194:                outerScript->filename, outerScript->lineno, outerScript->length);
76194: 
76194:     if (inlining()) {
76194:         JaegerSpew(JSpew_Inlining, "inlining calls in script (file \"%s\") (line \"%d\")\n",
76194:                    outerScript->filename, outerScript->lineno);
76194:     }
52558: 
52654: #ifdef JS_METHODJIT_SPEW
52558:     Profiler prof;
52558:     prof.start();
52654: #endif
52558: 
53590: #ifdef JS_METHODJIT
76194:     outerScript->debugMode = debugMode();
53590: #endif
53590: 
76194:     JS_ASSERT(cx->compartment->activeInference);
76194: 
76194:     {
76194:         types::AutoEnterCompilation enter(cx, outerScript);
76194: 
76194:         CHECK_STATUS(checkAnalysis(outerScript));
76194:         if (inlining())
76194:             CHECK_STATUS(scanInlineCalls(CrossScriptSSA::OUTER_FRAME, 0));
76194:         CHECK_STATUS(pushActiveFrame(outerScript, 0));
52558:         CHECK_STATUS(generatePrologue());
52558:         CHECK_STATUS(generateMethod());
52558:         CHECK_STATUS(generateEpilogue());
55503:         CHECK_STATUS(finishThisUp(jitp));
76194:     }
52558: 
52558: #ifdef JS_METHODJIT_SPEW
52558:     prof.stop();
52558:     JaegerSpew(JSpew_Prof, "compilation took %d us\n", prof.time_us());
52558: #endif
52558: 
77503:     JaegerSpew(JSpew_Scripts, "successfully compiled (code \"%p\") (size \"%u\")\n",
77503:                (*jitp)->code.m_code.executableAddress(), unsigned((*jitp)->code.m_size));
52558: 
76194:     if (!*jitp)
76194:         return Compile_Abort;
76194: 
52560:     return Compile_Okay;
52558: }
52558: 
52558: #undef CHECK_STATUS
52558: 
76194: mjit::Compiler::ActiveFrame::ActiveFrame(JSContext *cx)
76194:     : parent(NULL), parentPC(NULL), script(NULL), jumpMap(NULL),
76194:       inlineIndex(uint32(-1)), varTypes(NULL), needReturnValue(false),
76194:       syncReturnValue(false), returnValueDouble(false), returnSet(false),
76194:       returnEntry(NULL), returnJumps(NULL), exitState(NULL)
76194: {}
76194: 
76194: mjit::Compiler::ActiveFrame::~ActiveFrame()
76194: {
76194:     js::Foreground::free_(jumpMap);
76194:     if (varTypes)
76194:         js::Foreground::free_(varTypes);
76194: }
76194: 
52558: mjit::Compiler::~Compiler()
52558: {
76194:     if (outer)
76194:         cx->delete_(outer);
76194:     for (unsigned i = 0; i < inlineFrames.length(); i++)
76194:         cx->delete_(inlineFrames[i]);
76194:     while (loop) {
76194:         LoopState *nloop = loop->outer;
76194:         cx->delete_(loop);
76194:         loop = nloop;
76194:     }
76194: }
76194: 
76194: CompileStatus
76194: mjit::Compiler::prepareInferenceTypes(JSScript *script, ActiveFrame *a)
76194: {
76194:     /*
76194:      * During our walk of the script, we need to preserve the invariant that at
76194:      * join points the in memory type tag is always in sync with the known type
76194:      * tag of the variable's SSA value at that join point. In particular, SSA
76194:      * values inferred as (int|double) must in fact be doubles, stored either
76194:      * in floating point registers or in memory. There is an exception for
76194:      * locals whose value is currently dead, whose type might not be synced.
76194:      *
76194:      * To ensure this, we need to know the SSA values for each variable at each
76194:      * join point, which the SSA analysis does not store explicitly. These can
76194:      * be recovered, though. During the forward walk, the SSA value of a var
76194:      * (and its associated type set) change only when we see an explicit assign
76194:      * to the var or get to a join point with a phi node for that var. So we
76194:      * can duplicate the effects of that walk here by watching for writes to
76194:      * vars (updateVarTypes) and new phi nodes at join points.
76194:      *
76194:      * When we get to a branch and need to know a variable's value at the
76194:      * branch target, we know it will either be a phi node at the target or
76194:      * the variable's current value, as no phi node is created at the target
76194:      * only if a variable has the same value on all incoming edges.
76194:      */
76194: 
76194:     a->varTypes = (VarType *)
76194:         cx->calloc_(TotalSlots(script) * sizeof(VarType));
76194:     if (!a->varTypes)
76194:         return Compile_Error;
76194: 
76194:     for (uint32 slot = ArgSlot(0); slot < TotalSlots(script); slot++) {
76194:         VarType &vt = a->varTypes[slot];
77391:         vt.types = types::TypeScript::SlotTypes(script, slot);
76194:         vt.type = vt.types->getKnownTypeTag(cx);
76194:     }
76194: 
76194:     return Compile_Okay;
52558: }
52558: 
54626: CompileStatus JS_NEVER_INLINE
77461: mjit::TryCompile(JSContext *cx, JSScript *script, bool construct)
77461: {
57705: #if JS_HAS_SHARP_VARS
77461:     if (script->hasSharps)
57705:         return Compile_Abort;
57705: #endif
72551:     bool ok = cx->compartment->ensureJaegerCompartmentExists(cx);
72551:     if (!ok)
72551:         return Compile_Abort;
57705: 
56201:     // Ensure that constructors have at least one slot.
77461:     if (construct && !script->nslots)
77461:         script->nslots++;
76766: 
76194:     CompileStatus status;
76194:     {
76194:         types::AutoEnterTypeInference enter(cx, true);
76194: 
77461:         Compiler cc(cx, script, construct);
76194:         status = cc.compile();
76194:     }
76194: 
76194:     if (status == Compile_Okay) {
76194:         /*
76194:          * Compiling a script can occasionally trigger its own recompilation.
76194:          * Treat this the same way as a static overflow and wait for another
76194:          * attempt to compile the script.
76194:          */
77461:         JITScriptStatus status = script->getJITStatus(construct);
76194:         JS_ASSERT(status != JITScript_Invalid);
76194:         return (status == JITScript_Valid) ? Compile_Okay : Compile_Retry;
76194:     }
76194: 
76194:     /* Non-OOM errors should have an associated exception. */
76194:     JS_ASSERT_IF(status == Compile_Error,
76194:                  cx->isExceptionPending() || cx->runtime->hadOutOfMemory);
76194: 
76194:     return status;
57766: }
57766: 
52611: CompileStatus
52558: mjit::Compiler::generatePrologue()
52558: {
52873:     invokeLabel = masm.label();
53471: 
52873:     /*
52873:      * If there is no function, then this can only be called via JaegerShot(),
52873:      * which expects an existing frame to be initialized like the interpreter.
52873:      */
77391:     if (script->hasFunction) {
52873:         Jump j = masm.jump();
53590: 
53590:         /*
53590:          * Entry point #2: The caller has partially constructed a frame, and
53590:          * either argc >= nargs or the arity check has corrected the frame.
53590:          */
52873:         invokeLabel = masm.label();
53590: 
53590:         Label fastPath = masm.label();
53590: 
54832:         /* Store this early on so slow paths can access it. */
77391:         masm.storePtr(ImmPtr(script->function()), Address(JSFrameReg, StackFrame::offsetOfExec()));
53590: 
53590:         {
53590:             /*
53590:              * Entry point #3: The caller has partially constructed a frame,
53590:              * but argc might be != nargs, so an arity check might be called.
53590:              *
53590:              * This loops back to entry point #2.
53590:              */
53590:             arityLabel = stubcc.masm.label();
76194: 
53840:             Jump argMatch = stubcc.masm.branch32(Assembler::Equal, JSParamReg_Argc,
77391:                                                  Imm32(script->function()->nargs));
53590: 
53840:             if (JSParamReg_Argc != Registers::ArgReg1)
53840:                 stubcc.masm.move(JSParamReg_Argc, Registers::ArgReg1);
53840: 
53590:             /* Slow path - call the arity check function. Returns new fp. */
77391:             stubcc.masm.storePtr(ImmPtr(script->function()),
76194:                                  Address(JSFrameReg, StackFrame::offsetOfExec()));
76194:             OOL_STUBCALL(stubs::FixupArity, REJOIN_NONE);
53590:             stubcc.masm.move(Registers::ReturnReg, JSFrameReg);
76194:             argMatch.linkTo(stubcc.masm.label(), &stubcc.masm);
76194: 
76194:             argsCheckLabel = stubcc.masm.label();
76194: 
76194:             /* Type check the arguments as well. */
76194:             if (cx->typeInferenceEnabled()) {
76194: #ifdef JS_MONOIC
76194:                 this->argsCheckJump = stubcc.masm.jump();
76194:                 this->argsCheckStub = stubcc.masm.label();
76194:                 this->argsCheckJump.linkTo(this->argsCheckStub, &stubcc.masm);
76194: #endif
77391:                 stubcc.masm.storePtr(ImmPtr(script->function()), Address(JSFrameReg, StackFrame::offsetOfExec()));
76194:                 OOL_STUBCALL(stubs::CheckArgumentTypes, REJOIN_CHECK_ARGUMENTS);
76194: #ifdef JS_MONOIC
76194:                 this->argsCheckFallthrough = stubcc.masm.label();
76194: #endif
76194:             }
76194: 
53590:             stubcc.crossJump(stubcc.masm.jump(), fastPath);
53590:         }
53590: 
53590:         /*
76194:          * Guard that there is enough stack space. Note we reserve space for
76194:          * any inline frames we end up generating, or a callee's stack frame
76194:          * we write to before the callee checks the stack.
53590:          */
76194:         uint32 nvals = VALUES_PER_STACK_FRAME + script->nslots + StackSpace::STACK_JIT_EXTRA;
76194:         masm.addPtr(Imm32(nvals * sizeof(Value)), JSFrameReg, Registers::ReturnReg);
53590:         Jump stackCheck = masm.branchPtr(Assembler::AboveOrEqual, Registers::ReturnReg,
53590:                                          FrameAddress(offsetof(VMFrame, stackLimit)));
53590: 
74330:         /*
74330:          * If the stack check fails then we need to either commit more of the
74330:          * reserved stack space or throw an error. Specify that the number of
74330:          * local slots is 0 (instead of the default script->nfixed) since the
74330:          * range [fp->slots(), fp->base()) may not be commited. (The calling
74330:          * contract requires only that the caller has reserved space for fp.)
74330:          */
53590:         {
53590:             stubcc.linkExitDirect(stackCheck, stubcc.masm.label());
76194:             OOL_STUBCALL(stubs::HitStackQuota, REJOIN_NONE);
53590:             stubcc.crossJump(stubcc.masm.jump(), masm.label());
53590:         }
53590: 
56602:         /*
56602:          * Set locals to undefined, as in initCallFrameLatePrologue.
56602:          * Skip locals which aren't closed and are known to be defined before used,
56602:          * :FIXME: bug 604541: write undefined if we might be using the tracer, so it works.
56602:          */
53488:         for (uint32 i = 0; i < script->nfixed; i++) {
56602:             if (analysis->localHasUseBeforeDef(i) || addTraceHints) {
69223:                 Address local(JSFrameReg, sizeof(StackFrame) + i * sizeof(Value));
53081:                 masm.storeValue(UndefinedValue(), local);
52872:             }
56602:         }
52872: 
77884:         types::TypeScriptNesting *nesting = script->nesting();
77884: 
77884:         /*
77884:          * Run the function prologue if necessary. This is always done in a
77884:          * stub for heavyweight functions (including nesting outer functions).
77884:          */
77884:         JS_ASSERT_IF(nesting && nesting->children, script->function()->isHeavyweight());
77391:         if (script->function()->isHeavyweight()) {
53087:             prepareStubCall(Uses(0));
77884:             INLINE_STUBCALL(stubs::FunctionFramePrologue, REJOIN_FUNCTION_PROLOGUE);
77884:         } else {
77884:             /*
77884:              * Load the scope chain into the frame if it will be needed by NAME
77884:              * opcodes or by the nesting prologue below. The scope chain is
77884:              * always set for global and eval frames, and will have been set by
64364:              * CreateFunCallObject for heavyweight function frames.
55483:              */
77884:             if (analysis->usesScopeChain() || nesting) {
55483:                 RegisterID t0 = Registers::ReturnReg;
55483:                 Jump hasScope = masm.branchTest32(Assembler::NonZero,
76194:                                                   FrameFlagsAddress(), Imm32(StackFrame::HAS_SCOPECHAIN));
77391:                 masm.loadPayload(Address(JSFrameReg, StackFrame::offsetOfCallee(script->function())), t0);
55483:                 masm.loadPtr(Address(t0, offsetof(JSObject, parent)), t0);
69223:                 masm.storePtr(t0, Address(JSFrameReg, StackFrame::offsetOfScopeChain()));
55483:                 hasScope.linkTo(masm.label(), &masm);
55483:             }
76194: 
77884:             if (nesting) {
77884:                 /*
77884:                  * Inline the common case for the nesting prologue: the
77884:                  * function is a non-heavyweight inner function with no
77884:                  * children of its own. We ensure during inference that the
77884:                  * outer function does not add scope objects for 'let' or
77884:                  * 'with', so that the frame's scope chain will be
77884:                  * the parent's call object, and if it differs from the
77884:                  * parent's current activation then the parent is reentrant.
77884:                  */
77884:                 JSScript *parent = nesting->parent;
77884:                 JS_ASSERT(parent);
77884:                 JS_ASSERT_IF(parent->hasAnalysis() && parent->analysis()->ranBytecode(),
77884:                              !parent->analysis()->addsScopeObjects());
77884: 
77884:                 RegisterID t0 = Registers::ReturnReg;
77884:                 masm.move(ImmPtr(&parent->nesting()->activeCall), t0);
77884:                 masm.loadPtr(Address(t0), t0);
77884: 
77884:                 Address scopeChain(JSFrameReg, StackFrame::offsetOfScopeChain());
77884:                 Jump mismatch = masm.branchPtr(Assembler::NotEqual, t0, scopeChain);
77884:                 masm.add32(Imm32(1), AbsoluteAddress(&nesting->activeFrames));
77884: 
77884:                 stubcc.linkExitDirect(mismatch, stubcc.masm.label());
77884:                 OOL_STUBCALL(stubs::FunctionFramePrologue, REJOIN_FUNCTION_PROLOGUE);
77884:                 stubcc.crossJump(stubcc.masm.jump(), masm.label());
77884:             }
77884:         }
77884: 
77391:         if (outerScript->usesArguments && !script->function()->isHeavyweight()) {
76194:             /*
76194:              * Make sure that fp->args.nactual is always coherent. This may be
76194:              * inspected directly by JIT code, and is not guaranteed to be
76194:              * correct if the UNDERFLOW and OVERFLOW flags are not set.
76194:              */
76194:             Jump hasArgs = masm.branchTest32(Assembler::NonZero, FrameFlagsAddress(),
76194:                                              Imm32(StackFrame::OVERRIDE_ARGS |
76194:                                                    StackFrame::UNDERFLOW_ARGS |
76194:                                                    StackFrame::OVERFLOW_ARGS |
76194:                                                    StackFrame::HAS_ARGS_OBJ));
77436:             masm.storePtr(ImmPtr((void *)(size_t) script->function()->nargs),
76194:                           Address(JSFrameReg, StackFrame::offsetOfArgs()));
76194:             hasArgs.linkTo(masm.label(), &masm);
76194:         }
77884: 
77884:         j.linkTo(masm.label(), &masm);
74457:     }
74457: 
77431:     if (cx->typeInferenceEnabled()) {
77431: #ifdef DEBUG
77431:         if (script->hasFunction) {
77431:             prepareStubCall(Uses(0));
77431:             INLINE_STUBCALL(stubs::AssertArgumentTypes, REJOIN_NONE);
77431:         }
77431: #endif
77431:         ensureDoubleArguments();
77431:     }
77431: 
77414:     if (isConstructing) {
77414:         if (!constructThis())
77414:             return Compile_Error;
77414:     }
74457: 
77413:     if (debugMode()) {
77413:         prepareStubCall(Uses(0));
76194:         INLINE_STUBCALL(stubs::ScriptDebugPrologue, REJOIN_RESUME);
77413:     } else if (Probes::callTrackingActive(cx)) {
77413:         prepareStubCall(Uses(0));
76194:         INLINE_STUBCALL(stubs::ScriptProbeOnlyPrologue, REJOIN_RESUME);
77413:     }
76194: 
76194:     recompileCheckHelper();
74457: 
80222:     if (outerScript->pcCounters || Probes::wantNativeAddressInfo(cx)) {
77407:         size_t length = ssa.frameLength(ssa.numFrames() - 1);
77407:         pcLengths = (PCLengthEntry *) cx->calloc_(sizeof(pcLengths[0]) * length);
77407:         if (!pcLengths)
77407:             return Compile_Error;
77407:     }
52872: 
52558:     return Compile_Okay;
52558: }
52558: 
76194: void
76194: mjit::Compiler::ensureDoubleArguments()
76194: {
76194:     /* Convert integer arguments which were inferred as (int|double) to doubles. */
77391:     for (uint32 i = 0; script->hasFunction && i < script->function()->nargs; i++) {
76194:         uint32 slot = ArgSlot(i);
76194:         if (a->varTypes[slot].type == JSVAL_TYPE_DOUBLE && analysis->trackSlot(slot))
76194:             frame.ensureDouble(frame.getArg(i));
76194:     }
76194: }
76194: 
52611: CompileStatus
52611: mjit::Compiler::generateEpilogue()
52611: {
52611:     return Compile_Okay;
52611: }
52611: 
52611: CompileStatus
55503: mjit::Compiler::finishThisUp(JITScript **jitp)
52611: {
56037:     RETURN_IF_OOM(Compile_Error);
56037: 
76194:     /*
76194:      * Watch for reallocation of the global slots while we were in the middle
76194:      * of compiling due to, e.g. standard class initialization.
76194:      */
76194:     if (globalSlots && globalObj->getRawSlots() != globalSlots)
76194:         return Compile_Retry;
76194: 
77892:     /*
77892:      * Watch for GCs which occurred during compilation. These may have
77892:      * renumbered shapes baked into the jitcode.
77892:      */
77892:     if (cx->runtime->gcNumber != gcNumber)
77892:         return Compile_Retry;
77892: 
78457:     if (overflowICSpace) {
78457:         JaegerSpew(JSpew_Scripts, "dumped a constant pool while generating an IC\n");
78457:         return Compile_Abort;
78457:     }
78457: 
52611:     for (size_t i = 0; i < branchPatches.length(); i++) {
76194:         Label label = labelOf(branchPatches[i].pc, branchPatches[i].inlineIndex);
52611:         branchPatches[i].jump.linkTo(label, &masm);
52611:     }
52611: 
53147: #ifdef JS_CPU_ARM
53147:     masm.forceFlushConstantPool();
53147:     stubcc.masm.forceFlushConstantPool();
53147: #endif
75290:     JaegerSpew(JSpew_Insns, "## Fast code (masm) size = %lu, Slow code (stubcc) size = %lu.\n",
75290:                (unsigned long) masm.size(), (unsigned long) stubcc.size());
53147: 
69289:     size_t codeSize = masm.size() +
53183:                       stubcc.size() +
76194:                       (masm.numDoubles() * sizeof(double)) +
76194:                       (stubcc.masm.numDoubles() * sizeof(double)) +
59979:                       jumpTableOffsets.length() * sizeof(void *);
53183: 
64243:     JSC::ExecutablePool *execPool;
77659:     uint8 *result = (uint8 *)script->compartment()->jaegerCompartment()->execAlloc()->
77559:                     alloc(codeSize, &execPool, JSC::METHOD_CODE);
64243:     if (!result) {
61233:         js_ReportOutOfMemory(cx);
61233:         return Compile_Error;
61233:     }
64243:     JS_ASSERT(execPool);
69289:     JSC::ExecutableAllocator::makeWritable(result, codeSize);
53147:     masm.executableCopy(result);
53147:     stubcc.masm.executableCopy(result + masm.size());
52611: 
77559:     JSC::LinkBuffer fullCode(result, codeSize, JSC::METHOD_CODE);
77559:     JSC::LinkBuffer stubCode(result + masm.size(), stubcc.size(), JSC::METHOD_CODE);
53498: 
76194:     size_t nNmapLive = loopEntries.length();
58993:     for (size_t i = 0; i < script->length; i++) {
76194:         Bytecode *opinfo = analysis->maybeCode(i);
76194:         if (opinfo && opinfo->safePoint) {
76194:             /* loopEntries cover any safe points which are at loop heads. */
76194:             if (!cx->typeInferenceEnabled() || !opinfo->loopHead)
58993:                 nNmapLive++;
58993:         }
76194:     }
58993: 
60207:     /* Please keep in sync with JITScript::scriptDataSize! */
69289:     size_t dataSize = sizeof(JITScript) +
58993:                       sizeof(NativeMapEntry) * nNmapLive +
76194:                       sizeof(InlineFrame) * inlineFrames.length() +
76194:                       sizeof(CallSite) * callSites.length() +
78389:                       sizeof(JSObject *) * rootedObjects.length() +
53498: #if defined JS_MONOIC
62386:                       sizeof(ic::GetGlobalNameIC) * getGlobalNames.length() +
62386:                       sizeof(ic::SetGlobalNameIC) * setGlobalNames.length() +
68952:                       sizeof(ic::CallICInfo) * callICs.length() +
56192:                       sizeof(ic::EqualityICInfo) * equalityICs.length() +
56217:                       sizeof(ic::TraceICInfo) * traceICs.length() +
53498: #endif
53498: #if defined JS_POLYIC
53498:                       sizeof(ic::PICInfo) * pics.length() +
56738:                       sizeof(ic::GetElementIC) * getElemICs.length() +
57671:                       sizeof(ic::SetElementIC) * setElemICs.length() +
53498: #endif
76194:                       0;
53498: 
69289:     uint8 *cursor = (uint8 *)cx->calloc_(dataSize);
53498:     if (!cursor) {
52661:         execPool->release();
61233:         js_ReportOutOfMemory(cx);
52661:         return Compile_Error;
52661:     }
52661: 
57758:     JITScript *jit = new(cursor) JITScript;
53498:     cursor += sizeof(JITScript);
53498: 
76194:     JS_ASSERT(outerScript == script);
76194: 
76194:     jit->script = script;
55503:     jit->code = JSC::MacroAssemblerCodeRef(result, execPool, masm.size() + stubcc.size());
55503:     jit->invokeEntry = result;
77169:     jit->singleStepMode = script->stepModeEnabled();
77391:     if (script->hasFunction) {
62075:         jit->arityCheckEntry = stubCode.locationOf(arityLabel).executableAddress();
76194:         jit->argsCheckEntry = stubCode.locationOf(argsCheckLabel).executableAddress();
62075:         jit->fastEntry = fullCode.locationOf(invokeLabel).executableAddress();
62075:     }
77407:     jit->pcLengths = pcLengths;
62075: 
62075:     /*
62075:      * WARNING: mics(), callICs() et al depend on the ordering of these
62075:      * variable-length sections.  See JITScript's declaration for details.
62075:      */
53520: 
76194:     /* ICs can only refer to bytecodes in the outermost script, not inlined calls. */
76194:     Label *jumpMap = a->jumpMap;
76194: 
53498:     /* Build the pc -> ncode mapping. */
62075:     NativeMapEntry *jitNmap = (NativeMapEntry *)cursor;
62075:     jit->nNmapPairs = nNmapLive;
62075:     cursor += sizeof(NativeMapEntry) * jit->nNmapPairs;
58993:     size_t ix = 0;
62075:     if (jit->nNmapPairs > 0) {
52661:         for (size_t i = 0; i < script->length; i++) {
76194:             Bytecode *opinfo = analysis->maybeCode(i);
56602:             if (opinfo && opinfo->safePoint) {
76194:                 if (cx->typeInferenceEnabled() && opinfo->loopHead)
76194:                     continue;
58993:                 Label L = jumpMap[i];
71317:                 JS_ASSERT(L.isSet());
62075:                 jitNmap[ix].bcOff = i;
62075:                 jitNmap[ix].ncode = (uint8 *)(result + masm.distanceOf(L));
58993:                 ix++;
58993:             }
58993:         }
76194:         for (size_t i = 0; i < loopEntries.length(); i++) {
76194:             /* Insert the entry at the right position. */
76194:             const LoopEntry &entry = loopEntries[i];
76194:             size_t j;
76194:             for (j = 0; j < ix; j++) {
76194:                 if (jitNmap[j].bcOff > entry.pcOffset) {
76194:                     memmove(jitNmap + j + 1, jitNmap + j, (ix - j) * sizeof(NativeMapEntry));
76194:                     break;
76194:                 }
76194:             }
76194:             jitNmap[j].bcOff = entry.pcOffset;
76194:             jitNmap[j].ncode = (uint8 *) stubCode.locationOf(entry.label).executableAddress();
76194:             ix++;
76194:         }
58993:     }
62075:     JS_ASSERT(ix == jit->nNmapPairs);
53590: 
76194:     /* Build the table of inlined frames. */
76194:     InlineFrame *jitInlineFrames = (InlineFrame *)cursor;
76194:     jit->nInlineFrames = inlineFrames.length();
76194:     cursor += sizeof(InlineFrame) * jit->nInlineFrames;
76194:     for (size_t i = 0; i < jit->nInlineFrames; i++) {
76194:         InlineFrame &to = jitInlineFrames[i];
76194:         ActiveFrame *from = inlineFrames[i];
76194:         if (from->parent != outer)
76194:             to.parent = &jitInlineFrames[from->parent->inlineIndex];
76194:         else
76194:             to.parent = NULL;
76194:         to.parentpc = from->parentPC;
77391:         to.fun = from->script->function();
76194:         to.depth = ssa.getFrame(from->inlineIndex).depth;
76194:     }
76194: 
76194:     /* Build the table of call sites. */
76194:     CallSite *jitCallSites = (CallSite *)cursor;
76194:     jit->nCallSites = callSites.length();
76194:     cursor += sizeof(CallSite) * jit->nCallSites;
76194:     for (size_t i = 0; i < jit->nCallSites; i++) {
76194:         CallSite &to = jitCallSites[i];
76194:         InternalCallSite &from = callSites[i];
76194: 
76194:         /* Patch stores of f.regs.inlined for stubs called from within inline frames. */
76194:         if (cx->typeInferenceEnabled() &&
76194:             from.rejoin != REJOIN_TRAP &&
76194:             from.rejoin != REJOIN_SCRIPTED &&
76194:             from.inlineIndex != uint32(-1)) {
76194:             if (from.ool)
76194:                 stubCode.patch(from.inlinePatch, &to);
76194:             else
76194:                 fullCode.patch(from.inlinePatch, &to);
76194:         }
76194: 
76194:         JSScript *script =
76194:             (from.inlineIndex == uint32(-1)) ? outerScript : inlineFrames[from.inlineIndex]->script;
76194:         uint32 codeOffset = from.ool
76194:                             ? masm.size() + from.returnOffset
76194:                             : from.returnOffset;
76194:         to.initialize(codeOffset, from.inlineIndex, from.inlinepc - script->code, from.rejoin);
76194: 
76194:         /*
76194:          * Patch stores of the base call's return address for InvariantFailure
76194:          * calls. InvariantFailure will patch its own return address to this
76194:          * pointer before triggering recompilation.
76194:          */
76194:         if (from.loopPatch.hasPatch)
76194:             stubCode.patch(from.loopPatch.codePatch, result + codeOffset);
76194:     }
76194: 
78389:     /* Build the list of objects rooted by the script. */
78389:     JSObject **jitRooted = (JSObject **)cursor;
78389:     jit->nRootedObjects = rootedObjects.length();
78389:     cursor += sizeof(JSObject *) * jit->nRootedObjects;
78389:     for (size_t i = 0; i < jit->nRootedObjects; i++)
78389:         jitRooted[i] = rootedObjects[i];
78389: 
53119: #if defined JS_MONOIC
76194:     JS_INIT_CLIST(&jit->callers);
76194: 
77391:     if (script->hasFunction && cx->typeInferenceEnabled()) {
76194:         jit->argsCheckStub = stubCode.locationOf(argsCheckStub);
76194:         jit->argsCheckFallthrough = stubCode.locationOf(argsCheckFallthrough);
76194:         jit->argsCheckJump = stubCode.locationOf(argsCheckJump);
76194:         jit->argsCheckPool = NULL;
76194:     }
76194: 
62386:     ic::GetGlobalNameIC *getGlobalNames_ = (ic::GetGlobalNameIC *)cursor;
62386:     jit->nGetGlobalNames = getGlobalNames.length();
62386:     cursor += sizeof(ic::GetGlobalNameIC) * jit->nGetGlobalNames;
62386:     for (size_t i = 0; i < jit->nGetGlobalNames; i++) {
62386:         ic::GetGlobalNameIC &to = getGlobalNames_[i];
62386:         GetGlobalNameICInfo &from = getGlobalNames[i];
62386:         from.copyTo(to, fullCode, stubCode);
62386: 
62386:         int offset = fullCode.locationOf(from.load) - to.fastPathStart;
62386:         to.loadStoreOffset = offset;
62386:         JS_ASSERT(to.loadStoreOffset == offset);
62386: 
62386:         stubCode.patch(from.addrLabel, &to);
62386:     }
62386: 
62386:     ic::SetGlobalNameIC *setGlobalNames_ = (ic::SetGlobalNameIC *)cursor;
62386:     jit->nSetGlobalNames = setGlobalNames.length();
62386:     cursor += sizeof(ic::SetGlobalNameIC) * jit->nSetGlobalNames;
62386:     for (size_t i = 0; i < jit->nSetGlobalNames; i++) {
62386:         ic::SetGlobalNameIC &to = setGlobalNames_[i];
62386:         SetGlobalNameICInfo &from = setGlobalNames[i];
62386:         from.copyTo(to, fullCode, stubCode);
62386:         to.slowPathStart = stubCode.locationOf(from.slowPathStart);
62386: 
62386:         int offset = fullCode.locationOf(from.store).labelAtOffset(0) -
62386:                      to.fastPathStart;
62386:         to.loadStoreOffset = offset;
62386:         JS_ASSERT(to.loadStoreOffset == offset);
62386: 
62409:         to.hasExtraStub = 0;
62386:         to.objConst = from.objConst;
62386:         to.shapeReg = from.shapeReg;
62386:         to.objReg = from.objReg;
62386:         to.vr = from.vr;
62386: 
62386:         offset = fullCode.locationOf(from.shapeGuardJump) -
62386:                  to.fastPathStart;
62386:         to.inlineShapeJump = offset;
62386:         JS_ASSERT(to.inlineShapeJump == offset);
62386: 
62386:         offset = fullCode.locationOf(from.fastPathRejoin) -
62386:                  to.fastPathStart;
62386:         to.fastRejoinOffset = offset;
62386:         JS_ASSERT(to.fastRejoinOffset == offset);
62386: 
62386:         stubCode.patch(from.addrLabel, &to);
62075:     }
62075: 
68952:     ic::CallICInfo *jitCallICs = (ic::CallICInfo *)cursor;
68952:     jit->nCallICs = callICs.length();
68952:     cursor += sizeof(ic::CallICInfo) * jit->nCallICs;
68952:     for (size_t i = 0; i < jit->nCallICs; i++) {
68952:         jitCallICs[i].reset();
68952:         jitCallICs[i].funGuard = fullCode.locationOf(callICs[i].funGuard);
68952:         jitCallICs[i].funJump = fullCode.locationOf(callICs[i].funJump);
68952:         jitCallICs[i].slowPathStart = stubCode.locationOf(callICs[i].slowPathStart);
76194:         jitCallICs[i].typeMonitored = callICs[i].typeMonitored;
68952: 
68952:         /* Compute the hot call offset. */
68952:         uint32 offset = fullCode.locationOf(callICs[i].hotJump) -
68952:                         fullCode.locationOf(callICs[i].funGuard);
68952:         jitCallICs[i].hotJumpOffset = offset;
68952:         JS_ASSERT(jitCallICs[i].hotJumpOffset == offset);
68952: 
68952:         /* Compute the join point offset. */
68952:         offset = fullCode.locationOf(callICs[i].joinPoint) -
68952:                  fullCode.locationOf(callICs[i].funGuard);
68952:         jitCallICs[i].joinPointOffset = offset;
68952:         JS_ASSERT(jitCallICs[i].joinPointOffset == offset);
68952: 
68952:         /* Compute the OOL call offset. */
68952:         offset = stubCode.locationOf(callICs[i].oolCall) -
68952:                  stubCode.locationOf(callICs[i].slowPathStart);
68952:         jitCallICs[i].oolCallOffset = offset;
68952:         JS_ASSERT(jitCallICs[i].oolCallOffset == offset);
68952: 
68952:         /* Compute the OOL jump offset. */
68952:         offset = stubCode.locationOf(callICs[i].oolJump) -
68952:                  stubCode.locationOf(callICs[i].slowPathStart);
68952:         jitCallICs[i].oolJumpOffset = offset;
68952:         JS_ASSERT(jitCallICs[i].oolJumpOffset == offset);
68952: 
68952:         /* Compute the start of the OOL IC call. */
68952:         offset = stubCode.locationOf(callICs[i].icCall) -
68952:                  stubCode.locationOf(callICs[i].slowPathStart);
68952:         jitCallICs[i].icCallOffset = offset;
68952:         JS_ASSERT(jitCallICs[i].icCallOffset == offset);
68952: 
68952:         /* Compute the slow join point offset. */
68952:         offset = stubCode.locationOf(callICs[i].slowJoinPoint) -
68952:                  stubCode.locationOf(callICs[i].slowPathStart);
68952:         jitCallICs[i].slowJoinOffset = offset;
68952:         JS_ASSERT(jitCallICs[i].slowJoinOffset == offset);
68952: 
68952:         /* Compute the join point offset for continuing on the hot path. */
68952:         offset = stubCode.locationOf(callICs[i].hotPathLabel) -
68952:                  stubCode.locationOf(callICs[i].funGuard);
68952:         jitCallICs[i].hotPathOffset = offset;
68952:         JS_ASSERT(jitCallICs[i].hotPathOffset == offset);
68952: 
76194:         jitCallICs[i].call = &jitCallSites[callICs[i].callIndex];
68952:         jitCallICs[i].frameSize = callICs[i].frameSize;
68952:         jitCallICs[i].funObjReg = callICs[i].funObjReg;
68952:         jitCallICs[i].funPtrReg = callICs[i].funPtrReg;
68952:         stubCode.patch(callICs[i].addrLabel1, &jitCallICs[i]);
68952:         stubCode.patch(callICs[i].addrLabel2, &jitCallICs[i]);
68952:     }
68952: 
62075:     ic::EqualityICInfo *jitEqualityICs = (ic::EqualityICInfo *)cursor;
56192:     jit->nEqualityICs = equalityICs.length();
62075:     cursor += sizeof(ic::EqualityICInfo) * jit->nEqualityICs;
62075:     for (size_t i = 0; i < jit->nEqualityICs; i++) {
76194:         if (equalityICs[i].trampoline) {
76194:             jitEqualityICs[i].target = stubCode.locationOf(equalityICs[i].trampolineStart);
76194:         } else {
56192:             uint32 offs = uint32(equalityICs[i].jumpTarget - script->code);
71317:             JS_ASSERT(jumpMap[offs].isSet());
62075:             jitEqualityICs[i].target = fullCode.locationOf(jumpMap[offs]);
76194:         }
62075:         jitEqualityICs[i].stubEntry = stubCode.locationOf(equalityICs[i].stubEntry);
62075:         jitEqualityICs[i].stubCall = stubCode.locationOf(equalityICs[i].stubCall);
62075:         jitEqualityICs[i].stub = equalityICs[i].stub;
62075:         jitEqualityICs[i].lvr = equalityICs[i].lvr;
62075:         jitEqualityICs[i].rvr = equalityICs[i].rvr;
62075:         jitEqualityICs[i].tempReg = equalityICs[i].tempReg;
62075:         jitEqualityICs[i].cond = equalityICs[i].cond;
56192:         if (equalityICs[i].jumpToStub.isSet())
62075:             jitEqualityICs[i].jumpToStub = fullCode.locationOf(equalityICs[i].jumpToStub.get());
62075:         jitEqualityICs[i].fallThrough = fullCode.locationOf(equalityICs[i].fallThrough);
62075: 
62075:         stubCode.patch(equalityICs[i].addrLabel, &jitEqualityICs[i]);
62075:     }
62075: 
62075:     ic::TraceICInfo *jitTraceICs = (ic::TraceICInfo *)cursor;
56217:     jit->nTraceICs = traceICs.length();
62075:     cursor += sizeof(ic::TraceICInfo) * jit->nTraceICs;
62075:     for (size_t i = 0; i < jit->nTraceICs; i++) {
62075:         jitTraceICs[i].initialized = traceICs[i].initialized;
56220:         if (!traceICs[i].initialized)
56220:             continue;
56220: 
76194:         if (traceICs[i].fastTrampoline) {
76194:             jitTraceICs[i].fastTarget = stubCode.locationOf(traceICs[i].trampolineStart);
76194:         } else {
56217:             uint32 offs = uint32(traceICs[i].jumpTarget - script->code);
71317:             JS_ASSERT(jumpMap[offs].isSet());
76194:             jitTraceICs[i].fastTarget = fullCode.locationOf(jumpMap[offs]);
76194:         }
76194:         jitTraceICs[i].slowTarget = stubCode.locationOf(traceICs[i].trampolineStart);
76194: 
62075:         jitTraceICs[i].traceHint = fullCode.locationOf(traceICs[i].traceHint);
62075:         jitTraceICs[i].stubEntry = stubCode.locationOf(traceICs[i].stubEntry);
62075:         jitTraceICs[i].traceData = NULL;
56217: #ifdef DEBUG
62075:         jitTraceICs[i].jumpTargetPC = traceICs[i].jumpTarget;
56217: #endif
76194: 
62075:         jitTraceICs[i].hasSlowTraceHint = traceICs[i].slowTraceHint.isSet();
56217:         if (traceICs[i].slowTraceHint.isSet())
62075:             jitTraceICs[i].slowTraceHint = stubCode.locationOf(traceICs[i].slowTraceHint.get());
60534: #ifdef JS_TRACER
64280:         uint32 hotloop = GetHotloop(cx);
64280:         uint32 prevCount = cx->compartment->backEdgeCount(traceICs[i].jumpTarget);
64280:         jitTraceICs[i].loopCounterStart = hotloop;
64280:         jitTraceICs[i].loopCounter = hotloop < prevCount ? 1 : hotloop - prevCount;
60534: #endif
62075: 
62075:         stubCode.patch(traceICs[i].addrLabel, &jitTraceICs[i]);
53590:     }
53119: #endif /* JS_MONOIC */
53119: 
68952:     for (size_t i = 0; i < callPatches.length(); i++) {
68952:         CallPatchInfo &patch = callPatches[i];
68952: 
76194:         CodeLocationLabel joinPoint = patch.joinSlow
76194:             ? stubCode.locationOf(patch.joinPoint)
76194:             : fullCode.locationOf(patch.joinPoint);
76194: 
68952:         if (patch.hasFastNcode)
76194:             fullCode.patch(patch.fastNcodePatch, joinPoint);
68952:         if (patch.hasSlowNcode)
76194:             stubCode.patch(patch.slowNcodePatch, joinPoint);
68952:     }
68952: 
60598: #ifdef JS_POLYIC
62075:     ic::GetElementIC *jitGetElems = (ic::GetElementIC *)cursor;
56738:     jit->nGetElems = getElemICs.length();
62075:     cursor += sizeof(ic::GetElementIC) * jit->nGetElems;
62075:     for (size_t i = 0; i < jit->nGetElems; i++) {
62075:         ic::GetElementIC &to = jitGetElems[i];
56738:         GetElementICInfo &from = getElemICs[i];
57671: 
57671:         new (&to) ic::GetElementIC();
56738:         from.copyTo(to, fullCode, stubCode);
56738: 
56738:         to.typeReg = from.typeReg;
56738:         to.objReg = from.objReg;
56738:         to.idRemat = from.id;
56738: 
56738:         if (from.typeGuard.isSet()) {
56738:             int inlineTypeGuard = fullCode.locationOf(from.typeGuard.get()) -
56738:                                   fullCode.locationOf(from.fastPathStart);
56738:             to.inlineTypeGuard = inlineTypeGuard;
56738:             JS_ASSERT(to.inlineTypeGuard == inlineTypeGuard);
56738:         }
56738:         int inlineClaspGuard = fullCode.locationOf(from.claspGuard) -
56738:                                fullCode.locationOf(from.fastPathStart);
56738:         to.inlineClaspGuard = inlineClaspGuard;
56738:         JS_ASSERT(to.inlineClaspGuard == inlineClaspGuard);
56738: 
56738:         stubCode.patch(from.paramAddr, &to);
56738:     }
60598: 
62075:     ic::SetElementIC *jitSetElems = (ic::SetElementIC *)cursor;
57671:     jit->nSetElems = setElemICs.length();
62075:     cursor += sizeof(ic::SetElementIC) * jit->nSetElems;
62075:     for (size_t i = 0; i < jit->nSetElems; i++) {
62075:         ic::SetElementIC &to = jitSetElems[i];
57671:         SetElementICInfo &from = setElemICs[i];
57671: 
57671:         new (&to) ic::SetElementIC();
57671:         from.copyTo(to, fullCode, stubCode);
57671: 
57671:         to.strictMode = script->strictModeCode;
57671:         to.vr = from.vr;
57671:         to.objReg = from.objReg;
57671:         to.objRemat = from.objRemat.toInt32();
57671:         JS_ASSERT(to.objRemat == from.objRemat.toInt32());
57671: 
57671:         to.hasConstantKey = from.key.isConstant();
57671:         if (from.key.isConstant())
57671:             to.keyValue = from.key.index();
57671:         else
57671:             to.keyReg = from.key.reg();
57671: 
57671:         int inlineClaspGuard = fullCode.locationOf(from.claspGuard) -
57671:                                fullCode.locationOf(from.fastPathStart);
57671:         to.inlineClaspGuard = inlineClaspGuard;
57671:         JS_ASSERT(to.inlineClaspGuard == inlineClaspGuard);
57671: 
57671:         int inlineHoleGuard = fullCode.locationOf(from.holeGuard) -
57671:                                fullCode.locationOf(from.fastPathStart);
57671:         to.inlineHoleGuard = inlineHoleGuard;
57671:         JS_ASSERT(to.inlineHoleGuard == inlineHoleGuard);
57671: 
60597:         CheckIsStubCall(to.slowPathCall.labelAtOffset(0));
60597: 
60164:         to.volatileMask = from.volatileMask;
60164:         JS_ASSERT(to.volatileMask == from.volatileMask);
60164: 
57671:         stubCode.patch(from.paramAddr, &to);
57671:     }
60598: 
62075:     ic::PICInfo *jitPics = (ic::PICInfo *)cursor;
55503:     jit->nPICs = pics.length();
62075:     cursor += sizeof(ic::PICInfo) * jit->nPICs;
62075:     for (size_t i = 0; i < jit->nPICs; i++) {
62075:         new (&jitPics[i]) ic::PICInfo();
62075:         pics[i].copyTo(jitPics[i], fullCode, stubCode);
62075:         pics[i].copySimpleMembersTo(jitPics[i]);
62075: 
62075:         jitPics[i].shapeGuard = masm.distanceOf(pics[i].shapeGuard) -
53270:                                 masm.distanceOf(pics[i].fastPathStart);
62075:         JS_ASSERT(jitPics[i].shapeGuard == masm.distanceOf(pics[i].shapeGuard) -
53426:                                            masm.distanceOf(pics[i].fastPathStart));
62075:         jitPics[i].shapeRegHasBaseShape = true;
62075:         jitPics[i].pc = pics[i].pc;
52887: 
53620:         if (pics[i].kind == ic::PICInfo::SET ||
53620:             pics[i].kind == ic::PICInfo::SETMETHOD) {
62075:             jitPics[i].u.vr = pics[i].vr;
71340:         } else if (pics[i].kind != ic::PICInfo::NAME &&
71340:                    pics[i].kind != ic::PICInfo::CALLNAME) {
52884:             if (pics[i].hasTypeCheck) {
52884:                 int32 distance = stubcc.masm.distanceOf(pics[i].typeCheck) -
52884:                                  stubcc.masm.distanceOf(pics[i].slowPathStart);
53264:                 JS_ASSERT(distance <= 0);
62075:                 jitPics[i].u.get.typeCheckOffset = distance;
62075:             }
62075:         }
62075:         stubCode.patch(pics[i].paramAddr, &jitPics[i]);
52880:     }
60597: #endif
52880: 
76194:     JS_ASSERT(size_t(cursor - (uint8*)jit) == dataSize);
77853:     /* Pass in NULL here -- we don't want slop bytes to be counted. */
77853:     JS_ASSERT(jit->scriptDataSize(NULL) == dataSize);
76194: 
52613:     /* Link fast and slow paths together. */
52613:     stubcc.fixCrossJumps(result, masm.size(), masm.size() + stubcc.size());
52613: 
53183:     size_t doubleOffset = masm.size() + stubcc.size();
76194:     double *inlineDoubles = (double *) (result + doubleOffset);
76194:     double *oolDoubles = (double*) (result + doubleOffset +
76194:                                     masm.numDoubles() * sizeof(double));
53183: 
59979:     /* Generate jump tables. */
76194:     void **jumpVec = (void **)(oolDoubles + stubcc.masm.numDoubles());
59979: 
59979:     for (size_t i = 0; i < jumpTableOffsets.length(); i++) {
59979:         uint32 offset = jumpTableOffsets[i];
71317:         JS_ASSERT(jumpMap[offset].isSet());
59979:         jumpVec[i] = (void *)(result + masm.distanceOf(jumpMap[offset]));
59979:     }
59979: 
59979:     /* Patch jump table references. */
59979:     for (size_t i = 0; i < jumpTables.length(); i++) {
59979:         JumpTable &jumpTable = jumpTables[i];
59979:         fullCode.patch(jumpTable.label, &jumpVec[jumpTable.offsetIndex]);
59979:     }
59979: 
52613:     /* Patch all outgoing calls. */
76194:     masm.finalize(fullCode, inlineDoubles);
76194:     stubcc.masm.finalize(stubCode, oolDoubles);
52611: 
52611:     JSC::ExecutableAllocator::makeExecutable(result, masm.size() + stubcc.size());
52611:     JSC::ExecutableAllocator::cacheFlush(result, masm.size() + stubcc.size());
52611: 
80222:     Probes::registerMJITCode(cx, jit, script, script->hasFunction ? script->function() : NULL,
80222:                              (mjit::Compiler_ActiveFrame**) inlineFrames.begin(),
80222:                              result, masm.size(),
80222:                              result + masm.size(), stubcc.size());
80222: 
55503:     *jitp = jit;
53168: 
52611:     return Compile_Okay;
52611: }
52611: 
59882: class SrcNoteLineScanner {
80222:     /* offset of the current JSOp in the bytecode */
59882:     ptrdiff_t offset;
80222: 
80222:     /* next src note to process */
59882:     jssrcnote *sn;
59882: 
80222:     /* line number of the current JSOp */
80222:     uint32 lineno;
80222: 
80222:     /*
80222:      * Is the current op the first one after a line change directive? Note that
80222:      * multiple ops may be "first" if a line directive is used to return to a
80222:      * previous line (eg, with a for loop increment expression.)
80222:      */
80222:     bool lineHeader;
80222: 
59882: public:
80222:     SrcNoteLineScanner(jssrcnote *sn, uint32 lineno)
80222:         : offset(0), sn(sn), lineno(lineno)
80222:     {
80222:     }
80222: 
80222:     /*
80222:      * This is called repeatedly with always-advancing relpc values. The src
80222:      * notes are tuples of <PC offset from prev src note, type, args>. Scan
80222:      * through, updating the lineno, until the next src note is for a later
80222:      * bytecode.
80222:      *
80222:      * When looking at the desired PC offset ('relpc'), the op is first in that
80222:      * line iff there is a SRC_SETLINE or SRC_NEWLINE src note for that exact
80222:      * bytecode.
80222:      *
80222:      * Note that a single bytecode may have multiple line-modifying notes (even
80222:      * though only one should ever be needed.)
80222:      */
80222:     void advanceTo(ptrdiff_t relpc) {
80222:         // Must always advance! If the same or an earlier PC is erroneously
80222:         // passed in, we will already be past the relevant src notes
80222:         JS_ASSERT_IF(offset > 0, relpc > offset);
80222: 
80222:         // Next src note should be for after the current offset
80222:         JS_ASSERT_IF(offset > 0, SN_IS_TERMINATOR(sn) || SN_DELTA(sn) > 0);
80222: 
80222:         // The first PC requested is always considered to be a line header
80222:         lineHeader = (offset == 0);
80222: 
80222:         if (SN_IS_TERMINATOR(sn))
80222:             return;
80222: 
80222:         ptrdiff_t nextOffset;
80222:         while ((nextOffset = offset + SN_DELTA(sn)) <= relpc && !SN_IS_TERMINATOR(sn)) {
80222:             offset = nextOffset;
80222:             JSSrcNoteType type = (JSSrcNoteType) SN_TYPE(sn);
80222:             if (type == SRC_SETLINE || type == SRC_NEWLINE) {
80222:                 if (type == SRC_SETLINE)
80222:                     lineno = js_GetSrcNoteOffset(sn, 0);
80222:                 else
80222:                     lineno++;
80222: 
80222:                 if (offset == relpc)
80222:                     lineHeader = true;
80222:             }
80222: 
60153:             sn = SN_NEXT(sn);
80222:         }
80222:     }
80222: 
80222:     bool isLineHeader() const {
80222:         return lineHeader;
80222:     }
80222: 
80222:     uint32 getLine() const { return lineno; }
59882: };
59882: 
52776: #ifdef DEBUG
52776: #define SPEW_OPCODE()                                                         \
52776:     JS_BEGIN_MACRO                                                            \
52776:         if (IsJaegerSpewChannelActive(JSpew_JSOps)) {                         \
52776:             JaegerSpew(JSpew_JSOps, "    %2d ", frame.stackDepth());          \
79410:             LifoAllocScope las(&cx->tempLifoAlloc());                         \
64374:             Sprinter sprinter;                                                \
79410:             INIT_SPRINTER(cx, &sprinter, &cx->tempLifoAlloc(), 0);            \
52776:             js_Disassemble1(cx, script, PC, PC - script->code,                \
64374:                             JS_TRUE, &sprinter);                              \
64374:             fprintf(stdout, "%s", sprinter.base);                             \
52776:         }                                                                     \
52776:     JS_END_MACRO;
52776: #else
52776: #define SPEW_OPCODE()
52776: #endif /* DEBUG */
52776: 
52560: #define BEGIN_CASE(name)        case name:
52560: #define END_CASE(name)                      \
52560:     JS_BEGIN_MACRO                          \
52560:         PC += name##_LENGTH;                \
52560:     JS_END_MACRO;                           \
52560:     break;
52560: 
76194: static inline void
76194: FixDouble(Value &val)
76194: {
76194:     if (val.isInt32())
76194:         val.setDouble((double)val.toInt32());
76194: }
76194: 
52558: CompileStatus
52558: mjit::Compiler::generateMethod()
52558: {
53168:     mjit::AutoScriptRetrapper trapper(cx, script);
80222:     SrcNoteLineScanner scanner(script->notes(), script->lineno);
52558: 
76194:     /* For join points, whether there was fallthrough from the previous opcode. */
76194:     bool fallthrough = true;
76194: 
77407:     /* Last bytecode processed. */
77407:     jsbytecode *lastPC = NULL;
77407: 
52558:     for (;;) {
52558:         JSOp op = JSOp(*PC);
59882:         int trap = stubs::JSTRAP_NONE;
59882:         if (op == JSOP_TRAP) {
53168:             if (!trapper.untrap(PC))
53168:                 return Compile_Error;
53168:             op = JSOp(*PC);
59882:             trap |= stubs::JSTRAP_TRAP;
59882:         }
53168: 
76194:         Bytecode *opinfo = analysis->maybeCode(PC);
56602: 
56602:         if (!opinfo) {
52558:             if (op == JSOP_STOP)
52558:                 break;
52558:             if (js_CodeSpec[op].length != -1)
52558:                 PC += js_CodeSpec[op].length;
52558:             else
52558:                 PC += js_GetVariableBytecodeLength(PC);
52558:             continue;
52558:         }
52558: 
80222:         scanner.advanceTo(PC - script->code);
80222:         if (script->stepModeEnabled() &&
80222:             (scanner.isLineHeader() || opinfo->jumpTarget))
80222:         {
80222:             trap |= stubs::JSTRAP_SINGLESTEP;
80222:         }
80222: 
76194:         frame.setPC(PC);
56602:         frame.setInTryBlock(opinfo->inTryBlock);
76194: 
76194:         if (fallthrough) {
76194:             /*
76194:              * If there is fallthrough from the previous opcode and we changed
76194:              * any entries into doubles for a branch at that previous op,
77457:              * revert those entries into integers. Similarly, if we forgot that
77457:              * an entry is a double then make it a double again, as the frame
77457:              * may have assigned it a normal register.
76194:              */
77457:             for (unsigned i = 0; i < fixedIntToDoubleEntries.length(); i++) {
77457:                 FrameEntry *fe = frame.getSlotEntry(fixedIntToDoubleEntries[i]);
76194:                 frame.ensureInteger(fe);
76194:             }
77457:             for (unsigned i = 0; i < fixedDoubleToAnyEntries.length(); i++) {
77457:                 FrameEntry *fe = frame.getSlotEntry(fixedDoubleToAnyEntries[i]);
77457:                 frame.syncAndForgetFe(fe);
77457:             }
77457:         }
77457:         fixedIntToDoubleEntries.clear();
77457:         fixedDoubleToAnyEntries.clear();
76194: 
56602:         if (opinfo->jumpTarget || trap) {
76194:             if (fallthrough) {
76194:                 fixDoubleTypes(PC);
77457:                 fixedIntToDoubleEntries.clear();
77457:                 fixedDoubleToAnyEntries.clear();
76194: 
76194:                 /*
76194:                  * Watch for fallthrough to the head of a 'do while' loop.
76194:                  * We don't know what register state we will be using at the head
76194:                  * of the loop so sync, branch, and fix it up after the loop
76194:                  * has been processed.
76194:                  */
76194:                 if (cx->typeInferenceEnabled() && analysis->getCode(PC).loopHead) {
76194:                     frame.syncAndForgetEverything();
76194:                     Jump j = masm.jump();
76194:                     if (!startLoop(PC, j, PC))
76194:                         return Compile_Error;
76194:                 } else {
77407:                     Label start = masm.label();
76194:                     if (!frame.syncForBranch(PC, Uses(0)))
76194:                         return Compile_Error;
80222:                     if (pcLengths) {
77407:                         /* Track this sync code for the previous op. */
77407:                         size_t length = masm.size() - masm.distanceOf(start);
77407:                         uint32 offset = ssa.frameLength(a->inlineIndex) + lastPC - script->code;
77407:                         pcLengths[offset].codeLength += length;
77407:                     }
76194:                     JS_ASSERT(frame.consistentRegisters(PC));
76194:                 }
76194:             }
76194: 
76194:             if (!frame.discardForJoin(analysis->getAllocation(PC), opinfo->stackDepth))
76194:                 return Compile_Error;
76194:             updateJoinVarTypes();
76194:             fallthrough = true;
76194: 
76194:             if (!cx->typeInferenceEnabled()) {
76194:                 /* All join points have synced state if we aren't doing cross-branch regalloc. */
56602:                 opinfo->safePoint = true;
56602:             }
79910:         } else if (opinfo->safePoint && !cx->typeInferenceEnabled()) {
79910:             frame.syncAndForgetEverything();
76194:         }
76194:         frame.assertValidRegisterState();
76194:         a->jumpMap[uint32(PC - script->code)] = masm.label();
76766: 
77452:         // Now that we have the PC's register allocation, make sure it gets
77452:         // explicitly updated if this is the loop entry and new loop registers
77452:         // are allocated later on.
77452:         if (loop && !a->parent)
77452:             loop->setOuterPC(PC);
56602: 
52776:         SPEW_OPCODE();
56602:         JS_ASSERT(frame.stackDepth() == opinfo->stackDepth);
56602: 
77167:         // If this is an exception entry point, then jsl_InternalThrow has set
77167:         // VMFrame::fp to the correct fp for the entry point. We need to copy
77167:         // that value here to FpReg so that FpReg also has the correct sp.
77167:         // Otherwise, we would simply be using a stale FpReg value.
77360:         if (op == JSOP_ENTERBLOCK && analysis->getCode(PC).exceptionEntry)
76194:             masm.loadPtr(FrameAddress(VMFrame::offsetOfFp), JSFrameReg);
77167: 
56602:         if (trap) {
53168:             prepareStubCall(Uses(0));
59882:             masm.move(Imm32(trap), Registers::ArgReg1);
76194:             Call cl = emitStubCall(JS_FUNC_TO_DATA_PTR(void *, stubs::Trap), NULL);
76194:             InternalCallSite site(masm.callReturnOffset(cl), a->inlineIndex, PC,
76194:                                   REJOIN_TRAP, false);
57766:             addCallSite(site);
73894:         }
73894: 
77357:         /* Don't compile fat opcodes, run the decomposed version instead. */
77357:         if (js_CodeSpec[op].format & JOF_DECOMPOSE) {
77357:             PC += js_CodeSpec[op].length;
77357:             continue;
74457:         }
74457: 
77407:         Label codeStart = masm.label();
77407:         bool countersUpdated = false;
77407: 
77407:         /*
77407:          * Update PC counters for jump opcodes at their start, so that we don't
77407:          * miss them when taking the jump. This is delayed for other opcodes,
77407:          * as we want to skip updating for ops we didn't generate any code for.
77407:          */
77407:         if (script->pcCounters && JOF_OPTYPE(op) == JOF_JUMP)
77407:             updatePCCounters(PC, &codeStart, &countersUpdated);
53168: 
52560:     /**********************
52560:      * BEGIN COMPILER OPS *
52560:      **********************/
52560: 
77407:         lastPC = PC;
76194: 
52558:         switch (op) {
52647:           BEGIN_CASE(JSOP_NOP)
52647:           END_CASE(JSOP_NOP)
52647: 
52769:           BEGIN_CASE(JSOP_PUSH)
53081:             frame.push(UndefinedValue());
52769:           END_CASE(JSOP_PUSH)
52769: 
52662:           BEGIN_CASE(JSOP_POPV)
52806:           BEGIN_CASE(JSOP_SETRVAL)
52662:           {
54832:             RegisterID reg = frame.allocReg();
54832:             masm.load32(FrameFlagsAddress(), reg);
69223:             masm.or32(Imm32(StackFrame::HAS_RVAL), reg);
54832:             masm.store32(reg, FrameFlagsAddress());
54832:             frame.freeReg(reg);
54832: 
76194:             /* Scripts which write to the frame's return slot aren't inlined. */
76194:             JS_ASSERT(a == outer);
76194: 
52662:             FrameEntry *fe = frame.peek(-1);
69223:             frame.storeTo(fe, Address(JSFrameReg, StackFrame::offsetOfReturnValue()), true);
52662:             frame.pop();
52662:           }
52662:           END_CASE(JSOP_POPV)
52662: 
52650:           BEGIN_CASE(JSOP_RETURN)
77407:             if (script->pcCounters)
77407:                 updatePCCounters(PC, &codeStart, &countersUpdated);
54832:             emitReturn(frame.peek(-1));
76194:             fallthrough = false;
52650:           END_CASE(JSOP_RETURN)
52650: 
52599:           BEGIN_CASE(JSOP_GOTO)
77438:           BEGIN_CASE(JSOP_GOTOX)
64230:           BEGIN_CASE(JSOP_DEFAULT)
52599:           {
76194:             unsigned targetOffset = FollowBranch(cx, script, PC - script->code);
76194:             jsbytecode *target = script->code + targetOffset;
76194: 
76194:             fixDoubleTypes(target);
76194: 
76194:             /*
76194:              * Watch for gotos which are entering a 'for' or 'while' loop.
76194:              * These jump to the loop condition test and are immediately
76194:              * followed by the head of the loop.
76194:              */
77438:             jsbytecode *next = PC + js_CodeSpec[op].length;
76194:             if (cx->typeInferenceEnabled() && analysis->maybeCode(next) &&
76194:                 analysis->getCode(next).loopHead) {
54719:                 frame.syncAndForgetEverything();
52599:                 Jump j = masm.jump();
76194:                 if (!startLoop(next, j, target))
56766:                     return Compile_Error;
76194:             } else {
76194:                 if (!frame.syncForBranch(target, Uses(0)))
76194:                     return Compile_Error;
76194:                 Jump j = masm.jump();
76194:                 if (!jumpAndTrace(j, target))
76194:                     return Compile_Error;
76194:             }
76194:             fallthrough = false;
77438:             PC += js_CodeSpec[op].length;
77438:             break;
52599:           }
52599:           END_CASE(JSOP_GOTO)
52599: 
52686:           BEGIN_CASE(JSOP_IFEQ)
52686:           BEGIN_CASE(JSOP_IFNE)
77438:           BEGIN_CASE(JSOP_IFEQX)
77438:           BEGIN_CASE(JSOP_IFNEX)
77438:           {
77438:             jsbytecode *target = PC + GetJumpOffset(PC, PC);
76194:             fixDoubleTypes(target);
76194:             if (!jsop_ifneq(op, target))
56766:                 return Compile_Error;
77438:             PC += js_CodeSpec[op].length;
77438:             break;
76194:           }
52737:           END_CASE(JSOP_IFNE)
52737: 
52778:           BEGIN_CASE(JSOP_ARGUMENTS)
57718:             /*
57718:              * For calls of the form 'f.apply(x, arguments)' we can avoid
57718:              * creating an args object by having ic::SplatApplyArgs pull
57718:              * directly from the stack. To do this, we speculate here that
57718:              * 'apply' actually refers to js_fun_apply. If this is not true,
57718:              * the slow path in JSOP_FUNAPPLY will create the args object.
57718:              */
76194:             if (canUseApplyTricks()) {
77355:                 /*
77355:                  * Check for interrupts at the JSOP_ARGUMENTS when using
77355:                  * apply tricks, see inlineCallHelper().
77355:                  */
77355:                 interruptCheckHelper();
77355: 
57718:                 applyTricks = LazyArgsObj;
76194:                 pushSyncedEntry(0);
76194:             } else if (cx->typeInferenceEnabled() && !script->strictModeCode &&
77391:                        !script->function()->getType(cx)->hasAnyFlags(types::OBJECT_FLAG_CREATED_ARGUMENTS)) {
76194:                 frame.push(MagicValue(JS_LAZY_ARGUMENTS));
76194:             } else {
76194:                 jsop_arguments(REJOIN_FALLTHROUGH);
76194:                 pushSyncedEntry(0);
76194:             }
52778:           END_CASE(JSOP_ARGUMENTS)
52778: 
74052:           BEGIN_CASE(JSOP_ITERNEXT)
77824:             iterNext(GET_INT8(PC));
74052:           END_CASE(JSOP_ITERNEXT)
52686: 
52714:           BEGIN_CASE(JSOP_DUP)
52714:             frame.dup();
52714:           END_CASE(JSOP_DUP)
52714: 
52715:           BEGIN_CASE(JSOP_DUP2)
52715:             frame.dup2();
52715:           END_CASE(JSOP_DUP2)
52715: 
77357:           BEGIN_CASE(JSOP_SWAP)
77357:             frame.dup2();
77357:             frame.shift(-3);
77357:             frame.shift(-1);
77357:           END_CASE(JSOP_SWAP)
77357: 
77357:           BEGIN_CASE(JSOP_PICK)
77357:           {
77357:             int32 amt = GET_INT8(PC);
77357: 
77357:             // Push -(amt + 1), say amt == 2
77357:             // Stack before: X3 X2 X1
77357:             // Stack after:  X3 X2 X1 X3
77357:             frame.dupAt(-(amt + 1));
77357: 
77357:             // For each item X[i...1] push it then move it down.
77357:             // The above would transition like so:
77357:             //   X3 X2 X1 X3 X2 (dupAt)
77357:             //   X2 X2 X1 X3    (shift)
77357:             //   X2 X2 X1 X3 X1 (dupAt)
77357:             //   X2 X1 X1 X3    (shift)
77357:             for (int32 i = -amt; i < 0; i++) {
77357:                 frame.dupAt(i - 1);
77357:                 frame.shift(i - 2);
77357:             }
77357: 
77357:             // The stack looks like:
77357:             // Xn ... X1 X1 X{n+1}
77357:             // So shimmy the last value down.
77357:             frame.shimmy(1);
77357:           }
77357:           END_CASE(JSOP_PICK)
77357: 
52721:           BEGIN_CASE(JSOP_BITOR)
52718:           BEGIN_CASE(JSOP_BITXOR)
52685:           BEGIN_CASE(JSOP_BITAND)
52685:             jsop_bitop(op);
52685:           END_CASE(JSOP_BITAND)
52685: 
52651:           BEGIN_CASE(JSOP_LT)
52651:           BEGIN_CASE(JSOP_LE)
52651:           BEGIN_CASE(JSOP_GT)
52651:           BEGIN_CASE(JSOP_GE)
52679:           BEGIN_CASE(JSOP_EQ)
52679:           BEGIN_CASE(JSOP_NE)
52651:           {
52652:             /* Detect fusions. */
52652:             jsbytecode *next = &PC[JSOP_GE_LENGTH];
52652:             JSOp fused = JSOp(*next);
56602:             if ((fused != JSOP_IFEQ && fused != JSOP_IFNE) || analysis->jumpTarget(next))
52651:                 fused = JSOP_NOP;
52652: 
52652:             /* Get jump target, if any. */
52652:             jsbytecode *target = NULL;
76194:             if (fused != JSOP_NOP) {
77407:                 if (script->pcCounters)
77407:                     updatePCCounters(PC, &codeStart, &countersUpdated);
52652:                 target = next + GET_JUMP_OFFSET(next);
76194:                 fixDoubleTypes(target);
76194:             }
52651: 
52652:             BoolStub stub = NULL;
52652:             switch (op) {
52652:               case JSOP_LT:
52652:                 stub = stubs::LessThan;
52652:                 break;
52652:               case JSOP_LE:
52652:                 stub = stubs::LessEqual;
52652:                 break;
52652:               case JSOP_GT:
52652:                 stub = stubs::GreaterThan;
52652:                 break;
52652:               case JSOP_GE:
52652:                 stub = stubs::GreaterEqual;
52652:                 break;
52679:               case JSOP_EQ:
52679:                 stub = stubs::Equal;
52679:                 break;
52679:               case JSOP_NE:
52679:                 stub = stubs::NotEqual;
52679:                 break;
52652:               default:
52652:                 JS_NOT_REACHED("WAT");
52652:                 break;
52652:             }
52653: 
76194:             /*
76194:              * We need to ensure in the target case that we always rejoin
76194:              * before the rval test. In the non-target case we will rejoin
76194:              * correctly after the op finishes.
76194:              */
76194: 
52653:             FrameEntry *rhs = frame.peek(-1);
52653:             FrameEntry *lhs = frame.peek(-2);
52653: 
52653:             /* Check for easy cases that the parser does not constant fold. */
52653:             if (lhs->isConstant() && rhs->isConstant()) {
52653:                 /* Primitives can be trivially constant folded. */
52653:                 const Value &lv = lhs->getValue();
52653:                 const Value &rv = rhs->getValue();
52653: 
52653:                 if (lv.isPrimitive() && rv.isPrimitive()) {
52653:                     bool result = compareTwoValues(cx, op, lv, rv);
52653: 
52652:                     frame.pop();
52652:                     frame.pop();
52652: 
52652:                     if (!target) {
53081:                         frame.push(Value(BooleanValue(result)));
52652:                     } else {
52653:                         if (fused == JSOP_IFEQ)
52653:                             result = !result;
76194:                         if (!constantFoldBranch(target, result))
56766:                             return Compile_Error;
52652:                     }
52652:                 } else {
56766:                     if (!emitStubCmpOp(stub, target, fused))
56766:                         return Compile_Error;
52653:                 }
52653:             } else {
52651:                 /* Anything else should go through the fast path generator. */
56766:                 if (!jsop_relational(op, stub, target, fused))
56766:                     return Compile_Error;
52651:             }
52651: 
52651:             /* Advance PC manually. */
52679:             JS_STATIC_ASSERT(JSOP_LT_LENGTH == JSOP_GE_LENGTH);
52679:             JS_STATIC_ASSERT(JSOP_LE_LENGTH == JSOP_GE_LENGTH);
52679:             JS_STATIC_ASSERT(JSOP_GT_LENGTH == JSOP_GE_LENGTH);
52679:             JS_STATIC_ASSERT(JSOP_EQ_LENGTH == JSOP_GE_LENGTH);
52679:             JS_STATIC_ASSERT(JSOP_NE_LENGTH == JSOP_GE_LENGTH);
52679: 
52651:             PC += JSOP_GE_LENGTH;
52776:             if (fused != JSOP_NOP) {
52776:                 SPEW_OPCODE();
52651:                 PC += JSOP_IFNE_LENGTH;
52776:             }
52651:             break;
52651:           }
52651:           END_CASE(JSOP_GE)
52651: 
52685:           BEGIN_CASE(JSOP_LSH)
53230:             jsop_bitop(op);
53230:           END_CASE(JSOP_LSH)
53230: 
52685:           BEGIN_CASE(JSOP_RSH)
76194:             jsop_bitop(op);
52685:           END_CASE(JSOP_RSH)
52560: 
52725:           BEGIN_CASE(JSOP_URSH)
53581:             jsop_bitop(op);
52725:           END_CASE(JSOP_URSH)
52725: 
52692:           BEGIN_CASE(JSOP_ADD)
76194:             if (!jsop_binary(op, stubs::Add, knownPushedType(0), pushedTypeSet(0)))
76194:                 return Compile_Retry;
52692:           END_CASE(JSOP_ADD)
52692: 
52692:           BEGIN_CASE(JSOP_SUB)
76194:             if (!jsop_binary(op, stubs::Sub, knownPushedType(0), pushedTypeSet(0)))
76194:                 return Compile_Retry;
52692:           END_CASE(JSOP_SUB)
52692: 
52692:           BEGIN_CASE(JSOP_MUL)
76194:             if (!jsop_binary(op, stubs::Mul, knownPushedType(0), pushedTypeSet(0)))
76194:                 return Compile_Retry;
52692:           END_CASE(JSOP_MUL)
52692: 
52692:           BEGIN_CASE(JSOP_DIV)
76194:             if (!jsop_binary(op, stubs::Div, knownPushedType(0), pushedTypeSet(0)))
76194:                 return Compile_Retry;
52692:           END_CASE(JSOP_DIV)
52692: 
52692:           BEGIN_CASE(JSOP_MOD)
76194:             if (!jsop_mod())
76194:                 return Compile_Retry;
52692:           END_CASE(JSOP_MOD)
52692: 
52734:           BEGIN_CASE(JSOP_NOT)
52734:             jsop_not();
52734:           END_CASE(JSOP_NOT)
52734: 
52724:           BEGIN_CASE(JSOP_BITNOT)
52724:           {
52724:             FrameEntry *top = frame.peek(-1);
52724:             if (top->isConstant() && top->getValue().isPrimitive()) {
52724:                 int32_t i;
71341:                 JS_ALWAYS_TRUE(ValueToECMAInt32(cx, top->getValue(), &i));
52724:                 i = ~i;
52724:                 frame.pop();
53081:                 frame.push(Int32Value(i));
52724:             } else {
52724:                 jsop_bitnot();
52724:             }
52724:           }
52724:           END_CASE(JSOP_BITNOT)
52724: 
52713:           BEGIN_CASE(JSOP_NEG)
52713:           {
52713:             FrameEntry *top = frame.peek(-1);
52713:             if (top->isConstant() && top->getValue().isPrimitive()) {
52713:                 double d;
73894:                 JS_ALWAYS_TRUE(ToNumber(cx, top->getValue(), &d));
52713:                 d = -d;
76194:                 Value v = NumberValue(d);
76194: 
76194:                 /* Watch for overflow in constant propagation. */
76194:                 types::TypeSet *pushed = pushedTypeSet(0);
77353:                 if (!v.isInt32() && pushed && !pushed->hasType(types::Type::DoubleType())) {
77391:                     types::TypeScript::MonitorOverflow(cx, script, PC);
76194:                     return Compile_Retry;
76194:                 }
76194: 
52713:                 frame.pop();
76194:                 frame.push(v);
52713:             } else {
52713:                 jsop_neg();
52713:             }
52713:           }
52713:           END_CASE(JSOP_NEG)
52713: 
53039:           BEGIN_CASE(JSOP_POS)
53039:             jsop_pos();
53039:           END_CASE(JSOP_POS)
53039: 
54409:           BEGIN_CASE(JSOP_DELNAME)
54409:           {
54409:             uint32 index = fullAtomIndex(PC);
54409:             JSAtom *atom = script->getAtom(index);
54409: 
54409:             prepareStubCall(Uses(0));
54409:             masm.move(ImmPtr(atom), Registers::ArgReg1);
76194:             INLINE_STUBCALL(stubs::DelName, REJOIN_FALLTHROUGH);
76194:             pushSyncedEntry(0);
54409:           }
54409:           END_CASE(JSOP_DELNAME)
54409: 
54406:           BEGIN_CASE(JSOP_DELPROP)
54406:           {
54406:             uint32 index = fullAtomIndex(PC);
54406:             JSAtom *atom = script->getAtom(index);
54406: 
54406:             prepareStubCall(Uses(1));
54406:             masm.move(ImmPtr(atom), Registers::ArgReg1);
76194:             INLINE_STUBCALL(STRICT_VARIANT(stubs::DelProp), REJOIN_FALLTHROUGH);
54406:             frame.pop();
76194:             pushSyncedEntry(0);
54406:           }
54406:           END_CASE(JSOP_DELPROP)
54406: 
54406:           BEGIN_CASE(JSOP_DELELEM)
76194:           {
54406:             prepareStubCall(Uses(2));
76194:             INLINE_STUBCALL(STRICT_VARIANT(stubs::DelElem), REJOIN_FALLTHROUGH);
54406:             frame.popn(2);
76194:             pushSyncedEntry(0);
76194:           }
54406:           END_CASE(JSOP_DELELEM)
54406: 
52738:           BEGIN_CASE(JSOP_TYPEOF)
52784:           BEGIN_CASE(JSOP_TYPEOFEXPR)
52738:             jsop_typeof();
52738:           END_CASE(JSOP_TYPEOF)
52738: 
52676:           BEGIN_CASE(JSOP_VOID)
52676:             frame.pop();
53081:             frame.push(UndefinedValue());
52676:           END_CASE(JSOP_VOID)
52676: 
52741:           BEGIN_CASE(JSOP_GETPROP)
76194:           BEGIN_CASE(JSOP_LENGTH)
76194:             if (!jsop_getprop(script->getAtom(fullAtomIndex(PC)), knownPushedType(0)))
56037:                 return Compile_Error;
52741:           END_CASE(JSOP_GETPROP)
52741: 
52693:           BEGIN_CASE(JSOP_GETELEM)
57723:             if (!jsop_getelem(false))
56037:                 return Compile_Error;
52693:           END_CASE(JSOP_GETELEM)
52693: 
77357:           BEGIN_CASE(JSOP_TOID)
77357:             jsop_toid();
77357:           END_CASE(JSOP_TOID)
77357: 
52693:           BEGIN_CASE(JSOP_SETELEM)
78413:           {
60164:             jsbytecode *next = &PC[JSOP_SETELEM_LENGTH];
60164:             bool pop = (JSOp(*next) == JSOP_POP && !analysis->jumpTarget(next));
60164:             if (!jsop_setelem(pop))
57671:                 return Compile_Error;
60164:           }
52693:           END_CASE(JSOP_SETELEM);
52693: 
56556:           BEGIN_CASE(JSOP_EVAL)
56775:           {
56775:             JaegerSpew(JSpew_Insns, " --- EVAL --- \n");
56775:             emitEval(GET_ARGC(PC));
56775:             JaegerSpew(JSpew_Insns, " --- END EVAL --- \n");
56775:           }
56556:           END_CASE(JSOP_EVAL)
56556: 
52645:           BEGIN_CASE(JSOP_CALL)
76194:           BEGIN_CASE(JSOP_NEW)
57712:           BEGIN_CASE(JSOP_FUNAPPLY)
57712:           BEGIN_CASE(JSOP_FUNCALL)
52645:           {
76194:             bool callingNew = (op == JSOP_NEW);
76194: 
76194:             bool done = false;
77413:             if ((op == JSOP_CALL || op == JSOP_NEW) && !monitored(PC)) {
76194:                 CompileStatus status = inlineNativeFunction(GET_ARGC(PC), callingNew);
76194:                 if (status == Compile_Okay)
76194:                     done = true;
76194:                 else if (status != Compile_InlineAbort)
76194:                     return status;
76194:             }
76194:             if (!done && inlining()) {
76194:                 CompileStatus status = inlineScriptedFunction(GET_ARGC(PC), callingNew);
76194:                 if (status == Compile_Okay)
76194:                     done = true;
76194:                 else if (status != Compile_InlineAbort)
76194:                     return status;
77413:                 if (script->pcCounters) {
77413:                     /* Code generated while inlining has been accounted for. */
77413:                     updatePCCounters(PC, &codeStart, &countersUpdated);
77413:                 }
76194:             }
76194: 
76194:             FrameSize frameSize;
76194:             frameSize.initStatic(frame.totalDepth(), GET_ARGC(PC));
76194: 
76194:             if (!done) {
52648:                 JaegerSpew(JSpew_Insns, " --- SCRIPTED CALL --- \n");
76194:                 inlineCallHelper(GET_ARGC(PC), callingNew, frameSize);
52648:                 JaegerSpew(JSpew_Insns, " --- END SCRIPTED CALL --- \n");
52645:             }
76194:           }
52645:           END_CASE(JSOP_CALL)
52645: 
52615:           BEGIN_CASE(JSOP_NAME)
76194:           {
76194:             JSAtom *atom = script->getAtom(fullAtomIndex(PC));
76194:             jsop_name(atom, knownPushedType(0), false);
76194:             frame.extra(frame.peek(-1)).name = atom;
76194:           }
52615:           END_CASE(JSOP_NAME)
52615: 
71340:           BEGIN_CASE(JSOP_CALLNAME)
76194:           {
76194:             JSAtom *atom = script->getAtom(fullAtomIndex(PC));
76194:             jsop_name(atom, knownPushedType(0), true);
76194:             frame.extra(frame.peek(-2)).name = atom;
76194:           }
71340:           END_CASE(JSOP_CALLNAME)
71340: 
52605:           BEGIN_CASE(JSOP_DOUBLE)
52605:           {
52605:             uint32 index = fullAtomIndex(PC);
53081:             double d = script->getConst(index).toDouble();
53081:             frame.push(Value(DoubleValue(d)));
52605:           }
52605:           END_CASE(JSOP_DOUBLE)
52605: 
52653:           BEGIN_CASE(JSOP_STRING)
64345:             frame.push(StringValue(script->getAtom(fullAtomIndex(PC))));
52653:           END_CASE(JSOP_STRING)
52653: 
52560:           BEGIN_CASE(JSOP_ZERO)
78614:             frame.push(JSVAL_ZERO);
52560:           END_CASE(JSOP_ZERO)
52560: 
52560:           BEGIN_CASE(JSOP_ONE)
78614:             frame.push(JSVAL_ONE);
52560:           END_CASE(JSOP_ONE)
52560: 
52675:           BEGIN_CASE(JSOP_NULL)
53081:             frame.push(NullValue());
52675:           END_CASE(JSOP_NULL)
52675: 
52704:           BEGIN_CASE(JSOP_THIS)
52741:             jsop_this();
52704:           END_CASE(JSOP_THIS)
52704: 
52666:           BEGIN_CASE(JSOP_FALSE)
53081:             frame.push(Value(BooleanValue(false)));
52666:           END_CASE(JSOP_FALSE)
52666: 
52665:           BEGIN_CASE(JSOP_TRUE)
53081:             frame.push(Value(BooleanValue(true)));
52665:           END_CASE(JSOP_TRUE)
52665: 
52733:           BEGIN_CASE(JSOP_OR)
52733:           BEGIN_CASE(JSOP_AND)
76194:           {
76194:             jsbytecode *target = PC + GET_JUMP_OFFSET(PC);
76194:             fixDoubleTypes(target);
76194:             if (!jsop_andor(op, target))
56766:                 return Compile_Error;
76194:           }
52733:           END_CASE(JSOP_AND)
52733: 
52794:           BEGIN_CASE(JSOP_TABLESWITCH)
77438:           BEGIN_CASE(JSOP_TABLESWITCHX)
76194:             /*
76194:              * Note: there is no need to syncForBranch for the various targets of
76194:              * switch statement. The liveness analysis has already marked these as
76194:              * allocated with no registers in use. There is also no need to fix
76194:              * double types, as we don't track types of slots in scripts with
76194:              * switch statements (could be fixed).
76194:              */
77407:             if (script->pcCounters)
77407:                 updatePCCounters(PC, &codeStart, &countersUpdated);
59979: #if defined JS_CPU_ARM /* Need to implement jump(BaseIndex) for ARM */
76194:             frame.syncAndKillEverything();
52794:             masm.move(ImmPtr(PC), Registers::ArgReg1);
53338: 
54719:             /* prepareStubCall() is not needed due to syncAndForgetEverything() */
76194:             INLINE_STUBCALL(stubs::TableSwitch, REJOIN_NONE);
53338:             frame.pop();
53338: 
52794:             masm.jump(Registers::ReturnReg);
59979: #else
61233:             if (!jsop_tableswitch(PC))
61233:                 return Compile_Error;
59979: #endif
52794:             PC += js_GetVariableBytecodeLength(PC);
52794:             break;
52794:           END_CASE(JSOP_TABLESWITCH)
52794: 
52793:           BEGIN_CASE(JSOP_LOOKUPSWITCH)
77407:             if (script->pcCounters)
77407:                 updatePCCounters(PC, &codeStart, &countersUpdated);
54719:             frame.syncAndForgetEverything();
52793:             masm.move(ImmPtr(PC), Registers::ArgReg1);
53338: 
54719:             /* prepareStubCall() is not needed due to syncAndForgetEverything() */
76194:             INLINE_STUBCALL(stubs::LookupSwitch, REJOIN_NONE);
53338:             frame.pop();
53338: 
52793:             masm.jump(Registers::ReturnReg);
52793:             PC += js_GetVariableBytecodeLength(PC);
52793:             break;
52793:           END_CASE(JSOP_LOOKUPSWITCH)
52793: 
64230:           BEGIN_CASE(JSOP_CASE)
64230:             // X Y
64230: 
64230:             frame.dupAt(-2);
64230:             // X Y X
64230: 
64230:             jsop_stricteq(JSOP_STRICTEQ);
64230:             // X cond
64230: 
64230:             if (!jsop_ifneq(JSOP_IFNE, PC + GET_JUMP_OFFSET(PC)))
64230:                 return Compile_Error;
64230:           END_CASE(JSOP_CASE)
64230: 
52739:           BEGIN_CASE(JSOP_STRICTEQ)
52855:             jsop_stricteq(op);
52739:           END_CASE(JSOP_STRICTEQ)
52739: 
52739:           BEGIN_CASE(JSOP_STRICTNE)
52855:             jsop_stricteq(op);
52739:           END_CASE(JSOP_STRICTNE)
52739: 
52736:           BEGIN_CASE(JSOP_ITER)
61055:             if (!iter(PC[1]))
61055:                 return Compile_Error;
52736:           END_CASE(JSOP_ITER)
52736: 
52737:           BEGIN_CASE(JSOP_MOREITER)
76194:           {
61055:             /* At the byte level, this is always fused with IFNE or IFNEX. */
77407:             if (script->pcCounters)
77407:                 updatePCCounters(PC, &codeStart, &countersUpdated);
76194:             jsbytecode *target = &PC[JSOP_MOREITER_LENGTH];
76194:             JSOp next = JSOp(*target);
76194:             JS_ASSERT(next == JSOP_IFNE || next == JSOP_IFNEX);
76194: 
76194:             target += (next == JSOP_IFNE)
76194:                       ? GET_JUMP_OFFSET(target)
76194:                       : GET_JUMPX_OFFSET(target);
76194: 
76194:             fixDoubleTypes(target);
76194:             if (!iterMore(target))
61055:                 return Compile_Error;
76194:             PC += JSOP_MOREITER_LENGTH;
76194:             PC += js_CodeSpec[next].length;
60785:             break;
76194:           }
52737:           END_CASE(JSOP_MOREITER)
52737: 
52737:           BEGIN_CASE(JSOP_ENDITER)
53404:             iterEnd();
52737:           END_CASE(JSOP_ENDITER)
52737: 
52575:           BEGIN_CASE(JSOP_POP)
52575:             frame.pop();
52575:           END_CASE(JSOP_POP)
52575: 
52656:           BEGIN_CASE(JSOP_GETARG)
76194:           {
76194:             restoreVarType();
76194:             uint32 arg = GET_SLOTNO(PC);
76194:             frame.pushArg(arg);
76194:           }
76194:           END_CASE(JSOP_GETARG)
76194: 
52678:           BEGIN_CASE(JSOP_CALLARG)
52656:           {
76194:             restoreVarType();
76194:             uint32 arg = GET_SLOTNO(PC);
76194:             if (JSObject *singleton = pushedSingleton(0))
76194:                 frame.push(ObjectValue(*singleton));
76194:             else
76194:                 frame.pushArg(arg);
55712:             frame.push(UndefinedValue());
52656:           }
52656:           END_CASE(JSOP_GETARG)
52656: 
52825:           BEGIN_CASE(JSOP_BINDGNAME)
52826:             jsop_bindgname();
52825:           END_CASE(JSOP_BINDGNAME)
52825: 
52728:           BEGIN_CASE(JSOP_SETARG)
57787:           {
76194:             jsbytecode *next = &PC[JSOP_SETARG_LENGTH];
57787:             bool pop = JSOp(*next) == JSOP_POP && !analysis->jumpTarget(next);
57787:             frame.storeArg(GET_SLOTNO(PC), pop);
76194:             updateVarType();
76194: 
57787:             if (pop) {
57787:                 frame.pop();
57787:                 PC += JSOP_SETARG_LENGTH + JSOP_POP_LENGTH;
57787:                 break;
57787:             }
57787:           }
52728:           END_CASE(JSOP_SETARG)
52728: 
52670:           BEGIN_CASE(JSOP_GETLOCAL)
52670:           {
76194:             /*
76194:              * Update the var type unless we are about to pop the variable.
76194:              * Sync is not guaranteed for types of dead locals, and GETLOCAL
76194:              * followed by POP is not regarded as a use of the variable.
76194:              */
76194:             jsbytecode *next = &PC[JSOP_GETLOCAL_LENGTH];
76194:             if (JSOp(*next) != JSOP_POP || analysis->jumpTarget(next))
76194:                 restoreVarType();
52670:             uint32 slot = GET_SLOTNO(PC);
52670:             frame.pushLocal(slot);
52670:           }
52670:           END_CASE(JSOP_GETLOCAL)
52670: 
52670:           BEGIN_CASE(JSOP_SETLOCAL)
54719:           {
54719:             jsbytecode *next = &PC[JSOP_SETLOCAL_LENGTH];
56602:             bool pop = JSOp(*next) == JSOP_POP && !analysis->jumpTarget(next);
54719:             frame.storeLocal(GET_SLOTNO(PC), pop);
76194:             updateVarType();
76194: 
54719:             if (pop) {
54719:                 frame.pop();
54719:                 PC += JSOP_SETLOCAL_LENGTH + JSOP_POP_LENGTH;
54719:                 break;
54719:             }
54719:           }
54719:           END_CASE(JSOP_SETLOCAL)
54719: 
52786:           BEGIN_CASE(JSOP_SETLOCALPOP)
76194:           {
76194:             uint32 slot = GET_SLOTNO(PC);
76194:             frame.storeLocal(slot, true);
52786:             frame.pop();
76194:             updateVarType();
76194:           }
54719:           END_CASE(JSOP_SETLOCALPOP)
52670: 
52575:           BEGIN_CASE(JSOP_UINT16)
53081:             frame.push(Value(Int32Value((int32_t) GET_UINT16(PC))));
52575:           END_CASE(JSOP_UINT16)
52575: 
52719:           BEGIN_CASE(JSOP_NEWINIT)
76194:             if (!jsop_newinit())
76194:                 return Compile_Error;
52719:           END_CASE(JSOP_NEWINIT)
52719: 
58056:           BEGIN_CASE(JSOP_NEWARRAY)
76194:             if (!jsop_newinit())
76194:                 return Compile_Error;
58056:           END_CASE(JSOP_NEWARRAY)
58056: 
58056:           BEGIN_CASE(JSOP_NEWOBJECT)
76194:             if (!jsop_newinit())
76194:                 return Compile_Error;
58056:           END_CASE(JSOP_NEWOBJECT)
58056: 
52719:           BEGIN_CASE(JSOP_ENDINIT)
52719:           END_CASE(JSOP_ENDINIT)
52719: 
58056:           BEGIN_CASE(JSOP_INITMETHOD)
58056:             jsop_initmethod();
58056:             frame.pop();
58056:           END_CASE(JSOP_INITMETHOD)
58056: 
52735:           BEGIN_CASE(JSOP_INITPROP)
58056:             jsop_initprop();
52735:             frame.pop();
52735:           END_CASE(JSOP_INITPROP)
52735: 
52723:           BEGIN_CASE(JSOP_INITELEM)
58056:             jsop_initelem();
52723:             frame.popn(2);
52723:           END_CASE(JSOP_INITELEM)
52723: 
52804:           BEGIN_CASE(JSOP_INCARG)
52804:           BEGIN_CASE(JSOP_DECARG)
52804:           BEGIN_CASE(JSOP_ARGINC)
52804:           BEGIN_CASE(JSOP_ARGDEC)
76194:             if (!jsop_arginc(op, GET_SLOTNO(PC)))
76194:                 return Compile_Retry;
52804:           END_CASE(JSOP_ARGDEC)
52804: 
52808:           BEGIN_CASE(JSOP_INCLOCAL)
52808:           BEGIN_CASE(JSOP_DECLOCAL)
52808:           BEGIN_CASE(JSOP_LOCALINC)
52808:           BEGIN_CASE(JSOP_LOCALDEC)
76194:             if (!jsop_localinc(op, GET_SLOTNO(PC)))
76194:                 return Compile_Retry;
52808:           END_CASE(JSOP_LOCALDEC)
52808: 
52560:           BEGIN_CASE(JSOP_BINDNAME)
60526:             jsop_bindname(script->getAtom(fullAtomIndex(PC)), true);
52560:           END_CASE(JSOP_BINDNAME)
52560: 
52886:           BEGIN_CASE(JSOP_SETPROP)
78413:           {
78413:             jsbytecode *next = &PC[JSOP_SETPROP_LENGTH];
78413:             bool pop = JSOp(*next) == JSOP_POP && !analysis->jumpTarget(next);
78413:             if (!jsop_setprop(script->getAtom(fullAtomIndex(PC)), true, pop))
78413:                 return Compile_Error;
78413:           }
78413:           END_CASE(JSOP_SETPROP)
78413: 
52611:           BEGIN_CASE(JSOP_SETNAME)
52747:           BEGIN_CASE(JSOP_SETMETHOD)
76194:           {
78413:             jsbytecode *next = &PC[JSOP_SETNAME_LENGTH];
76194:             bool pop = JSOp(*next) == JSOP_POP && !analysis->jumpTarget(next);
76194:             if (!jsop_setprop(script->getAtom(fullAtomIndex(PC)), true, pop))
56037:                 return Compile_Error;
76194:           }
52611:           END_CASE(JSOP_SETNAME)
52611: 
52772:           BEGIN_CASE(JSOP_THROW)
53087:             prepareStubCall(Uses(1));
76194:             INLINE_STUBCALL(stubs::Throw, REJOIN_NONE);
52772:             frame.pop();
52772:           END_CASE(JSOP_THROW)
52772: 
54413:           BEGIN_CASE(JSOP_IN)
76194:           {
54413:             prepareStubCall(Uses(2));
76194:             INLINE_STUBCALL(stubs::In, REJOIN_PUSH_BOOLEAN);
54413:             frame.popn(2);
54413:             frame.takeReg(Registers::ReturnReg);
54413:             frame.pushTypedPayload(JSVAL_TYPE_BOOLEAN, Registers::ReturnReg);
76194:           }
54413:           END_CASE(JSOP_IN)
54413: 
52780:           BEGIN_CASE(JSOP_INSTANCEOF)
56037:             if (!jsop_instanceof())
56037:                 return Compile_Error;
52780:           END_CASE(JSOP_INSTANCEOF)
52780: 
52786:           BEGIN_CASE(JSOP_EXCEPTION)
76194:           {
60211:             prepareStubCall(Uses(0));
76194:             INLINE_STUBCALL(stubs::Exception, REJOIN_FALLTHROUGH);
76194:             frame.pushSynced(JSVAL_TYPE_UNKNOWN);
76194:           }
52786:           END_CASE(JSOP_EXCEPTION)
52786: 
52768:           BEGIN_CASE(JSOP_LINENO)
52768:           END_CASE(JSOP_LINENO)
52768: 
56586:           BEGIN_CASE(JSOP_ENUMELEM)
56586:             // Normally, SETELEM transforms the stack
56586:             //  from: OBJ ID VALUE
56586:             //  to:   VALUE
56586:             //
56586:             // Here, the stack transition is
56586:             //  from: VALUE OBJ ID
56586:             //  to:
56586:             // So we make the stack look like a SETELEM, and re-use it.
56586: 
56586:             // Before: VALUE OBJ ID
56586:             // After:  VALUE OBJ ID VALUE
56586:             frame.dupAt(-3);
56586: 
56586:             // Before: VALUE OBJ ID VALUE
56586:             // After:  VALUE VALUE
60164:             if (!jsop_setelem(true))
57671:                 return Compile_Error;
56586: 
56586:             // Before: VALUE VALUE
56586:             // After:
56586:             frame.popn(2);
56586:           END_CASE(JSOP_ENUMELEM)
56586: 
54855:           BEGIN_CASE(JSOP_BLOCKCHAIN)
54855:           END_CASE(JSOP_BLOCKCHAIN)
54855: 
54855:           BEGIN_CASE(JSOP_NULLBLOCKCHAIN)
54855:           END_CASE(JSOP_NULLBLOCKCHAIN)
54855: 
54415:           BEGIN_CASE(JSOP_CONDSWITCH)
54415:             /* No-op for the decompiler. */
54415:           END_CASE(JSOP_CONDSWITCH)
54415: 
52647:           BEGIN_CASE(JSOP_DEFFUN)
53591:           {
53591:             uint32 index = fullAtomIndex(PC);
59968:             JSFunction *innerFun = script->getFunction(index);
59968: 
53087:             prepareStubCall(Uses(0));
59968:             masm.move(ImmPtr(innerFun), Registers::ArgReg1);
76194:             INLINE_STUBCALL(STRICT_VARIANT(stubs::DefFun), REJOIN_FALLTHROUGH);
53591:           }
52647:           END_CASE(JSOP_DEFFUN)
52647: 
54408:           BEGIN_CASE(JSOP_DEFVAR)
57795:           BEGIN_CASE(JSOP_DEFCONST)
54408:           {
54408:             uint32 index = fullAtomIndex(PC);
54408:             JSAtom *atom = script->getAtom(index);
54408: 
54408:             prepareStubCall(Uses(0));
54408:             masm.move(ImmPtr(atom), Registers::ArgReg1);
76194:             INLINE_STUBCALL(stubs::DefVarOrConst, REJOIN_FALLTHROUGH);
54408:           }
54408:           END_CASE(JSOP_DEFVAR)
54408: 
57795:           BEGIN_CASE(JSOP_SETCONST)
57795:           {
57795:             uint32 index = fullAtomIndex(PC);
57795:             JSAtom *atom = script->getAtom(index);
57795: 
57795:             prepareStubCall(Uses(1));
57795:             masm.move(ImmPtr(atom), Registers::ArgReg1);
76194:             INLINE_STUBCALL(stubs::SetConst, REJOIN_FALLTHROUGH);
57795:           }
57795:           END_CASE(JSOP_SETCONST)
57795: 
53038:           BEGIN_CASE(JSOP_DEFLOCALFUN_FC)
53038:           {
53038:             uint32 slot = GET_SLOTNO(PC);
53038:             JSFunction *fun = script->getFunction(fullAtomIndex(&PC[SLOTNO_LEN]));
57787:             prepareStubCall(Uses(frame.frameSlots()));
53038:             masm.move(ImmPtr(fun), Registers::ArgReg1);
76194:             INLINE_STUBCALL(stubs::DefLocalFun_FC, REJOIN_DEFLOCALFUN);
53038:             frame.takeReg(Registers::ReturnReg);
53038:             frame.pushTypedPayload(JSVAL_TYPE_OBJECT, Registers::ReturnReg);
53309:             frame.storeLocal(slot, true);
53038:             frame.pop();
76194:             updateVarType();
53038:           }
53340:           END_CASE(JSOP_DEFLOCALFUN_FC)
53038: 
52730:           BEGIN_CASE(JSOP_LAMBDA)
52730:           {
52730:             JSFunction *fun = script->getFunction(fullAtomIndex(PC));
53249: 
53249:             JSObjStubFun stub = stubs::Lambda;
53249:             uint32 uses = 0;
53249: 
77544:             jsbytecode *pc2 = NULL;
77544:             if (fun->joinable()) {
77544:                 pc2 = AdvanceOverBlockchainOp(PC + JSOP_LAMBDA_LENGTH);
54855:                 JSOp next = JSOp(*pc2);
54855: 
53249:                 if (next == JSOP_INITMETHOD) {
77544:                     stub = stubs::LambdaJoinableForInit;
53249:                 } else if (next == JSOP_SETMETHOD) {
77544:                     stub = stubs::LambdaJoinableForSet;
53249:                     uses = 1;
77544:                 } else if (next == JSOP_CALL) {
77544:                     int iargc = GET_ARGC(pc2);
77544:                     if (iargc == 1 || iargc == 2) {
53249:                         stub = stubs::LambdaJoinableForCall;
57787:                         uses = frame.frameSlots();
77544:                     }
53249:                 } else if (next == JSOP_NULL) {
77544:                     pc2 += JSOP_NULL_LENGTH;
77544:                     if (JSOp(*pc2) == JSOP_CALL && GET_ARGC(pc2) == 0)
53249:                         stub = stubs::LambdaJoinableForNull;
53249:                 }
53249:             }
53249: 
53249:             prepareStubCall(Uses(uses));
52730:             masm.move(ImmPtr(fun), Registers::ArgReg1);
53128: 
77395:             if (stub != stubs::Lambda)
77395:                 masm.storePtr(ImmPtr(pc2), FrameAddress(offsetof(VMFrame, scratch)));
77395: 
76194:             INLINE_STUBCALL(stub, REJOIN_PUSH_OBJECT);
53249: 
52730:             frame.takeReg(Registers::ReturnReg);
53025:             frame.pushTypedPayload(JSVAL_TYPE_OBJECT, Registers::ReturnReg);
52730:           }
52730:           END_CASE(JSOP_LAMBDA)
52730: 
52774:           BEGIN_CASE(JSOP_TRY)
54719:             frame.syncAndForgetEverything();
52774:           END_CASE(JSOP_TRY)
52774: 
53531:           BEGIN_CASE(JSOP_GETFCSLOT)
53531:           BEGIN_CASE(JSOP_CALLFCSLOT)
52716:           {
53531:             uintN index = GET_UINT16(PC);
57787: 
57787:             // Load the callee's payload into a register.
57787:             frame.pushCallee();
57787:             RegisterID reg = frame.copyDataIntoReg(frame.peek(-1));
57787:             frame.pop();
57787: 
53531:             // obj->getFlatClosureUpvars()
76194:             Address upvarAddress(reg, JSObject::getFlatClosureUpvarsOffset());
53531:             masm.loadPrivate(upvarAddress, reg);
53531:             // push ((Value *) reg)[index]
77439: 
77439:             BarrierState barrier = pushAddressMaybeBarrier(Address(reg, index * sizeof(Value)),
77439:                                                            knownPushedType(0), true);
77439:             finishBarrier(barrier, REJOIN_GETTER, 0);
77439: 
53531:             if (op == JSOP_CALLFCSLOT)
55712:                 frame.push(UndefinedValue());
52716:           }
53531:           END_CASE(JSOP_CALLFCSLOT)
52716: 
53037:           BEGIN_CASE(JSOP_ARGSUB)
76194:           {
53087:             prepareStubCall(Uses(0));
53037:             masm.move(Imm32(GET_ARGNO(PC)), Registers::ArgReg1);
76194:             INLINE_STUBCALL(stubs::ArgSub, REJOIN_FALLTHROUGH);
76194:             pushSyncedEntry(0);
76194:           }
53037:           END_CASE(JSOP_ARGSUB)
53037: 
52781:           BEGIN_CASE(JSOP_ARGCNT)
76194:           {
53087:             prepareStubCall(Uses(0));
76194:             INLINE_STUBCALL(stubs::ArgCnt, REJOIN_FALLTHROUGH);
76194:             pushSyncedEntry(0);
76194:           }
52781:           END_CASE(JSOP_ARGCNT)
52781: 
52727:           BEGIN_CASE(JSOP_DEFLOCALFUN)
52727:           {
52727:             uint32 slot = GET_SLOTNO(PC);
52727:             JSFunction *fun = script->getFunction(fullAtomIndex(&PC[SLOTNO_LEN]));
53087:             prepareStubCall(Uses(0));
52727:             masm.move(ImmPtr(fun), Registers::ArgReg1);
76194:             INLINE_STUBCALL(stubs::DefLocalFun, REJOIN_DEFLOCALFUN);
52727:             frame.takeReg(Registers::ReturnReg);
53025:             frame.pushTypedPayload(JSVAL_TYPE_OBJECT, Registers::ReturnReg);
53309:             frame.storeLocal(slot, true);
52727:             frame.pop();
76194:             updateVarType();
52727:           }
52727:           END_CASE(JSOP_DEFLOCALFUN)
52727: 
52806:           BEGIN_CASE(JSOP_RETRVAL)
54832:             emitReturn(NULL);
52806:           END_CASE(JSOP_RETRVAL)
52806: 
52825:           BEGIN_CASE(JSOP_GETGNAME)
52825:           BEGIN_CASE(JSOP_CALLGNAME)
76194:           {
76194:             uint32 index = fullAtomIndex(PC);
76194:             jsop_getgname(index);
76194:             frame.extra(frame.peek(-1)).name = script->getAtom(index);
52825:             if (op == JSOP_CALLGNAME)
63236:                 jsop_callgname_epilogue();
76194:           }
52825:           END_CASE(JSOP_GETGNAME)
52825: 
52825:           BEGIN_CASE(JSOP_SETGNAME)
76194:           {
76194:             jsbytecode *next = &PC[JSOP_SETGNAME_LENGTH];
76194:             bool pop = JSOp(*next) == JSOP_POP && !analysis->jumpTarget(next);
76194:             jsop_setgname(script->getAtom(fullAtomIndex(PC)), true, pop);
76194:           }
52825:           END_CASE(JSOP_SETGNAME)
52825: 
52729:           BEGIN_CASE(JSOP_REGEXP)
52729:           {
52729:             JSObject *regex = script->getRegExp(fullAtomIndex(PC));
53087:             prepareStubCall(Uses(0));
52729:             masm.move(ImmPtr(regex), Registers::ArgReg1);
77356:             INLINE_STUBCALL(stubs::RegExp, REJOIN_PUSH_OBJECT);
52729:             frame.takeReg(Registers::ReturnReg);
53025:             frame.pushTypedPayload(JSVAL_TYPE_OBJECT, Registers::ReturnReg);
52729:           }
52729:           END_CASE(JSOP_REGEXP)
52729: 
59962:           BEGIN_CASE(JSOP_OBJECT)
59962:           {
59962:             JSObject *object = script->getObject(fullAtomIndex(PC));
59962:             RegisterID reg = frame.allocReg();
59962:             masm.move(ImmPtr(object), reg);
59962:             frame.pushTypedPayload(JSVAL_TYPE_OBJECT, reg);
59962:           }
59962:           END_CASE(JSOP_OBJECT)
59962: 
52761:           BEGIN_CASE(JSOP_CALLPROP)
52903:             if (!jsop_callprop(script->getAtom(fullAtomIndex(PC))))
52903:                 return Compile_Error;
52761:           END_CASE(JSOP_CALLPROP)
52761: 
52575:           BEGIN_CASE(JSOP_UINT24)
53081:             frame.push(Value(Int32Value((int32_t) GET_UINT24(PC))));
52575:           END_CASE(JSOP_UINT24)
52575: 
52771:           BEGIN_CASE(JSOP_CALLELEM)
57723:             jsop_getelem(true);
52771:           END_CASE(JSOP_CALLELEM)
52771: 
52560:           BEGIN_CASE(JSOP_STOP)
77407:             if (script->pcCounters)
77407:                 updatePCCounters(PC, &codeStart, &countersUpdated);
54832:             emitReturn(NULL);
52560:             goto done;
52560:           END_CASE(JSOP_STOP)
52560: 
54847:           BEGIN_CASE(JSOP_GETXPROP)
56037:             if (!jsop_xname(script->getAtom(fullAtomIndex(PC))))
56037:                 return Compile_Error;
54847:           END_CASE(JSOP_GETXPROP)
54847: 
52785:           BEGIN_CASE(JSOP_ENTERBLOCK)
54840:             enterBlock(script->getObject(fullAtomIndex(PC)));
54840:           END_CASE(JSOP_ENTERBLOCK);
52785: 
52785:           BEGIN_CASE(JSOP_LEAVEBLOCK)
54840:             leaveBlock();
52785:           END_CASE(JSOP_LEAVEBLOCK)
52785: 
52742:           BEGIN_CASE(JSOP_CALLLOCAL)
76194:           {
76194:             restoreVarType();
76194:             uint32 slot = GET_SLOTNO(PC);
76194:             if (JSObject *singleton = pushedSingleton(0))
76194:                 frame.push(ObjectValue(*singleton));
76194:             else
76194:                 frame.pushLocal(slot);
55712:             frame.push(UndefinedValue());
76194:           }
52742:           END_CASE(JSOP_CALLLOCAL)
52742: 
52575:           BEGIN_CASE(JSOP_INT8)
53081:             frame.push(Value(Int32Value(GET_INT8(PC))));
52575:           END_CASE(JSOP_INT8)
52575: 
52575:           BEGIN_CASE(JSOP_INT32)
53081:             frame.push(Value(Int32Value(GET_INT32(PC))));
52575:           END_CASE(JSOP_INT32)
52575: 
54415:           BEGIN_CASE(JSOP_HOLE)
54415:             frame.push(MagicValue(JS_ARRAY_HOLE));
54415:           END_CASE(JSOP_HOLE)
54415: 
52775:           BEGIN_CASE(JSOP_LAMBDA_FC)
52775:           {
52775:             JSFunction *fun = script->getFunction(fullAtomIndex(PC));
57787:             prepareStubCall(Uses(frame.frameSlots()));
52775:             masm.move(ImmPtr(fun), Registers::ArgReg1);
76194:             INLINE_STUBCALL(stubs::FlatLambda, REJOIN_PUSH_OBJECT);
52775:             frame.takeReg(Registers::ReturnReg);
53025:             frame.pushTypedPayload(JSVAL_TYPE_OBJECT, Registers::ReturnReg);
52775:           }
52775:           END_CASE(JSOP_LAMBDA_FC)
52775: 
52617:           BEGIN_CASE(JSOP_TRACE)
56217:           BEGIN_CASE(JSOP_NOTRACE)
52753:           {
76194:             if (analysis->jumpTarget(PC)) {
53223:                 interruptCheckHelper();
76194:                 recompileCheckHelper();
76194:             }
52753:           }
52617:           END_CASE(JSOP_TRACE)
52617: 
53395:           BEGIN_CASE(JSOP_DEBUGGER)
76194:           {
53395:             prepareStubCall(Uses(0));
53396:             masm.move(ImmPtr(PC), Registers::ArgReg1);
77434:             INLINE_STUBCALL(stubs::DebuggerStatement, REJOIN_FALLTHROUGH);
76194:           }
53395:           END_CASE(JSOP_DEBUGGER)
53395: 
52874:           BEGIN_CASE(JSOP_UNBRAND)
52874:             jsop_unbrand();
52874:           END_CASE(JSOP_UNBRAND)
52874: 
52874:           BEGIN_CASE(JSOP_UNBRANDTHIS)
77890:             prepareStubCall(Uses(1));
77890:             INLINE_STUBCALL(stubs::UnbrandThis, REJOIN_FALLTHROUGH);
52874:           END_CASE(JSOP_UNBRANDTHIS)
52874: 
52558:           default:
52558:            /* Sorry, this opcode isn't implemented yet. */
52558: #ifdef JS_METHODJIT_SPEW
52825:             JaegerSpew(JSpew_Abort, "opcode %s not handled yet (%s line %d)\n", OpcodeNames[op],
52825:                        script->filename, js_PCToLineNumber(cx, script, PC));
52558: #endif
52558:             return Compile_Abort;
52558:         }
52560: 
52560:     /**********************
52560:      *  END COMPILER OPS  *
52560:      **********************/
52578: 
77434:         if (cx->typeInferenceEnabled() && PC == lastPC + analyze::GetBytecodeLength(lastPC)) {
76194:             /*
76194:              * Inform the frame of the type sets for values just pushed. Skip
76194:              * this if we did any opcode fusions, we don't keep track of the
76194:              * associated type sets in such cases.
76194:              */
77407:             unsigned nuses = GetUseCount(script, lastPC - script->code);
77407:             unsigned ndefs = GetDefCount(script, lastPC - script->code);
76194:             for (unsigned i = 0; i < ndefs; i++) {
76194:                 FrameEntry *fe = frame.getStack(opinfo->stackDepth - nuses + i);
76194:                 if (fe) {
76194:                     /* fe may be NULL for conditionally pushed entries, e.g. JSOP_AND */
77407:                     frame.extra(fe).types = analysis->pushedTypes(lastPC - script->code, i);
77407:                 }
77407:             }
77407:         }
77407: 
80222:         if (script->pcCounters || pcLengths) {
77407:             size_t length = masm.size() - masm.distanceOf(codeStart);
77407:             if (countersUpdated || length != 0) {
80222:                 if (!countersUpdated && script->pcCounters)
77407:                     updatePCCounters(lastPC, &codeStart, &countersUpdated);
77407: 
80222:                 if (pcLengths) {
77407:                     /* Fill in the amount of inline code generated for the op. */
77407:                     uint32 offset = ssa.frameLength(a->inlineIndex) + lastPC - script->code;
77407:                     pcLengths[offset].codeLength += length;
76194:                 }
76194:             }
80222:         }
76194: 
52578:         frame.assertValidRegisterState();
52558:     }
52558: 
52560:   done:
52558:     return Compile_Okay;
52558: }
52558: 
52560: #undef END_CASE
52560: #undef BEGIN_CASE
52560: 
77407: void
77407: mjit::Compiler::updatePCCounters(jsbytecode *pc, Label *start, bool *updated)
77407: {
77407:     /*
77407:      * Bump the METHODJIT count for the opcode, read the METHODJIT_CODE_LENGTH
77407:      * and METHODJIT_PICS_LENGTH counts, indicating the amounts of inline path
77407:      * code and generated code, respectively, and add them to the accumulated
77407:      * total for the op.
77407:      */
77407:     uint32 offset = ssa.frameLength(a->inlineIndex) + pc - script->code;
77407: 
77407:     /*
77407:      * Base register for addresses, we can't use AbsoluteAddress in all places.
77407:      * This may hold a live value, so write it out to the top of the stack
77407:      * first. This cannot overflow the stack, as space is always reserved for
77407:      * an extra callee frame.
77407:      */
77407:     RegisterID reg = Registers::ReturnReg;
77407:     masm.storePtr(reg, frame.addressOfTop());
77407: 
77407:     double *code = &script->pcCounters.get(JSPCCounters::METHODJIT_CODE, pc - script->code);
77407:     double *codeLength = &pcLengths[offset].codeLength;
77407:     masm.addCounter(codeLength, code, reg);
77407: 
77407:     double *pics = &script->pcCounters.get(JSPCCounters::METHODJIT_PICS, pc - script->code);
77407:     double *picsLength = &pcLengths[offset].picsLength;
77407:     masm.addCounter(picsLength, pics, reg);
77407: 
77407:     static const double oneDouble = 1.0;
77407:     double *counter = &script->pcCounters.get(JSPCCounters::METHODJIT, pc - script->code);
77407:     masm.addCounter(&oneDouble, counter, reg);
77407: 
77407:     /* Reload the base register's original value. */
77407:     masm.loadPtr(frame.addressOfTop(), reg);
77407: 
77407:     /* The start label should reflect the code for the op, not instrumentation. */
77407:     *start = masm.label();
77407:     *updated = true;
77407: }
77407: 
52653: JSC::MacroAssembler::Label
76194: mjit::Compiler::labelOf(jsbytecode *pc, uint32 inlineIndex)
76194: {
76194:     ActiveFrame *a = (inlineIndex == uint32(-1)) ? outer : inlineFrames[inlineIndex];
76194:     JS_ASSERT(uint32(pc - a->script->code) < a->script->length);
76194: 
76194:     uint32 offs = uint32(pc - a->script->code);
76194:     JS_ASSERT(a->jumpMap[offs].isSet());
76194:     return a->jumpMap[offs];
52599: }
52599: 
52560: uint32
52560: mjit::Compiler::fullAtomIndex(jsbytecode *pc)
52560: {
52560:     return GET_SLOTNO(pc);
52560: 
52560:     /* If we ever enable INDEXBASE garbage, use this below. */
52560: #if 0
77659:     return GET_SLOTNO(pc) + (atoms - script->atoms);
52560: #endif
52560: }
52560: 
52653: bool
52653: mjit::Compiler::knownJump(jsbytecode *pc)
52653: {
52653:     return pc < PC;
52653: }
52653: 
56766: bool
52599: mjit::Compiler::jumpInScript(Jump j, jsbytecode *pc)
52599: {
52599:     JS_ASSERT(pc >= script->code && uint32(pc - script->code) < script->length);
52599: 
56766:     if (pc < PC) {
76194:         j.linkTo(a->jumpMap[uint32(pc - script->code)], &masm);
56766:         return true;
56766:     }
76194:     return branchPatches.append(BranchPatch(j, pc, a->inlineIndex));
52599: }
52599: 
52560: void
54832: mjit::Compiler::emitFinalReturn(Assembler &masm)
52560: {
69223:     masm.loadPtr(Address(JSFrameReg, StackFrame::offsetOfNcode()), Registers::ReturnReg);
54832:     masm.jump(Registers::ReturnReg);
54832: }
54832: 
55503: // Emits code to load a return value of the frame into the scripted-ABI
55503: // type & data register pair. If the return value is in fp->rval, then |fe|
55503: // is NULL. Otherwise, |fe| contains the return value.
55503: //
55503: // If reading from fp->rval, |undefined| is loaded optimistically, before
55503: // checking if fp->rval is set in the frame flags and loading that instead.
55503: //
55503: // Otherwise, if |masm| is the inline path, it is loaded as efficiently as
55503: // the FrameState can manage. If |masm| is the OOL path, the value is simply
55503: // loaded from its slot in the frame, since the caller has guaranteed it's
55503: // been synced.
55503: //
54832: void
55503: mjit::Compiler::loadReturnValue(Assembler *masm, FrameEntry *fe)
54427: {
55503:     RegisterID typeReg = JSReturnReg_Type;
55503:     RegisterID dataReg = JSReturnReg_Data;
55503: 
55503:     if (fe) {
55503:         // If using the OOL assembler, the caller signifies that the |fe| is
55503:         // synced, but not to rely on its register state.
55503:         if (masm != &this->masm) {
55503:             if (fe->isConstant()) {
55503:                 stubcc.masm.loadValueAsComponents(fe->getValue(), typeReg, dataReg);
55503:             } else {
55503:                 Address rval(frame.addressOf(fe));
76194:                 if (fe->isTypeKnown() && !fe->isType(JSVAL_TYPE_DOUBLE)) {
55503:                     stubcc.masm.loadPayload(rval, dataReg);
55503:                     stubcc.masm.move(ImmType(fe->getKnownType()), typeReg);
55503:                 } else {
55503:                     stubcc.masm.loadValueAsComponents(rval, typeReg, dataReg);
55503:                 }
55503:             }
55503:         } else {
56572:             frame.loadForReturn(fe, typeReg, dataReg, Registers::ReturnReg);
55503:         }
55503:     } else {
55503:          // Load a return value from POPV or SETRVAL into the return registers,
55503:          // otherwise return undefined.
55503:         masm->loadValueAsComponents(UndefinedValue(), typeReg, dataReg);
56602:         if (analysis->usesReturnValue()) {
55503:             Jump rvalClear = masm->branchTest32(Assembler::Zero,
55503:                                                FrameFlagsAddress(),
69223:                                                Imm32(StackFrame::HAS_RVAL));
69223:             Address rvalAddress(JSFrameReg, StackFrame::offsetOfReturnValue());
55503:             masm->loadValueAsComponents(rvalAddress, typeReg, dataReg);
55503:             rvalClear.linkTo(masm->label(), masm);
55487:         }
55487:     }
55503: }
55503: 
55503: // This ensures that constructor return values are an object. If a non-object
55503: // is returned, either explicitly or implicitly, the newly created object is
55503: // loaded out of the frame. Otherwise, the explicitly returned object is kept.
55503: //
55503: void
55503: mjit::Compiler::fixPrimitiveReturn(Assembler *masm, FrameEntry *fe)
55503: {
55503:     JS_ASSERT(isConstructing);
55503: 
57787:     bool ool = (masm != &this->masm);
77391:     Address thisv(JSFrameReg, StackFrame::offsetOfThis(script->function()));
55503: 
58700:     // We can just load |thisv| if either of the following is true:
58700:     //  (1) There is no explicit return value, AND fp->rval is not used.
58700:     //  (2) There is an explicit return value, and it's known to be primitive.
58700:     if ((!fe && !analysis->usesReturnValue()) ||
58700:         (fe && fe->isTypeKnown() && fe->getKnownType() != JSVAL_TYPE_OBJECT))
58700:     {
57787:         if (ool)
55503:             masm->loadValueAsComponents(thisv, JSReturnReg_Type, JSReturnReg_Data);
57787:         else
57787:             frame.loadThisForReturn(JSReturnReg_Type, JSReturnReg_Data, Registers::ReturnReg);
55503:         return;
55503:     }
55503: 
55503:     // If the type is known to be an object, just load the return value as normal.
58700:     if (fe && fe->isTypeKnown() && fe->getKnownType() == JSVAL_TYPE_OBJECT) {
55503:         loadReturnValue(masm, fe);
55503:         return;
55503:     }
55503: 
55503:     // There's a return value, and its type is unknown. Test the type and load
77431:     // |thisv| if necessary. Sync the 'this' entry before doing so, as it may
77431:     // be stored in registers if we constructed it inline.
77431:     frame.syncThis();
55503:     loadReturnValue(masm, fe);
55503:     Jump j = masm->testObject(Assembler::Equal, JSReturnReg_Type);
55503:     masm->loadValueAsComponents(thisv, JSReturnReg_Type, JSReturnReg_Data);
55503:     j.linkTo(masm->label(), masm);
55503: }
55503: 
55503: // Loads the return value into the scripted ABI register pair, such that JS
55503: // semantics in constructors are preserved.
55503: //
55503: void
55503: mjit::Compiler::emitReturnValue(Assembler *masm, FrameEntry *fe)
55503: {
55503:     if (isConstructing)
55503:         fixPrimitiveReturn(masm, fe);
55503:     else
55503:         loadReturnValue(masm, fe);
55503: }
54832: 
54832: void
76194: mjit::Compiler::emitInlineReturnValue(FrameEntry *fe)
76194: {
76194:     JS_ASSERT(!isConstructing && a->needReturnValue);
76194: 
76194:     if (a->syncReturnValue) {
76194:         /* Needed return value with unknown type, the caller's entry is synced. */
76194:         Address address = frame.addressForInlineReturn();
76194:         if (fe)
76194:             frame.storeTo(fe, address);
76194:         else
76194:             masm.storeValue(UndefinedValue(), address);
76194:         return;
76194:     }
76194: 
76194:     /*
76194:      * For inlined functions that simply return an entry present in the outer
76194:      * script (e.g. a loop invariant term), mark the copy and propagate it
76194:      * after popping the frame.
76194:      */
76194:     if (!a->exitState && fe && fe->isCopy() && frame.isOuterSlot(fe->backing())) {
76194:         a->returnEntry = fe->backing();
76194:         return;
76194:     }
76194: 
76194:     if (a->returnValueDouble) {
76194:         JS_ASSERT(fe);
76194:         frame.ensureDouble(fe);
76194:         Registers mask(a->returnSet
76194:                        ? Registers::maskReg(a->returnRegister)
76194:                        : Registers::AvailFPRegs);
76194:         FPRegisterID fpreg;
76194:         if (!fe->isConstant()) {
76194:             fpreg = frame.tempRegInMaskForData(fe, mask.freeMask).fpreg();
76194:             frame.syncAndForgetFe(fe, true);
76194:             frame.takeReg(fpreg);
76194:         } else {
76194:             fpreg = frame.allocReg(mask.freeMask).fpreg();
76194:             masm.slowLoadConstantDouble(fe->getValue().toDouble(), fpreg);
76194:         }
76194:         JS_ASSERT_IF(a->returnSet, fpreg == a->returnRegister.fpreg());
76194:         a->returnRegister = fpreg;
76194:     } else {
76194:         Registers mask(a->returnSet
76194:                        ? Registers::maskReg(a->returnRegister)
76194:                        : Registers::AvailRegs);
76194:         RegisterID reg;
76194:         if (fe && !fe->isConstant()) {
76194:             reg = frame.tempRegInMaskForData(fe, mask.freeMask).reg();
76194:             frame.syncAndForgetFe(fe, true);
76194:             frame.takeReg(reg);
76194:         } else {
76194:             reg = frame.allocReg(mask.freeMask).reg();
76194:             Value val = fe ? fe->getValue() : UndefinedValue();
76194:             masm.loadValuePayload(val, reg);
76194:         }
76194:         JS_ASSERT_IF(a->returnSet, reg == a->returnRegister.reg());
76194:         a->returnRegister = reg;
76194:     }
76194: 
76194:     a->returnSet = true;
76194:     if (a->exitState)
76194:         a->exitState->setUnassigned(a->returnRegister);
76194: }
76194: 
76194: void
54832: mjit::Compiler::emitReturn(FrameEntry *fe)
54832: {
77391:     JS_ASSERT_IF(!script->hasFunction, JSOp(*PC) == JSOP_STOP);
52847: 
54832:     /* Only the top of the stack can be returned. */
54832:     JS_ASSERT_IF(fe, fe == frame.peek(-1));
54832: 
57766:     if (debugMode() || Probes::callTrackingActive(cx)) {
56201:         prepareStubCall(Uses(0));
76194:         INLINE_STUBCALL(stubs::ScriptDebugEpilogue, REJOIN_RESUME);
76194:     }
76194: 
76194:     if (a != outer) {
76194:         /*
76194:          * Returning from an inlined script. The checks we do for inlineability
76194:          * and recompilation triggered by args object construction ensure that
76194:          * there can't be an arguments or call object.
76194:          */
76194: 
76194:         if (a->needReturnValue)
76194:             emitInlineReturnValue(fe);
76194: 
76194:         if (a->exitState) {
76194:             /*
76194:              * Restore the register state to reflect that at the original call,
76194:              * modulo entries which will be popped once the call finishes and any
76194:              * entry which will be clobbered by the return value register.
76194:              */
76194:             frame.syncForAllocation(a->exitState, true, Uses(0));
76194:         }
76194: 
76194:         /*
76194:          * Simple tests to see if we are at the end of the script and will
76194:          * fallthrough after the script body finishes, thus won't need to jump.
76194:          */
76194:         bool endOfScript =
76194:             (JSOp(*PC) == JSOP_STOP) ||
76194:             (JSOp(*PC) == JSOP_RETURN &&
76194:              (JSOp(*(PC + JSOP_RETURN_LENGTH)) == JSOP_STOP &&
76194:               !analysis->maybeCode(PC + JSOP_RETURN_LENGTH)));
76194:         if (!endOfScript)
76194:             a->returnJumps->append(masm.jump());
76194: 
76194:         if (a->returnSet)
76194:             frame.freeReg(a->returnRegister);
76194:         return;
56201:     }
56201: 
52847:     /*
64365:      * Outside the mjit, activation objects are put by StackSpace::pop*
64365:      * members. For JSOP_RETURN, the interpreter only calls popInlineFrame if
64365:      * fp != entryFrame since the VM protocol is that Invoke/Execute are
64365:      * responsible for pushing/popping the initial frame. The mjit does not
64365:      * perform this branch (by instead using a trampoline at the return address
64365:      * to handle exiting mjit code) and thus always puts activation objects,
64365:      * even on the entry frame. To avoid double-putting, EnterMethodJIT clears
64365:      * out the entry frame's activation objects.
52847:      */
77884:     if (script->hasFunction) {
77884:         types::TypeScriptNesting *nesting = script->nesting();
77884:         if (script->function()->isHeavyweight() || (nesting && nesting->children)) {
54832:             prepareStubCall(Uses(fe ? 1 : 0));
77884:             INLINE_STUBCALL(stubs::FunctionFrameEpilogue, REJOIN_NONE);
52847:         } else {
64364:             /* if (hasCallObj() || hasArgsObj()) */
53840:             Jump putObjs = masm.branchTest32(Assembler::NonZero,
69223:                                              Address(JSFrameReg, StackFrame::offsetOfFlags()),
69223:                                              Imm32(StackFrame::HAS_CALL_OBJ | StackFrame::HAS_ARGS_OBJ));
57787:             stubcc.linkExit(putObjs, Uses(frame.frameSlots()));
52847: 
52847:             stubcc.leave();
77884:             OOL_STUBCALL(stubs::FunctionFrameEpilogue, REJOIN_NONE);
54832: 
55503:             emitReturnValue(&stubcc.masm, fe);
54832:             emitFinalReturn(stubcc.masm);
77884: 
77884:             /*
77884:              * Do frame count balancing inline for inner functions in a nesting
77884:              * with no children of their own.
77884:              */
77884:             if (nesting)
77884:                 masm.sub32(Imm32(1), AbsoluteAddress(&nesting->activeFrames));
77884:         }
54427:     }
54832: 
55503:     emitReturnValue(&masm, fe);
54832:     emitFinalReturn(masm);
64365: 
64365:     /*
64365:      * After we've placed the call object, all tracked state can be
64365:      * thrown away. This will happen anyway because the next live opcode (if
64365:      * any) must have an incoming edge. It's an optimization to throw it away
64365:      * early - the tracker won't be spilled on further exits or join points.
64365:      */
54832:     frame.discardFrame();
52560: }
52560: 
52611: void
53087: mjit::Compiler::prepareStubCall(Uses uses)
52611: {
52613:     JaegerSpew(JSpew_Insns, " ---- STUB CALL, SYNCING FRAME ---- \n");
76194:     frame.syncAndKill(Registers(Registers::TempAnyRegs), uses);
52611:     JaegerSpew(JSpew_Insns, " ---- FRAME SYNCING DONE ---- \n");
52611: }
52611: 
52611: JSC::MacroAssembler::Call
76194: mjit::Compiler::emitStubCall(void *ptr, DataLabelPtr *pinline)
52611: {
52611:     JaegerSpew(JSpew_Insns, " ---- CALLING STUB ---- \n");
77407: 
77412:     masm.bumpStubCounter(script, PC, Registers::tempCallReg());
77407: 
76194:     Call cl = masm.fallibleVMCall(cx->typeInferenceEnabled(),
78413:                                   ptr, outerPC(), pinline, frame.totalDepth());
52613:     JaegerSpew(JSpew_Insns, " ---- END STUB CALL ---- \n");
52611:     return cl;
52611: }
52611: 
52645: void
53223: mjit::Compiler::interruptCheckHelper()
53223: {
56559:     /*
56559:      * Bake in and test the address of the interrupt counter for the runtime.
56559:      * This is faster than doing two additional loads for the context's
56559:      * thread data, but will cause this thread to run slower if there are
56559:      * pending interrupts on some other thread.  For non-JS_THREADSAFE builds
56559:      * we can skip this, as there is only one flag to poll.
56559:      */
53223: #ifdef JS_THREADSAFE
56559:     void *interrupt = (void*) &cx->runtime->interruptCounter;
53223: #else
56559:     void *interrupt = (void*) &JS_THREAD_DATA(cx)->interruptFlags;
53223: #endif
56559: 
56559: #if defined(JS_CPU_X86) || defined(JS_CPU_ARM)
56559:     Jump jump = masm.branch32(Assembler::NotEqual, AbsoluteAddress(interrupt), Imm32(0));
56559: #else
56559:     /* Handle processors that can't load from absolute addresses. */
76194:     RegisterID reg = frame.allocReg();
56559:     masm.move(ImmPtr(interrupt), reg);
56559:     Jump jump = masm.branchTest32(Assembler::NonZero, Address(reg, 0));
76194:     frame.freeReg(reg);
56559: #endif
56559: 
56559:     stubcc.linkExitDirect(jump, stubcc.masm.label());
56559: 
56559:     frame.sync(stubcc.masm, Uses(0));
53223:     stubcc.masm.move(ImmPtr(PC), Registers::ArgReg1);
76194:     OOL_STUBCALL(stubs::Interrupt, REJOIN_RESUME);
53223:     stubcc.rejoin(Changes(0));
76194: }
76194: 
76194: void
76194: mjit::Compiler::recompileCheckHelper()
76194: {
76194:     if (inlining() || debugMode() || !globalObj ||
76194:         !analysis->hasFunctionCalls() || !cx->typeInferenceEnabled()) {
76194:         return;
76194:     }
76194: 
76194:     size_t *addr = script->addressOfUseCount();
76194:     masm.add32(Imm32(1), AbsoluteAddress(addr));
76194: #if defined(JS_CPU_X86) || defined(JS_CPU_ARM)
76194:     Jump jump = masm.branch32(Assembler::GreaterThanOrEqual, AbsoluteAddress(addr),
76194:                               Imm32(USES_BEFORE_INLINING));
76194: #else
76194:     /* Handle processors that can't load from absolute addresses. */
76194:     RegisterID reg = frame.allocReg();
76194:     masm.move(ImmPtr(addr), reg);
76194:     Jump jump = masm.branch32(Assembler::GreaterThanOrEqual, Address(reg, 0),
76194:                               Imm32(USES_BEFORE_INLINING));
76194:     frame.freeReg(reg);
56559: #endif
76194:     stubcc.linkExit(jump, Uses(0));
76194:     stubcc.leave();
76194: 
76194:     OOL_STUBCALL(stubs::RecompileForInline, REJOIN_RESUME);
76194:     stubcc.rejoin(Changes(0));
53223: }
53223: 
53223: void
76194: mjit::Compiler::addReturnSite()
76194: {
76194:     InternalCallSite site(masm.distanceOf(masm.label()), a->inlineIndex, PC,
76194:                           REJOIN_SCRIPTED, false);
57766:     addCallSite(site);
76194:     masm.loadPtr(Address(JSFrameReg, StackFrame::offsetOfPrev()), JSFrameReg);
57766: }
57766: 
68952: void
68952: mjit::Compiler::emitUncachedCall(uint32 argc, bool callingNew)
68952: {
68952:     CallPatchInfo callPatch;
68952: 
68952:     RegisterID r0 = Registers::ReturnReg;
68952:     VoidPtrStubUInt32 stub = callingNew ? stubs::UncachedNew : stubs::UncachedCall;
68952: 
76194:     frame.syncAndKill(Uses(argc + 2));
68952:     prepareStubCall(Uses(argc + 2));
68952:     masm.move(Imm32(argc), Registers::ArgReg1);
76194:     INLINE_STUBCALL(stub, REJOIN_CALL_PROLOGUE);
68952: 
68952:     Jump notCompiled = masm.branchTestPtr(Assembler::Zero, r0, r0);
68952: 
76194:     masm.loadPtr(FrameAddress(offsetof(VMFrame, regs.sp)), JSFrameReg);
68952:     callPatch.hasFastNcode = true;
68952:     callPatch.fastNcodePatch =
68952:         masm.storePtrWithPatch(ImmPtr(NULL),
69223:                                Address(JSFrameReg, StackFrame::offsetOfNcode()));
68952: 
68952:     masm.jump(r0);
68952:     callPatch.joinPoint = masm.label();
76194:     addReturnSite();
68952: 
68952:     frame.popn(argc + 2);
76194: 
68952:     frame.takeReg(JSReturnReg_Type);
68952:     frame.takeReg(JSReturnReg_Data);
76194:     frame.pushRegs(JSReturnReg_Type, JSReturnReg_Data, knownPushedType(0));
76194: 
76194:     BarrierState barrier = testBarrier(JSReturnReg_Type, JSReturnReg_Data,
76194:                                        /* testUndefined = */ false,
76194:                                        /* testReturn = */ true);
68952: 
68952:     stubcc.linkExitDirect(notCompiled, stubcc.masm.label());
76194:     stubcc.rejoin(Changes(1));
68952:     callPatches.append(callPatch);
76194: 
76194:     finishBarrier(barrier, REJOIN_FALLTHROUGH, 0);
68952: }
68952: 
57713: static bool
57717: IsLowerableFunCallOrApply(jsbytecode *pc)
57713: {
57713: #ifdef JS_MONOIC
57717:     return (*pc == JSOP_FUNCALL && GET_ARGC(pc) >= 1) ||
57717:            (*pc == JSOP_FUNAPPLY && GET_ARGC(pc) == 2);
57713: #else
57713:     return false;
57713: #endif
57713: }
57713: 
68952: void
68952: mjit::Compiler::checkCallApplySpeculation(uint32 callImmArgc, uint32 speculatedArgc,
68952:                                           FrameEntry *origCallee, FrameEntry *origThis,
68952:                                           MaybeRegisterID origCalleeType, RegisterID origCalleeData,
68952:                                           MaybeRegisterID origThisType, RegisterID origThisData,
68952:                                           Jump *uncachedCallSlowRejoin, CallPatchInfo *uncachedCallPatch)
68952: {
68952:     JS_ASSERT(IsLowerableFunCallOrApply(PC));
68952: 
68952:     /*
68952:      * if (origCallee.isObject() &&
68952:      *     origCallee.toObject().isFunction &&
68952:      *     origCallee.toObject().getFunctionPrivate() == js_fun_{call,apply})
68952:      */
68952:     MaybeJump isObj;
68952:     if (origCalleeType.isSet())
68952:         isObj = masm.testObject(Assembler::NotEqual, origCalleeType.reg());
68952:     Jump isFun = masm.testFunction(Assembler::NotEqual, origCalleeData);
68952:     masm.loadObjPrivate(origCalleeData, origCalleeData);
68952:     Native native = *PC == JSOP_FUNCALL ? js_fun_call : js_fun_apply;
68952:     Jump isNative = masm.branchPtr(Assembler::NotEqual,
68952:                                    Address(origCalleeData, JSFunction::offsetOfNativeOrScript()),
68952:                                    ImmPtr(JS_FUNC_TO_DATA_PTR(void *, native)));
68952: 
68952:     /*
68952:      * If speculation fails, we can't use the ic, since it is compiled on the
68952:      * assumption that speculation succeeds. Instead, just do an uncached call.
68952:      */
68952:     {
68952:         if (isObj.isSet())
68952:             stubcc.linkExitDirect(isObj.getJump(), stubcc.masm.label());
68952:         stubcc.linkExitDirect(isFun, stubcc.masm.label());
68952:         stubcc.linkExitDirect(isNative, stubcc.masm.label());
68952: 
68952:         int32 frameDepthAdjust;
68952:         if (applyTricks == LazyArgsObj) {
77356:             OOL_STUBCALL(stubs::Arguments, REJOIN_RESUME);
68952:             frameDepthAdjust = +1;
68952:         } else {
68952:             frameDepthAdjust = 0;
68952:         }
68952: 
68952:         stubcc.masm.move(Imm32(callImmArgc), Registers::ArgReg1);
68952:         JaegerSpew(JSpew_Insns, " ---- BEGIN SLOW CALL CODE ---- \n");
76194:         OOL_STUBCALL_LOCAL_SLOTS(JS_FUNC_TO_DATA_PTR(void *, stubs::SlowCall),
76194:                                  REJOIN_FALLTHROUGH, frame.totalDepth() + frameDepthAdjust);
68952:         JaegerSpew(JSpew_Insns, " ---- END SLOW CALL CODE ---- \n");
68952: 
68952:         /*
68952:          * inlineCallHelper will link uncachedCallSlowRejoin to the join point
68952:          * at the end of the ic. At that join point, the return value of the
68952:          * call is assumed to be in registers, so load them before jumping.
68952:          */
68952:         JaegerSpew(JSpew_Insns, " ---- BEGIN SLOW RESTORE CODE ---- \n");
68952:         Address rval = frame.addressOf(origCallee);  /* vp[0] == rval */
76194:         if (knownPushedType(0) == JSVAL_TYPE_DOUBLE)
76194:             stubcc.masm.ensureInMemoryDouble(rval);
68952:         stubcc.masm.loadValueAsComponents(rval, JSReturnReg_Type, JSReturnReg_Data);
68952:         *uncachedCallSlowRejoin = stubcc.masm.jump();
68952:         JaegerSpew(JSpew_Insns, " ---- END SLOW RESTORE CODE ---- \n");
68952:     }
68952: 
68952:     /*
68952:      * For simplicity, we don't statically specialize calls to
68952:      * ic::SplatApplyArgs based on applyTricks. Rather, this state is
68952:      * communicated dynamically through the VMFrame.
68952:      */
68952:     if (*PC == JSOP_FUNAPPLY) {
68952:         masm.store32(Imm32(applyTricks == LazyArgsObj),
68952:                      FrameAddress(offsetof(VMFrame, u.call.lazyArgsObj)));
68952:     }
68952: }
68952: 
57718: /* This predicate must be called before the current op mutates the FrameState. */
57718: bool
57718: mjit::Compiler::canUseApplyTricks()
57718: {
57718:     JS_ASSERT(*PC == JSOP_ARGUMENTS);
57718:     jsbytecode *nextpc = PC + JSOP_ARGUMENTS_LENGTH;
57718:     return *nextpc == JSOP_FUNAPPLY &&
57718:            IsLowerableFunCallOrApply(nextpc) &&
58124:            !analysis->jumpTarget(nextpc) &&
76194:            !debugMode() && !a->parent;
57713: }
57713: 
68952: /* See MonoIC.cpp, CallCompiler for more information on call ICs. */
76194: bool
76194: mjit::Compiler::inlineCallHelper(uint32 callImmArgc, bool callingNew, FrameSize &callFrameSize)
73894: {
68952:     int32 speculatedArgc;
57718:     if (applyTricks == LazyArgsObj) {
57718:         frame.pop();
68952:         speculatedArgc = 1;
57718:     } else {
77355:         /*
77355:          * Check for interrupts on function call. We don't do this for lazy
77355:          * arguments objects as the interrupt may kick this frame into the
77355:          * interpreter, which doesn't know about the apply tricks. Instead, we
77355:          * do the interrupt check at the start of the JSOP_ARGUMENTS.
77355:          */
77355:         interruptCheckHelper();
77355: 
68952:         speculatedArgc = callImmArgc;
68952:     }
68952: 
68952:     FrameEntry *origCallee = frame.peek(-(speculatedArgc + 2));
68952:     FrameEntry *origThis = frame.peek(-(speculatedArgc + 1));
68952: 
76194:     /*
76194:      * 'this' does not need to be synced for constructing. :FIXME: is it
76194:      * possible that one of the arguments is directly copying the 'this'
76194:      * entry (something like 'new x.f(x)')?
76194:      */
77361:     if (callingNew) {
57713:         frame.discardFe(origThis);
57713: 
68952:         /*
77361:          * If inference is enabled, the 'this' value of the pushed frame always
77361:          * needs to be coherent. If a GC gets triggered before the callee can
77361:          * fill in the slot (i.e. the GC happens on constructing the 'new'
77361:          * object or the call object for a heavyweight callee), it needs to be
77361:          * able to read the 'this' value to tell whether newScript constraints
77361:          * will need to be regenerated afterwards.
77361:          */
77361:         if (cx->typeInferenceEnabled())
77361:             masm.storeValue(NullValue(), frame.addressOf(origThis));
77361:     }
77361: 
76194:     if (!cx->typeInferenceEnabled()) {
76194:         CompileStatus status = callArrayBuiltin(callImmArgc, callingNew);
76194:         if (status != Compile_InlineAbort)
76194:             return status;
76194:     }
76194: 
73894:     /*
68952:      * From the presence of JSOP_FUN{CALL,APPLY}, we speculate that we are
68952:      * going to call js_fun_{call,apply}. Normally, this call would go through
68952:      * js::Invoke to ultimately call 'this'. We can do much better by having
68952:      * the callIC cache and call 'this' directly. However, if it turns out that
68952:      * we are not actually calling js_fun_call, the callIC must act as normal.
76194:      *
76194:      * Note: do *NOT* use type information or inline state in any way when
76194:      * deciding whether to lower a CALL or APPLY. The stub calls here store
76194:      * their return values in a different slot, so when recompiling we need
76194:      * to go down the exact same path.
68952:      */
68952:     bool lowerFunCallOrApply = IsLowerableFunCallOrApply(PC);
68952: 
76194:     bool newType = callingNew && cx->typeInferenceEnabled() && types::UseNewType(cx, script, PC);
76194: 
68952: #ifdef JS_MONOIC
76194:     if (debugMode() || newType) {
68952: #endif
68952:         if (applyTricks == LazyArgsObj) {
68952:             /* frame.pop() above reset us to pre-JSOP_ARGUMENTS state */
76194:             jsop_arguments(REJOIN_RESUME);
76194:             frame.pushSynced(JSVAL_TYPE_UNKNOWN);
68952:         }
68952:         emitUncachedCall(callImmArgc, callingNew);
76194:         applyTricks = NoApplyTricks;
76194:         return true;
68952: #ifdef JS_MONOIC
68952:     }
68952: 
76194:     frame.forgetMismatchedObject(origCallee);
76194:     if (lowerFunCallOrApply)
76194:         frame.forgetMismatchedObject(origThis);
76194: 
68952:     /* Initialized by both branches below. */
76194:     CallGenInfo     callIC;
68952:     CallPatchInfo   callPatch;
68952:     MaybeRegisterID icCalleeType; /* type to test for function-ness */
68952:     RegisterID      icCalleeData; /* data to call */
68952:     Address         icRvalAddr;   /* return slot on slow-path rejoin */
68952: 
68952:     /*
68952:      * IC space must be reserved (using RESERVE_IC_SPACE or RESERVE_OOL_SPACE) between the
68952:      * following labels (as used in finishThisUp):
68952:      *  - funGuard -> hotJump
68952:      *  - funGuard -> joinPoint
68952:      *  - funGuard -> hotPathLabel
68952:      *  - slowPathStart -> oolCall
68952:      *  - slowPathStart -> oolJump
68952:      *  - slowPathStart -> icCall
68952:      *  - slowPathStart -> slowJoinPoint
68952:      * Because the call ICs are fairly long (compared to PICs), we don't reserve the space in each
68952:      * path until the first usage of funGuard (for the in-line path) or slowPathStart (for the
68952:      * out-of-line path).
68952:      */
68952: 
68952:     /* Initialized only on lowerFunCallOrApply branch. */
68952:     Jump            uncachedCallSlowRejoin;
68952:     CallPatchInfo   uncachedCallPatch;
68952: 
57713:     {
68952:         MaybeRegisterID origCalleeType, maybeOrigCalleeData;
68952:         RegisterID origCalleeData;
68952: 
68952:         /* Get the callee in registers. */
68952:         frame.ensureFullRegs(origCallee, &origCalleeType, &maybeOrigCalleeData);
68952:         origCalleeData = maybeOrigCalleeData.reg();
57713:         PinRegAcrossSyncAndKill p1(frame, origCalleeData), p2(frame, origCalleeType);
57713: 
68952:         if (lowerFunCallOrApply) {
68952:             MaybeRegisterID origThisType, maybeOrigThisData;
68952:             RegisterID origThisData;
57713:             {
68952:                 /* Get thisv in registers. */
68952:                 frame.ensureFullRegs(origThis, &origThisType, &maybeOrigThisData);
68952:                 origThisData = maybeOrigThisData.reg();
57713:                 PinRegAcrossSyncAndKill p3(frame, origThisData), p4(frame, origThisType);
57713: 
68952:                 /* Leaves pinned regs untouched. */
76194:                 frame.syncAndKill(Uses(speculatedArgc + 2));
68952:             }
68952: 
68952:             checkCallApplySpeculation(callImmArgc, speculatedArgc,
68952:                                       origCallee, origThis,
68952:                                       origCalleeType, origCalleeData,
68952:                                       origThisType, origThisData,
68952:                                       &uncachedCallSlowRejoin, &uncachedCallPatch);
68952: 
68952:             icCalleeType = origThisType;
68952:             icCalleeData = origThisData;
68952:             icRvalAddr = frame.addressOf(origThis);
68952: 
68952:             /*
68952:              * For f.call(), since we compile the ic under the (checked)
68952:              * assumption that call == js_fun_call, we still have a static
68952:              * frame size. For f.apply(), the frame size depends on the dynamic
68952:              * length of the array passed to apply.
68952:              */
68952:             if (*PC == JSOP_FUNCALL)
76194:                 callIC.frameSize.initStatic(frame.totalDepth(), speculatedArgc - 1);
68952:             else
68952:                 callIC.frameSize.initDynamic();
68952:         } else {
68952:             /* Leaves pinned regs untouched. */
76194:             frame.syncAndKill(Uses(speculatedArgc + 2));
68952: 
68952:             icCalleeType = origCalleeType;
68952:             icCalleeData = origCalleeData;
68952:             icRvalAddr = frame.addressOf(origCallee);
76194:             callIC.frameSize.initStatic(frame.totalDepth(), speculatedArgc);
76194:         }
76194:     }
76194: 
76194:     callFrameSize = callIC.frameSize;
76194: 
76194:     callIC.typeMonitored = monitored(PC) || hasTypeBarriers(PC);
68952: 
68952:     /* Test the type if necessary. Failing this always takes a really slow path. */
68952:     MaybeJump notObjectJump;
68952:     if (icCalleeType.isSet())
68952:         notObjectJump = masm.testObject(Assembler::NotEqual, icCalleeType.reg());
68952: 
68952:     /*
68952:      * For an optimized apply, keep icCalleeData and funPtrReg in a
68952:      * callee-saved registers for the subsequent ic::SplatApplyArgs call.
68952:      */
76194:     Registers tempRegs(Registers::AvailRegs);
68952:     if (callIC.frameSize.isDynamic() && !Registers::isSaved(icCalleeData)) {
76194:         RegisterID x = tempRegs.takeAnyReg(Registers::SavedRegs).reg();
68952:         masm.move(icCalleeData, x);
68952:         icCalleeData = x;
68952:     } else {
68952:         tempRegs.takeReg(icCalleeData);
68952:     }
76194:     RegisterID funPtrReg = tempRegs.takeAnyReg(Registers::SavedRegs).reg();
68952: 
68952:     /* Reserve space just before initialization of funGuard. */
68952:     RESERVE_IC_SPACE(masm);
68952: 
68952:     /*
68952:      * Guard on the callee identity. This misses on the first run. If the
68952:      * callee is scripted, compiled/compilable, and argc == nargs, then this
68952:      * guard is patched, and the compiled code address is baked in.
68952:      */
68952:     Jump j = masm.branchPtrWithPatch(Assembler::NotEqual, icCalleeData, callIC.funGuard);
68952:     callIC.funJump = j;
68952: 
68952:     /* Reserve space just before initialization of slowPathStart. */
68952:     RESERVE_OOL_SPACE(stubcc.masm);
68952: 
68952:     Jump rejoin1, rejoin2;
68945:     {
68945:         RESERVE_OOL_SPACE(stubcc.masm);
68952:         stubcc.linkExitDirect(j, stubcc.masm.label());
68952:         callIC.slowPathStart = stubcc.masm.label();
68952: 
68952:         /*
68952:          * Test if the callee is even a function. If this doesn't match, we
68952:          * take a _really_ slow path later.
68952:          */
68952:         Jump notFunction = stubcc.masm.testFunction(Assembler::NotEqual, icCalleeData);
68952: 
68952:         /* Test if the function is scripted. */
76194:         RegisterID tmp = tempRegs.takeAnyReg().reg();
68952:         stubcc.masm.loadObjPrivate(icCalleeData, funPtrReg);
68952:         stubcc.masm.load16(Address(funPtrReg, offsetof(JSFunction, flags)), tmp);
68952:         stubcc.masm.and32(Imm32(JSFUN_KINDMASK), tmp);
68952:         Jump isNative = stubcc.masm.branch32(Assembler::Below, tmp, Imm32(JSFUN_INTERPRETED));
68952:         tempRegs.putReg(tmp);
68952: 
68952:         /*
68952:          * N.B. After this call, the frame will have a dynamic frame size.
68952:          * Check after the function is known not to be a native so that the
68952:          * catch-all/native path has a static depth.
68952:          */
68952:         if (callIC.frameSize.isDynamic())
76194:             OOL_STUBCALL(ic::SplatApplyArgs, REJOIN_CALL_SPLAT);
68952: 
68952:         /*
68952:          * No-op jump that gets patched by ic::New/Call to the stub generated
68952:          * by generateFullCallStub.
68952:          */
68952:         Jump toPatch = stubcc.masm.jump();
68952:         toPatch.linkTo(stubcc.masm.label(), &stubcc.masm);
68952:         callIC.oolJump = toPatch;
68952:         callIC.icCall = stubcc.masm.label();
68952: 
76194:         RejoinState rejoinState = callIC.frameSize.rejoinState(PC, false);
76194: 
68952:         /*
68952:          * At this point the function is definitely scripted, so we try to
68952:          * compile it and patch either funGuard/funJump or oolJump. This code
68952:          * is only executed once.
68952:          */
68952:         callIC.addrLabel1 = stubcc.masm.moveWithPatch(ImmPtr(NULL), Registers::ArgReg1);
68952:         void *icFunPtr = JS_FUNC_TO_DATA_PTR(void *, callingNew ? ic::New : ic::Call);
76194:         if (callIC.frameSize.isStatic()) {
76194:             callIC.oolCall = OOL_STUBCALL_LOCAL_SLOTS(icFunPtr, rejoinState, frame.totalDepth());
76194:         } else {
76194:             callIC.oolCall = OOL_STUBCALL_LOCAL_SLOTS(icFunPtr, rejoinState, -1);
76194:         }
68952: 
68952:         callIC.funObjReg = icCalleeData;
68952:         callIC.funPtrReg = funPtrReg;
68952: 
68952:         /*
68952:          * The IC call either returns NULL, meaning call completed, or a
76194:          * function pointer to jump to.
68952:          */
68952:         rejoin1 = stubcc.masm.branchTestPtr(Assembler::Zero, Registers::ReturnReg,
68952:                                             Registers::ReturnReg);
68952:         if (callIC.frameSize.isStatic())
68952:             stubcc.masm.move(Imm32(callIC.frameSize.staticArgc()), JSParamReg_Argc);
68952:         else
68952:             stubcc.masm.load32(FrameAddress(offsetof(VMFrame, u.call.dynamicArgc)), JSParamReg_Argc);
76194:         stubcc.masm.loadPtr(FrameAddress(offsetof(VMFrame, regs.sp)), JSFrameReg);
68952:         callPatch.hasSlowNcode = true;
68952:         callPatch.slowNcodePatch =
68952:             stubcc.masm.storePtrWithPatch(ImmPtr(NULL),
69223:                                           Address(JSFrameReg, StackFrame::offsetOfNcode()));
68952:         stubcc.masm.jump(Registers::ReturnReg);
68952: 
76194: 
76194: 
68952:         /*
68952:          * This ool path is the catch-all for everything but scripted function
68952:          * callees. For native functions, ic::NativeNew/NativeCall will repatch
68952:          * funGaurd/funJump with a fast call stub. All other cases
68952:          * (non-function callable objects and invalid callees) take the slow
68952:          * path through js::Invoke.
68952:          */
68952:         if (notObjectJump.isSet())
68952:             stubcc.linkExitDirect(notObjectJump.get(), stubcc.masm.label());
68952:         notFunction.linkTo(stubcc.masm.label(), &stubcc.masm);
68952:         isNative.linkTo(stubcc.masm.label(), &stubcc.masm);
68952: 
68952:         callIC.addrLabel2 = stubcc.masm.moveWithPatch(ImmPtr(NULL), Registers::ArgReg1);
76194:         OOL_STUBCALL(callingNew ? ic::NativeNew : ic::NativeCall, rejoinState);
68952: 
68952:         rejoin2 = stubcc.masm.jump();
68952:     }
68952: 
68952:     /*
68952:      * If the call site goes to a closure over the same function, it will
68952:      * generate an out-of-line stub that joins back here.
68952:      */
68952:     callIC.hotPathLabel = masm.label();
68952: 
68952:     uint32 flags = 0;
68952:     if (callingNew)
69223:         flags |= StackFrame::CONSTRUCTING;
68952: 
68952:     InlineFrameAssembler inlFrame(masm, callIC, flags);
68952:     callPatch.hasFastNcode = true;
77341:     callPatch.fastNcodePatch = inlFrame.assemble(NULL, PC);
68952: 
68952:     callIC.hotJump = masm.jump();
68952:     callIC.joinPoint = callPatch.joinPoint = masm.label();
76194:     callIC.callIndex = callSites.length();
76194:     addReturnSite();
68952:     if (lowerFunCallOrApply)
68952:         uncachedCallPatch.joinPoint = callIC.joinPoint;
53590: 
68952:     /*
68952:      * We've placed hotJump, joinPoint and hotPathLabel, and no other labels are located by offset
68952:      * in the in-line path so we can check the IC space now.
68952:      */
68952:     CHECK_IC_SPACE();
68952: 
76194:     JSValueType type = knownPushedType(0);
76194: 
68952:     frame.popn(speculatedArgc + 2);
53590:     frame.takeReg(JSReturnReg_Type);
53590:     frame.takeReg(JSReturnReg_Data);
76194:     frame.pushRegs(JSReturnReg_Type, JSReturnReg_Data, type);
76194: 
76194:     BarrierState barrier = testBarrier(JSReturnReg_Type, JSReturnReg_Data,
76194:                                        /* testUndefined = */ false,
76194:                                        /* testReturn = */ true);
53590: 
68952:     /*
68952:      * Now that the frame state is set, generate the rejoin path. Note that, if
68952:      * lowerFunCallOrApply, we cannot just call 'stubcc.rejoin' since the return
68952:      * value has been placed at vp[1] which is not the stack address associated
68952:      * with frame.peek(-1).
68952:      */
68952:     callIC.slowJoinPoint = stubcc.masm.label();
68952:     rejoin1.linkTo(callIC.slowJoinPoint, &stubcc.masm);
68952:     rejoin2.linkTo(callIC.slowJoinPoint, &stubcc.masm);
68952:     JaegerSpew(JSpew_Insns, " ---- BEGIN SLOW RESTORE CODE ---- \n");
76194:     frame.reloadEntry(stubcc.masm, icRvalAddr, frame.peek(-1));
68952:     stubcc.crossJump(stubcc.masm.jump(), masm.label());
68952:     JaegerSpew(JSpew_Insns, " ---- END SLOW RESTORE CODE ---- \n");
68952: 
62361:     CHECK_OOL_SPACE();
68952: 
68952:     if (lowerFunCallOrApply)
68952:         stubcc.crossJump(uncachedCallSlowRejoin, masm.label());
68952: 
68952:     callICs.append(callIC);
68952:     callPatches.append(callPatch);
68952:     if (lowerFunCallOrApply)
68952:         callPatches.append(uncachedCallPatch);
57718: 
76194:     finishBarrier(barrier, REJOIN_FALLTHROUGH, 0);
76194: 
57718:     applyTricks = NoApplyTricks;
76194:     return true;
68952: #endif
52645: }
52645: 
76194: CompileStatus
76194: mjit::Compiler::callArrayBuiltin(uint32 argc, bool callingNew)
76194: {
76194:     if (!globalObj)
76194:         return Compile_InlineAbort;
76194: 
76194:     if (applyTricks == LazyArgsObj)
76194:         return Compile_InlineAbort;
76194: 
76194:     FrameEntry *origCallee = frame.peek(-((int)argc + 2));
76194:     if (origCallee->isNotType(JSVAL_TYPE_OBJECT))
76194:         return Compile_InlineAbort;
76194: 
76194:     if (frame.extra(origCallee).name != cx->runtime->atomState.classAtoms[JSProto_Array])
76194:         return Compile_InlineAbort;
76194: 
76194:     JSObject *arrayObj;
76194:     if (!js_GetClassObject(cx, globalObj, JSProto_Array, &arrayObj))
76194:         return Compile_Error;
76194: 
76194:     JSObject *arrayProto;
76194:     if (!js_GetClassPrototype(cx, globalObj, JSProto_Array, &arrayProto))
76194:         return Compile_Error;
76194: 
76194:     if (argc > 1)
76194:         return Compile_InlineAbort;
76194:     FrameEntry *origArg = (argc == 1) ? frame.peek(-1) : NULL;
76194:     if (origArg) {
76194:         if (origArg->isNotType(JSVAL_TYPE_INT32))
76194:             return Compile_InlineAbort;
76194:         if (origArg->isConstant() && origArg->getValue().toInt32() < 0)
76194:             return Compile_InlineAbort;
76194:     }
76194: 
76194:     if (!origCallee->isTypeKnown()) {
76194:         Jump notObject = frame.testObject(Assembler::NotEqual, origCallee);
76194:         stubcc.linkExit(notObject, Uses(argc + 2));
76194:     }
76194: 
76194:     RegisterID reg = frame.tempRegForData(origCallee);
76194:     Jump notArray = masm.branchPtr(Assembler::NotEqual, reg, ImmPtr(arrayObj));
76194:     stubcc.linkExit(notArray, Uses(argc + 2));
76194: 
76194:     int32 knownSize = 0;
76194:     MaybeRegisterID sizeReg;
76194:     if (origArg) {
76194:         if (origArg->isConstant()) {
76194:             knownSize = origArg->getValue().toInt32();
76194:         } else {
76194:             if (!origArg->isTypeKnown()) {
76194:                 Jump notInt = frame.testInt32(Assembler::NotEqual, origArg);
76194:                 stubcc.linkExit(notInt, Uses(argc + 2));
76194:             }
76194:             sizeReg = frame.tempRegForData(origArg);
76194:             Jump belowZero = masm.branch32(Assembler::LessThan, sizeReg.reg(), Imm32(0));
76194:             stubcc.linkExit(belowZero, Uses(argc + 2));
76194:         }
76194:     } else {
76194:         knownSize = 0;
76194:     }
76194: 
76194:     stubcc.leave();
76194:     stubcc.masm.move(Imm32(argc), Registers::ArgReg1);
76194:     OOL_STUBCALL(callingNew ? stubs::SlowNew : stubs::SlowCall, REJOIN_FALLTHROUGH);
76194: 
76194:     {
76194:         PinRegAcrossSyncAndKill p1(frame, sizeReg);
76194:         frame.popn(argc + 2);
76194:         frame.syncAndKill(Uses(0));
76194:     }
76194: 
76194:     prepareStubCall(Uses(0));
76194:     masm.storePtr(ImmPtr(arrayProto), FrameAddress(offsetof(VMFrame, scratch)));
76194:     if (sizeReg.isSet())
76194:         masm.move(sizeReg.reg(), Registers::ArgReg1);
76194:     else
76194:         masm.move(Imm32(knownSize), Registers::ArgReg1);
76194:     INLINE_STUBCALL(stubs::NewDenseUnallocatedArray, REJOIN_PUSH_OBJECT);
76194: 
76194:     frame.takeReg(Registers::ReturnReg);
76194:     frame.pushTypedPayload(JSVAL_TYPE_OBJECT, Registers::ReturnReg);
76194:     frame.forgetType(frame.peek(-1));
76194: 
76194:     stubcc.rejoin(Changes(1));
76194: 
76194:     return Compile_Okay;
76194: }
76194: 
76194: /* Maximum number of calls we will inline at the same site. */
76194: static const uint32 INLINE_SITE_LIMIT = 5;
76194: 
76194: CompileStatus
76194: mjit::Compiler::inlineScriptedFunction(uint32 argc, bool callingNew)
76194: {
76194:     JS_ASSERT(inlining());
76194: 
76194:     /* We already know which frames we are inlining at each PC, so scan the list of inline frames. */
76194:     bool calleeMultipleReturns = false;
76194:     Vector<JSScript *> inlineCallees(CompilerAllocPolicy(cx, *this));
76194:     for (unsigned i = 0; i < ssa.numFrames(); i++) {
76194:         if (ssa.iterFrame(i).parent == a->inlineIndex && ssa.iterFrame(i).parentpc == PC) {
76194:             JSScript *script = ssa.iterFrame(i).script;
76194:             inlineCallees.append(script);
77391:             if (script->analysis()->numReturnSites() > 1)
76194:                 calleeMultipleReturns = true;
76194:         }
76194:     }
76194: 
76194:     if (inlineCallees.empty())
76194:         return Compile_InlineAbort;
76194: 
76194:     JS_ASSERT(!monitored(PC));
76194: 
76194:     /*
76194:      * Remove all dead entries from the frame's tracker. We will not recognize
76194:      * them as dead after pushing the new frame.
76194:      */
76194:     frame.pruneDeadEntries();
76194: 
76194:     RegisterAllocation *exitState = NULL;
76194:     if (inlineCallees.length() > 1 || calleeMultipleReturns) {
76194:         /*
76194:          * Multiple paths through the callees, get a register allocation for
76194:          * the various incoming edges.
76194:          */
76194:         exitState = frame.computeAllocation(PC + JSOP_CALL_LENGTH);
76194:     }
76194: 
76194:     /*
76194:      * If this is a polymorphic callsite, get a register for the callee too.
76194:      * After this, do not touch the register state in the current frame until
76194:      * stubs for all callees have been generated.
76194:      */
76194:     FrameEntry *origCallee = frame.peek(-((int)argc + 2));
76194:     FrameEntry *entrySnapshot = NULL;
76194:     MaybeRegisterID calleeReg;
76194:     if (inlineCallees.length() > 1) {
76194:         frame.forgetMismatchedObject(origCallee);
76194:         calleeReg = frame.tempRegForData(origCallee);
76194: 
76194:         entrySnapshot = frame.snapshotState();
76194:         if (!entrySnapshot)
76194:             return Compile_Error;
76194:     }
76194:     MaybeJump calleePrevious;
76194: 
76194:     JSValueType returnType = knownPushedType(0);
76194: 
76194:     bool needReturnValue = JSOP_POP != (JSOp)*(PC + JSOP_CALL_LENGTH);
76194:     bool syncReturnValue = needReturnValue && returnType == JSVAL_TYPE_UNKNOWN;
76194: 
76194:     /* Track register state after the call. */
76194:     bool returnSet = false;
76194:     AnyRegisterID returnRegister;
76194:     const FrameEntry *returnEntry = NULL;
76194: 
76194:     Vector<Jump, 4, CompilerAllocPolicy> returnJumps(CompilerAllocPolicy(cx, *this));
76194: 
76194:     for (unsigned i = 0; i < inlineCallees.length(); i++) {
76194:         if (entrySnapshot)
76194:             frame.restoreFromSnapshot(entrySnapshot);
76194: 
76194:         JSScript *script = inlineCallees[i];
76194:         CompileStatus status;
76194: 
76194:         status = pushActiveFrame(script, argc);
76194:         if (status != Compile_Okay)
76194:             return status;
76194: 
76194:         a->exitState = exitState;
76194: 
76194:         JaegerSpew(JSpew_Inlining, "inlining call to script (file \"%s\") (line \"%d\")\n",
76194:                    script->filename, script->lineno);
76194: 
76194:         if (calleePrevious.isSet()) {
76194:             calleePrevious.get().linkTo(masm.label(), &masm);
76194:             calleePrevious = MaybeJump();
76194:         }
76194: 
76194:         if (i + 1 != inlineCallees.length()) {
76194:             /* Guard on the callee, except when this object must be the callee. */
76194:             JS_ASSERT(calleeReg.isSet());
77391:             calleePrevious = masm.branchPtr(Assembler::NotEqual, calleeReg.reg(), ImmPtr(script->function()));
76194:         }
76194: 
76194:         a->returnJumps = &returnJumps;
76194:         a->needReturnValue = needReturnValue;
76194:         a->syncReturnValue = syncReturnValue;
76194:         a->returnValueDouble = returnType == JSVAL_TYPE_DOUBLE;
76194:         if (returnSet) {
76194:             a->returnSet = true;
76194:             a->returnRegister = returnRegister;
76194:         }
76194: 
76194:         /*
76194:          * Update the argument frame entries in place if the callee has had an
76194:          * argument inferred as double but we are passing an int.
76194:          */
76194:         ensureDoubleArguments();
76194: 
76194:         status = generateMethod();
76194:         if (status != Compile_Okay) {
76194:             popActiveFrame();
76194:             if (status == Compile_Abort) {
76194:                 /* The callee is uncompileable, mark it as uninlineable and retry. */
77361:                 script->uninlineable = true;
77391:                 types::MarkTypeObjectFlags(cx, script->function(),
76194:                                            types::OBJECT_FLAG_UNINLINEABLE);
76194:                 return Compile_Retry;
76194:             }
76194:             return status;
76194:         }
76194: 
76194:         if (needReturnValue && !returnSet) {
76194:             if (a->returnSet) {
76194:                 returnSet = true;
76194:                 returnRegister = a->returnRegister;
76194:             } else {
76194:                 returnEntry = a->returnEntry;
76194:             }
76194:         }
76194: 
76194:         popActiveFrame();
76194: 
76194:         if (i + 1 != inlineCallees.length())
76194:             returnJumps.append(masm.jump());
76194:     }
76194: 
76194:     for (unsigned i = 0; i < returnJumps.length(); i++)
76194:         returnJumps[i].linkTo(masm.label(), &masm);
76194: 
76194:     frame.popn(argc + 2);
76194: 
76194:     if (entrySnapshot)
76194:         cx->array_delete(entrySnapshot);
76194: 
76194:     if (exitState)
76194:         frame.discardForJoin(exitState, analysis->getCode(PC).stackDepth - (argc + 2));
76194: 
76194:     if (returnSet) {
76194:         frame.takeReg(returnRegister);
76194:         if (returnRegister.isReg())
76194:             frame.pushTypedPayload(returnType, returnRegister.reg());
76194:         else
76194:             frame.pushDouble(returnRegister.fpreg());
76194:     } else if (returnEntry) {
76194:         frame.pushCopyOf((FrameEntry *) returnEntry);
76194:     } else {
76194:         frame.pushSynced(JSVAL_TYPE_UNKNOWN);
76194:     }
76194: 
76194:     JaegerSpew(JSpew_Inlining, "finished inlining call to script (file \"%s\") (line \"%d\")\n",
76194:                script->filename, script->lineno);
76194: 
76194:     return Compile_Okay;
76194: }
76194: 
53168: /*
53168:  * This function must be called immediately after any instruction which could
69223:  * cause a new StackFrame to be pushed and could lead to a new debug trap
53168:  * being set. This includes any API callbacks and any scripted or native call.
53168:  */
53168: void
57766: mjit::Compiler::addCallSite(const InternalCallSite &site)
53168: {
53168:     callSites.append(site);
53168: }
53168: 
52645: void
78454: mjit::Compiler::inlineStubCall(void *stub, RejoinState rejoin, Uses uses)
76194: {
76194:     DataLabelPtr inlinePatch;
76194:     Call cl = emitStubCall(stub, &inlinePatch);
76194:     InternalCallSite site(masm.callReturnOffset(cl), a->inlineIndex, PC,
76194:                           rejoin, false);
76194:     site.inlinePatch = inlinePatch;
76194:     if (loop && loop->generatingInvariants()) {
76194:         Jump j = masm.jump();
76194:         Label l = masm.label();
78454:         loop->addInvariantCall(j, l, false, false, callSites.length(), uses);
76194:     }
76194:     addCallSite(site);
52645: }
52645: 
52651: bool
52651: mjit::Compiler::compareTwoValues(JSContext *cx, JSOp op, const Value &lhs, const Value &rhs)
52651: {
52651:     JS_ASSERT(lhs.isPrimitive());
52651:     JS_ASSERT(rhs.isPrimitive());
52651: 
52651:     if (lhs.isString() && rhs.isString()) {
59890:         int32 cmp;
59890:         CompareStrings(cx, lhs.toString(), rhs.toString(), &cmp);
52651:         switch (op) {
52651:           case JSOP_LT:
52651:             return cmp < 0;
52651:           case JSOP_LE:
52651:             return cmp <= 0;
52651:           case JSOP_GT:
52651:             return cmp > 0;
52651:           case JSOP_GE:
52651:             return cmp >= 0;
52679:           case JSOP_EQ:
52679:             return cmp == 0;
52679:           case JSOP_NE:
52679:             return cmp != 0;
52651:           default:
52651:             JS_NOT_REACHED("NYI");
52651:         }
52651:     } else {
52651:         double ld, rd;
52651: 
52651:         /* These should be infallible w/ primitives. */
73894:         JS_ALWAYS_TRUE(ToNumber(cx, lhs, &ld));
73894:         JS_ALWAYS_TRUE(ToNumber(cx, rhs, &rd));
52651:         switch(op) {
52651:           case JSOP_LT:
52651:             return ld < rd;
52651:           case JSOP_LE:
52651:             return ld <= rd;
52651:           case JSOP_GT:
52651:             return ld > rd;
52651:           case JSOP_GE:
52651:             return ld >= rd;
52679:           case JSOP_EQ: /* fall through */
52679:           case JSOP_NE:
52679:             /* Special case null/undefined/void comparisons. */
52679:             if (lhs.isNullOrUndefined()) {
52679:                 if (rhs.isNullOrUndefined())
52679:                     return op == JSOP_EQ;
52679:                 return op == JSOP_NE;
52679:             }
52679:             if (rhs.isNullOrUndefined())
52679:                 return op == JSOP_NE;
52679: 
52679:             /* Normal return. */
52679:             return (op == JSOP_EQ) ? (ld == rd) : (ld != rd);
52651:           default:
52651:             JS_NOT_REACHED("NYI");
52651:         }
52651:     }
52651: 
52651:     JS_NOT_REACHED("NYI");
52651:     return false;
52651: }
52651: 
56766: bool
76194: mjit::Compiler::constantFoldBranch(jsbytecode *target, bool taken)
76194: {
76194:     if (taken) {
76194:         if (!frame.syncForBranch(target, Uses(0)))
76194:             return false;
76194:         Jump j = masm.jump();
76194:         if (!jumpAndTrace(j, target))
76194:             return false;
76194:     } else {
76194:         /*
76194:          * Branch is never taken, but clean up any loop
76194:          * if this is a backedge.
76194:          */
76194:         if (target < PC && !finishLoop(target))
76194:             return false;
76194:     }
76194:     return true;
76194: }
76194: 
76194: bool
52653: mjit::Compiler::emitStubCmpOp(BoolStub stub, jsbytecode *target, JSOp fused)
52653: {
76194:     if (target)
76194:         frame.syncAndKillEverything();
76194:     else
76194:         frame.syncAndKill(Uses(2));
76194: 
53087:     prepareStubCall(Uses(2));
76194:     INLINE_STUBCALL(stub, target ? REJOIN_BRANCH : REJOIN_PUSH_BOOLEAN);
71314:     frame.popn(2);
76194: 
76194:     if (!target) {
71314:         frame.takeReg(Registers::ReturnReg);
53025:         frame.pushTypedPayload(JSVAL_TYPE_BOOLEAN, Registers::ReturnReg);
56766:         return true;
56766:     }
56766: 
52679:     JS_ASSERT(fused == JSOP_IFEQ || fused == JSOP_IFNE);
76194:     Jump j = masm.branchTest32(GetStubCompareCondition(fused), Registers::ReturnReg,
52653:                                Registers::ReturnReg);
56766:     return jumpAndTrace(j, target);
52653: }
52653: 
52692: void
56201: mjit::Compiler::jsop_setprop_slow(JSAtom *atom, bool usePropCache)
52886: {
53087:     prepareStubCall(Uses(2));
52886:     masm.move(ImmPtr(atom), Registers::ArgReg1);
56201:     if (usePropCache)
76194:         INLINE_STUBCALL(STRICT_VARIANT(stubs::SetName), REJOIN_FALLTHROUGH);
56201:     else
76194:         INLINE_STUBCALL(STRICT_VARIANT(stubs::SetPropNoCache), REJOIN_FALLTHROUGH);
52886:     JS_STATIC_ASSERT(JSOP_SETNAME_LENGTH == JSOP_SETPROP_LENGTH);
52886:     frame.shimmy(1);
52886: }
52886: 
52886: void
56201: mjit::Compiler::jsop_getprop_slow(JSAtom *atom, bool usePropCache)
52743: {
76194:     /* See ::jsop_getprop */
76194:     RejoinState rejoin = usePropCache ? REJOIN_GETTER : REJOIN_THIS_PROTOTYPE;
76194: 
53087:     prepareStubCall(Uses(1));
56201:     if (usePropCache) {
76194:         INLINE_STUBCALL(stubs::GetProp, rejoin);
78456:         testPushedType(rejoin, -1, /* ool = */ false);
56201:     } else {
56201:         masm.move(ImmPtr(atom), Registers::ArgReg1);
76194:         INLINE_STUBCALL(stubs::GetPropNoCache, rejoin);
76194:     }
76194: 
52743:     frame.pop();
76194:     frame.pushSynced(JSVAL_TYPE_UNKNOWN);
52743: }
52743: 
52903: bool
52896: mjit::Compiler::jsop_callprop_slow(JSAtom *atom)
52896: {
53087:     prepareStubCall(Uses(1));
52896:     masm.move(ImmPtr(atom), Registers::ArgReg1);
76194:     INLINE_STUBCALL(stubs::CallProp, REJOIN_FALLTHROUGH);
78456:     testPushedType(REJOIN_FALLTHROUGH, -1, /* ool = */ false);
52896:     frame.pop();
76194:     pushSyncedEntry(0);
76194:     pushSyncedEntry(1);
52903:     return true;
52896: }
52896: 
76194: #ifdef JS_MONOIC
76194: void
76194: mjit::Compiler::passMICAddress(GlobalNameICInfo &ic)
76194: {
76194:     ic.addrLabel = stubcc.masm.moveWithPatch(ImmPtr(NULL), Registers::ArgReg1);
76194: }
76194: #endif
76194: 
76194: #if defined JS_POLYIC
76194: void
76194: mjit::Compiler::passICAddress(BaseICInfo *ic)
76194: {
76194:     ic->paramAddr = stubcc.masm.moveWithPatch(ImmPtr(NULL), Registers::ArgReg1);
76194: }
76194: 
56037: bool
76194: mjit::Compiler::jsop_getprop(JSAtom *atom, JSValueType knownType,
76194:                              bool doTypeCheck, bool usePropCache)
52884: {
52884:     FrameEntry *top = frame.peek(-1);
52884: 
76194:     /*
76194:      * Use a different rejoin for GETPROP computing the 'this' object, as we
76194:      * can't use the current bytecode within InternalInterpret to tell this is
76194:      * fetching the 'this' value.
76194:      */
76194:     RejoinState rejoin = REJOIN_GETTER;
76194:     if (!usePropCache) {
76194:         JS_ASSERT(top->isType(JSVAL_TYPE_OBJECT) &&
76194:                   atom == cx->runtime->atomState.classPrototypeAtom);
76194:         rejoin = REJOIN_THIS_PROTOTYPE;
76194:     }
76194: 
76194:     /* Handle length accesses on known strings without using a PIC. */
76194:     if (atom == cx->runtime->atomState.lengthAtom &&
76194:         top->isType(JSVAL_TYPE_STRING) &&
77355:         (!cx->typeInferenceEnabled() || knownPushedType(0) == JSVAL_TYPE_INT32)) {
52884:         if (top->isConstant()) {
53081:             JSString *str = top->getValue().toString();
52884:             Value v;
52885:             v.setNumber(uint32(str->length()));
52884:             frame.pop();
52884:             frame.push(v);
52884:         } else {
52884:             RegisterID str = frame.ownRegForData(top);
59888:             masm.loadPtr(Address(str, JSString::offsetOfLengthAndFlags()), str);
59977:             masm.urshift32(Imm32(JSString::LENGTH_SHIFT), str);
52884:             frame.pop();
53025:             frame.pushTypedPayload(JSVAL_TYPE_INT32, str);
52884:         }
56037:         return true;
52884:     }
52884: 
52884:     /* If the incoming type will never PIC, take slow path. */
77364:     if (top->isNotType(JSVAL_TYPE_OBJECT)) {
56201:         jsop_getprop_slow(atom, usePropCache);
56037:         return true;
52880:     }
52880: 
77364:     frame.forgetMismatchedObject(top);
77364: 
77353:     if (JSOp(*PC) == JSOP_LENGTH && cx->typeInferenceEnabled() &&
77353:         !hasTypeBarriers(PC) && knownPushedType(0) == JSVAL_TYPE_INT32) {
76194:         /* Check if this is an array we can make a loop invariant entry for. */
76194:         if (loop && loop->generatingInvariants()) {
76194:             CrossSSAValue topv(a->inlineIndex, analysis->poppedValue(PC, 0));
76194:             FrameEntry *fe = loop->invariantLength(topv);
76194:             if (fe) {
76194:                 frame.learnType(fe, JSVAL_TYPE_INT32, false);
74052:                 frame.pop();
76194:                 frame.pushCopyOf(fe);
74052:                 return true;
76194:             }
76194:         }
76194: 
76194:         types::TypeSet *types = analysis->poppedTypes(PC, 0);
76194: 
76194:         /*
76194:          * Check if we are accessing the 'length' property of a known dense array.
76194:          * Note that if the types are known to indicate dense arrays, their lengths
76194:          * must fit in an int32.
76194:          */
76194:         if (!types->hasObjectFlags(cx, types::OBJECT_FLAG_NON_DENSE_ARRAY)) {
76194:             bool isObject = top->isTypeKnown();
76194:             if (!isObject) {
76194:                 Jump notObject = frame.testObject(Assembler::NotEqual, top);
76194:                 stubcc.linkExit(notObject, Uses(1));
76194:                 stubcc.leave();
76194:                 OOL_STUBCALL(stubs::GetProp, rejoin);
78456:                 if (rejoin == REJOIN_GETTER)
78456:                     testPushedType(rejoin, -1);
76194:             }
76194:             RegisterID reg = frame.tempRegForData(top);
76194:             frame.pop();
77435:             frame.pushWord(Address(reg, offsetof(JSObject, privateData)), JSVAL_TYPE_INT32);
76194:             if (!isObject)
76194:                 stubcc.rejoin(Changes(1));
76194:             return true;
76194:         }
76194: 
76194:         /*
77344:          * Check if we're accessing the 'length' property of a typed array.
77344:          * The typed array length always fits in an int32.
77344:          */
77344:         if (!types->hasObjectFlags(cx, types::OBJECT_FLAG_NON_TYPED_ARRAY)) {
77344:             bool isObject = top->isTypeKnown();
77344:             if (!isObject) {
77344:                 Jump notObject = frame.testObject(Assembler::NotEqual, top);
77344:                 stubcc.linkExit(notObject, Uses(1));
77344:                 stubcc.leave();
77344:                 OOL_STUBCALL(stubs::GetProp, rejoin);
78456:                 if (rejoin == REJOIN_GETTER)
78456:                     testPushedType(rejoin, -1);
77344:             }
77344:             RegisterID reg = frame.copyDataIntoReg(top);
77344:             frame.pop();
80240:             masm.loadPayload(Address(reg, TypedArray::lengthOffset()), reg);
80240:             frame.pushTypedPayload(JSVAL_TYPE_INT32, reg);
77344:             if (!isObject)
77344:                 stubcc.rejoin(Changes(1));
77344:             return true;
77344:         }
77344: 
77344:         /*
76194:          * Check if we are accessing the 'length' of the lazy arguments for the
76194:          * current frame. No actual arguments object has ever been constructed
76194:          * for the script, so we can go straight to nactual.
76194:          */
76194:         if (types->isLazyArguments(cx)) {
76194:             frame.pop();
77435:             frame.pushWord(Address(JSFrameReg, StackFrame::offsetOfArgs()), JSVAL_TYPE_INT32);
76194:             return true;
76194:         }
76194:     }
74052: 
76194:     /* Check if this is a property access we can make a loop invariant entry for. */
76194:     if (loop && loop->generatingInvariants() && !hasTypeBarriers(PC)) {
76194:         CrossSSAValue topv(a->inlineIndex, analysis->poppedValue(PC, 0));
76194:         FrameEntry *fe = loop->invariantProperty(topv, ATOM_TO_JSID(atom));
76194:         if (fe) {
76194:             if (knownType != JSVAL_TYPE_UNKNOWN && knownType != JSVAL_TYPE_DOUBLE)
76194:                 frame.learnType(fe, knownType, false);
76194:             frame.pop();
76194:             frame.pushCopyOf(fe);
76194:             return true;
76194:         }
76194:     }
76194: 
76194:     /*
76194:      * Check if we are accessing a known type which always has the property
76194:      * in a particular inline slot. Get the property directly in this case,
76194:      * without using an IC.
76194:      */
76194:     jsid id = ATOM_TO_JSID(atom);
76194:     types::TypeSet *types = frame.extra(top).types;
77357:     if (types && !types->unknownObject() &&
77353:         types->getObjectCount() == 1 &&
77353:         types->getTypeObject(0) != NULL &&
77353:         !types->getTypeObject(0)->unknownProperties() &&
77353:         id == types::MakeTypeId(cx, id)) {
76194:         JS_ASSERT(usePropCache);
77353:         types::TypeObject *object = types->getTypeObject(0);
76194:         types::TypeSet *propertyTypes = object->getProperty(cx, id, false);
76194:         if (!propertyTypes)
76194:             return false;
77361:         if (propertyTypes->isDefiniteProperty() &&
77361:             !propertyTypes->isOwnProperty(cx, object, true)) {
76194:             types->addFreeze(cx);
76194:             uint32 slot = propertyTypes->definiteSlot();
76194:             bool isObject = top->isTypeKnown();
76194:             if (!isObject) {
76194:                 Jump notObject = frame.testObject(Assembler::NotEqual, top);
76194:                 stubcc.linkExit(notObject, Uses(1));
76194:                 stubcc.leave();
76194:                 OOL_STUBCALL(stubs::GetProp, rejoin);
78456:                 if (rejoin == REJOIN_GETTER)
78456:                     testPushedType(rejoin, -1);
76194:             }
76194:             RegisterID reg = frame.tempRegForData(top);
76194:             frame.pop();
76194: 
76194:             Address address(reg, JSObject::getFixedSlotOffset(slot));
76194:             BarrierState barrier = pushAddressMaybeBarrier(address, knownType, false);
76194:             if (!isObject)
76194:                 stubcc.rejoin(Changes(1));
76194:             finishBarrier(barrier, rejoin, 0);
76194: 
76194:             return true;
76194:         }
76194:     }
76194: 
52884:     /*
52884:      * These two must be loaded first. The objReg because the string path
52884:      * wants to read it, and the shapeReg because it could cause a spill that
52884:      * the string path wouldn't sink back.
52884:      */
52884:     RegisterID objReg = Registers::ReturnReg;
52884:     RegisterID shapeReg = Registers::ReturnReg;
52892:     if (atom == cx->runtime->atomState.lengthAtom) {
52884:         objReg = frame.copyDataIntoReg(top);
52884:         shapeReg = frame.allocReg();
52884:     }
52884: 
60592:     RESERVE_IC_SPACE(masm);
60592: 
56738:     PICGenInfo pic(ic::PICInfo::GET, JSOp(*PC), usePropCache);
52880: 
52880:     /* Guard that the type is an object. */
60592:     Label typeCheck;
52894:     if (doTypeCheck && !top->isTypeKnown()) {
52880:         RegisterID reg = frame.tempRegForType(top);
52884:         pic.typeReg = reg;
52884: 
52884:         /* Start the hot path where it's easy to patch it. */
53270:         pic.fastPathStart = masm.label();
53023:         Jump j = masm.testObject(Assembler::NotEqual, reg);
60592:         typeCheck = masm.label();
56037:         RETURN_IF_OOM(false);
53270: 
53479:         pic.typeCheck = stubcc.linkExit(j, Uses(1));
52884:         pic.hasTypeCheck = true;
52884:     } else {
53270:         pic.fastPathStart = masm.label();
52884:         pic.hasTypeCheck = false;
52884:         pic.typeReg = Registers::ReturnReg;
52880:     }
52880: 
52892:     if (atom != cx->runtime->atomState.lengthAtom) {
52884:         objReg = frame.copyDataIntoReg(top);
52884:         shapeReg = frame.allocReg();
52884:     }
52884: 
78454:     /*
78454:      * If this access has been on a shape with a getter hook, make preparations
78454:      * so that we can generate a stub to call the hook directly (rather than be
78454:      * forced to make a stub call). Sync the stack up front and kill all
78454:      * registers so that PIC stubs can contain calls, and always generate a
78454:      * type barrier if inference is enabled (known property types do not
78454:      * reflect properties with getter hooks).
78454:      */
78456:     pic.canCallHook = pic.forcedTypeBarrier =
78822:         usePropCache &&
78822:         JSOp(*PC) == JSOP_GETPROP &&
78822:         atom != cx->runtime->atomState.lengthAtom &&
78822:         analysis->getCode(PC).accessGetter;
78454:     if (pic.canCallHook)
78454:         frame.syncAndKillEverything();
78454: 
52880:     pic.shapeReg = shapeReg;
52892:     pic.atom = atom;
52880: 
52880:     /* Guard on shape. */
53445:     masm.loadShape(objReg, shapeReg);
52880:     pic.shapeGuard = masm.label();
53270: 
53408:     DataLabel32 inlineShapeLabel;
53270:     Jump j = masm.branch32WithPatch(Assembler::NotEqual, shapeReg,
68935:                                     Imm32(int32(INVALID_SHAPE)),
53315:                                     inlineShapeLabel);
60592:     Label inlineShapeJump = masm.label();
60592: 
60592:     RESERVE_OOL_SPACE(stubcc.masm);
53479:     pic.slowPathStart = stubcc.linkExit(j, Uses(1));
52880: 
52880:     stubcc.leave();
56738:     passICAddress(&pic);
76194:     pic.slowPathCall = OOL_STUBCALL(usePropCache ? ic::GetProp : ic::GetPropNoCache, rejoin);
60592:     CHECK_OOL_SPACE();
78456:     if (rejoin == REJOIN_GETTER)
78456:         testPushedType(rejoin, -1);
60592: 
60592:     /* Load the base slot address. */
60592:     Label dslotsLoadLabel = masm.loadPtrWithPatchToLEA(Address(objReg, offsetof(JSObject, slots)),
60592:                                                                objReg);
52880: 
52880:     /* Copy the slot value to the expression stack. */
52880:     Address slot(objReg, 1 << 24);
52880:     frame.pop();
53270: 
60592:     Label fastValueLoad = masm.loadValueWithAddressOffsetPatch(slot, shapeReg, objReg);
60592:     pic.fastPathRejoin = masm.label();
60592: 
60592:     RETURN_IF_OOM(false);
60592: 
60592:     /* Initialize op labels. */
60592:     GetPropLabels &labels = pic.getPropLabels();
60592:     labels.setDslotsLoad(masm, pic.fastPathRejoin, dslotsLoadLabel);
60592:     labels.setInlineShapeData(masm, pic.shapeGuard, inlineShapeLabel);
60592: 
60592:     labels.setValueLoad(masm, pic.fastPathRejoin, fastValueLoad);
60592:     if (pic.hasTypeCheck)
60592:         labels.setInlineTypeJump(masm, pic.fastPathStart, typeCheck);
60592: #ifdef JS_CPU_X64
60592:     labels.setInlineShapeJump(masm, inlineShapeLabel, inlineShapeJump);
60592: #else
60592:     labels.setInlineShapeJump(masm, pic.shapeGuard, inlineShapeJump);
53315: #endif
53270: 
78455:     CHECK_IC_SPACE();
78455: 
52880:     pic.objReg = objReg;
76194:     frame.pushRegs(shapeReg, objReg, knownType);
78454:     BarrierState barrier = testBarrier(pic.shapeReg, pic.objReg, false, false,
78454:                                        /* force = */ pic.canCallHook);
52880: 
53088:     stubcc.rejoin(Changes(1));
52880:     pics.append(pic);
76194: 
76194:     finishBarrier(barrier, rejoin, 0);
56037:     return true;
52880: }
60598: 
52903: bool
52906: mjit::Compiler::jsop_callprop_generic(JSAtom *atom)
52906: {
52906:     FrameEntry *top = frame.peek(-1);
52906: 
52906:     /*
52906:      * These two must be loaded first. The objReg because the string path
52906:      * wants to read it, and the shapeReg because it could cause a spill that
52906:      * the string path wouldn't sink back.
52906:      */
52906:     RegisterID objReg = frame.copyDataIntoReg(top);
52906:     RegisterID shapeReg = frame.allocReg();
52906: 
56738:     PICGenInfo pic(ic::PICInfo::CALL, JSOp(*PC), true);
52906: 
56551:     pic.pc = PC;
56551: 
52906:     /* Guard that the type is an object. */
52906:     pic.typeReg = frame.copyTypeIntoReg(top);
52906: 
78822:     pic.canCallHook = pic.forcedTypeBarrier = analysis->getCode(PC).accessGetter;
78822:     if (pic.canCallHook)
78822:         frame.syncAndKillEverything();
78822: 
60596:     RESERVE_IC_SPACE(masm);
60596: 
52906:     /* Start the hot path where it's easy to patch it. */
53270:     pic.fastPathStart = masm.label();
52906: 
52906:     /*
52906:      * Guard that the value is an object. This part needs some extra gunk
52906:      * because the leave() after the shape guard will emit a jump from this
52906:      * path to the final call. We need a label in between that jump, which
52906:      * will be the target of patched jumps in the PIC.
52906:      */
60596:     Jump typeCheckJump = masm.testObject(Assembler::NotEqual, pic.typeReg);
60596:     Label typeCheck = masm.label();
60596:     RETURN_IF_OOM(false);
60596: 
60596:     pic.typeCheck = stubcc.linkExit(typeCheckJump, Uses(1));
52906:     pic.hasTypeCheck = true;
52906:     pic.objReg = objReg;
52906:     pic.shapeReg = shapeReg;
52906:     pic.atom = atom;
52906: 
52906:     /*
52906:      * Store the type and object back. Don't bother keeping them in registers,
52906:      * since a sync will be needed for the upcoming call.
52906:      */
76194:     uint32 thisvSlot = frame.totalDepth();
69223:     Address thisv = Address(JSFrameReg, sizeof(StackFrame) + thisvSlot * sizeof(Value));
60596: 
53315: #if defined JS_NUNBOX32
54582:     masm.storeValueFromComponents(pic.typeReg, pic.objReg, thisv);
53315: #elif defined JS_PUNBOX64
53315:     masm.orPtr(pic.objReg, pic.typeReg);
53315:     masm.storePtr(pic.typeReg, thisv);
53315: #endif
60596: 
52906:     frame.freeReg(pic.typeReg);
52906: 
52906:     /* Guard on shape. */
53445:     masm.loadShape(objReg, shapeReg);
52906:     pic.shapeGuard = masm.label();
53307: 
53408:     DataLabel32 inlineShapeLabel;
53307:     Jump j = masm.branch32WithPatch(Assembler::NotEqual, shapeReg,
68935:                            Imm32(int32(INVALID_SHAPE)),
53315:                            inlineShapeLabel);
60596:     Label inlineShapeJump = masm.label();
60596: 
60596:     /* Slow path. */
60596:     RESERVE_OOL_SPACE(stubcc.masm);
53479:     pic.slowPathStart = stubcc.linkExit(j, Uses(1));
52906:     stubcc.leave();
56738:     passICAddress(&pic);
76194:     pic.slowPathCall = OOL_STUBCALL(ic::CallProp, REJOIN_FALLTHROUGH);
60596:     CHECK_OOL_SPACE();
52906: 
78456:     testPushedType(REJOIN_FALLTHROUGH, -1);
78456: 
60596:     /* Load the base slot address. */
60596:     Label dslotsLoadLabel = masm.loadPtrWithPatchToLEA(Address(objReg, offsetof(JSObject, slots)),
60596:                                                                objReg);
52906: 
52906:     /* Copy the slot value to the expression stack. */
52906:     Address slot(objReg, 1 << 24);
53307: 
60592:     Label fastValueLoad = masm.loadValueWithAddressOffsetPatch(slot, shapeReg, objReg);
56575:     pic.fastPathRejoin = masm.label();
52906: 
56037:     RETURN_IF_OOM(false);
60596: 
60596:     /*
60596:      * Initialize op labels. We use GetPropLabels here because we have the same patching
60596:      * requirements for CallProp.
60596:      */
60590:     GetPropLabels &labels = pic.getPropLabels();
60590:     labels.setDslotsLoadOffset(masm.differenceBetween(pic.fastPathRejoin, dslotsLoadLabel));
60590:     labels.setInlineShapeOffset(masm.differenceBetween(pic.shapeGuard, inlineShapeLabel));
60592:     labels.setValueLoad(masm, pic.fastPathRejoin, fastValueLoad);
60596:     labels.setInlineTypeJump(masm, pic.fastPathStart, typeCheck);
60596: #ifdef JS_CPU_X64
60596:     labels.setInlineShapeJump(masm, inlineShapeLabel, inlineShapeJump);
60596: #else
60596:     labels.setInlineShapeJump(masm, pic.shapeGuard, inlineShapeJump);
53315: #endif
53307: 
78455:     CHECK_IC_SPACE();
78455: 
76194:     /* Adjust the frame. */
76194:     frame.pop();
76194:     frame.pushRegs(shapeReg, objReg, knownPushedType(0));
78454:     BarrierState barrier = testBarrier(pic.shapeReg, pic.objReg, false, false,
78454:                                        /* force = */ pic.canCallHook);
76194: 
76194:     pushSyncedEntry(1);
76194: 
53088:     stubcc.rejoin(Changes(2));
52906:     pics.append(pic);
52906: 
76194:     finishBarrier(barrier, REJOIN_FALLTHROUGH, 1);
52906:     return true;
52906: }
52906: 
52906: bool
52903: mjit::Compiler::jsop_callprop_str(JSAtom *atom)
52903: {
76194:     if (!globalObj) {
52903:         jsop_callprop_slow(atom);
52903:         return true;
52903:     }
52903: 
60240:     /*
60240:      * Bake in String.prototype. This is safe because of compileAndGo.
60240:      * We must pass an explicit scope chain only because JSD calls into
60240:      * here via the recompiler with a dummy context, and we need to use
60240:      * the global object for the script we are now compiling.
60240:      */
52903:     JSObject *obj;
76194:     if (!js_GetClassPrototype(cx, globalObj, JSProto_String, &obj))
52903:         return false;
52903: 
78389:     /*
78389:      * Root the proto, since JS_ClearScope might overwrite the global object's
78389:      * copy.
78389:      */
78389:     rootedObjects.append(obj);
78389: 
52903:     /* Force into a register because getprop won't expect a constant. */
52903:     RegisterID reg = frame.allocReg();
54832: 
52903:     masm.move(ImmPtr(obj), reg);
53025:     frame.pushTypedPayload(JSVAL_TYPE_OBJECT, reg);
52903: 
52903:     /* Get the property. */
76194:     if (!jsop_getprop(atom, knownPushedType(0)))
56037:         return false;
52903: 
52903:     /* Perform a swap. */
52903:     frame.dup2();
52903:     frame.shift(-3);
52903:     frame.shift(-1);
52903: 
53374:     /*
53374:      * See bug 584579 - need to forget string type, since wrapping could
53374:      * create an object. forgetType() alone is not valid because it cannot be
53374:      * used on copies or constants.
53374:      */
53374:     RegisterID strReg;
53374:     FrameEntry *strFe = frame.peek(-1);
53374:     if (strFe->isConstant()) {
53374:         strReg = frame.allocReg();
53374:         masm.move(ImmPtr(strFe->getValue().toString()), strReg);
53374:     } else {
53374:         strReg = frame.ownRegForData(strFe);
53374:     }
53374:     frame.pop();
53374:     frame.pushTypedPayload(JSVAL_TYPE_STRING, strReg);
53374:     frame.forgetType(frame.peek(-1));
53374: 
52903:     return true;
52903: }
52903: 
52903: bool
52896: mjit::Compiler::jsop_callprop_obj(JSAtom *atom)
52896: {
52896:     FrameEntry *top = frame.peek(-1);
52896: 
56738:     PICGenInfo pic(ic::PICInfo::CALL, JSOp(*PC), true);
52896: 
52896:     JS_ASSERT(top->isTypeKnown());
53025:     JS_ASSERT(top->getKnownType() == JSVAL_TYPE_OBJECT);
52896: 
60596:     RESERVE_IC_SPACE(masm);
60596: 
56551:     pic.pc = PC;
53270:     pic.fastPathStart = masm.label();
52896:     pic.hasTypeCheck = false;
52896:     pic.typeReg = Registers::ReturnReg;
52896: 
52896:     RegisterID shapeReg = frame.allocReg();
52896:     pic.shapeReg = shapeReg;
52896:     pic.atom = atom;
52896: 
76194:     RegisterID objReg;
76194:     if (top->isConstant()) {
76194:         objReg = frame.allocReg();
76194:         masm.move(ImmPtr(&top->getValue().toObject()), objReg);
76194:     } else {
76194:         objReg = frame.copyDataIntoReg(top);
76194:     }
76194: 
78456:     pic.canCallHook = pic.forcedTypeBarrier = analysis->getCode(PC).accessGetter;
78454:     if (pic.canCallHook)
78454:         frame.syncAndKillEverything();
78454: 
52896:     /* Guard on shape. */
53445:     masm.loadShape(objReg, shapeReg);
52896:     pic.shapeGuard = masm.label();
53307: 
53408:     DataLabel32 inlineShapeLabel;
53307:     Jump j = masm.branch32WithPatch(Assembler::NotEqual, shapeReg,
68935:                            Imm32(int32(INVALID_SHAPE)),
53315:                            inlineShapeLabel);
60596:     Label inlineShapeJump = masm.label();
60596: 
60596:     /* Slow path. */
60596:     RESERVE_OOL_SPACE(stubcc.masm);
53588:     pic.slowPathStart = stubcc.linkExit(j, Uses(1));
52896:     stubcc.leave();
56738:     passICAddress(&pic);
76194:     pic.slowPathCall = OOL_STUBCALL(ic::CallProp, REJOIN_FALLTHROUGH);
60596:     CHECK_OOL_SPACE();
60596: 
78456:     testPushedType(REJOIN_FALLTHROUGH, -1);
78456: 
60596:     /* Load the base slot address. */
60596:     Label dslotsLoadLabel = masm.loadPtrWithPatchToLEA(Address(objReg, offsetof(JSObject, slots)),
60596:                                                                objReg);
52896: 
52896:     /* Copy the slot value to the expression stack. */
52896:     Address slot(objReg, 1 << 24);
53307: 
60592:     Label fastValueLoad = masm.loadValueWithAddressOffsetPatch(slot, shapeReg, objReg);
53307: 
56575:     pic.fastPathRejoin = masm.label();
52896:     pic.objReg = objReg;
52896: 
78455:     CHECK_IC_SPACE();
78455: 
52896:     /*
52896:      * 1) Dup the |this| object.
76194:      * 2) Store the property value below the |this| value.
76194:      * This is safe as a stack transition, because JSOP_CALLPROP has
76194:      * JOF_TMPSLOT. It is also safe for correctness, because if we know the LHS
76194:      * is an object, it is the resulting vp[1].
52896:      */
52896:     frame.dup();
76194:     frame.storeRegs(-2, shapeReg, objReg, knownPushedType(0));
78454:     BarrierState barrier = testBarrier(shapeReg, objReg, false, false,
78454:                                        /* force = */ pic.canCallHook);
52896: 
53315:     /*
53315:      * Assert correctness of hardcoded offsets.
53315:      * No type guard: type is asserted.
53315:      */
56037:     RETURN_IF_OOM(false);
60590: 
60590:     GetPropLabels &labels = pic.getPropLabels();
60590:     labels.setDslotsLoadOffset(masm.differenceBetween(pic.fastPathRejoin, dslotsLoadLabel));
60590:     labels.setInlineShapeOffset(masm.differenceBetween(pic.shapeGuard, inlineShapeLabel));
60592:     labels.setValueLoad(masm, pic.fastPathRejoin, fastValueLoad);
60596: #ifdef JS_CPU_X64
60596:     labels.setInlineShapeJump(masm, inlineShapeLabel, inlineShapeJump);
60596: #else
60596:     labels.setInlineShapeJump(masm, pic.shapeGuard, inlineShapeJump);
53315: #endif
53307: 
53088:     stubcc.rejoin(Changes(2));
52896:     pics.append(pic);
52903: 
76194:     finishBarrier(barrier, REJOIN_FALLTHROUGH, 1);
76194:     return true;
76194: }
76194: 
76194: bool
76194: mjit::Compiler::testSingletonProperty(JSObject *obj, jsid id)
76194: {
76194:     /*
76194:      * We would like to completely no-op property/global accesses which can
76194:      * produce only a particular JSObject or undefined, provided we can
76194:      * determine the pushed value must not be undefined (or, if it could be
76194:      * undefined, a recompilation will be triggered).
76194:      *
76194:      * If the access definitely goes through obj, either directly or on the
76194:      * prototype chain, then if obj has a defined property now, and the
76194:      * property has a default or method shape, the only way it can produce
76194:      * undefined in the future is if it is deleted. Deletion causes type
76194:      * properties to be explicitly marked with undefined.
76194:      */
76194: 
77384:     JSObject *nobj = obj;
77384:     while (nobj) {
77384:         if (!nobj->isNative())
76194:             return false;
77384:         if (nobj->getClass()->ops.lookupProperty)
76194:             return false;
77384:         nobj = nobj->getProto();
77384:     }
76194: 
76194:     JSObject *holder;
76194:     JSProperty *prop = NULL;
76194:     if (!obj->lookupProperty(cx, id, &holder, &prop))
76194:         return false;
76194:     if (!prop)
76194:         return false;
76194: 
76194:     Shape *shape = (Shape *) prop;
76194:     if (shape->hasDefaultGetter()) {
76194:         if (!shape->hasSlot())
76194:             return false;
76194:         if (holder->getSlot(shape->slot).isUndefined())
76194:             return false;
76194:     } else if (!shape->isMethod()) {
76194:         return false;
76194:     }
76194: 
76194:     return true;
76194: }
76194: 
76194: bool
76194: mjit::Compiler::testSingletonPropertyTypes(FrameEntry *top, jsid id, bool *testObject)
76194: {
76194:     *testObject = false;
76194: 
76194:     types::TypeSet *types = frame.extra(top).types;
77353:     if (!types || types->unknownObject())
76194:         return false;
76194: 
76194:     JSObject *singleton = types->getSingleton(cx);
76194:     if (singleton)
76194:         return testSingletonProperty(singleton, id);
76194: 
76194:     if (!globalObj)
76194:         return false;
76194: 
76194:     JSProtoKey key;
76194:     JSValueType type = types->getKnownTypeTag(cx);
76194:     switch (type) {
76194:       case JSVAL_TYPE_STRING:
76194:         key = JSProto_String;
76194:         break;
76194: 
76194:       case JSVAL_TYPE_INT32:
76194:       case JSVAL_TYPE_DOUBLE:
76194:         key = JSProto_Number;
76194:         break;
76194: 
76194:       case JSVAL_TYPE_BOOLEAN:
76194:         key = JSProto_Boolean;
76194:         break;
76194: 
76194:       case JSVAL_TYPE_OBJECT:
76194:       case JSVAL_TYPE_UNKNOWN:
76194:         if (types->getObjectCount() == 1 && !top->isNotType(JSVAL_TYPE_OBJECT)) {
76194:             JS_ASSERT_IF(top->isTypeKnown(), top->isType(JSVAL_TYPE_OBJECT));
77353:             types::TypeObject *object = types->getTypeObject(0);
77353:             if (object && object->proto) {
76194:                 if (!testSingletonProperty(object->proto, id))
76194:                     return false;
76194:                 types->addFreeze(cx);
76194: 
76194:                 /* If we don't know this is an object, we will need a test. */
76194:                 *testObject = (type != JSVAL_TYPE_OBJECT) && !top->isTypeKnown();
76194:                 return true;
76194:             }
76194:         }
76194:         return false;
76194: 
76194:       default:
76194:         return false;
76194:     }
76194: 
76194:     JSObject *proto;
76194:     if (!js_GetClassPrototype(cx, globalObj, key, &proto, NULL))
76194:         return NULL;
76194: 
76194:     return testSingletonProperty(proto, id);
76194: }
76194: 
76194: bool
76194: mjit::Compiler::jsop_callprop_dispatch(JSAtom *atom)
76194: {
76194:     /*
76194:      * Check for a CALLPROP which is a dynamic dispatch: every value it can
76194:      * push is a singleton, and the pushed value is determined by the type of
76194:      * the object being accessed. Return true if the CALLPROP has been fully
76194:      * processed, false if no code was generated.
76194:      */
76194:     FrameEntry *top = frame.peek(-1);
76194:     if (top->isNotType(JSVAL_TYPE_OBJECT))
76194:         return false;
76194: 
76194:     jsid id = ATOM_TO_JSID(atom);
76194:     if (id != types::MakeTypeId(cx, id))
76194:         return false;
76194: 
76194:     types::TypeSet *pushedTypes = pushedTypeSet(0);
77353:     if (pushedTypes->unknownObject() || pushedTypes->baseFlags() != 0)
76194:         return false;
76194: 
76194:     /* Check every pushed value is a singleton. */
76194:     for (unsigned i = 0; i < pushedTypes->getObjectCount(); i++) {
77353:         if (pushedTypes->getTypeObject(i) != NULL)
76194:             return false;
76194:     }
76194: 
76194:     types::TypeSet *objTypes = analysis->poppedTypes(PC, 0);
77353:     if (objTypes->unknownObject() || objTypes->getObjectCount() == 0)
76194:         return false;
76194: 
76194:     pushedTypes->addFreeze(cx);
76194: 
76194:     /* Map each type in the object to the resulting pushed value. */
76194:     Vector<JSObject *> results(CompilerAllocPolicy(cx, *this));
76194: 
76194:     /*
76194:      * For each type of the base object, check it has no 'own' property for the
76194:      * accessed id and that its prototype does have such a property.
76194:      */
76194:     uint32 last = 0;
76194:     for (unsigned i = 0; i < objTypes->getObjectCount(); i++) {
77353:         if (objTypes->getSingleObject(i) != NULL)
77353:             return false;
77353:         types::TypeObject *object = objTypes->getTypeObject(i);
76194:         if (!object) {
77403:             results.append((JSObject *) NULL);
76194:             continue;
76194:         }
76194:         if (object->unknownProperties() || !object->proto)
76194:             return false;
76194:         types::TypeSet *ownTypes = object->getProperty(cx, id, false);
77361:         if (ownTypes->isOwnProperty(cx, object, false))
76194:             return false;
76194: 
76194:         if (!testSingletonProperty(object->proto, id))
76194:             return false;
76194: 
77363:         if (object->proto->getType(cx)->unknownProperties())
77363:             return false;
77363:         types::TypeSet *protoTypes = object->proto->type()->getProperty(cx, id, false);
77353:         if (!protoTypes)
77353:             return false;
76194:         JSObject *singleton = protoTypes->getSingleton(cx);
76194:         if (!singleton)
76194:             return false;
76194: 
76194:         results.append(singleton);
76194:         last = i;
76194:     }
76194: 
76194:     if (oomInVector)
76194:         return false;
76194: 
76194:     objTypes->addFreeze(cx);
76194: 
76194:     /* Done filtering, now generate code which dispatches on the type. */
76194: 
76194:     frame.forgetMismatchedObject(top);
76194: 
76194:     if (!top->isType(JSVAL_TYPE_OBJECT)) {
76194:         Jump notObject = frame.testObject(Assembler::NotEqual, top);
76194:         stubcc.linkExit(notObject, Uses(1));
76194:     }
76194: 
76194:     RegisterID reg = frame.tempRegForData(top);
76194:     frame.pinReg(reg);
76194:     RegisterID pushreg = frame.allocReg();
76194:     frame.unpinReg(reg);
76194: 
77353:     Address typeAddress(reg, JSObject::offsetOfType());
76194: 
76194:     Vector<Jump> rejoins(CompilerAllocPolicy(cx, *this));
76194:     MaybeJump lastMiss;
76194: 
76194:     for (unsigned i = 0; i < objTypes->getObjectCount(); i++) {
77353:         types::TypeObject *object = objTypes->getTypeObject(i);
76194:         if (!object) {
76194:             JS_ASSERT(results[i] == NULL);
76194:             continue;
76194:         }
76194:         if (lastMiss.isSet())
76194:             lastMiss.get().linkTo(masm.label(), &masm);
76194: 
76194:         /*
76194:          * Check that the pushed result is actually in the known pushed types
76194:          * for the bytecode; this bytecode may have type barriers. Redirect to
76194:          * the stub to update said pushed types.
76194:          */
77353:         if (!pushedTypes->hasType(types::Type::ObjectType(results[i]))) {
76194:             JS_ASSERT(hasTypeBarriers(PC));
76194:             if (i == last) {
76194:                 stubcc.linkExit(masm.jump(), Uses(1));
76194:                 break;
76194:             } else {
76194:                 lastMiss.setJump(masm.branchPtr(Assembler::NotEqual, typeAddress, ImmPtr(object)));
76194:                 stubcc.linkExit(masm.jump(), Uses(1));
76194:                 continue;
76194:             }
76194:         }
76194: 
76194:         if (i == last) {
76194:             masm.move(ImmPtr(results[i]), pushreg);
76194:             break;
76194:         } else {
76194:             lastMiss.setJump(masm.branchPtr(Assembler::NotEqual, typeAddress, ImmPtr(object)));
76194:             masm.move(ImmPtr(results[i]), pushreg);
76194:             rejoins.append(masm.jump());
76194:         }
76194:     }
76194: 
76194:     for (unsigned i = 0; i < rejoins.length(); i++)
76194:         rejoins[i].linkTo(masm.label(), &masm);
76194: 
76194:     stubcc.leave();
76194:     stubcc.masm.move(ImmPtr(atom), Registers::ArgReg1);
76194:     OOL_STUBCALL(stubs::CallProp, REJOIN_FALLTHROUGH);
78456:     testPushedType(REJOIN_FALLTHROUGH, -1);
76194: 
76194:     frame.dup();
76194:     // THIS THIS
76194: 
76194:     frame.pushTypedPayload(JSVAL_TYPE_OBJECT, pushreg);
76194:     // THIS THIS FUN
76194: 
76194:     frame.shift(-2);
76194:     // FUN THIS
76194: 
76194:     stubcc.rejoin(Changes(2));
52903:     return true;
52896: }
52896: 
52903: bool
52896: mjit::Compiler::jsop_callprop(JSAtom *atom)
52896: {
52896:     FrameEntry *top = frame.peek(-1);
52896: 
76194:     /* If the CALLPROP will definitely be fetching a particular value, nop it. */
76194:     bool testObject;
76194:     JSObject *singleton = pushedSingleton(0);
76194:     if (singleton && singleton->isFunction() && !hasTypeBarriers(PC) &&
76194:         testSingletonPropertyTypes(top, ATOM_TO_JSID(atom), &testObject)) {
76194:         if (testObject) {
76194:             Jump notObject = frame.testObject(Assembler::NotEqual, top);
76194:             stubcc.linkExit(notObject, Uses(1));
76194:             stubcc.leave();
76194:             stubcc.masm.move(ImmPtr(atom), Registers::ArgReg1);
76194:             OOL_STUBCALL(stubs::CallProp, REJOIN_FALLTHROUGH);
78456:             testPushedType(REJOIN_FALLTHROUGH, -1);
76194:         }
76194: 
76194:         // THIS
76194: 
76194:         frame.dup();
76194:         // THIS THIS
76194: 
76194:         frame.push(ObjectValue(*singleton));
76194:         // THIS THIS FUN
76194: 
76194:         frame.shift(-2);
76194:         // FUN THIS
76194: 
76194:         if (testObject)
76194:             stubcc.rejoin(Changes(2));
76194: 
76194:         return true;
76194:     }
76194: 
76194:     /* Check for a dynamic dispatch. */
76194:     if (cx->typeInferenceEnabled()) {
76194:         if (jsop_callprop_dispatch(atom))
76194:             return true;
76194:     }
76194: 
52896:     /* If the incoming type will never PIC, take slow path. */
53025:     if (top->isTypeKnown() && top->getKnownType() != JSVAL_TYPE_OBJECT) {
53025:         if (top->getKnownType() == JSVAL_TYPE_STRING)
52903:             return jsop_callprop_str(atom);
52903:         return jsop_callprop_slow(atom);
52896:     }
52896: 
52896:     if (top->isTypeKnown())
52903:         return jsop_callprop_obj(atom);
52906:     return jsop_callprop_generic(atom);
52896: }
60598: 
56037: bool
76194: mjit::Compiler::jsop_setprop(JSAtom *atom, bool usePropCache, bool popGuaranteed)
52886: {
52887:     FrameEntry *lhs = frame.peek(-2);
52887:     FrameEntry *rhs = frame.peek(-1);
52887: 
52887:     /* If the incoming type will never PIC, take slow path. */
53025:     if (lhs->isTypeKnown() && lhs->getKnownType() != JSVAL_TYPE_OBJECT) {
56201:         jsop_setprop_slow(atom, usePropCache);
56037:         return true;
52887:     }
52887: 
76194:     /*
77884:      * If this is a SETNAME to a variable of a non-reentrant outer function,
77884:      * set the variable's slot directly for the active call object.
77884:      */
77884:     if (cx->typeInferenceEnabled() && js_CodeSpec[*PC].format & JOF_NAME) {
77884:         ScriptAnalysis::NameAccess access =
77884:             analysis->resolveNameAccess(cx, ATOM_TO_JSID(atom), true);
77884:         if (access.nesting) {
77884:             Address address = frame.loadNameAddress(access);
77884:             frame.storeTo(rhs, address, popGuaranteed);
77884:             frame.shimmy(1);
77884:             frame.freeReg(address.base);
77884:             return true;
77884:         }
77884:     }
77884: 
77884:     /*
76194:      * Set the property directly if we are accessing a known object which
76194:      * always has the property in a particular inline slot.
76194:      */
76194:     jsid id = ATOM_TO_JSID(atom);
76194:     types::TypeSet *types = frame.extra(lhs).types;
76194:     if (JSOp(*PC) == JSOP_SETPROP && id == types::MakeTypeId(cx, id) &&
77353:         types && !types->unknownObject() &&
77353:         types->getObjectCount() == 1 &&
77353:         types->getTypeObject(0) != NULL &&
77353:         !types->getTypeObject(0)->unknownProperties()) {
76194:         JS_ASSERT(usePropCache);
77353:         types::TypeObject *object = types->getTypeObject(0);
76194:         types::TypeSet *propertyTypes = object->getProperty(cx, id, false);
76194:         if (!propertyTypes)
76194:             return false;
77361:         if (propertyTypes->isDefiniteProperty() &&
77361:             !propertyTypes->isOwnProperty(cx, object, true)) {
76194:             types->addFreeze(cx);
76194:             uint32 slot = propertyTypes->definiteSlot();
76194:             bool isObject = lhs->isTypeKnown();
76194:             if (!isObject) {
76194:                 Jump notObject = frame.testObject(Assembler::NotEqual, lhs);
76194:                 stubcc.linkExit(notObject, Uses(2));
76194:                 stubcc.leave();
76194:                 stubcc.masm.move(ImmPtr(atom), Registers::ArgReg1);
76194:                 OOL_STUBCALL(STRICT_VARIANT(stubs::SetName), REJOIN_FALLTHROUGH);
76194:             }
76194:             RegisterID reg = frame.tempRegForData(lhs);
76194:             frame.storeTo(rhs, Address(reg, JSObject::getFixedSlotOffset(slot)), popGuaranteed);
76194:             frame.shimmy(1);
76194:             if (!isObject)
76194:                 stubcc.rejoin(Changes(1));
76194:             return true;
76194:         }
76194:     }
76194: 
53620:     JSOp op = JSOp(*PC);
53620: 
56738:     ic::PICInfo::Kind kind = (op == JSOP_SETMETHOD)
56738:                              ? ic::PICInfo::SETMETHOD
56738:                              : ic::PICInfo::SET;
56738:     PICGenInfo pic(kind, op, usePropCache);
52892:     pic.atom = atom;
52887: 
76194:     if (monitored(PC)) {
77353:         pic.typeMonitored = true;
76194:         types::TypeSet *types = frame.extra(rhs).types;
77353:         if (!types) {
77353:             /* Handle FORNAME and other compound opcodes. Yuck. */
77353:             types = types::TypeSet::make(cx, "unknownRHS");
77353:             if (!types)
76194:                 return false;
77353:             types->addType(cx, types::Type::UnknownType());
77353:         }
77353:         pic.rhsTypes = types;
76194:     } else {
76194:         pic.typeMonitored = false;
76194:         pic.rhsTypes = NULL;
76194:     }
76194: 
60595:     RESERVE_IC_SPACE(masm);
60595:     RESERVE_OOL_SPACE(stubcc.masm);
60595: 
52887:     /* Guard that the type is an object. */
52887:     Jump typeCheck;
52887:     if (!lhs->isTypeKnown()) {
52887:         RegisterID reg = frame.tempRegForType(lhs);
52887:         pic.typeReg = reg;
52887: 
52887:         /* Start the hot path where it's easy to patch it. */
53270:         pic.fastPathStart = masm.label();
53025:         Jump j = masm.testObject(Assembler::NotEqual, reg);
52887: 
53588:         pic.typeCheck = stubcc.linkExit(j, Uses(2));
52887:         stubcc.leave();
53492: 
52892:         stubcc.masm.move(ImmPtr(atom), Registers::ArgReg1);
56201:         if (usePropCache)
76194:             OOL_STUBCALL(STRICT_VARIANT(stubs::SetName), REJOIN_FALLTHROUGH);
56201:         else
76194:             OOL_STUBCALL(STRICT_VARIANT(stubs::SetPropNoCache), REJOIN_FALLTHROUGH);
52887:         typeCheck = stubcc.masm.jump();
52887:         pic.hasTypeCheck = true;
52887:     } else {
53270:         pic.fastPathStart = masm.label();
52887:         pic.hasTypeCheck = false;
52887:         pic.typeReg = Registers::ReturnReg;
52887:     }
52887: 
76194:     frame.forgetMismatchedObject(lhs);
76194: 
52887:     /* Get the object into a mutable register. */
52887:     RegisterID objReg = frame.copyDataIntoReg(lhs);
52887:     pic.objReg = objReg;
52887: 
52887:     /* Get info about the RHS and pin it. */
52887:     ValueRemat vr;
54160:     frame.pinEntry(rhs, vr);
52887:     pic.vr = vr;
52887: 
52887:     RegisterID shapeReg = frame.allocReg();
52887:     pic.shapeReg = shapeReg;
52887: 
54160:     frame.unpinEntry(vr);
52887: 
52887:     /* Guard on shape. */
53445:     masm.loadShape(objReg, shapeReg);
52887:     pic.shapeGuard = masm.label();
60595:     DataLabel32 inlineShapeData;
53270:     Jump j = masm.branch32WithPatch(Assembler::NotEqual, shapeReg,
68935:                                     Imm32(int32(INVALID_SHAPE)),
60595:                                     inlineShapeData);
60595:     Label afterInlineShapeJump = masm.label();
52887: 
52887:     /* Slow path. */
52887:     {
53588:         pic.slowPathStart = stubcc.linkExit(j, Uses(2));
52887: 
52887:         stubcc.leave();
56738:         passICAddress(&pic);
76194:         pic.slowPathCall = OOL_STUBCALL(ic::SetProp, REJOIN_FALLTHROUGH);
60595:         CHECK_OOL_SPACE();
52887:     }
52887: 
52887:     /* Load dslots. */
60595:     Label dslotsLoadLabel = masm.loadPtrWithPatchToLEA(Address(objReg, offsetof(JSObject, slots)),
60595:                                                        objReg);
52887: 
52887:     /* Store RHS into object slot. */
53315:     Address slot(objReg, 1 << 24);
60781:     DataLabel32 inlineValueStore = masm.storeValueWithAddressOffsetPatch(vr, slot);
56575:     pic.fastPathRejoin = masm.label();
53315: 
52887:     frame.freeReg(objReg);
52887:     frame.freeReg(shapeReg);
52887: 
52887:     /* "Pop under", taking out object (LHS) and leaving RHS. */
52887:     frame.shimmy(1);
52887: 
52887:     /* Finish slow path. */
52887:     {
52887:         if (pic.hasTypeCheck)
52887:             typeCheck.linkTo(stubcc.masm.label(), &stubcc.masm);
53088:         stubcc.rejoin(Changes(1));
52887:     }
52887: 
56037:     RETURN_IF_OOM(false);
60590: 
60590:     SetPropLabels &labels = pic.setPropLabels();
60595:     labels.setInlineShapeData(masm, pic.shapeGuard, inlineShapeData);
60595:     labels.setDslotsLoad(masm, pic.fastPathRejoin, dslotsLoadLabel, vr);
60595:     labels.setInlineValueStore(masm, pic.fastPathRejoin, inlineValueStore, vr);
60595:     labels.setInlineShapeJump(masm, pic.shapeGuard, afterInlineShapeJump);
53270: 
52887:     pics.append(pic);
56037:     return true;
52886: }
60598: 
53054: void
76194: mjit::Compiler::jsop_name(JSAtom *atom, JSValueType type, bool isCall)
53054: {
77884:     /*
77884:      * If this is a NAME for a variable of a non-reentrant outer function, get
77884:      * the variable's slot directly for the active call object. We always need
77884:      * to check for undefined, however.
77884:      */
77884:     if (cx->typeInferenceEnabled()) {
77884:         ScriptAnalysis::NameAccess access =
77884:             analysis->resolveNameAccess(cx, ATOM_TO_JSID(atom), true);
77884:         if (access.nesting) {
77884:             Address address = frame.loadNameAddress(access);
77884:             JSValueType type = knownPushedType(0);
77884:             BarrierState barrier = pushAddressMaybeBarrier(address, type, true,
77884:                                                            /* testUndefined = */ true);
77884:             finishBarrier(barrier, REJOIN_GETTER, 0);
77884:             if (isCall)
77884:                 jsop_callgname_epilogue();
77884:             return;
77884:         }
77884:     }
77884: 
71340:     PICGenInfo pic(isCall ? ic::PICInfo::CALLNAME : ic::PICInfo::NAME, JSOp(*PC), true);
53054: 
60594:     RESERVE_IC_SPACE(masm);
60594: 
53054:     pic.shapeReg = frame.allocReg();
53054:     pic.objReg = frame.allocReg();
53054:     pic.typeReg = Registers::ReturnReg;
53054:     pic.atom = atom;
53054:     pic.hasTypeCheck = false;
53270:     pic.fastPathStart = masm.label();
53054: 
76194:     RejoinState rejoin = isCall ? REJOIN_FALLTHROUGH : REJOIN_GETTER;
76194: 
60594:     /* There is no inline implementation, so we always jump to the slow path or to a stub. */
53054:     pic.shapeGuard = masm.label();
60594:     Jump inlineJump = masm.jump();
53054:     {
60594:         RESERVE_OOL_SPACE(stubcc.masm);
60594:         pic.slowPathStart = stubcc.linkExit(inlineJump, Uses(0));
53054:         stubcc.leave();
56738:         passICAddress(&pic);
76194:         pic.slowPathCall = OOL_STUBCALL(isCall ? ic::CallName : ic::Name, rejoin);
60594:         CHECK_OOL_SPACE();
78456:         testPushedType(rejoin, 0);
60594:     }
56575:     pic.fastPathRejoin = masm.label();
60594: 
60594:     /* Initialize op labels. */
60594:     ScopeNameLabels &labels = pic.scopeNameLabels();
60594:     labels.setInlineJump(masm, pic.fastPathStart, inlineJump);
60594: 
78455:     CHECK_IC_SPACE();
78455: 
76194:     /*
76194:      * We can't optimize away the PIC for the NAME access itself, but if we've
76194:      * only seen a single value pushed by this access, mark it as such and
76194:      * recompile if a different value becomes possible.
76194:      */
76194:     JSObject *singleton = pushedSingleton(0);
76194:     if (singleton) {
76194:         frame.push(ObjectValue(*singleton));
76194:         frame.freeReg(pic.shapeReg);
76194:         frame.freeReg(pic.objReg);
76194:     } else {
76194:         frame.pushRegs(pic.shapeReg, pic.objReg, type);
76194:     }
71340:     if (isCall)
76194:         frame.pushSynced(JSVAL_TYPE_UNKNOWN);
76194:     BarrierState barrier = testBarrier(pic.shapeReg, pic.objReg, /* testUndefined = */ true);
71340: 
71340:     stubcc.rejoin(Changes(isCall ? 2 : 1));
53054: 
53054:     pics.append(pic);
76194: 
76194:     finishBarrier(barrier, rejoin, isCall ? 1 : 0);
53054: }
53054: 
56037: bool
54847: mjit::Compiler::jsop_xname(JSAtom *atom)
54847: {
77884:     /*
77884:      * If this is a GETXPROP for a variable of a non-reentrant outer function,
77884:      * treat in the same way as a NAME.
77884:      */
77884:     if (cx->typeInferenceEnabled()) {
77884:         ScriptAnalysis::NameAccess access =
77884:             analysis->resolveNameAccess(cx, ATOM_TO_JSID(atom), true);
77884:         if (access.nesting) {
77884:             frame.pop();
77884:             Address address = frame.loadNameAddress(access);
77884:             JSValueType type = knownPushedType(0);
77884:             BarrierState barrier = pushAddressMaybeBarrier(address, type, true,
77884:                                                            /* testUndefined = */ true);
77884:             finishBarrier(barrier, REJOIN_GETTER, 0);
77884:             return true;
77884:         }
77884:     }
77884: 
56738:     PICGenInfo pic(ic::PICInfo::XNAME, JSOp(*PC), true);
54847: 
54847:     FrameEntry *fe = frame.peek(-1);
54847:     if (fe->isNotType(JSVAL_TYPE_OBJECT)) {
76194:         return jsop_getprop(atom, knownPushedType(0));
54847:     }
54847: 
54847:     if (!fe->isTypeKnown()) {
54847:         Jump notObject = frame.testObject(Assembler::NotEqual, fe);
54847:         stubcc.linkExit(notObject, Uses(1));
54847:     }
54847: 
76194:     frame.forgetMismatchedObject(fe);
76194: 
60594:     RESERVE_IC_SPACE(masm);
60594: 
54847:     pic.shapeReg = frame.allocReg();
54847:     pic.objReg = frame.copyDataIntoReg(fe);
54847:     pic.typeReg = Registers::ReturnReg;
54847:     pic.atom = atom;
54847:     pic.hasTypeCheck = false;
54847:     pic.fastPathStart = masm.label();
54847: 
60594:     /* There is no inline implementation, so we always jump to the slow path or to a stub. */
54847:     pic.shapeGuard = masm.label();
60594:     Jump inlineJump = masm.jump();
54847:     {
60594:         RESERVE_OOL_SPACE(stubcc.masm);
60594:         pic.slowPathStart = stubcc.linkExit(inlineJump, Uses(1));
54847:         stubcc.leave();
56738:         passICAddress(&pic);
76194:         pic.slowPathCall = OOL_STUBCALL(ic::XName, REJOIN_GETTER);
60594:         CHECK_OOL_SPACE();
78456:         testPushedType(REJOIN_GETTER, -1);
56575:     }
56575: 
56575:     pic.fastPathRejoin = masm.label();
60594: 
60594:     RETURN_IF_OOM(false);
60594: 
60594:     /* Initialize op labels. */
60598:     ScopeNameLabels &labels = pic.scopeNameLabels();
60594:     labels.setInlineJumpOffset(masm.differenceBetween(pic.fastPathStart, inlineJump));
60594: 
78455:     CHECK_IC_SPACE();
78455: 
54847:     frame.pop();
76194:     frame.pushRegs(pic.shapeReg, pic.objReg, knownPushedType(0));
76194: 
76194:     BarrierState barrier = testBarrier(pic.shapeReg, pic.objReg, /* testUndefined = */ true);
54847: 
54847:     stubcc.rejoin(Changes(1));
54847: 
54847:     pics.append(pic);
76194: 
76194:     finishBarrier(barrier, REJOIN_FALLTHROUGH, 0);
56037:     return true;
54847: }
54847: 
53055: void
60526: mjit::Compiler::jsop_bindname(JSAtom *atom, bool usePropCache)
53055: {
77884:     /*
77884:      * If this is a BINDNAME for a variable of a non-reentrant outer function,
77884:      * the object is definitely the outer function's active call object.
77884:      */
77884:     if (cx->typeInferenceEnabled()) {
77884:         ScriptAnalysis::NameAccess access =
77884:             analysis->resolveNameAccess(cx, ATOM_TO_JSID(atom), true);
77884:         if (access.nesting) {
77884:             RegisterID reg = frame.allocReg();
77884:             JSObject **pobj = &access.nesting->activeCall;
77884:             masm.move(ImmPtr(pobj), reg);
77884:             masm.loadPtr(Address(reg), reg);
77884:             frame.pushTypedPayload(JSVAL_TYPE_OBJECT, reg);
77884:             return;
77884:         }
77884:     }
77884: 
56738:     PICGenInfo pic(ic::PICInfo::BIND, JSOp(*PC), usePropCache);
53055: 
56586:     // This code does not check the frame flags to see if scopeChain has been
56586:     // set. Rather, it relies on the up-front analysis statically determining
56586:     // whether BINDNAME can be used, which reifies the scope chain at the
56586:     // prologue.
56602:     JS_ASSERT(analysis->usesScopeChain());
56586: 
53055:     pic.shapeReg = frame.allocReg();
53055:     pic.objReg = frame.allocReg();
53055:     pic.typeReg = Registers::ReturnReg;
60526:     pic.atom = atom;
53055:     pic.hasTypeCheck = false;
60593: 
60593:     RESERVE_IC_SPACE(masm);
53270:     pic.fastPathStart = masm.label();
53055: 
53244:     Address parent(pic.objReg, offsetof(JSObject, parent));
69223:     masm.loadPtr(Address(JSFrameReg, StackFrame::offsetOfScopeChain()), pic.objReg);
53055: 
53055:     pic.shapeGuard = masm.label();
68931:     Jump inlineJump = masm.branchPtr(Assembler::NotEqual, parent, ImmPtr(0));
53055:     {
60593:         RESERVE_OOL_SPACE(stubcc.masm);
60593:         pic.slowPathStart = stubcc.linkExit(inlineJump, Uses(0));
53055:         stubcc.leave();
56738:         passICAddress(&pic);
77409:         pic.slowPathCall = OOL_STUBCALL(ic::BindName, REJOIN_FALLTHROUGH);
60593:         CHECK_OOL_SPACE();
56575:     }
56575: 
56575:     pic.fastPathRejoin = masm.label();
60593: 
60593:     /* Initialize op labels. */
60593:     BindNameLabels &labels = pic.bindNameLabels();
60593:     labels.setInlineJump(masm, pic.shapeGuard, inlineJump);
60593: 
53055:     frame.pushTypedPayload(JSVAL_TYPE_OBJECT, pic.objReg);
53055:     frame.freeReg(pic.shapeReg);
53055: 
53088:     stubcc.rejoin(Changes(1));
53055: 
53055:     pics.append(pic);
53055: }
60598: 
60598: #else /* !JS_POLYIC */
60598: 
52880: void
76194: mjit::Compiler::jsop_name(JSAtom *atom, JSValueType type, bool isCall)
53054: {
53087:     prepareStubCall(Uses(0));
76194:     INLINE_STUBCALL(isCall ? stubs::CallName : stubs::Name, REJOIN_FALLTHROUGH);
78456:     testPushedType(REJOIN_FALLTHROUGH, 0, /* ool = */ false);
76194:     frame.pushSynced(type);
71340:     if (isCall)
76194:         frame.pushSynced(JSVAL_TYPE_UNKNOWN);
53054: }
53054: 
56037: bool
54860: mjit::Compiler::jsop_xname(JSAtom *atom)
52880: {
76194:     return jsop_getprop(atom, knownPushedType(0), pushedTypeSet(0));
54860: }
60598: 
56037: bool
76194: mjit::Compiler::jsop_getprop(JSAtom *atom, JSValueType knownType, types::TypeSet *typeSet,
76194:                              bool typecheck, bool usePropCache)
54427: {
56201:     jsop_getprop_slow(atom, usePropCache);
56037:     return true;
52886: }
60598: 
53119: bool
52896: mjit::Compiler::jsop_callprop(JSAtom *atom)
52896: {
53119:     return jsop_callprop_slow(atom);
52896: }
60598: 
56037: bool
56201: mjit::Compiler::jsop_setprop(JSAtom *atom, bool usePropCache)
54427: {
56201:     jsop_setprop_slow(atom, usePropCache);
56037:     return true;
54427: }
60598: 
52896: void
60526: mjit::Compiler::jsop_bindname(JSAtom *atom, bool usePropCache)
53055: {
53055:     RegisterID reg = frame.allocReg();
69223:     Address scopeChain(JSFrameReg, StackFrame::offsetOfScopeChain());
53162:     masm.loadPtr(scopeChain, reg);
53055: 
53246:     Address address(reg, offsetof(JSObject, parent));
53246: 
68931:     Jump j = masm.branchPtr(Assembler::NotEqual, address, ImmPtr(0));
53055: 
53088:     stubcc.linkExit(j, Uses(0));
53055:     stubcc.leave();
56201:     if (usePropCache) {
77409:         OOL_STUBCALL(stubs::BindName, REJOIN_FALLTHROUGH);
56201:     } else {
60526:         stubcc.masm.move(ImmPtr(atom), Registers::ArgReg1);
77409:         OOL_STUBCALL(stubs::BindNameNoCache, REJOIN_FALLTHROUGH);
56201:     }
53055: 
53055:     frame.pushTypedPayload(JSVAL_TYPE_OBJECT, reg);
53055: 
53088:     stubcc.rejoin(Changes(1));
53055: }
52880: #endif
52880: 
52743: void
52741: mjit::Compiler::jsop_this()
52741: {
57787:     frame.pushThis();
57787: 
55713:     /*
55713:      * In strict mode code, we don't wrap 'this'.
55713:      * In direct-call eval code, we wrapped 'this' before entering the eval.
55713:      * In global code, 'this' is always an object.
55713:      */
77391:     if (script->hasFunction && !script->strictModeCode) {
57787:         FrameEntry *thisFe = frame.peek(-1);
76194: 
76194:         /*
76194:          * We don't inline calls to scripts which use 'this' but might require
76194:          * 'this' to be wrapped.
76194:          */
76194:         JS_ASSERT(!thisFe->isNotType(JSVAL_TYPE_OBJECT));
76194: 
76194:         if (!thisFe->isType(JSVAL_TYPE_OBJECT)) {
76194:             JSValueType type = cx->typeInferenceEnabled()
77391:                 ? types::TypeScript::ThisTypes(script)->getKnownTypeTag(cx)
76194:                 : JSVAL_TYPE_UNKNOWN;
76194:             if (type != JSVAL_TYPE_OBJECT) {
57787:                 Jump notObj = frame.testObject(Assembler::NotEqual, thisFe);
55713:                 stubcc.linkExit(notObj, Uses(1));
52854:                 stubcc.leave();
76194:                 OOL_STUBCALL(stubs::This, REJOIN_FALLTHROUGH);
53088:                 stubcc.rejoin(Changes(1));
76194:             }
57787: 
77890:             /*
77890:              * Watch out for an obscure case where we don't know we are pushing
77890:              * an object: the script has not yet had a 'this' value assigned,
77890:              * so no pushed 'this' type has been inferred. Don't mark the type
77890:              * as known in this case, preserving the invariant that compiler
77890:              * types reflect inferred types.
77890:              */
77890:             if (cx->typeInferenceEnabled() && knownPushedType(0) != JSVAL_TYPE_OBJECT)
77890:                 return;
77890: 
57787:             // Now we know that |this| is an object.
57787:             frame.pop();
76194:             frame.learnThisIsObject(type != JSVAL_TYPE_OBJECT);
57787:             frame.pushThis();
57787:         }
57787: 
57787:         JS_ASSERT(thisFe->isType(JSVAL_TYPE_OBJECT));
52741:     }
53023: }
52741: 
61055: bool
53404: mjit::Compiler::iter(uintN flags)
53404: {
53404:     FrameEntry *fe = frame.peek(-1);
53404: 
53404:     /*
53404:      * Stub the call if this is not a simple 'for in' loop or if the iterated
53404:      * value is known to not be an object.
53404:      */
53404:     if ((flags != JSITER_ENUMERATE) || fe->isNotType(JSVAL_TYPE_OBJECT)) {
53404:         prepareStubCall(Uses(1));
53404:         masm.move(Imm32(flags), Registers::ArgReg1);
76194:         INLINE_STUBCALL(stubs::Iter, REJOIN_FALLTHROUGH);
53404:         frame.pop();
76194:         frame.pushSynced(JSVAL_TYPE_UNKNOWN);
61055:         return true;
53404:     }
53404: 
53404:     if (!fe->isTypeKnown()) {
53404:         Jump notObject = frame.testObject(Assembler::NotEqual, fe);
53404:         stubcc.linkExit(notObject, Uses(1));
53404:     }
53404: 
76194:     frame.forgetMismatchedObject(fe);
76194: 
53404:     RegisterID reg = frame.tempRegForData(fe);
53404: 
53404:     frame.pinReg(reg);
53404:     RegisterID ioreg = frame.allocReg();  /* Will hold iterator JSObject */
53404:     RegisterID nireg = frame.allocReg();  /* Will hold NativeIterator */
53404:     RegisterID T1 = frame.allocReg();
53404:     RegisterID T2 = frame.allocReg();
53404:     frame.unpinReg(reg);
53404: 
59954:     /* Fetch the most recent iterator. */
77659:     masm.loadPtr(&script->compartment()->nativeIterCache.last, ioreg);
53404: 
53404:     /* Test for NULL. */
53404:     Jump nullIterator = masm.branchTest32(Assembler::Zero, ioreg, ioreg);
53404:     stubcc.linkExit(nullIterator, Uses(1));
53404: 
60777:     /* Get NativeIterator from iter obj. */
60777:     masm.loadObjPrivate(ioreg, nireg);
53404: 
53404:     /* Test for active iterator. */
53404:     Address flagsAddr(nireg, offsetof(NativeIterator, flags));
53404:     masm.load32(flagsAddr, T1);
62573:     Jump activeIterator = masm.branchTest32(Assembler::NonZero, T1,
62573:                                             Imm32(JSITER_ACTIVE|JSITER_UNREUSABLE));
53404:     stubcc.linkExit(activeIterator, Uses(1));
53404: 
53404:     /* Compare shape of object with iterator. */
53404:     masm.loadShape(reg, T1);
53404:     masm.loadPtr(Address(nireg, offsetof(NativeIterator, shapes_array)), T2);
53404:     masm.load32(Address(T2, 0), T2);
53404:     Jump mismatchedObject = masm.branch32(Assembler::NotEqual, T1, T2);
53404:     stubcc.linkExit(mismatchedObject, Uses(1));
53404: 
53404:     /* Compare shape of object's prototype with iterator. */
77353:     masm.loadPtr(Address(reg, JSObject::offsetOfType()), T1);
76194:     masm.loadPtr(Address(T1, offsetof(types::TypeObject, proto)), T1);
53404:     masm.loadShape(T1, T1);
53404:     masm.loadPtr(Address(nireg, offsetof(NativeIterator, shapes_array)), T2);
53404:     masm.load32(Address(T2, sizeof(uint32)), T2);
53404:     Jump mismatchedProto = masm.branch32(Assembler::NotEqual, T1, T2);
53404:     stubcc.linkExit(mismatchedProto, Uses(1));
53404: 
53404:     /*
53404:      * Compare object's prototype's prototype with NULL. The last native
53404:      * iterator will always have a prototype chain length of one
53404:      * (i.e. it must be a plain object), so we do not need to generate
53404:      * a loop here.
53404:      */
77353:     masm.loadPtr(Address(reg, JSObject::offsetOfType()), T1);
76194:     masm.loadPtr(Address(T1, offsetof(types::TypeObject, proto)), T1);
77353:     masm.loadPtr(Address(T1, JSObject::offsetOfType()), T1);
76194:     masm.loadPtr(Address(T1, offsetof(types::TypeObject, proto)), T1);
53404:     Jump overlongChain = masm.branchPtr(Assembler::NonZero, T1, T1);
53404:     stubcc.linkExit(overlongChain, Uses(1));
53404: 
53404:     /* Found a match with the most recent iterator. Hooray! */
53404: 
53404:     /* Mark iterator as active. */
60777:     masm.storePtr(reg, Address(nireg, offsetof(NativeIterator, obj)));
53404:     masm.load32(flagsAddr, T1);
53404:     masm.or32(Imm32(JSITER_ACTIVE), T1);
53404:     masm.store32(T1, flagsAddr);
53404: 
53404:     /* Chain onto the active iterator stack. */
53404:     masm.loadPtr(FrameAddress(offsetof(VMFrame, cx)), T1);
53404:     masm.loadPtr(Address(T1, offsetof(JSContext, enumerators)), T2);
53404:     masm.storePtr(T2, Address(nireg, offsetof(NativeIterator, next)));
53404:     masm.storePtr(ioreg, Address(T1, offsetof(JSContext, enumerators)));
53404: 
53404:     frame.freeReg(nireg);
53404:     frame.freeReg(T1);
53404:     frame.freeReg(T2);
53404: 
53404:     stubcc.leave();
53404:     stubcc.masm.move(Imm32(flags), Registers::ArgReg1);
76194:     OOL_STUBCALL(stubs::Iter, REJOIN_FALLTHROUGH);
53404: 
53404:     /* Push the iterator object. */
53404:     frame.pop();
53404:     frame.pushTypedPayload(JSVAL_TYPE_OBJECT, ioreg);
53404: 
53404:     stubcc.rejoin(Changes(1));
61055: 
61055:     return true;
53404: }
53404: 
52737: /*
74052:  * This big nasty function implements JSOP_ITERNEXT, which is used in the head
74052:  * of a for-in loop to put the next value on the stack.
52737:  */
52737: void
77824: mjit::Compiler::iterNext(ptrdiff_t offset)
77824: {
77824:     FrameEntry *fe = frame.peek(-offset);
52737:     RegisterID reg = frame.tempRegForData(fe);
52737: 
52737:     /* Is it worth trying to pin this longer? Prolly not. */
52737:     frame.pinReg(reg);
52737:     RegisterID T1 = frame.allocReg();
52737:     frame.unpinReg(reg);
52737: 
52737:     /* Test clasp */
77817:     Jump notFast = masm.testObjClass(Assembler::NotEqual, reg, &IteratorClass);
53088:     stubcc.linkExit(notFast, Uses(1));
52737: 
53178:     /* Get private from iter obj. */
60777:     masm.loadObjPrivate(reg, T1);
52737: 
52737:     RegisterID T3 = frame.allocReg();
52973:     RegisterID T4 = frame.allocReg();
52737: 
61055:     /* Test for a value iterator, which could come through an Iterator object. */
52737:     masm.load32(Address(T1, offsetof(NativeIterator, flags)), T3);
53841:     notFast = masm.branchTest32(Assembler::NonZero, T3, Imm32(JSITER_FOREACH));
53088:     stubcc.linkExit(notFast, Uses(1));
52973: 
52973:     RegisterID T2 = frame.allocReg();
52973: 
52973:     /* Get cursor. */
52973:     masm.loadPtr(Address(T1, offsetof(NativeIterator, props_cursor)), T2);
52973: 
52973:     /* Test if the jsid is a string. */
52973:     masm.loadPtr(T2, T3);
52973:     masm.move(T3, T4);
52973:     masm.andPtr(Imm32(JSID_TYPE_MASK), T4);
52973:     notFast = masm.branchTestPtr(Assembler::NonZero, T4, T4);
53088:     stubcc.linkExit(notFast, Uses(1));
52737: 
52737:     /* It's safe to increase the cursor now. */
52973:     masm.addPtr(Imm32(sizeof(jsid)), T2, T4);
52973:     masm.storePtr(T4, Address(T1, offsetof(NativeIterator, props_cursor)));
52973: 
52973:     frame.freeReg(T4);
52737:     frame.freeReg(T1);
52973:     frame.freeReg(T2);
52737: 
52737:     stubcc.leave();
77824:     stubcc.masm.move(Imm32(offset), Registers::ArgReg1);
77397:     OOL_STUBCALL(stubs::IterNext, REJOIN_FALLTHROUGH);
52737: 
53025:     frame.pushUntypedPayload(JSVAL_TYPE_STRING, T3);
52737: 
52737:     /* Join with the stub call. */
53088:     stubcc.rejoin(Changes(1));
52737: }
52737: 
56766: bool
76194: mjit::Compiler::iterMore(jsbytecode *target)
76194: {
76194:     if (!frame.syncForBranch(target, Uses(1)))
76194:         return false;
76194: 
52737:     FrameEntry *fe = frame.peek(-1);
52737:     RegisterID reg = frame.tempRegForData(fe);
76194:     RegisterID tempreg = frame.allocReg();
52737: 
52737:     /* Test clasp */
77817:     Jump notFast = masm.testObjClass(Assembler::NotEqual, reg, &IteratorClass);
53277:     stubcc.linkExitForBranch(notFast);
52737: 
53178:     /* Get private from iter obj. */
76194:     masm.loadObjPrivate(reg, reg);
52737: 
61055:     /* Test that the iterator supports fast iteration. */
76194:     notFast = masm.branchTest32(Assembler::NonZero, Address(reg, offsetof(NativeIterator, flags)),
61055:                                 Imm32(JSITER_FOREACH));
61055:     stubcc.linkExitForBranch(notFast);
61055: 
52737:     /* Get props_cursor, test */
76194:     masm.loadPtr(Address(reg, offsetof(NativeIterator, props_cursor)), tempreg);
76194:     masm.loadPtr(Address(reg, offsetof(NativeIterator, props_end)), reg);
76194: 
76194:     Jump jFast = masm.branchPtr(Assembler::LessThan, tempreg, reg);
52737: 
52737:     stubcc.leave();
76194:     OOL_STUBCALL(stubs::IterMore, REJOIN_BRANCH);
53133:     Jump j = stubcc.masm.branchTest32(Assembler::NonZero, Registers::ReturnReg,
53133:                                       Registers::ReturnReg);
53133: 
53088:     stubcc.rejoin(Changes(1));
76194:     frame.freeReg(tempreg);
53133: 
56766:     return jumpAndTrace(jFast, target, &j);
52737: }
52737: 
52815: void
53404: mjit::Compiler::iterEnd()
53404: {
53404:     FrameEntry *fe= frame.peek(-1);
53404:     RegisterID reg = frame.tempRegForData(fe);
53404: 
53404:     frame.pinReg(reg);
53404:     RegisterID T1 = frame.allocReg();
53404:     frame.unpinReg(reg);
53404: 
53404:     /* Test clasp */
77817:     Jump notIterator = masm.testObjClass(Assembler::NotEqual, reg, &IteratorClass);
53404:     stubcc.linkExit(notIterator, Uses(1));
53404: 
60777:     /* Get private from iter obj. */
60777:     masm.loadObjPrivate(reg, T1);
53404: 
53404:     RegisterID T2 = frame.allocReg();
53404: 
53404:     /* Load flags. */
53404:     Address flagAddr(T1, offsetof(NativeIterator, flags));
53404:     masm.loadPtr(flagAddr, T2);
53404: 
62573:     /* Test for a normal enumerate iterator. */
62573:     Jump notEnumerate = masm.branchTest32(Assembler::Zero, T2, Imm32(JSITER_ENUMERATE));
62413:     stubcc.linkExit(notEnumerate, Uses(1));
53404: 
53404:     /* Clear active bit. */
53404:     masm.and32(Imm32(~JSITER_ACTIVE), T2);
53404:     masm.storePtr(T2, flagAddr);
53404: 
53404:     /* Reset property cursor. */
53404:     masm.loadPtr(Address(T1, offsetof(NativeIterator, props_array)), T2);
53404:     masm.storePtr(T2, Address(T1, offsetof(NativeIterator, props_cursor)));
53404: 
53404:     /* Advance enumerators list. */
53404:     masm.loadPtr(FrameAddress(offsetof(VMFrame, cx)), T2);
53404:     masm.loadPtr(Address(T1, offsetof(NativeIterator, next)), T1);
53404:     masm.storePtr(T1, Address(T2, offsetof(JSContext, enumerators)));
53404: 
53404:     frame.freeReg(T1);
53404:     frame.freeReg(T2);
53404: 
53404:     stubcc.leave();
76194:     OOL_STUBCALL(stubs::EndIter, REJOIN_FALLTHROUGH);
53404: 
53404:     frame.pop();
53404: 
53404:     stubcc.rejoin(Changes(1));
53404: }
53404: 
53404: void
52826: mjit::Compiler::jsop_getgname_slow(uint32 index)
52826: {
53087:     prepareStubCall(Uses(0));
76194:     INLINE_STUBCALL(stubs::GetGlobalName, REJOIN_GETTER);
78456:     testPushedType(REJOIN_GETTER, 0, /* ool = */ false);
76194:     frame.pushSynced(JSVAL_TYPE_UNKNOWN);
52826: }
52826: 
52826: void
52826: mjit::Compiler::jsop_bindgname()
52826: {
76194:     if (globalObj) {
53081:         frame.push(ObjectValue(*globalObj));
52826:         return;
52826:     }
52826: 
52826:     /* :TODO: this is slower than it needs to be. */
53087:     prepareStubCall(Uses(0));
76194:     INLINE_STUBCALL(stubs::BindGlobalName, REJOIN_NONE);
52826:     frame.takeReg(Registers::ReturnReg);
53025:     frame.pushTypedPayload(JSVAL_TYPE_OBJECT, Registers::ReturnReg);
52826: }
52826: 
52826: void
52826: mjit::Compiler::jsop_getgname(uint32 index)
52826: {
57823:     /* Optimize undefined, NaN and Infinity. */
57823:     JSAtom *atom = script->getAtom(index);
57823:     if (atom == cx->runtime->atomState.typeAtoms[JSTYPE_VOID]) {
57823:         frame.push(UndefinedValue());
57823:         return;
57823:     }
57823:     if (atom == cx->runtime->atomState.NaNAtom) {
57823:         frame.push(cx->runtime->NaNValue);
57823:         return;
57823:     }
57823:     if (atom == cx->runtime->atomState.InfinityAtom) {
57823:         frame.push(cx->runtime->positiveInfinityValue);
57823:         return;
57823:     }
76194: 
76194:     /* Optimize singletons like Math for JSOP_CALLPROP. */
76194:     JSObject *obj = pushedSingleton(0);
76194:     if (obj && !hasTypeBarriers(PC) && testSingletonProperty(globalObj, ATOM_TO_JSID(atom))) {
76194:         frame.push(ObjectValue(*obj));
76194:         return;
76194:     }
76194: 
76194:     jsid id = ATOM_TO_JSID(atom);
77357:     JSValueType type = knownPushedType(0);
76194:     if (cx->typeInferenceEnabled() && globalObj->isGlobal() && id == types::MakeTypeId(cx, id) &&
77361:         !globalObj->getType(cx)->unknownProperties()) {
77361:         types::TypeSet *propertyTypes = globalObj->getType(cx)->getProperty(cx, id, false);
76194:         if (!propertyTypes)
76194:             return;
77357: 
77357:         /*
77357:          * If we are accessing a defined global which is a normal data property
77357:          * then bake its address into the jitcode and guard against future
77357:          * reallocation of the global object's slots.
77357:          */
78194:         const js::Shape *shape = globalObj->nativeLookup(cx, ATOM_TO_JSID(atom));
76194:         if (shape && shape->hasDefaultGetterOrIsMethod() && shape->hasSlot()) {
76194:             Value *value = &globalObj->getSlotRef(shape->slot);
77361:             if (!value->isUndefined() &&
77361:                 !propertyTypes->isOwnProperty(cx, globalObj->getType(cx), true)) {
76194:                 watchGlobalReallocation();
76194:                 RegisterID reg = frame.allocReg();
76194:                 masm.move(ImmPtr(value), reg);
76194: 
76194:                 BarrierState barrier = pushAddressMaybeBarrier(Address(reg), type, true);
76194:                 finishBarrier(barrier, REJOIN_GETTER, 0);
76194:                 return;
76194:             }
76194:         }
76194:     }
76194: 
53119: #if defined JS_MONOIC
52826:     jsop_bindgname();
52826: 
52826:     FrameEntry *fe = frame.peek(-1);
53025:     JS_ASSERT(fe->isTypeKnown() && fe->getKnownType() == JSVAL_TYPE_OBJECT);
52826: 
62386:     GetGlobalNameICInfo ic;
58198:     RESERVE_IC_SPACE(masm);
52826:     RegisterID objReg;
52826:     Jump shapeGuard;
52826: 
62386:     ic.usePropertyCache = true;
62386: 
62386:     ic.fastPathStart = masm.label();
52826:     if (fe->isConstant()) {
53081:         JSObject *obj = &fe->getValue().toObject();
52826:         frame.pop();
52826:         JS_ASSERT(obj->isNative());
52826: 
52826:         objReg = frame.allocReg();
52826: 
53531:         masm.load32FromImm(&obj->objShape, objReg);
53408:         shapeGuard = masm.branch32WithPatch(Assembler::NotEqual, objReg,
68935:                                             Imm32(int32(INVALID_SHAPE)), ic.shape);
52826:         masm.move(ImmPtr(obj), objReg);
52826:     } else {
52826:         objReg = frame.ownRegForData(fe);
52826:         frame.pop();
52826:         RegisterID reg = frame.allocReg();
52826: 
53445:         masm.loadShape(objReg, reg);
53408:         shapeGuard = masm.branch32WithPatch(Assembler::NotEqual, reg,
68935:                                             Imm32(int32(INVALID_SHAPE)), ic.shape);
52826:         frame.freeReg(reg);
52826:     }
62386:     stubcc.linkExit(shapeGuard, Uses(0));
52826: 
52826:     stubcc.leave();
62386:     passMICAddress(ic);
76194:     ic.slowPathCall = OOL_STUBCALL(ic::GetGlobalName, REJOIN_GETTER);
52826: 
78456:     CHECK_IC_SPACE();
78456: 
78456:     testPushedType(REJOIN_GETTER, 0);
78456: 
52826:     /* Garbage value. */
52826:     uint32 slot = 1 << 24;
52826: 
55746:     masm.loadPtr(Address(objReg, offsetof(JSObject, slots)), objReg);
52826:     Address address(objReg, slot);
53269: 
53269:     /* Allocate any register other than objReg. */
60592:     RegisterID treg = frame.allocReg();
53269:     /* After dreg is loaded, it's safe to clobber objReg. */
60592:     RegisterID dreg = objReg;
53269: 
62386:     ic.load = masm.loadValueWithAddressOffsetPatch(address, treg, dreg);
53269: 
76194:     frame.pushRegs(treg, dreg, type);
76194: 
76194:     /*
76194:      * Note: no undefined check is needed for GNAME opcodes. These were not
76194:      * declared with 'var', so cannot be undefined without triggering an error
76194:      * or having been a pre-existing global whose value is undefined (which
76194:      * type inference will know about).
76194:      */
76194:     BarrierState barrier = testBarrier(treg, dreg);
52826: 
53088:     stubcc.rejoin(Changes(1));
62385: 
62386:     getGlobalNames.append(ic);
76194:     finishBarrier(barrier, REJOIN_GETTER, 0);
53269: 
52826: #else
52826:     jsop_getgname_slow(index);
52826: #endif
76194: 
52826: }
52826: 
63236: /*
63236:  * Generate just the epilogue code that is specific to callgname. The rest
63236:  * is shared with getgname.
63236:  */
63236: void
63236: mjit::Compiler::jsop_callgname_epilogue()
63236: {
63236:     /*
63236:      * This slow path does the same thing as the interpreter.
63236:      */
76194:     if (!globalObj) {
63236:         prepareStubCall(Uses(1));
76194:         INLINE_STUBCALL(stubs::PushImplicitThisForGlobal, REJOIN_NONE);
76194:         frame.pushSynced(JSVAL_TYPE_UNKNOWN);
63236:         return;
63236:     }
63236: 
63236:     /* Fast path for known-not-an-object callee. */
63236:     FrameEntry *fval = frame.peek(-1);
63236:     if (fval->isNotType(JSVAL_TYPE_OBJECT)) {
63236:         frame.push(UndefinedValue());
63236:         return;
63236:     }
63236: 
76194:     /* Paths for known object callee. */
76194:     if (fval->isConstant()) {
76194:         JSObject *obj = &fval->getValue().toObject();
77884:         if (obj->getGlobal() == globalObj) {
76194:             frame.push(UndefinedValue());
76194:         } else {
76194:             prepareStubCall(Uses(1));
76194:             INLINE_STUBCALL(stubs::PushImplicitThisForGlobal, REJOIN_NONE);
76194:             frame.pushSynced(JSVAL_TYPE_UNKNOWN);
76194:         }
76194:         return;
76194:     }
76194: 
63236:     /*
77884:      * Fast path for functions whose global is statically known to be the
77884:      * current global. This is primarily for calls on inner functions within
77884:      * nestings, whose direct parent is a call object rather than the global
77884:      * and which will make a stub call in the path below.
77884:      */
77884:     if (cx->typeInferenceEnabled()) {
77884:         types::TypeSet *types = analysis->pushedTypes(PC, 0);
77884:         if (types->hasGlobalObject(cx, globalObj)) {
77884:             frame.push(UndefinedValue());
77884:             return;
77884:         }
77884:     }
77884: 
77884:     /*
63236:      * Optimized version. This inlines the common case, calling a
63236:      * (non-proxied) function that has the same global as the current
63236:      * script. To make the code simpler, we:
63236:      *      1. test the stronger property that the callee's parent is
63236:      *         equal to the global of the current script, and
63236:      *      2. bake in the global of the current script, which is why
63236:      *         this optimized path requires compile-and-go.
63236:      */
63236: 
63236:     /* If the callee is not an object, jump to the inline fast path. */
63236:     MaybeRegisterID typeReg = frame.maybePinType(fval);
63236:     RegisterID objReg = frame.copyDataIntoReg(fval);
63236: 
63236:     MaybeJump isNotObj;
63236:     if (!fval->isType(JSVAL_TYPE_OBJECT)) {
63236:         isNotObj = frame.testObject(Assembler::NotEqual, fval);
63236:         frame.maybeUnpinReg(typeReg);
63236:     }
63236: 
63236:     /*
63236:      * If the callee is not a function, jump to OOL slow path.
63236:      */
63236:     Jump notFunction = masm.testFunction(Assembler::NotEqual, objReg);
63236:     stubcc.linkExit(notFunction, Uses(1));
63236: 
63236:     /*
63236:      * If the callee's parent is not equal to the global, jump to
63236:      * OOL slow path.
63236:      */
63236:     masm.loadPtr(Address(objReg, offsetof(JSObject, parent)), objReg);
63236:     Jump globalMismatch = masm.branchPtr(Assembler::NotEqual, objReg, ImmPtr(globalObj));
63236:     stubcc.linkExit(globalMismatch, Uses(1));
63236:     frame.freeReg(objReg);
63236: 
63236:     /* OOL stub call path. */
63236:     stubcc.leave();
76194:     OOL_STUBCALL(stubs::PushImplicitThisForGlobal, REJOIN_NONE);
63236: 
63236:     /* Fast path. */
63236:     if (isNotObj.isSet())
63236:         isNotObj.getJump().linkTo(masm.label(), &masm);
63236:     frame.pushUntypedValue(UndefinedValue());
63236: 
63236:     stubcc.rejoin(Changes(1));
63236: }
63236: 
52831: void
60526: mjit::Compiler::jsop_setgname_slow(JSAtom *atom, bool usePropertyCache)
52831: {
53087:     prepareStubCall(Uses(2));
52831:     masm.move(ImmPtr(atom), Registers::ArgReg1);
57784:     if (usePropertyCache)
76194:         INLINE_STUBCALL(STRICT_VARIANT(stubs::SetGlobalName), REJOIN_FALLTHROUGH);
57784:     else
76194:         INLINE_STUBCALL(STRICT_VARIANT(stubs::SetGlobalNameNoCache), REJOIN_FALLTHROUGH);
52831:     frame.popn(2);
76194:     pushSyncedEntry(0);
52831: }
52831: 
52831: void
76194: mjit::Compiler::jsop_setgname(JSAtom *atom, bool usePropertyCache, bool popGuaranteed)
76194: {
76194:     if (monitored(PC)) {
76194:         /* Global accesses are monitored only for a few names like __proto__. */
76194:         jsop_setgname_slow(atom, usePropertyCache);
76194:         return;
76194:     }
76194: 
76194:     jsid id = ATOM_TO_JSID(atom);
76194:     if (cx->typeInferenceEnabled() && globalObj->isGlobal() && id == types::MakeTypeId(cx, id) &&
77361:         !globalObj->getType(cx)->unknownProperties()) {
76194:         /*
76194:          * Note: object branding is disabled when inference is enabled. With
76194:          * branding there is no way to ensure that a non-function property
76194:          * can't get a function later and cause the global object to become
76194:          * branded, requiring a shape change if it changes again.
76194:          */
77361:         types::TypeSet *types = globalObj->getType(cx)->getProperty(cx, id, false);
76194:         if (!types)
76194:             return;
78194:         const js::Shape *shape = globalObj->nativeLookup(cx, ATOM_TO_JSID(atom));
76194:         if (shape && !shape->isMethod() && shape->hasDefaultSetter() &&
77361:             shape->writable() && shape->hasSlot() &&
77361:             !types->isOwnProperty(cx, globalObj->getType(cx), true)) {
76194:             watchGlobalReallocation();
76194:             Value *value = &globalObj->getSlotRef(shape->slot);
76194:             RegisterID reg = frame.allocReg();
76194:             masm.move(ImmPtr(value), reg);
76194:             frame.storeTo(frame.peek(-1), Address(reg), popGuaranteed);
76194:             frame.shimmy(1);
76194:             frame.freeReg(reg);
76194:             return;
76194:         }
76194:     }
76194: 
53119: #if defined JS_MONOIC
52831:     FrameEntry *objFe = frame.peek(-2);
62385:     FrameEntry *fe = frame.peek(-1);
53025:     JS_ASSERT_IF(objFe->isTypeKnown(), objFe->getKnownType() == JSVAL_TYPE_OBJECT);
52831: 
76194:     if (!fe->isConstant() && fe->isType(JSVAL_TYPE_DOUBLE))
76194:         frame.forgetKnownDouble(fe);
76194: 
62386:     SetGlobalNameICInfo ic;
62386: 
62386:     frame.pinEntry(fe, ic.vr);
62386:     Jump shapeGuard;
62385: 
58198:     RESERVE_IC_SPACE(masm);
62386:     ic.fastPathStart = masm.label();
52831:     if (objFe->isConstant()) {
53081:         JSObject *obj = &objFe->getValue().toObject();
52831:         JS_ASSERT(obj->isNative());
52831: 
62386:         ic.objReg = frame.allocReg();
62386:         ic.shapeReg = ic.objReg;
62386:         ic.objConst = true;
62386: 
62386:         masm.load32FromImm(&obj->objShape, ic.shapeReg);
62386:         shapeGuard = masm.branch32WithPatch(Assembler::NotEqual, ic.shapeReg,
68935:                                             Imm32(int32(INVALID_SHAPE)),
62386:                                             ic.shape);
62386:         masm.move(ImmPtr(obj), ic.objReg);
52831:     } else {
62386:         ic.objReg = frame.copyDataIntoReg(objFe);
62386:         ic.shapeReg = frame.allocReg();
62386:         ic.objConst = false;
62386: 
62386:         masm.loadShape(ic.objReg, ic.shapeReg);
62386:         shapeGuard = masm.branch32WithPatch(Assembler::NotEqual, ic.shapeReg,
68935:                                             Imm32(int32(INVALID_SHAPE)),
62386:                                             ic.shape);
62386:         frame.freeReg(ic.shapeReg);
62386:     }
62386:     ic.shapeGuardJump = shapeGuard;
62386:     ic.slowPathStart = stubcc.linkExit(shapeGuard, Uses(2));
52831: 
52831:     stubcc.leave();
62386:     passMICAddress(ic);
76194:     ic.slowPathCall = OOL_STUBCALL(ic::SetGlobalName, REJOIN_FALLTHROUGH);
52831: 
52831:     /* Garbage value. */
52831:     uint32 slot = 1 << 24;
52831: 
62386:     ic.usePropertyCache = usePropertyCache;
62386: 
62386:     masm.loadPtr(Address(ic.objReg, offsetof(JSObject, slots)), ic.objReg);
62386:     Address address(ic.objReg, slot);
62386: 
62386:     if (ic.vr.isConstant()) {
62386:         ic.store = masm.storeValueWithAddressOffsetPatch(ic.vr.value(), address);
62386:     } else if (ic.vr.isTypeKnown()) {
62386:         ic.store = masm.storeValueWithAddressOffsetPatch(ImmType(ic.vr.knownType()),
62386:                                                           ic.vr.dataReg(), address);
52831:     } else {
62386:         ic.store = masm.storeValueWithAddressOffsetPatch(ic.vr.typeReg(), ic.vr.dataReg(), address);
62386:     }
62386: 
62386:     frame.freeReg(ic.objReg);
62386:     frame.unpinEntry(ic.vr);
62385:     frame.shimmy(1);
52831: 
53088:     stubcc.rejoin(Changes(1));
52831: 
62386:     ic.fastPathRejoin = masm.label();
62386:     setGlobalNames.append(ic);
52831: #else
60526:     jsop_setgname_slow(atom, usePropertyCache);
52831: #endif
52831: }
52831: 
52838: void
52838: mjit::Compiler::jsop_setelem_slow()
52838: {
53087:     prepareStubCall(Uses(3));
76194:     INLINE_STUBCALL(STRICT_VARIANT(stubs::SetElem), REJOIN_FALLTHROUGH);
52838:     frame.popn(3);
76194:     frame.pushSynced(JSVAL_TYPE_UNKNOWN);
52838: }
52838: 
52843: void
52843: mjit::Compiler::jsop_getelem_slow()
52843: {
53087:     prepareStubCall(Uses(2));
76194:     INLINE_STUBCALL(stubs::GetElem, REJOIN_FALLTHROUGH);
78456:     testPushedType(REJOIN_FALLTHROUGH, -2, /* ool = */ false);
52843:     frame.popn(2);
76194:     pushSyncedEntry(0);
52843: }
52843: 
52874: void
52874: mjit::Compiler::jsop_unbrand()
52874: {
53087:     prepareStubCall(Uses(1));
76194:     INLINE_STUBCALL(stubs::Unbrand, REJOIN_FALLTHROUGH);
52874: }
52874: 
56037: bool
52894: mjit::Compiler::jsop_instanceof()
52894: {
53124:     FrameEntry *lhs = frame.peek(-2);
52894:     FrameEntry *rhs = frame.peek(-1);
52894: 
53124:     // The fast path applies only when both operands are objects.
53124:     if (rhs->isNotType(JSVAL_TYPE_OBJECT) || lhs->isNotType(JSVAL_TYPE_OBJECT)) {
76194:         stubcc.linkExit(masm.jump(), Uses(2));
76194:         frame.discardFe(lhs);
76194:         frame.discardFe(rhs);
52894:     }
52894: 
53124:     MaybeJump firstSlow;
53124:     if (!rhs->isTypeKnown()) {
53023:         Jump j = frame.testObject(Assembler::NotEqual, rhs);
53088:         stubcc.linkExit(j, Uses(2));
64390:     }
64390: 
76194:     frame.forgetMismatchedObject(lhs);
76194:     frame.forgetMismatchedObject(rhs);
76194: 
64390:     RegisterID obj = frame.tempRegForData(rhs);
64390:     Jump notFunction = masm.testFunction(Assembler::NotEqual, obj);
64390:     stubcc.linkExit(notFunction, Uses(2));
54410: 
54410:     /* Test for bound functions. */
54410:     Jump isBound = masm.branchTest32(Assembler::NonZero, Address(obj, offsetof(JSObject, flags)),
54410:                                      Imm32(JSObject::BOUND_FUNCTION));
54410:     {
54410:         stubcc.linkExit(isBound, Uses(2));
52894:         stubcc.leave();
76194:         OOL_STUBCALL(stubs::InstanceOf, REJOIN_FALLTHROUGH);
52911:         firstSlow = stubcc.masm.jump();
52911:     }
52911: 
54410: 
52911:     /* This is sadly necessary because the error case needs the object. */
52894:     frame.dup();
52894: 
76194:     if (!jsop_getprop(cx->runtime->atomState.classPrototypeAtom, JSVAL_TYPE_UNKNOWN, false))
56037:         return false;
52894: 
52911:     /* Primitive prototypes are invalid. */
52911:     rhs = frame.peek(-1);
52911:     Jump j = frame.testPrimitive(Assembler::Equal, rhs);
53088:     stubcc.linkExit(j, Uses(3));
52911: 
52911:     /* Allocate registers up front, because of branchiness. */
54410:     obj = frame.copyDataIntoReg(lhs);
52911:     RegisterID proto = frame.copyDataIntoReg(rhs);
52911:     RegisterID temp = frame.allocReg();
52911: 
53124:     MaybeJump isFalse;
53124:     if (!lhs->isTypeKnown())
53124:         isFalse = frame.testPrimitive(Assembler::Equal, lhs);
52911: 
52911:     Label loop = masm.label();
52911: 
52911:     /* Walk prototype chain, break out on NULL or hit. */
77353:     masm.loadPtr(Address(obj, JSObject::offsetOfType()), obj);
76194:     masm.loadPtr(Address(obj, offsetof(types::TypeObject, proto)), obj);
52911:     Jump isFalse2 = masm.branchTestPtr(Assembler::Zero, obj, obj);
52911:     Jump isTrue = masm.branchPtr(Assembler::NotEqual, obj, proto);
52911:     isTrue.linkTo(loop, &masm);
52911:     masm.move(Imm32(1), temp);
52911:     isTrue = masm.jump();
52911: 
53124:     if (isFalse.isSet())
53124:         isFalse.getJump().linkTo(masm.label(), &masm);
52911:     isFalse2.linkTo(masm.label(), &masm);
52911:     masm.move(Imm32(0), temp);
52911:     isTrue.linkTo(masm.label(), &masm);
52911: 
52911:     frame.freeReg(proto);
52911:     frame.freeReg(obj);
52911: 
52911:     stubcc.leave();
76194:     OOL_STUBCALL(stubs::FastInstanceOf, REJOIN_FALLTHROUGH);
52911: 
52894:     frame.popn(3);
53025:     frame.pushTypedPayload(JSVAL_TYPE_BOOLEAN, temp);
52911: 
53124:     if (firstSlow.isSet())
53124:         firstSlow.getJump().linkTo(stubcc.masm.label(), &stubcc.masm);
53088:     stubcc.rejoin(Changes(1));
56037:     return true;
52894: }
52894: 
56556: void
56775: mjit::Compiler::emitEval(uint32 argc)
56556: {
56775:     /* Check for interrupts on function call */
56775:     interruptCheckHelper();
56775: 
76194:     frame.syncAndKill(Uses(argc + 2));
56775:     prepareStubCall(Uses(argc + 2));
56775:     masm.move(Imm32(argc), Registers::ArgReg1);
76194:     INLINE_STUBCALL(stubs::Eval, REJOIN_FALLTHROUGH);
56775:     frame.popn(argc + 2);
76194:     pushSyncedEntry(0);
56556: }
56556: 
57718: void
76194: mjit::Compiler::jsop_arguments(RejoinState rejoin)
57718: {
57718:     prepareStubCall(Uses(0));
76194:     INLINE_STUBCALL(stubs::Arguments, rejoin);
76194: }
76194: 
76194: bool
58056: mjit::Compiler::jsop_newinit()
58056: {
58056:     bool isArray;
58056:     unsigned count = 0;
58056:     JSObject *baseobj = NULL;
58056:     switch (*PC) {
58056:       case JSOP_NEWINIT:
58056:         isArray = (PC[1] == JSProto_Array);
58056:         break;
58056:       case JSOP_NEWARRAY:
58056:         isArray = true;
58056:         count = GET_UINT24(PC);
58056:         break;
58056:       case JSOP_NEWOBJECT:
76194:         /*
76194:          * Scripts with NEWOBJECT must be compileAndGo, but treat these like
76194:          * NEWINIT if the script's associated global is not known (or is not
76194:          * actually a global object). This should only happen in chrome code.
76194:          */
58056:         isArray = false;
76194:         baseobj = globalObj ? script->getObject(fullAtomIndex(PC)) : NULL;
58056:         break;
58056:       default:
58056:         JS_NOT_REACHED("Bad op");
76194:         return false;
74457:     }
74457: 
77413:     void *stub, *stubArg;
77413:     if (isArray) {
77413:         stub = JS_FUNC_TO_DATA_PTR(void *, stubs::NewInitArray);
77413:         stubArg = (void *) count;
77413:     } else {
77413:         stub = JS_FUNC_TO_DATA_PTR(void *, stubs::NewInitObject);
77413:         stubArg = (void *) baseobj;
77413:     }
76194: 
76194:     /* Don't bake in types for non-compileAndGo scripts. */
76194:     types::TypeObject *type = NULL;
76194:     if (globalObj) {
77391:         type = types::TypeScript::InitObject(cx, script, PC,
77391:                                              isArray ? JSProto_Array : JSProto_Object);
76194:         if (!type)
76194:             return false;
76194:     }
77413: 
77413:     if (!cx->typeInferenceEnabled() ||
77413:         !globalObj ||
77413:         (isArray && count >= gc::GetGCKindSlots(gc::FINALIZE_OBJECT_LAST)) ||
77413:         (!isArray && !baseobj) ||
77413:         (!isArray && baseobj->hasSlotsArray())) {
58056:         prepareStubCall(Uses(0));
76194:         masm.storePtr(ImmPtr(type), FrameAddress(offsetof(VMFrame, scratch)));
77413:         masm.move(ImmPtr(stubArg), Registers::ArgReg1);
77413:         INLINE_STUBCALL(stub, REJOIN_FALLTHROUGH);
77413:         frame.pushSynced(JSVAL_TYPE_OBJECT);
77413: 
77413:         frame.extra(frame.peek(-1)).initArray = (*PC == JSOP_NEWARRAY);
77413:         frame.extra(frame.peek(-1)).initObject = baseobj;
77413: 
77413:         return true;
77413:     }
77413: 
77413:     JSObject *templateObject;
58056:     if (isArray) {
77413:         templateObject = NewDenseUnallocatedArray(cx, count);
77413:         if (!templateObject)
77413:             return false;
77413:         templateObject->setType(type);
58056:     } else {
77413:         templateObject = CopyInitializerObject(cx, baseobj, type);
77413:         if (!templateObject)
77413:             return false;
77413:     }
77413: 
77413:     RegisterID result = frame.allocReg();
77413:     Jump emptyFreeList = masm.getNewObject(cx, result, templateObject);
77413: 
77413:     stubcc.linkExit(emptyFreeList, Uses(0));
77413:     stubcc.leave();
77413: 
77413:     stubcc.masm.storePtr(ImmPtr(type), FrameAddress(offsetof(VMFrame, scratch)));
77413:     stubcc.masm.move(ImmPtr(stubArg), Registers::ArgReg1);
77413:     OOL_STUBCALL(stub, REJOIN_FALLTHROUGH);
77413: 
77413:     frame.pushTypedPayload(JSVAL_TYPE_OBJECT, result);
77413: 
77413:     stubcc.rejoin(Changes(1));
76194: 
76194:     frame.extra(frame.peek(-1)).initArray = (*PC == JSOP_NEWARRAY);
76194:     frame.extra(frame.peek(-1)).initObject = baseobj;
76194: 
76194:     return true;
76194: }
76194: 
76194: bool
76194: mjit::Compiler::startLoop(jsbytecode *head, Jump entry, jsbytecode *entryTarget)
76194: {
76194:     JS_ASSERT(cx->typeInferenceEnabled() && script == outerScript);
76194: 
76194:     if (loop) {
76194:         /*
76194:          * Convert all loop registers in the outer loop into unassigned registers.
76194:          * We don't keep track of which registers the inner loop uses, so the only
76194:          * registers that can be carried in the outer loop must be mentioned before
76194:          * the inner loop starts.
76194:          */
76194:         loop->clearLoopRegisters();
76194:     }
76194: 
76194:     LoopState *nloop = cx->new_<LoopState>(cx, &ssa, this, &frame);
76194:     if (!nloop || !nloop->init(head, entry, entryTarget))
76194:         return false;
76194: 
76194:     nloop->outer = loop;
76194:     loop = nloop;
76194:     frame.setLoop(loop);
76194: 
76194:     return true;
76194: }
76194: 
76194: bool
76194: mjit::Compiler::finishLoop(jsbytecode *head)
76194: {
76194:     if (!cx->typeInferenceEnabled())
76194:         return true;
76194: 
76194:     /*
76194:      * We're done processing the current loop. Every loop has exactly one backedge
76194:      * at the end ('continue' statements are forward jumps to the loop test),
76194:      * and after jumpAndTrace'ing on that edge we can pop it from the frame.
76194:      */
76194:     JS_ASSERT(loop && loop->headOffset() == uint32(head - script->code));
76194: 
76194:     jsbytecode *entryTarget = script->code + loop->entryOffset();
76194: 
76194:     /*
76194:      * Fix up the jump entering the loop. We are doing this after all code has
76194:      * been emitted for the backedge, so that we are now in the loop's fallthrough
76194:      * (where we will emit the entry code).
76194:      */
76194:     Jump fallthrough = masm.jump();
76194: 
76194: #ifdef DEBUG
76194:     if (IsJaegerSpewChannelActive(JSpew_Regalloc)) {
76194:         RegisterAllocation *alloc = analysis->getAllocation(head);
77530:         JaegerSpew(JSpew_Regalloc, "loop allocation at %u:", unsigned(head - script->code));
76194:         frame.dumpAllocation(alloc);
76194:     }
76194: #endif
76194: 
76194:     loop->entryJump().linkTo(masm.label(), &masm);
76194: 
76194:     jsbytecode *oldPC = PC;
76194: 
76194:     PC = entryTarget;
76194:     {
76194:         OOL_STUBCALL(stubs::MissedBoundsCheckEntry, REJOIN_RESUME);
76194: 
76194:         if (loop->generatingInvariants()) {
76194:             /*
76194:              * To do the initial load of the invariants, jump to the invariant
76194:              * restore point after the call just emitted. :XXX: fix hackiness.
76194:              */
76194:             if (oomInVector)
76194:                 return false;
76194:             Label label = callSites[callSites.length() - 1].loopJumpLabel;
76194:             stubcc.linkExitDirect(masm.jump(), label);
76194:         }
76194:         stubcc.crossJump(stubcc.masm.jump(), masm.label());
76194:     }
76194:     PC = oldPC;
76194: 
76194:     frame.prepareForJump(entryTarget, masm, true);
76194: 
76194:     if (!jumpInScript(masm.jump(), entryTarget))
76194:         return false;
76194: 
76194:     PC = head;
76194:     if (!analysis->getCode(head).safePoint) {
76194:         /*
76194:          * Emit a stub into the OOL path which loads registers from a synced state
76194:          * and jumps to the loop head, for rejoining from the interpreter.
76194:          */
76194:         LoopEntry entry;
76194:         entry.pcOffset = head - script->code;
76194: 
76194:         OOL_STUBCALL(stubs::MissedBoundsCheckHead, REJOIN_RESUME);
76194: 
76194:         if (loop->generatingInvariants()) {
76194:             if (oomInVector)
76194:                 return false;
76194:             entry.label = callSites[callSites.length() - 1].loopJumpLabel;
76194:         } else {
76194:             entry.label = stubcc.masm.label();
76194:         }
76194: 
76194:         /*
76194:          * The interpreter may store integers in slots we assume are doubles,
76194:          * make sure state is consistent before joining. Note that we don't
76194:          * need any handling for other safe points the interpreter can enter
76194:          * from, i.e. from switch and try blocks, as we don't assume double
76194:          * variables are coherent in such cases.
76194:          */
76194:         for (uint32 slot = ArgSlot(0); slot < TotalSlots(script); slot++) {
76194:             if (a->varTypes[slot].type == JSVAL_TYPE_DOUBLE) {
76194:                 FrameEntry *fe = frame.getSlotEntry(slot);
76194:                 stubcc.masm.ensureInMemoryDouble(frame.addressOf(fe));
76194:             }
76194:         }
76194: 
76194:         frame.prepareForJump(head, stubcc.masm, true);
76194:         if (!stubcc.jumpInScript(stubcc.masm.jump(), head))
76194:             return false;
76194: 
76194:         loopEntries.append(entry);
76194:     }
76194:     PC = oldPC;
76194: 
76194:     /* Write out loads and tests of loop invariants at all calls in the loop body. */
76194:     loop->flushLoop(stubcc);
76194: 
76194:     LoopState *nloop = loop->outer;
76194:     cx->delete_(loop);
76194:     loop = nloop;
76194:     frame.setLoop(loop);
76194: 
76194:     fallthrough.linkTo(masm.label(), &masm);
76194: 
76194:     /*
76194:      * Clear all registers used for loop temporaries. In the case of loop
76194:      * nesting, we do not allocate temporaries for the outer loop.
76194:      */
76194:     frame.clearTemporaries();
76194: 
76194:     return true;
58056: }
58056: 
53133: /*
53133:  * Note: This function emits tracer hooks into the OOL path. This means if
53460:  * it is used in the middle of an in-progress slow path, the stream will be
53133:  * hopelessly corrupted. Take care to only call this before linkExits() and
53133:  * after rejoin()s.
76194:  *
76194:  * The state at the fast jump must reflect the frame's current state. If specified
76194:  * the state at the slow jump must be fully synced.
76194:  *
76194:  * The 'trampoline' argument indicates whether a trampoline was emitted into
76194:  * the OOL path loading some registers for the target. If this is the case,
76194:  * the fast path jump was redirected to the stub code's initial label, and the
76194:  * same must happen for any other fast paths for the target (i.e. paths from
76194:  * inline caches).
53133:  */
56766: bool
76194: mjit::Compiler::jumpAndTrace(Jump j, jsbytecode *target, Jump *slow, bool *trampoline)
76194: {
76194:     if (trampoline)
76194:         *trampoline = false;
76194: 
76194:     /*
76194:      * Unless we are coming from a branch which synced everything, syncForBranch
76194:      * must have been called and ensured an allocation at the target.
76194:      */
76194:     RegisterAllocation *lvtarget = NULL;
76194:     bool consistent = true;
76194:     if (cx->typeInferenceEnabled()) {
76194:         RegisterAllocation *&alloc = analysis->getAllocation(target);
76194:         if (!alloc) {
79410:             alloc = cx->typeLifoAlloc().new_<RegisterAllocation>(false);
76194:             if (!alloc)
56766:                 return false;
76194:         }
76194:         lvtarget = alloc;
76194:         consistent = frame.consistentRegisters(target);
76194:     }
76194: 
59895:     if (!addTraceHints || target >= PC ||
59895:         (JSOp(*target) != JSOP_TRACE && JSOp(*target) != JSOP_NOTRACE)
56218: #ifdef JS_MONOIC
56218:         || GET_UINT16(target) == BAD_TRACEIC_INDEX
56218: #endif
56218:         )
56217:     {
76194:         if (!lvtarget || lvtarget->synced()) {
76194:             JS_ASSERT(consistent);
56766:             if (!jumpInScript(j, target))
56766:                 return false;
59895:             if (slow && !stubcc.jumpInScript(*slow, target))
59895:                 return false;
76194:         } else {
76194:             if (consistent) {
76194:                 if (!jumpInScript(j, target))
76194:                     return false;
76194:             } else {
76194:                 /*
76194:                  * Make a trampoline to issue remaining loads for the register
76194:                  * state at target.
76194:                  */
77407:                 Label start = stubcc.masm.label();
77407:                 stubcc.linkExitDirect(j, start);
76194:                 frame.prepareForJump(target, stubcc.masm, false);
76194:                 if (!stubcc.jumpInScript(stubcc.masm.jump(), target))
76194:                     return false;
76194:                 if (trampoline)
76194:                     *trampoline = true;
80222:                 if (pcLengths) {
77407:                     /*
77407:                      * This is OOL code but will usually be executed, so track
77407:                      * it in the CODE_LENGTH for the opcode.
77407:                      */
77407:                     uint32 offset = ssa.frameLength(a->inlineIndex) + PC - script->code;
77407:                     size_t length = stubcc.masm.size() - stubcc.masm.distanceOf(start);
77407:                     pcLengths[offset].codeLength += length;
77407:                 }
76194:             }
76194: 
76194:             if (slow) {
76194:                 slow->linkTo(stubcc.masm.label(), &stubcc.masm);
76194:                 frame.prepareForJump(target, stubcc.masm, true);
76194:                 if (!stubcc.jumpInScript(stubcc.masm.jump(), target))
76194:                     return false;
76194:             }
76194:         }
76194: 
76194:         if (target < PC)
76194:             return finishLoop(target);
56766:         return true;
53133:     }
53133: 
76194:     /* The trampoline should not be specified if we need to generate a trace IC. */
76194:     JS_ASSERT(!trampoline);
76194: 
76194: #ifndef JS_TRACER
76194:     JS_NOT_REACHED("Bad addTraceHints");
76194:     return false;
76194: #else
76194: 
53133: # if JS_MONOIC
56217:     TraceGenInfo ic;
56217: 
56220:     ic.initialized = true;
56217:     ic.stubEntry = stubcc.masm.label();
56217:     ic.traceHint = j;
56192:     if (slow)
56217:         ic.slowTraceHint = *slow;
56217: 
56217:     uint16 index = GET_UINT16(target);
56217:     if (traceICs.length() <= index)
57679:         if (!traceICs.resize(index+1))
57679:             return false;
53133: # endif
53133: 
54160:     Label traceStart = stubcc.masm.label();
54160: 
54160:     stubcc.linkExitDirect(j, traceStart);
56192:     if (slow)
56192:         slow->linkTo(traceStart, &stubcc.masm);
59895: 
53133: # if JS_MONOIC
56217:     ic.addrLabel = stubcc.masm.moveWithPatch(ImmPtr(NULL), Registers::ArgReg1);
60534: 
60534:     Jump nonzero = stubcc.masm.branchSub32(Assembler::NonZero, Imm32(1),
60534:                                            Address(Registers::ArgReg1,
60534:                                                    offsetof(TraceICInfo, loopCounter)));
53133: # endif
53133: 
53133:     /* Save and restore compiler-tracked PC, so cx->regs is right in InvokeTracer. */
53133:     {
53133:         jsbytecode* pc = PC;
53133:         PC = target;
53133: 
78389:         OOL_STUBCALL(stubs::InvokeTracer, REJOIN_NONE);
53133: 
53133:         PC = pc;
53133:     }
53133: 
53133:     Jump no = stubcc.masm.branchTestPtr(Assembler::Zero, Registers::ReturnReg,
53133:                                         Registers::ReturnReg);
76194:     if (!cx->typeInferenceEnabled())
76194:         stubcc.masm.loadPtr(FrameAddress(VMFrame::offsetOfFp), JSFrameReg);
76194:     stubcc.masm.jump(Registers::ReturnReg);
76194:     no.linkTo(stubcc.masm.label(), &stubcc.masm);
76194: 
76194: #ifdef JS_MONOIC
76194:     nonzero.linkTo(stubcc.masm.label(), &stubcc.masm);
76194: 
76194:     ic.jumpTarget = target;
76194:     ic.fastTrampoline = !consistent;
76194:     ic.trampolineStart = stubcc.masm.label();
76194: 
76194:     traceICs[index] = ic;
76194: #endif
76194: 
76194:     /*
76194:      * Jump past the tracer call if the trace has been blacklisted. We still make
76194:      * a trace IC in such cases, in case it is un-blacklisted later.
76194:      */
76194:     if (JSOp(*target) == JSOP_NOTRACE) {
76194:         if (consistent) {
76194:             if (!jumpInScript(j, target))
60534:                 return false;
76194:         } else {
76194:             stubcc.linkExitDirect(j, stubcc.masm.label());
76194:         }
76194:         if (slow)
76194:             slow->linkTo(stubcc.masm.label(), &stubcc.masm);
76194:     }
76194: 
76194:     /*
76194:      * Reload any registers needed at the head of the loop. Note that we didn't
76194:      * need to do syncing before calling InvokeTracer, as state is always synced
76194:      * on backwards jumps.
76194:      */
76194:     frame.prepareForJump(target, stubcc.masm, true);
76194: 
76194:     if (!stubcc.jumpInScript(stubcc.masm.jump(), target))
76194:         return false;
53133: #endif
76194: 
76194:     return finishLoop(target);
54427: }
54427: 
54840: void
54840: mjit::Compiler::enterBlock(JSObject *obj)
54840: {
54840:     /* For now, don't bother doing anything for this opcode. */
54840:     frame.syncAndForgetEverything();
54840:     masm.move(ImmPtr(obj), Registers::ArgReg1);
54840:     uint32 n = js_GetEnterBlockStackDefs(cx, script, PC);
76194:     INLINE_STUBCALL(stubs::EnterBlock, REJOIN_NONE);
54840:     frame.enterBlock(n);
54840: }
54840: 
54840: void
54840: mjit::Compiler::leaveBlock()
54840: {
54840:     /*
54840:      * Note: After bug 535912, we can pass the block obj directly, inline
54840:      * PutBlockObject, and do away with the muckiness in PutBlockObject.
54840:      */
54840:     uint32 n = js_GetVariableStackUses(JSOP_LEAVEBLOCK, PC);
54855:     JSObject *obj = script->getObject(fullAtomIndex(PC + UINT16_LEN));
54840:     prepareStubCall(Uses(n));
54855:     masm.move(ImmPtr(obj), Registers::ArgReg1);
76194:     INLINE_STUBCALL(stubs::LeaveBlock, REJOIN_NONE);
54840:     frame.leaveBlock(n);
54840: }
54840: 
55503: // Creates the new object expected for constructors, and places it in |thisv|.
55503: // It is broken down into the following operations:
55503: //   CALLEE
55503: //   GETPROP "prototype"
55503: //   IFPRIMTOP:
55503: //       NULL
55503: //   call js_CreateThisFromFunctionWithProto(...)
55503: //
56037: bool
55503: mjit::Compiler::constructThis()
55503: {
55503:     JS_ASSERT(isConstructing);
55503: 
77415:     JSFunction *fun = script->function();
77415: 
78457:     do {
78457:         if (!cx->typeInferenceEnabled() || fun->getType(cx)->unknownProperties())
78457:             break;
78457: 
77413:         jsid id = ATOM_TO_JSID(cx->runtime->atomState.classPrototypeAtom);
77415:         types::TypeSet *protoTypes = fun->getType(cx)->getProperty(cx, id, false);
77413: 
77413:         JSObject *proto = protoTypes->getSingleton(cx, true);
78457:         if (!proto)
78457:             break;
78457: 
78457:         /*
78457:          * Generate an inline path to create a 'this' object with the given
78457:          * prototype. Only do this if the type is actually known as a possible
78457:          * 'this' type of the script.
78457:          */
78457:         types::TypeObject *type = proto->getNewType(cx, fun);
78457:         if (!type)
78457:             return false;
78457:         if (!types::TypeScript::ThisTypes(script)->hasType(types::Type::ObjectType(type)))
78457:             break;
78457: 
77415:         JSObject *templateObject = js_CreateThisForFunctionWithProto(cx, fun, proto);
77414:         if (!templateObject)
77414:             return false;
77414: 
77414:         /*
77414:          * The template incorporates a shape and/or fixed slots from any
77414:          * newScript on its type, so make sure recompilation is triggered
77414:          * should this information change later.
77414:          */
77414:         if (templateObject->type()->newScript)
77414:             types::TypeSet::WatchObjectStateChange(cx, templateObject->type());
77413: 
77413:         RegisterID result = frame.allocReg();
77413:         Jump emptyFreeList = masm.getNewObject(cx, result, templateObject);
77413: 
77413:         stubcc.linkExit(emptyFreeList, Uses(0));
77413:         stubcc.leave();
77413: 
77413:         stubcc.masm.move(ImmPtr(proto), Registers::ArgReg1);
77413:         OOL_STUBCALL(stubs::CreateThis, REJOIN_RESUME);
77413: 
77413:         frame.setThis(result);
77413: 
77413:         stubcc.rejoin(Changes(1));
77413:         return true;
78457:     } while (false);
77413: 
55503:     // Load the callee.
57787:     frame.pushCallee();
55503: 
55503:     // Get callee.prototype.
76194:     if (!jsop_getprop(cx->runtime->atomState.classPrototypeAtom, JSVAL_TYPE_UNKNOWN, false, false))
56037:         return false;
55503: 
55503:     // Reach into the proto Value and grab a register for its data.
55503:     FrameEntry *protoFe = frame.peek(-1);
55503:     RegisterID protoReg = frame.ownRegForData(protoFe);
55503: 
55503:     // Now, get the type. If it's not an object, set protoReg to NULL.
76194:     JS_ASSERT_IF(protoFe->isTypeKnown(), protoFe->isType(JSVAL_TYPE_OBJECT));
76194:     if (!protoFe->isType(JSVAL_TYPE_OBJECT)) {
55503:         Jump isNotObject = frame.testObject(Assembler::NotEqual, protoFe);
55503:         stubcc.linkExitDirect(isNotObject, stubcc.masm.label());
55503:         stubcc.masm.move(ImmPtr(NULL), protoReg);
55503:         stubcc.crossJump(stubcc.masm.jump(), masm.label());
76194:     }
55503: 
55503:     // Done with the protoFe.
55503:     frame.pop();
55503: 
55503:     prepareStubCall(Uses(0));
55503:     if (protoReg != Registers::ArgReg1)
55503:         masm.move(protoReg, Registers::ArgReg1);
76194:     INLINE_STUBCALL(stubs::CreateThis, REJOIN_RESUME);
55503:     frame.freeReg(protoReg);
56037:     return true;
55503: }
55503: 
61233: bool
59979: mjit::Compiler::jsop_tableswitch(jsbytecode *pc)
59979: {
59979: #if defined JS_CPU_ARM
59979:     JS_NOT_REACHED("Implement jump(BaseIndex) for ARM");
61248:     return true;
59979: #else
59979:     jsbytecode *originalPC = pc;
77438:     JSOp op = JSOp(*originalPC);
77438:     JS_ASSERT(op == JSOP_TABLESWITCH || op == JSOP_TABLESWITCHX);
77438: 
77438:     uint32 defaultTarget = GetJumpOffset(pc, pc);
77438:     unsigned jumpLength = (op == JSOP_TABLESWITCHX) ? JUMPX_OFFSET_LEN : JUMP_OFFSET_LEN;
77438:     pc += jumpLength;
59979: 
59979:     jsint low = GET_JUMP_OFFSET(pc);
59979:     pc += JUMP_OFFSET_LEN;
59979:     jsint high = GET_JUMP_OFFSET(pc);
59979:     pc += JUMP_OFFSET_LEN;
59979:     int numJumps = high + 1 - low;
59979:     JS_ASSERT(numJumps >= 0);
59979: 
59979:     /*
59979:      * If there are no cases, this is a no-op. The default case immediately
59979:      * follows in the bytecode and is always taken.
59979:      */
59979:     if (numJumps == 0) {
59979:         frame.pop();
61233:         return true;
59979:     }
59979: 
59979:     FrameEntry *fe = frame.peek(-1);
59979:     if (fe->isNotType(JSVAL_TYPE_INT32) || numJumps > 256) {
59979:         frame.syncAndForgetEverything();
59979:         masm.move(ImmPtr(originalPC), Registers::ArgReg1);
59979: 
59979:         /* prepareStubCall() is not needed due to forgetEverything() */
76194:         INLINE_STUBCALL(stubs::TableSwitch, REJOIN_NONE);
59979:         frame.pop();
59979:         masm.jump(Registers::ReturnReg);
61233:         return true;
59979:     }
59979: 
59979:     RegisterID dataReg;
59979:     if (fe->isConstant()) {
59979:         JS_ASSERT(fe->isType(JSVAL_TYPE_INT32));
59979:         dataReg = frame.allocReg();
59979:         masm.move(Imm32(fe->getValue().toInt32()), dataReg);
59979:     } else {
59979:         dataReg = frame.copyDataIntoReg(fe);
59979:     }
59979: 
59979:     RegisterID reg = frame.allocReg();
59979:     frame.syncAndForgetEverything();
59979: 
59979:     MaybeJump notInt;
59979:     if (!fe->isType(JSVAL_TYPE_INT32))
59979:         notInt = masm.testInt32(Assembler::NotEqual, frame.addressOf(fe));
59979: 
59979:     JumpTable jt;
59979:     jt.offsetIndex = jumpTableOffsets.length();
59979:     jt.label = masm.moveWithPatch(ImmPtr(NULL), reg);
59979:     jumpTables.append(jt);
59979: 
59979:     for (int i = 0; i < numJumps; i++) {
77438:         uint32 target = GetJumpOffset(originalPC, pc);
59979:         if (!target)
59979:             target = defaultTarget;
59979:         uint32 offset = (originalPC + target) - script->code;
59979:         jumpTableOffsets.append(offset);
77438:         pc += jumpLength;
59979:     }
59979:     if (low != 0)
59979:         masm.sub32(Imm32(low), dataReg);
59979:     Jump defaultCase = masm.branch32(Assembler::AboveOrEqual, dataReg, Imm32(numJumps));
59979:     BaseIndex jumpTarget(reg, dataReg, Assembler::ScalePtr);
59979:     masm.jump(jumpTarget);
59979: 
59979:     if (notInt.isSet()) {
59979:         stubcc.linkExitDirect(notInt.get(), stubcc.masm.label());
59979:         stubcc.leave();
59979:         stubcc.masm.move(ImmPtr(originalPC), Registers::ArgReg1);
76194:         OOL_STUBCALL(stubs::TableSwitch, REJOIN_NONE);
59979:         stubcc.masm.jump(Registers::ReturnReg);
59979:     }
59979:     frame.pop();
61233:     return jumpAndTrace(defaultCase, originalPC + defaultTarget);
59979: #endif
59979: }
59979: 
59979: void
57723: mjit::Compiler::jsop_callelem_slow()
57723: {
57723:     prepareStubCall(Uses(2));
76194:     INLINE_STUBCALL(stubs::CallElem, REJOIN_FALLTHROUGH);
78456:     testPushedType(REJOIN_FALLTHROUGH, -2, /* ool = */ false);
57723:     frame.popn(2);
76194:     pushSyncedEntry(0);
76194:     pushSyncedEntry(1);
73894: }
73894: 
73894: void
77357: mjit::Compiler::jsop_toid()
77357: {
77386:     /* Leave integers alone, stub everything else. */
77357:     FrameEntry *top = frame.peek(-1);
77357: 
77357:     if (top->isType(JSVAL_TYPE_INT32))
77357:         return;
77357: 
77357:     if (top->isNotType(JSVAL_TYPE_INT32)) {
77357:         prepareStubCall(Uses(2));
77357:         INLINE_STUBCALL(stubs::ToId, REJOIN_FALLTHROUGH);
77357:         frame.pop();
77357:         pushSyncedEntry(0);
77357:         return;
77357:     }
77357: 
77357:     frame.syncAt(-1);
77357: 
77357:     Jump j = frame.testInt32(Assembler::NotEqual, top);
77357:     stubcc.linkExit(j, Uses(2));
77357: 
77357:     stubcc.leave();
77357:     OOL_STUBCALL(stubs::ToId, REJOIN_FALLTHROUGH);
77357: 
77357:     frame.pop();
77357:     pushSyncedEntry(0);
77357: 
77357:     stubcc.rejoin(Changes(1));
77357: }
77357: 
76194: /*
76194:  * For any locals or args which we know to be integers but are treated as
76194:  * doubles by the type inference, convert to double. These will be assumed to be
76194:  * doubles at control flow join points. This function must be called before
76194:  * branching to another opcode.
76194:  *
76194:  * We can only carry entries as doubles when we can track all incoming edges to
76194:  * a join point (no try blocks etc.) and when we can track all writes to the
76194:  * local/arg (the slot does not escape) and ensure the Compiler representation
76194:  * matches the inferred type for the variable's SSA value. These properties are
76194:  * both ensured by analysis->trackSlot.
76194:  */
76194: void
76194: mjit::Compiler::fixDoubleTypes(jsbytecode *target)
76194: {
76194:     if (!cx->typeInferenceEnabled())
76194:         return;
76194: 
76194:     /*
77457:      * Fill fixedIntToDoubleEntries with all variables that are known to be an
77457:      * int here and a double at the branch target, and fixedDoubleToAnyEntries
77457:      * with all variables that are known to be a double here but not at the
77457:      * branch target.
77457:      *
77457:      * Per prepareInferenceTypes, the target state consists of the current
77457:      * state plus any phi nodes or other new values introduced at the target.
76194:      */
77457:     JS_ASSERT(fixedIntToDoubleEntries.empty());
77457:     JS_ASSERT(fixedDoubleToAnyEntries.empty());
76194:     const SlotValue *newv = analysis->newValues(target);
76194:     if (newv) {
76194:         while (newv->slot) {
76194:             if (newv->value.kind() != SSAValue::PHI ||
77457:                 newv->value.phiOffset() != uint32(target - script->code) ||
77457:                 !analysis->trackSlot(newv->slot)) {
76194:                 newv++;
76194:                 continue;
76194:             }
77457:             JS_ASSERT(newv->slot < TotalSlots(script));
76194:             types::TypeSet *targetTypes = analysis->getValueTypes(newv->value);
77457:             FrameEntry *fe = frame.getSlotEntry(newv->slot);
76194:             VarType &vt = a->varTypes[newv->slot];
77457:             if (targetTypes->getKnownTypeTag(cx) == JSVAL_TYPE_DOUBLE) {
76194:                 if (vt.type == JSVAL_TYPE_INT32) {
77457:                     fixedIntToDoubleEntries.append(newv->slot);
76194:                     frame.ensureDouble(fe);
77471:                     frame.forgetLoopReg(fe);
76194:                 } else if (vt.type == JSVAL_TYPE_UNKNOWN) {
76194:                     /*
76194:                      * Unknown here but a double at the target. The type
76194:                      * set for the existing value must be empty, so this
76194:                      * code is doomed and we can just mark the value as
76194:                      * a double.
76194:                      */
76194:                     frame.ensureDouble(fe);
76194:                 } else {
76194:                     JS_ASSERT(vt.type == JSVAL_TYPE_DOUBLE);
76194:                 }
77894:             } else if (vt.type == JSVAL_TYPE_DOUBLE) {
77457:                 fixedDoubleToAnyEntries.append(newv->slot);
77457:                 frame.syncAndForgetFe(fe);
77471:                 frame.forgetLoopReg(fe);
76194:             }
76194:             newv++;
76194:         }
76194:     }
76194: }
76194: 
76194: void
76194: mjit::Compiler::watchGlobalReallocation()
76194: {
76194:     JS_ASSERT(cx->typeInferenceEnabled());
76194:     if (hasGlobalReallocation)
76194:         return;
77414:     types::TypeSet::WatchObjectStateChange(cx, globalObj->getType(cx));
76194:     hasGlobalReallocation = true;
76194: }
76194: 
76194: void
76194: mjit::Compiler::updateVarType()
76194: {
76194:     if (!cx->typeInferenceEnabled())
76194:         return;
76194: 
76194:     /*
76194:      * For any non-escaping variable written at the current opcode, update the
76194:      * associated type sets according to the written type, keeping the type set
76194:      * for each variable in sync with what the SSA analysis has determined
76194:      * (see prepareInferenceTypes).
76194:      */
76194: 
77363:     types::TypeSet *types = pushedTypeSet(0);
76194:     uint32 slot = GetBytecodeSlot(script, PC);
76194: 
76194:     if (analysis->trackSlot(slot)) {
76194:         VarType &vt = a->varTypes[slot];
76194:         vt.types = types;
76194:         vt.type = types->getKnownTypeTag(cx);
76194: 
76194:         /*
76194:          * Variables whose type has been inferred as a double need to be
76194:          * maintained by the frame as a double. We might forget the exact
76194:          * representation used by the next call to fixDoubleTypes, fix it now.
76194:          */
76194:         if (vt.type == JSVAL_TYPE_DOUBLE)
76194:             frame.ensureDouble(frame.getSlotEntry(slot));
76194:     }
76194: }
76194: 
76194: void
76194: mjit::Compiler::updateJoinVarTypes()
76194: {
76194:     if (!cx->typeInferenceEnabled())
76194:         return;
76194: 
76194:     /* Update variable types for all new values at this bytecode. */
76194:     const SlotValue *newv = analysis->newValues(PC);
76194:     if (newv) {
76194:         while (newv->slot) {
76194:             if (newv->slot < TotalSlots(script)) {
76194:                 VarType &vt = a->varTypes[newv->slot];
76194:                 vt.types = analysis->getValueTypes(newv->value);
78457:                 JSValueType newType = vt.types->getKnownTypeTag(cx);
78457:                 if (newType != vt.type) {
78457:                     FrameEntry *fe = frame.getSlotEntry(newv->slot);
78457:                     frame.forgetLoopReg(fe);
78457:                 }
78457:                 vt.type = newType;
76194:             }
76194:             newv++;
76194:         }
76194:     }
76194: }
76194: 
76194: void
76194: mjit::Compiler::restoreVarType()
76194: {
76194:     if (!cx->typeInferenceEnabled())
76194:         return;
76194: 
76194:     uint32 slot = GetBytecodeSlot(script, PC);
76194: 
76194:     if (slot >= analyze::TotalSlots(script))
76194:         return;
76194: 
76194:     /*
76194:      * Restore the known type of a live local or argument. We ensure that types
76194:      * of tracked variables match their inferred type (as tracked in varTypes),
76194:      * but may have forgotten it due to a branch or syncAndForgetEverything.
76194:      */
76194:     JSValueType type = a->varTypes[slot].type;
76194:     if (type != JSVAL_TYPE_UNKNOWN &&
76194:         (type != JSVAL_TYPE_DOUBLE || analysis->trackSlot(slot))) {
76194:         FrameEntry *fe = frame.getSlotEntry(slot);
76194:         JS_ASSERT_IF(fe->isTypeKnown(), fe->isType(type));
76194:         if (!fe->isTypeKnown())
76194:             frame.learnType(fe, type, false);
76194:     }
76194: }
76194: 
76194: JSValueType
76194: mjit::Compiler::knownPushedType(uint32 pushed)
76194: {
76194:     if (!cx->typeInferenceEnabled())
76194:         return JSVAL_TYPE_UNKNOWN;
76194:     types::TypeSet *types = analysis->pushedTypes(PC, pushed);
76194:     return types->getKnownTypeTag(cx);
76194: }
76194: 
76194: bool
76194: mjit::Compiler::mayPushUndefined(uint32 pushed)
76194: {
76194:     JS_ASSERT(cx->typeInferenceEnabled());
76194: 
76194:     /*
76194:      * This should only be used when the compiler is checking if it is OK to push
76194:      * undefined without going to a stub that can trigger recompilation.
76194:      * If this returns false and undefined subsequently becomes a feasible
76194:      * value pushed by the bytecode, recompilation will *NOT* be triggered.
76194:      */
76194:     types::TypeSet *types = analysis->pushedTypes(PC, pushed);
77353:     return types->hasType(types::Type::UndefinedType());
76194: }
76194: 
76194: types::TypeSet *
76194: mjit::Compiler::pushedTypeSet(uint32 pushed)
76194: {
76194:     if (!cx->typeInferenceEnabled())
76194:         return NULL;
76194:     return analysis->pushedTypes(PC, pushed);
76194: }
76194: 
76194: bool
76194: mjit::Compiler::monitored(jsbytecode *pc)
76194: {
76194:     if (!cx->typeInferenceEnabled())
76194:         return false;
76194:     return analysis->getCode(pc).monitoredTypes;
76194: }
76194: 
76194: bool
76194: mjit::Compiler::hasTypeBarriers(jsbytecode *pc)
76194: {
76194:     if (!cx->typeInferenceEnabled())
76194:         return false;
76194: 
78194:     return analysis->typeBarriers(cx, pc) != NULL;
76194: }
76194: 
76194: void
76194: mjit::Compiler::pushSyncedEntry(uint32 pushed)
76194: {
76194:     frame.pushSynced(knownPushedType(pushed));
76194: }
76194: 
76194: JSObject *
76194: mjit::Compiler::pushedSingleton(unsigned pushed)
76194: {
76194:     if (!cx->typeInferenceEnabled())
76194:         return NULL;
76194: 
76194:     types::TypeSet *types = analysis->pushedTypes(PC, pushed);
76194:     return types->getSingleton(cx);
76194: }
76194: 
76194: bool
76194: mjit::Compiler::arrayPrototypeHasIndexedProperty()
76194: {
76194:     if (!cx->typeInferenceEnabled() || !globalObj)
76194:         return true;
76194: 
76194:     JSObject *proto;
76194:     if (!js_GetClassPrototype(cx, NULL, JSProto_Array, &proto, NULL))
76194:         return false;
76194: 
76194:     /*
76194:      * It is sufficient to check just Array.prototype; if Object.prototype is
76194:      * unknown or has an indexed property, those will be reflected in
76194:      * Array.prototype.
76194:      */
77353:     if (proto->getType(cx)->unknownProperties())
76194:         return true;
77353:     types::TypeSet *arrayTypes = proto->getType(cx)->getProperty(cx, JSID_VOID, false);
76194:     return !arrayTypes || arrayTypes->knownNonEmpty(cx);
76194: }
76194: 
76194: /*
76194:  * Barriers overview.
76194:  *
76194:  * After a property fetch finishes, we may need to do type checks on it to make
76194:  * sure it matches the pushed type set for this bytecode. This can be either
76194:  * because there is a type barrier at the bytecode, or because we cannot rule
76194:  * out an undefined result. For such accesses, we push a register pair, and
76194:  * then use those registers to check the fetched type matches the inferred
76194:  * types for the pushed set. The flow here is tricky:
76194:  *
76194:  * frame.pushRegs(type, data, knownType);
76194:  * --- Depending on knownType, the frame's representation for the pushed entry
76194:  *     may not be a register pair anymore. knownType is based on the observed
76194:  *     types that have been pushed here and may not actually match type/data.
76194:  *     pushRegs must not clobber either register, for the test below.
76194:  *
76194:  * testBarrier(type, data)
76194:  * --- Use the type/data regs and generate a single jump taken if the barrier
76194:  *     has been violated.
76194:  *
76194:  * --- Rearrange stack, rejoin from stub paths. No code must be emitted into
76194:  *     the inline path between testBarrier and finishBarrier. Since a stub path
76194:  *     may be in progress we can't call finishBarrier before stubcc.rejoin,
76194:  *     and since typeReg/dataReg may not be intact after the stub call rejoin
76194:  *     (if knownType != JSVAL_TYPE_UNKNOWN) we can't testBarrier after calling
76194:  *     stubcc.rejoin.
76194:  *
76194:  * finishBarrier()
76194:  * --- Link the barrier jump to a new stub code path which updates the pushed
76194:  *     types (possibly triggering recompilation). The frame has changed since
76194:  *     pushRegs to reflect the final state of the op, which is OK as no inline
76194:  *     code has been emitted since the barrier jump.
76194:  */
76194: 
76194: mjit::Compiler::BarrierState
76194: mjit::Compiler::pushAddressMaybeBarrier(Address address, JSValueType type, bool reuseBase,
76194:                                         bool testUndefined)
76194: {
76194:     if (!hasTypeBarriers(PC) && !testUndefined) {
76194:         frame.push(address, type, reuseBase);
76194:         return BarrierState();
76194:     }
76194: 
76194:     RegisterID typeReg, dataReg;
76194:     frame.loadIntoRegisters(address, reuseBase, &typeReg, &dataReg);
76194: 
76194:     frame.pushRegs(typeReg, dataReg, type);
76194:     return testBarrier(typeReg, dataReg, testUndefined);
76194: }
76194: 
76194: MaybeJump
76194: mjit::Compiler::trySingleTypeTest(types::TypeSet *types, RegisterID typeReg)
76194: {
76194:     /*
76194:      * If a type set we have a barrier on is monomorphic, generate a single
76194:      * jump taken if a type register has a match. This doesn't handle type sets
76194:      * containing objects, as these require two jumps regardless (test for
76194:      * object, then test the type of the object).
76194:      */
76194:     MaybeJump res;
76194: 
76194:     switch (types->getKnownTypeTag(cx)) {
76194:       case JSVAL_TYPE_INT32:
76194:         res.setJump(masm.testInt32(Assembler::NotEqual, typeReg));
76194:         return res;
76194: 
76194:       case JSVAL_TYPE_DOUBLE:
76194:         res.setJump(masm.testNumber(Assembler::NotEqual, typeReg));
76194:         return res;
76194: 
76194:       case JSVAL_TYPE_BOOLEAN:
76194:         res.setJump(masm.testBoolean(Assembler::NotEqual, typeReg));
76194:         return res;
76194: 
76194:       case JSVAL_TYPE_STRING:
76194:         res.setJump(masm.testString(Assembler::NotEqual, typeReg));
76194:         return res;
76194: 
76194:       default:
76194:         return res;
76194:     }
76194: }
76194: 
76194: JSC::MacroAssembler::Jump
76194: mjit::Compiler::addTypeTest(types::TypeSet *types, RegisterID typeReg, RegisterID dataReg)
76194: {
76194:     /*
76194:      * :TODO: It would be good to merge this with GenerateTypeCheck, but the
76194:      * two methods have a different format for the tested value (in registers
76194:      * vs. in memory).
76194:      */
76194: 
76194:     Vector<Jump> matches(CompilerAllocPolicy(cx, *this));
76194: 
77353:     if (types->hasType(types::Type::Int32Type()))
76194:         matches.append(masm.testInt32(Assembler::Equal, typeReg));
76194: 
77353:     if (types->hasType(types::Type::DoubleType()))
76194:         matches.append(masm.testDouble(Assembler::Equal, typeReg));
76194: 
77353:     if (types->hasType(types::Type::UndefinedType()))
76194:         matches.append(masm.testUndefined(Assembler::Equal, typeReg));
76194: 
77353:     if (types->hasType(types::Type::BooleanType()))
76194:         matches.append(masm.testBoolean(Assembler::Equal, typeReg));
76194: 
77353:     if (types->hasType(types::Type::StringType()))
76194:         matches.append(masm.testString(Assembler::Equal, typeReg));
76194: 
77353:     if (types->hasType(types::Type::NullType()))
76194:         matches.append(masm.testNull(Assembler::Equal, typeReg));
76194: 
77353:     unsigned count = 0;
77353:     if (types->hasType(types::Type::AnyObjectType()))
77353:         matches.append(masm.testObject(Assembler::Equal, typeReg));
77353:     else
77353:         count = types->getObjectCount();
77353: 
76194:     if (count != 0) {
76194:         Jump notObject = masm.testObject(Assembler::NotEqual, typeReg);
77353:         Address typeAddress(dataReg, JSObject::offsetOfType());
76194: 
77455:         /*
77455:          * Test for a singleton objects first. If singletons have lazy types
77464:          * then they may share their raw type pointer with another type object
77464:          * in the observed set and we can get a spurious match.
77455:          */
77455:         Jump notSingleton = masm.branchTest32(Assembler::Zero,
77455:                                               Address(dataReg, offsetof(JSObject, flags)),
77455:                                               Imm32(JSObject::SINGLETON_TYPE));
77455: 
76194:         for (unsigned i = 0; i < count; i++) {
77353:             if (JSObject *object = types->getSingleObject(i))
77353:                 matches.append(masm.branchPtr(Assembler::Equal, dataReg, ImmPtr(object)));
77455:         }
77455: 
77455:         Jump singletonMismatch = masm.jump();
77455: 
77455:         notSingleton.linkTo(masm.label(), &masm);
77455: 
77455:         for (unsigned i = 0; i < count; i++) {
77455:             if (types::TypeObject *object = types->getTypeObject(i))
76194:                 matches.append(masm.branchPtr(Assembler::Equal, typeAddress, ImmPtr(object)));
76194:         }
76194: 
77455:         singletonMismatch.linkTo(masm.label(), &masm);
76194:         notObject.linkTo(masm.label(), &masm);
76194:     }
76194: 
76194:     Jump mismatch = masm.jump();
76194: 
76194:     for (unsigned i = 0; i < matches.length(); i++)
76194:         matches[i].linkTo(masm.label(), &masm);
76194: 
76194:     return mismatch;
76194: }
76194: 
76194: mjit::Compiler::BarrierState
76194: mjit::Compiler::testBarrier(RegisterID typeReg, RegisterID dataReg,
78454:                             bool testUndefined, bool testReturn, bool force)
76194: {
76194:     BarrierState state;
76194:     state.typeReg = typeReg;
76194:     state.dataReg = dataReg;
76194: 
76194:     if (!cx->typeInferenceEnabled() || !(js_CodeSpec[*PC].format & JOF_TYPESET))
76194:         return state;
76194: 
77391:     types::TypeSet *types = analysis->bytecodeTypes(PC);
76194:     if (types->unknown()) {
76194:         /*
76194:          * If the result of this opcode is already unknown, there is no way for
76194:          * a type barrier to fail.
76194:          */
76194:         return state;
76194:     }
76194: 
76194:     if (testReturn) {
76194:         JS_ASSERT(!testUndefined);
76194:         if (!analysis->getCode(PC).monitoredTypesReturn)
76194:             return state;
78454:     } else if (!hasTypeBarriers(PC) && !force) {
77459:         if (testUndefined && !types->hasType(types::Type::UndefinedType()))
76194:             state.jump.setJump(masm.testUndefined(Assembler::Equal, typeReg));
76194:         return state;
76194:     }
76194: 
76194:     types->addFreeze(cx);
76194: 
76194:     /* Cannot have type barriers when the result of the operation is already unknown. */
76194:     JS_ASSERT(!types->unknown());
76194: 
76194:     state.jump = trySingleTypeTest(types, typeReg);
76194:     if (!state.jump.isSet())
76194:         state.jump.setJump(addTypeTest(types, typeReg, dataReg));
76194: 
76194:     return state;
76194: }
76194: 
76194: void
76194: mjit::Compiler::finishBarrier(const BarrierState &barrier, RejoinState rejoin, uint32 which)
76194: {
76194:     if (!barrier.jump.isSet())
76194:         return;
76194: 
76194:     stubcc.linkExitDirect(barrier.jump.get(), stubcc.masm.label());
76194: 
76194:     /*
76194:      * Before syncing, store the entry to sp[0]. (scanInlineCalls accounted for
76194:      * this when making sure there is enough froom for all frames). The known
76194:      * type in the frame may be wrong leading to an incorrect sync, and this
76194:      * sync may also clobber typeReg and/or dataReg.
76194:      */
76194:     frame.pushSynced(JSVAL_TYPE_UNKNOWN);
76194:     stubcc.masm.storeValueFromComponents(barrier.typeReg, barrier.dataReg,
76194:                                          frame.addressOf(frame.peek(-1)));
76194:     frame.pop();
76194: 
76194:     stubcc.syncExit(Uses(0));
76194:     stubcc.leave();
76194: 
76194:     stubcc.masm.move(ImmPtr((void *) which), Registers::ArgReg1);
76194:     OOL_STUBCALL(stubs::TypeBarrierHelper, rejoin);
76194:     stubcc.rejoin(Changes(0));
76194: }
78456: 
78456: void
78456: mjit::Compiler::testPushedType(RejoinState rejoin, int which, bool ool)
78456: {
78456:     if (!cx->typeInferenceEnabled() || !(js_CodeSpec[*PC].format & JOF_TYPESET))
78456:         return;
78456: 
78456:     types::TypeSet *types = analysis->bytecodeTypes(PC);
78456:     if (types->unknown())
78456:         return;
78456: 
78456:     Assembler &masm = ool ? stubcc.masm : this->masm;
78456: 
78456:     JS_ASSERT(which <= 0);
78456:     Address address = (which == 0) ? frame.addressOfTop() : frame.addressOf(frame.peek(which));
78456: 
78456:     Vector<Jump> mismatches(cx);
78456:     if (!masm.generateTypeCheck(cx, address, types, &mismatches)) {
78456:         oomInVector = true;
78456:         return;
78456:     }
78456: 
78456:     Jump j = masm.jump();
78456: 
78456:     for (unsigned i = 0; i < mismatches.length(); i++)
78456:         mismatches[i].linkTo(masm.label(), &masm);
78456: 
78456:     masm.move(Imm32(which), Registers::ArgReg1);
78456:     if (ool)
78456:         OOL_STUBCALL(stubs::StubTypeHelper, rejoin);
78456:     else
78456:         INLINE_STUBCALL(stubs::StubTypeHelper, rejoin);
78456: 
78456:     j.linkTo(masm.label(), &masm);
78456: }
