130924: # This Source Code Form is subject to the terms of the Mozilla Public
130924: # License, v. 2.0. If a copy of the MPL was not distributed with this
130924: # file, You can obtain one at http://mozilla.org/MPL/2.0/.
130924: 
130924: from __future__ import unicode_literals
130924: 
163583: import itertools
130924: import logging
130924: import os
171969: import re
134922: import types
130924: 
162878: from collections import namedtuple
162878: 
163581: import mozbuild.makeutil as mozmakeutil
150021: from mozpack.copier import FilePurger
154196: from mozpack.manifests import (
154196:     InstallManifest,
154196: )
156517: import mozpack.path as mozpath
150021: 
156517: from .common import CommonBackend
130928: from ..frontend.data import (
130928:     ConfigFileSubstitution,
162726:     Defines,
130928:     DirectoryTraversal,
160248:     Exports,
171969:     GeneratedEventWebIDLFile,
166712:     GeneratedInclude,
171969:     GeneratedWebIDLFile,
168104:     HeaderFileSubstitution,
168744:     HostProgram,
168744:     HostSimpleProgram,
166365:     InstallationTarget,
150550:     IPDLFile,
167037:     JavaJarData,
171741:     LibraryDefinition,
159584:     LocalInclude,
171969:     PreprocessedTestWebIDLFile,
171969:     PreprocessedWebIDLFile,
160248:     Program,
134922:     SandboxDerived,
167037:     SandboxWrapped,
168743:     SimpleProgram,
171969:     TestWebIDLFile,
135313:     VariablePassthru,
156517:     XPIDLFile,
171969:     TestManifest,
171969:     WebIDLFile,
130928: )
163236: from ..util import (
163236:     ensureParentDir,
163236:     FileAvoidWrite,
163236: )
162878: from ..makeutil import Makefile
130924: 
130924: class BackendMakeFile(object):
130924:     """Represents a generated backend.mk file.
130924: 
134431:     This is both a wrapper around a file handle as well as a container that
130924:     holds accumulated state.
134431: 
134431:     It's worth taking a moment to explain the make dependencies. The
134431:     generated backend.mk as well as the Makefile.in (if it exists) are in the
134431:     GLOBAL_DEPS list. This means that if one of them changes, all targets
134431:     in that Makefile are invalidated. backend.mk also depends on all of its
134431:     input files.
134431: 
134431:     It's worth considering the effect of file mtimes on build behavior.
134431: 
134431:     Since we perform an "all or none" traversal of moz.build files (the whole
134431:     tree is scanned as opposed to individual files), if we were to blindly
134431:     write backend.mk files, the net effect of updating a single mozbuild file
134431:     in the tree is all backend.mk files have new mtimes. This would in turn
134431:     invalidate all make targets across the whole tree! This would effectively
134431:     undermine incremental builds as any mozbuild change would cause the entire
134431:     tree to rebuild!
134431: 
134431:     The solution is to not update the mtimes of backend.mk files unless they
144630:     actually change. We use FileAvoidWrite to accomplish this.
130924:     """
130924: 
134481:     def __init__(self, srcdir, objdir, environment):
130924:         self.srcdir = srcdir
130924:         self.objdir = objdir
162878:         self.relobjdir = objdir[len(environment.topobjdir) + 1:]
134481:         self.environment = environment
168106:         self.name = os.path.join(objdir, 'backend.mk')
130924: 
156517:         # XPIDLFiles attached to this file.
156517:         self.idls = []
156517:         self.xpt_name = None
156517: 
168106:         self.fh = FileAvoidWrite(self.name)
134431:         self.fh.write('# THIS FILE WAS AUTOMATICALLY GENERATED. DO NOT EDIT.\n')
134431:         self.fh.write('\n')
134431:         self.fh.write('MOZBUILD_DERIVED := 1\n')
130924: 
130924:     def write(self, buf):
130924:         self.fh.write(buf)
130924: 
168109:     # For compatibility with makeutil.Makefile
168109:     def add_statement(self, stmt):
168109:         self.write('%s\n' % stmt)
168109: 
130924:     def close(self):
156517:         if self.xpt_name:
156517:             self.fh.write('XPT_NAME := %s\n' % self.xpt_name)
156517: 
158111:             # We just recompile all xpidls because it's easier and less error
158111:             # prone.
156517:             self.fh.write('NONRECURSIVE_TARGETS += export\n')
156517:             self.fh.write('NONRECURSIVE_TARGETS_export += xpidl\n')
156517:             self.fh.write('NONRECURSIVE_TARGETS_export_xpidl_DIRECTORY = '
163679:                 '$(DEPTH)/xpcom/xpidl\n')
156517:             self.fh.write('NONRECURSIVE_TARGETS_export_xpidl_TARGETS += '
163679:                 'export\n')
156517: 
144630:         return self.fh.close()
134922: 
130924: 
162878: class RecursiveMakeTraversal(object):
162878:     """
162878:     Helper class to keep track of how the "traditional" recursive make backend
162878:     recurses subdirectories. This is useful until all adhoc rules are removed
162878:     from Makefiles.
162878: 
162878:     Each directory may have one or more types of subdirectories:
162878:         - parallel
162878:         - static
162878:         - (normal) dirs
162878:         - tests
162878:         - tools
162878: 
162878:     The "traditional" recursive make backend recurses through those by first
162878:     building the current directory, followed by parallel directories (in
162878:     parallel), then static directories, dirs, tests and tools (all
162878:     sequentially).
162878:     """
162878:     SubDirectoryCategories = ['parallel', 'static', 'dirs', 'tests', 'tools']
162878:     SubDirectoriesTuple = namedtuple('SubDirectories', SubDirectoryCategories)
162878:     class SubDirectories(SubDirectoriesTuple):
162878:         def __new__(self):
162878:             return RecursiveMakeTraversal.SubDirectoriesTuple.__new__(self, [], [], [], [], [])
162878: 
162878:     def __init__(self):
162878:         self._traversal = {}
162878: 
162878:     def add(self, dir, **kargs):
162878:         """
162878:         Function signature is, in fact:
162878:             def add(self, dir, parallel=[], static=[], dirs=[],
162878:                                tests=[], tools=[])
162878:         but it's done with **kargs to avoid repetitive code.
162878: 
162878:         Adds a directory to traversal, registering its subdirectories,
162878:         sorted by categories. If the directory was already added to
162878:         traversal, adds the new subdirectories to the already known lists.
162878:         """
162878:         subdirs = self._traversal.setdefault(dir, self.SubDirectories())
162878:         for key, value in kargs.items():
162878:             assert(key in self.SubDirectoryCategories)
162878:             getattr(subdirs, key).extend(value)
162878: 
162878:     @staticmethod
162878:     def default_filter(current, subdirs):
162878:         """
162878:         Default filter for use with compute_dependencies and traverse.
162878:         """
162878:         return current, subdirs.parallel, \
162878:                subdirs.static + subdirs.dirs + subdirs.tests + subdirs.tools
162878: 
162878:     def call_filter(self, current, filter):
162878:         """
162878:         Helper function to call a filter from compute_dependencies and
162878:         traverse.
162878:         """
162878:         return filter(current, self._traversal.get(current,
162878:             self.SubDirectories()))
162878: 
162878:     def compute_dependencies(self, filter=None):
162878:         """
162878:         Compute make dependencies corresponding to the registered directory
162878:         traversal.
162878: 
162878:         filter is a function with the following signature:
162878:             def filter(current, subdirs)
162878:         where current is the directory being traversed, and subdirs the
162878:         SubDirectories instance corresponding to it.
162878:         The filter function returns a tuple (filtered_current, filtered_parallel,
162878:         filtered_dirs) where filtered_current is either current or None if
162878:         the current directory is to be skipped, and filtered_parallel and
162878:         filtered_dirs are lists of parallel directories and sequential
162878:         directories, which can be rearranged from whatever is given in the
162878:         SubDirectories members.
162878: 
162878:         The default filter corresponds to a default recursive traversal.
162878:         """
162878:         filter = filter or self.default_filter
162878: 
162878:         deps = {}
162878: 
162878:         def recurse(start_node, prev_nodes=None):
162878:             current, parallel, sequential = self.call_filter(start_node, filter)
162878:             if current is not None:
162878:                 if start_node != '':
162878:                     deps[start_node] = prev_nodes
162878:                 prev_nodes = (start_node,)
162878:             if not start_node in self._traversal:
162878:                 return prev_nodes
162878:             parallel_nodes = []
162878:             for node in parallel:
162878:                 nodes = recurse(node, prev_nodes)
164351:                 if nodes and nodes != ('',):
162878:                     parallel_nodes.extend(nodes)
162878:             if parallel_nodes:
162878:                 prev_nodes = tuple(parallel_nodes)
162878:             for dir in sequential:
162878:                 prev_nodes = recurse(dir, prev_nodes)
162878:             return prev_nodes
162878: 
162878:         return recurse(''), deps
162878: 
162878:     def traverse(self, start, filter=None):
162878:         """
162878:         Iterate over the filtered subdirectories, following the traditional
162878:         make traversal order.
162878:         """
162878:         if filter is None:
162878:             filter = self.default_filter
162878: 
162878:         current, parallel, sequential = self.call_filter(start, filter)
162878:         if current is not None:
162878:             yield start
162878:         if not start in self._traversal:
162878:             return
162878:         for node in parallel:
162878:             for n in self.traverse(node, filter):
162878:                 yield n
162878:         for dir in sequential:
162878:             for d in self.traverse(dir, filter):
162878:                 yield d
162878: 
162878:     def get_subdirs(self, dir):
162878:         """
162878:         Returns all direct subdirectories under the given directory.
162878:         """
162878:         return self._traversal.get(dir, self.SubDirectories())
162878: 
162878: 
156517: class RecursiveMakeBackend(CommonBackend):
130924:     """Backend that integrates with the existing recursive make build system.
130924: 
130924:     This backend facilitates the transition from Makefile.in to moz.build
130924:     files.
130924: 
130924:     This backend performs Makefile.in -> Makefile conversion. It also writes
130924:     out .mk files containing content derived from moz.build files. Both are
130924:     consumed by the recursive make builder.
130924: 
130924:     This backend may eventually evolve to write out non-recursive make files.
130924:     However, as long as there are Makefile.in files in the tree, we are tied to
130924:     recursive make and thus will need this backend.
130924:     """
130924: 
130924:     def _init(self):
156517:         CommonBackend._init(self)
156517: 
130924:         self._backend_files = {}
150550:         self._ipdl_sources = set()
171969:         self._webidl_sources = set()
171969:         self._generated_events_webidl_sources = set()
171969:         self._test_webidl_sources = set()
171969:         self._preprocessed_test_webidl_sources = set()
171969:         self._preprocessed_webidl_sources = set()
171969:         self._generated_webidl_sources = set()
130924: 
134922:         def detailed(summary):
168107:             s = '{:d} total backend files. {:d} created; {:d} updated; {:d} unchanged'.format(
168106:                 summary.created_count + summary.updated_count +
168106:                 summary.unchanged_count, summary.created_count,
134922:                 summary.updated_count, summary.unchanged_count)
168107:             if summary.deleted_count:
168107:                 s+= '; {:d} deleted'.format(summary.deleted_count)
168107:             return s
134922: 
134922:         # This is a little kludgy and could be improved with a better API.
134922:         self.summary.backend_detailed_summary = types.MethodType(detailed,
134922:             self.summary)
134922: 
163244:         self._test_manifests = {}
147165: 
144630:         self.backend_input_files.add(os.path.join(self.environment.topobjdir,
144630:             'config', 'autoconf.mk'))
144630: 
162784:         self._install_manifests = {
162784:             k: InstallManifest() for k in [
162784:                 'dist_bin',
162784:                 'dist_idl',
162784:                 'dist_include',
162784:                 'dist_public',
162784:                 'dist_private',
162784:                 'dist_sdk',
162784:                 'tests',
162784:                 'xpidl',
162784:             ]}
150021: 
162878:         self._traversal = RecursiveMakeTraversal()
164351:         self._may_skip = {
164351:             'export': set(),
164351:             'compile': set(),
164351:             'binaries': set(),
164351:             'libs': set(),
164351:             'tools': set(),
164351:         }
162878: 
162880:         derecurse = self.environment.substs.get('MOZ_PSEUDO_DERECURSE', '').split(',')
162880:         self._parallel_export = False
164351:         self._no_skip = False
164351:         if derecurse != ['']:
164351:             self._parallel_export = 'no-parallel-export' not in derecurse
164351:             self._no_skip = 'no-skip' in derecurse
162880: 
130924:     def consume_object(self, obj):
130924:         """Write out build files necessary to build with recursive make."""
130924: 
156517:         CommonBackend.consume_object(self, obj)
156517: 
134922:         if not isinstance(obj, SandboxDerived):
134922:             return
134922: 
168106:         if obj.srcdir not in self._backend_files:
168106:             self._backend_files[obj.srcdir] = \
168106:                 BackendMakeFile(obj.srcdir, obj.objdir, self.get_environment(obj))
168106:         backend_file = self._backend_files[obj.srcdir]
130924: 
130924:         if isinstance(obj, DirectoryTraversal):
130924:             self._process_directory_traversal(obj, backend_file)
130928:         elif isinstance(obj, ConfigFileSubstitution):
168106:             with self._write_file(obj.output_path) as fh:
168106:                 backend_file.environment.create_config_file(fh)
144630:             self.backend_input_files.add(obj.input_path)
168104:         elif isinstance(obj, HeaderFileSubstitution):
168106:             with self._write_file(obj.output_path) as fh:
168106:                 backend_file.environment.create_config_header(fh)
168104:             self.backend_input_files.add(obj.input_path)
156517:         elif isinstance(obj, XPIDLFile):
156517:             backend_file.idls.append(obj)
156517:             backend_file.xpt_name = '%s.xpt' % obj.module
135313:         elif isinstance(obj, VariablePassthru):
168109:             unified_suffixes = dict(
168109:                 UNIFIED_CSRCS='c',
170581:                 UNIFIED_CMMSRCS='mm',
168109:                 UNIFIED_CPPSRCS='cpp',
168109:             )
172510: 
172510:             files_per_unification = 16
172510:             if 'FILES_PER_UNIFIED_FILE' in obj.variables.keys():
172510:                 files_per_unification = obj.variables['FILES_PER_UNIFIED_FILE']
172510: 
172459:             do_unify = not self.environment.substs.get(
172510:                 'MOZ_DISABLE_UNIFIED_COMPILATION') and files_per_unification > 1
172510: 
135313:             # Sorted so output is consistent and we don't bump mtimes.
135313:             for k, v in sorted(obj.variables.items()):
172459:                 if k in unified_suffixes:
172459:                     if do_unify:
168109:                         self._add_unified_build_rules(backend_file, v,
168109:                             backend_file.objdir,
172459:                             unified_prefix='Unified_%s_%s' % (
172459:                                 unified_suffixes[k],
168109:                                 backend_file.relobjdir.replace('/', '_')),
168109:                             unified_suffix=unified_suffixes[k],
168109:                             unified_files_makefile_variable=k,
172510:                             include_curdir_build_rules=False,
173305:                             files_per_unified_file=files_per_unification)
168109:                         backend_file.write('%s += $(%s)\n' % (k[len('UNIFIED_'):], k))
172459:                     else:
172459:                         backend_file.write('%s += %s\n' % (
172459:                             k[len('UNIFIED_'):], ' '.join(sorted(v))))
168109:                 elif isinstance(v, list):
135313:                     for item in v:
135313:                         backend_file.write('%s += %s\n' % (k, item))
157586:                 elif isinstance(v, bool):
157586:                     if v:
157586:                         backend_file.write('%s := 1\n' % k)
135313:                 else:
135313:                     backend_file.write('%s := %s\n' % (k, v))
162726: 
162726:         elif isinstance(obj, Defines):
162726:             defines = obj.get_defines()
162726:             if defines:
162726:                 backend_file.write('DEFINES +=')
162726:                 for define in defines:
162726:                     backend_file.write(' %s' % define)
162726:                 backend_file.write('\n')
162726: 
139977:         elif isinstance(obj, Exports):
159250:             self._process_exports(obj, obj.exports, backend_file)
130924: 
150550:         elif isinstance(obj, IPDLFile):
156517:             self._ipdl_sources.add(mozpath.join(obj.srcdir, obj.basename))
150550: 
171969:         elif isinstance(obj, WebIDLFile):
171969:             self._webidl_sources.add(mozpath.join(obj.srcdir, obj.basename))
171969:             self._process_webidl_basename(obj.basename)
171969: 
171969:         elif isinstance(obj, GeneratedEventWebIDLFile):
171969:             self._generated_events_webidl_sources.add(mozpath.join(obj.srcdir, obj.basename))
171969: 
171969:         elif isinstance(obj, TestWebIDLFile):
171969:             self._test_webidl_sources.add(mozpath.join(obj.srcdir,
171969:                                                        obj.basename))
171969:             # Test WebIDL files are not exported.
171969: 
171969:         elif isinstance(obj, PreprocessedTestWebIDLFile):
171969:             self._preprocessed_test_webidl_sources.add(mozpath.join(obj.srcdir,
171969:                                                                     obj.basename))
171969:             # Test WebIDL files are not exported.
171969: 
171969:         elif isinstance(obj, GeneratedWebIDLFile):
171969:             self._generated_webidl_sources.add(mozpath.join(obj.srcdir,
171969:                                                             obj.basename))
171969:             self._process_webidl_basename(obj.basename)
171969: 
171969:         elif isinstance(obj, PreprocessedWebIDLFile):
171969:             self._preprocessed_webidl_sources.add(mozpath.join(obj.srcdir,
171969:                                                                obj.basename))
171969:             self._process_webidl_basename(obj.basename)
171969: 
143209:         elif isinstance(obj, Program):
143209:             self._process_program(obj.program, backend_file)
143209: 
168744:         elif isinstance(obj, HostProgram):
168744:             self._process_host_program(obj.program, backend_file)
168744: 
168743:         elif isinstance(obj, SimpleProgram):
168743:             self._process_simple_program(obj.program, backend_file)
168743: 
168744:         elif isinstance(obj, HostSimpleProgram):
168744:             self._process_host_simple_program(obj.program, backend_file)
168744: 
163244:         elif isinstance(obj, TestManifest):
163244:             self._process_test_manifest(obj, backend_file)
142045: 
159584:         elif isinstance(obj, LocalInclude):
159584:             self._process_local_include(obj.path, backend_file)
159584: 
166712:         elif isinstance(obj, GeneratedInclude):
166712:             self._process_generated_include(obj.path, backend_file)
166712: 
166365:         elif isinstance(obj, InstallationTarget):
166365:             self._process_installation_target(obj, backend_file)
166365: 
167037:         elif isinstance(obj, SandboxWrapped):
167037:             # Process a rich build system object from the front-end
167037:             # as-is.  Please follow precedent and handle CamelCaseData
167037:             # in a function named _process_camel_case_data.  At some
167037:             # point in the future, this unwrapping process may be
167037:             # automated.
167037:             if isinstance(obj.wrapped, JavaJarData):
167037:                 self._process_java_jar_data(obj.wrapped, backend_file)
169808:             else:
169808:                 return
167037: 
171741:         elif isinstance(obj, LibraryDefinition):
171741:             self._process_library_definition(obj, backend_file)
171741: 
169808:         else:
169808:             return
169808:         obj.ack()
130924: 
162878:     def _fill_root_mk(self):
162878:         """
162878:         Create two files, root.mk and root-deps.mk, the first containing
162878:         convenience variables, and the other dependency definitions for a
162878:         hopefully proper directory traversal.
162878:         """
164351:         if not self._no_skip:
164351:             for tier, skip in self._may_skip.items():
164351:                 self.log(logging.DEBUG, 'fill_root_mk', {
164351:                     'number': len(skip), 'tier': tier
164351:                     }, 'Ignoring {number} directories during {tier}')
164351: 
162880:         # Traverse directories in parallel, and skip static dirs
162880:         def parallel_filter(current, subdirs):
162879:             all_subdirs = subdirs.parallel + subdirs.dirs + \
162879:                           subdirs.tests + subdirs.tools
164351:             if current in self._may_skip[tier]:
164351:                 current = None
162879:             # subtiers/*_start and subtiers/*_finish, under subtiers/*, are
162879:             # kept sequential. Others are all forced parallel.
164351:             if current and current.startswith('subtiers/') and all_subdirs and \
162879:                     all_subdirs[0].startswith('subtiers/'):
162879:                 return current, [], all_subdirs
162879:             return current, all_subdirs, []
162878: 
162880:         # Skip static dirs during export traversal, or build everything in
162880:         # parallel when enabled.
162880:         def export_filter(current, subdirs):
162880:             if self._parallel_export:
162880:                 return parallel_filter(current, subdirs)
162880:             return current, subdirs.parallel, \
162880:                 subdirs.dirs + subdirs.tests + subdirs.tools
162880: 
165176:         # Skip tools dirs during libs traversal. Because of bug 925236 and
165176:         # possible other unknown race conditions, don't parallelize the libs
165176:         # tier.
162878:         def libs_filter(current, subdirs):
164351:             if current in self._may_skip[tier]:
164351:                 current = None
165176:             return current, [], subdirs.parallel + \
165176:                 subdirs.static + subdirs.dirs + subdirs.tests
165176: 
165176:         # Because of bug 925236 and possible other unknown race conditions,
165176:         # don't parallelize the tools tier.
165176:         def tools_filter(current, subdirs):
165176:             if current in self._may_skip[tier]:
165176:                 current = None
162878:             return current, subdirs.parallel, \
165176:                 subdirs.static + subdirs.dirs + subdirs.tests + subdirs.tools
162878: 
164350:         # compile, binaries and tools tiers use the same traversal as export
162878:         filters = {
162878:             'export': export_filter,
163679:             'compile': parallel_filter,
164350:             'binaries': parallel_filter,
162878:             'libs': libs_filter,
165176:             'tools': tools_filter,
162878:         }
162878: 
162878:         root_deps_mk = Makefile()
162878: 
162878:         # Fill the dependencies for traversal of each tier.
162878:         for tier, filter in filters.items():
162878:             main, all_deps = \
162878:                 self._traversal.compute_dependencies(filter)
162878:             for dir, deps in all_deps.items():
162878:                 rule = root_deps_mk.create_rule(['%s/%s' % (dir, tier)])
162878:                 if deps is not None:
162878:                     rule.add_dependencies('%s/%s' % (d, tier) for d in deps if d)
164351:             rule = root_deps_mk.create_rule(['recurse_%s' % tier])
164351:             if main:
164351:                 rule.add_dependencies('%s/%s' % (d, tier) for d in main)
162878: 
162878:         root_mk = Makefile()
162878: 
162878:         # Fill root.mk with the convenience variables.
162878:         for tier, filter in filters.items() + [('all', self._traversal.default_filter)]:
162878:             # Gather filtered subtiers for the given tier
162878:             all_direct_subdirs = reduce(lambda x, y: x + y,
162878:                                         self._traversal.get_subdirs(''), [])
162878:             direct_subdirs = [d for d in all_direct_subdirs
162878:                               if filter(d, self._traversal.get_subdirs(d))[0]]
162878:             subtiers = [d.replace('subtiers/', '') for d in direct_subdirs
162878:                         if d.startswith('subtiers/')]
162878: 
162878:             if tier != 'all':
162878:                 # Gather filtered directories for the given tier
162878:                 dirs = [d for d in direct_subdirs if not d.startswith('subtiers/')]
162878:                 if dirs:
162878:                     # For build systems without tiers (js/src), output a list
162878:                     # of directories for each tier.
162878:                     root_mk.add_statement('%s_dirs := %s' % (tier, ' '.join(dirs)))
162878:                     continue
162878:                 if subtiers:
162878:                     # Output the list of filtered subtiers for the given tier.
162878:                     root_mk.add_statement('%s_subtiers := %s' % (tier, ' '.join(subtiers)))
162878: 
162878:             for subtier in subtiers:
162878:                 # subtier_dirs[0] is 'subtiers/%s_start' % subtier, skip it
162878:                 subtier_dirs = list(self._traversal.traverse('subtiers/%s_start' % subtier, filter))[1:]
162878:                 if tier == 'all':
162878:                     for dir in subtier_dirs:
162878:                         # Output convenience variables to be able to map directories
162878:                         # to subtier names from Makefiles.
162878:                         stamped = dir.replace('/', '_')
162878:                         root_mk.add_statement('subtier_of_%s := %s' % (stamped, subtier))
162878: 
162878:                 else:
162878:                     # Output the list of filtered directories for each tier/subtier
162878:                     # pair.
162878:                     root_mk.add_statement('%s_subtier_%s := %s' % (tier, subtier, ' '.join(subtier_dirs)))
162878: 
162878:         root_mk.add_statement('$(call include_deps,root-deps.mk)')
162878: 
168106:         with self._write_file(
168106:                 os.path.join(self.environment.topobjdir, 'root.mk')) as root:
162878:             root_mk.dump(root, removal_guard=False)
168106: 
168106:         with self._write_file(
168106:                 os.path.join(self.environment.topobjdir, 'root-deps.mk')) as root_deps:
162878:             root_deps_mk.dump(root_deps, removal_guard=False)
162878: 
163586:     def _add_unified_build_rules(self, makefile, files, output_directory,
163586:                                  unified_prefix='Unified',
168109:                                  unified_suffix='cpp',
163586:                                  extra_dependencies=[],
163586:                                  unified_files_makefile_variable='unified_files',
168709:                                  include_curdir_build_rules=True,
172510:                                  poison_windows_h=False,
172510:                                  files_per_unified_file=16):
163586: 
163586:         explanation = "\n" \
163586:             "# We build files in 'unified' mode by including several files\n" \
163586:             "# together into a single source file.  This cuts down on\n" \
163586:             "# compilation times and debug information size.  %d was chosen as\n" \
163586:             "# a reasonable compromise between clobber rebuild time, incremental\n" \
168109:             "# rebuild time, and compiler memory usage." % files_per_unified_file
163586:         makefile.add_statement(explanation)
163586: 
163586:         def unified_files():
163586:             "Return an iterator of (unified_filename, source_filenames) tuples."
163586:             # Our last returned list of source filenames may be short, and we
163586:             # don't want the fill value inserted by izip_longest to be an
163586:             # issue.  So we do a little dance to filter it out ourselves.
163586:             dummy_fill_value = ("dummy",)
163586:             def filter_out_dummy(iterable):
163586:                 return itertools.ifilter(lambda x: x != dummy_fill_value,
163586:                                          iterable)
163586: 
163586:             # From the itertools documentation, slightly modified:
163586:             def grouper(n, iterable):
163586:                 "grouper(3, 'ABCDEFG', 'x') --> ABC DEF Gxx"
163586:                 args = [iter(iterable)] * n
163586:                 return itertools.izip_longest(fillvalue=dummy_fill_value, *args)
163586: 
163586:             for i, unified_group in enumerate(grouper(files_per_unified_file,
171969:                                                       sorted(files))):
163586:                 just_the_filenames = list(filter_out_dummy(unified_group))
168109:                 yield '%s%d.%s' % (unified_prefix, i, unified_suffix), just_the_filenames
163586: 
163586:         all_sources = ' '.join(source for source, _ in unified_files())
168109:         makefile.add_statement('%s := %s' % (unified_files_makefile_variable,
163586:                                                all_sources))
163586: 
163586:         for unified_file, source_filenames in unified_files():
168108:             if extra_dependencies:
163586:                 rule = makefile.create_rule([unified_file])
168108:                 rule.add_dependencies(extra_dependencies)
163586: 
163586:             # The rule we just defined is only for cases where the cpp files get
163586:             # blown away and we need to regenerate them.  The rule doesn't correctly
163586:             # handle source files being added/removed/renamed.  Therefore, we
163586:             # generate them here also to make sure everything's up-to-date.
168106:             with self._write_file(os.path.join(output_directory, unified_file)) as f:
172511:                 f.write('#define MOZ_UNIFIED_BUILD\n')
168709:                 includeTemplate = '#include "%(cppfile)s"'
168709:                 if poison_windows_h:
168709:                     includeTemplate += (
168709:                         '\n'
168709:                         '#ifdef _WINDOWS_\n'
168709:                         '#error "%(cppfile)s included windows.h"\n'
168709:                         "#endif")
173062:                 includeTemplate += (
173062:                     '\n'
173062:                     '#ifdef PL_ARENA_CONST_ALIGN_MASK\n'
173062:                     '#error "%(cppfile)s uses PL_ARENA_CONST_ALIGN_MASK, '
173062:                     'so it cannot be built in unified mode."\n'
173062:                     '#undef PL_ARENA_CONST_ALIGN_MASK\n'
173062:                     '#endif\n'
173062:                     '#ifdef FORCE_PR_LOG\n'
173062:                     '#error "%(cppfile)s forces NSPR logging, '
173062:                     'so it cannot be built in unified mode."\n'
173062:                     '#undef FORCE_PR_LOG\n'
173062:                     '#endif')
168709:                 f.write('\n'.join(includeTemplate % { "cppfile": s } for
168709:                                   s in source_filenames))
163586: 
163586:         if include_curdir_build_rules:
163586:             makefile.add_statement('\n'
163586:                 '# Make sometimes gets confused between "foo" and "$(CURDIR)/foo".\n'
163586:                 '# Help it out by explicitly specifiying dependencies.')
163586:             makefile.add_statement('all_absolute_unified_files := \\\n'
163586:                                    '  $(addprefix $(CURDIR)/,$(%s))'
163586:                                    % unified_files_makefile_variable)
163586:             rule = makefile.create_rule(['$(all_absolute_unified_files)'])
163586:             rule.add_dependencies(['$(CURDIR)/%: %'])
163586: 
130924:     def consume_finished(self):
156517:         CommonBackend.consume_finished(self)
156517: 
130924:         for srcdir in sorted(self._backend_files.keys()):
168106:             with self._write_file(fh=self._backend_files[srcdir]) as bf:
130924:                 makefile_in = os.path.join(srcdir, 'Makefile.in')
136385:                 makefile = os.path.join(bf.objdir, 'Makefile')
130924: 
168106:                 # If Makefile.in exists, use it as a template. Otherwise,
168106:                 # create a stub.
159435:                 stub = not os.path.exists(makefile_in)
159435:                 if not stub:
136385:                     self.log(logging.DEBUG, 'substitute_makefile',
136385:                         {'path': makefile}, 'Substituting makefile: {path}')
134814: 
168106:                     # Adding the Makefile.in here has the desired side-effect
168106:                     # that if the Makefile.in disappears, this will force
168106:                     # moz.build traversal. This means that when we remove empty
168106:                     # Makefile.in files, the old file will get replaced with
168106:                     # the autogenerated one automatically.
144630:                     self.backend_input_files.add(makefile_in)
164351: 
169435:                     for tier, skiplist in self._may_skip.items():
169435:                         if tier in ('compile', 'binaries'):
169435:                             continue
164351:                         if bf.relobjdir in skiplist:
164351:                             skiplist.remove(bf.relobjdir)
136385:                 else:
136385:                     self.log(logging.DEBUG, 'stub_makefile',
136385:                         {'path': makefile}, 'Creating stub Makefile: {path}')
136385: 
164351:                 # Can't skip directories with a jar.mn for the 'libs' tier.
164351:                 if bf.relobjdir in self._may_skip['libs'] and \
164351:                         os.path.exists(os.path.join(srcdir, 'jar.mn')):
164351:                     self._may_skip['libs'].remove(bf.relobjdir)
164351: 
168106:                 with self._write_file(makefile) as fh:
168106:                     bf.environment.create_makefile(fh, stub=stub)
130924: 
150550:         # Write out a master list of all IPDL source files.
164302:         ipdl_dir = os.path.join(self.environment.topobjdir, 'ipc', 'ipdl')
163581:         mk = mozmakeutil.Makefile()
163581: 
163583:         sorted_ipdl_sources = list(sorted(self._ipdl_sources))
168109:         mk.add_statement('ALL_IPDLSRCS := %s' % ' '.join(sorted_ipdl_sources))
163583: 
163582:         def files_from(ipdl):
163582:             base = os.path.basename(ipdl)
150550:             root, ext = os.path.splitext(base)
150550: 
150550:             # Both .ipdl and .ipdlh become .cpp files
163582:             files = ['%s.cpp' % root]
150550:             if ext == '.ipdl':
150550:                 # .ipdl also becomes Child/Parent.cpp files
163582:                 files.extend(['%sChild.cpp' % root,
163582:                               '%sParent.cpp' % root])
163582:             return files
163582: 
164302:         ipdl_cppsrcs = list(itertools.chain(*[files_from(p) for p in sorted_ipdl_sources]))
164302:         self._add_unified_build_rules(mk, ipdl_cppsrcs, ipdl_dir,
164302:                                       unified_prefix='UnifiedProtocols',
164302:                                       unified_files_makefile_variable='CPPSRCS')
161037: 
168109:         mk.add_statement('IPDLDIRS := %s' % ' '.join(sorted(set(os.path.dirname(p)
150550:             for p in self._ipdl_sources))))
150550: 
168106:         with self._write_file(os.path.join(ipdl_dir, 'ipdlsrcs.mk')) as ipdls:
164302:             mk.dump(ipdls, removal_guard=False)
163581: 
171969:         self._may_skip['compile'] -= set(['ipc/ipdl'])
171969: 
171969:         # Write out master lists of WebIDL source files.
171969:         bindings_dir = os.path.join(self.environment.topobjdir, 'dom', 'bindings')
171969: 
171969:         mk = mozmakeutil.Makefile()
171969: 
171969:         def write_var(variable, sources):
171969:             files = [os.path.basename(f) for f in sorted(sources)]
171969:             mk.add_statement('%s += %s' % (variable, ' '.join(files)))
171969:         write_var('webidl_files', self._webidl_sources)
171969:         write_var('generated_events_webidl_files', self._generated_events_webidl_sources)
171969:         write_var('test_webidl_files', self._test_webidl_sources)
171969:         write_var('preprocessed_test_webidl_files', self._preprocessed_test_webidl_sources)
171969:         write_var('generated_webidl_files', self._generated_webidl_sources)
171969:         write_var('preprocessed_webidl_files', self._preprocessed_webidl_sources)
171969: 
171969:         all_webidl_files = itertools.chain(iter(self._webidl_sources),
171969:                                            iter(self._generated_events_webidl_sources),
171969:                                            iter(self._generated_webidl_sources),
171969:                                            iter(self._preprocessed_webidl_sources))
171969:         all_webidl_files = [os.path.basename(x) for x in all_webidl_files]
171969:         all_webidl_sources = [re.sub(r'\.webidl$', 'Binding.cpp', x) for x in all_webidl_files]
171969: 
171969:         self._add_unified_build_rules(mk, all_webidl_sources,
171969:                                       bindings_dir,
171969:                                       unified_prefix='UnifiedBindings',
171969:                                       unified_files_makefile_variable='unified_binding_cpp_files',
171969:                                       poison_windows_h=True)
171969: 
171969:         # Assume that Somebody Else has responsibility for correctly
171969:         # specifying removal dependencies for |all_webidl_sources|.
171969:         with self._write_file(os.path.join(bindings_dir, 'webidlsrcs.mk')) as webidls:
171969:             mk.dump(webidls, removal_guard=False)
171969: 
171969:         self._may_skip['compile'] -= set(['dom/bindings', 'dom/bindings/test'])
169435: 
169435:         self._fill_root_mk()
169435: 
144630:         # Write out a dependency file used to determine whether a config.status
144630:         # re-run is needed.
144735:         inputs = sorted(p.replace(os.sep, '/') for p in self.backend_input_files)
144847: 
144847:         # We need to use $(DEPTH) so the target here matches what's in
144847:         # rules.mk. If they are different, the dependencies don't get pulled in
144847:         # properly.
168107:         with self._write_file('%s.pp' % self._backend_output_list_file) as backend_deps:
168107:             backend_deps.write('$(DEPTH)/backend.%s: %s\n' %
168107:                 (self.__class__.__name__, ' '.join(inputs)))
144630:             for path in inputs:
144630:                 backend_deps.write('%s:\n' % path)
144630: 
168448:         with open(self._backend_output_list_file, 'a'):
168448:             pass
168448:         os.utime(self._backend_output_list_file, None)
168448: 
163244:         # Make the master test manifest files.
163244:         for flavor, t in self._test_manifests.items():
163244:             install_prefix, manifests = t
163244:             manifest_stem = os.path.join(install_prefix, '%s.ini' % flavor)
163244:             self._write_master_test_manifest(os.path.join(
163244:                 self.environment.topobjdir, '_tests', manifest_stem),
163244:                 manifests)
163244: 
163244:             # Catch duplicate inserts.
163244:             try:
163244:                 self._install_manifests['tests'].add_optional_exists(manifest_stem)
163244:             except ValueError:
163244:                 pass
147165: 
154196:         self._write_manifests('install', self._install_manifests)
150021: 
163236:         ensureParentDir(os.path.join(self.environment.topobjdir, 'dist', 'foo'))
163236: 
130924:     def _process_directory_traversal(self, obj, backend_file):
130924:         """Process a data.DirectoryTraversal instance."""
130924:         fh = backend_file.fh
130924: 
162878:         def relativize(dirs):
162878:             return [mozpath.normpath(mozpath.join(backend_file.relobjdir, d))
162878:                 for d in dirs]
162878: 
130924:         for tier, dirs in obj.tier_dirs.iteritems():
130924:             fh.write('TIERS += %s\n' % tier)
162878:             # For pseudo derecursification, subtiers are treated as pseudo
162878:             # directories, with a special hierarchy:
162878:             # - subtier1 - subtier1_start - dirA - dirAA
162878:             # |          |                |      + dirAB
162878:             # |          |                ...
162878:             # |          |                + dirB
162878:             # |          + subtier1_finish
162878:             # + subtier2 - subtier2_start ...
162878:             # ...        + subtier2_finish
162878:             self._traversal.add('subtiers/%s' % tier,
162878:                                 dirs=['subtiers/%s_start' % tier,
162878:                                       'subtiers/%s_finish' % tier])
130924: 
130924:             if dirs:
130924:                 fh.write('tier_%s_dirs += %s\n' % (tier, ' '.join(dirs)))
157431:                 fh.write('DIRS += $(tier_%s_dirs)\n' % tier)
162878:                 self._traversal.add('subtiers/%s_start' % tier,
162878:                                     dirs=relativize(dirs))
130924: 
130924:             # tier_static_dirs should have the same keys as tier_dirs.
130924:             if obj.tier_static_dirs[tier]:
130924:                 fh.write('tier_%s_staticdirs += %s\n' % (
130924:                     tier, ' '.join(obj.tier_static_dirs[tier])))
162878:                 self._traversal.add('subtiers/%s_start' % tier,
162878:                                     static=relativize(obj.tier_static_dirs[tier]))
162878: 
162878:             self._traversal.add('subtiers/%s_start' % tier)
162878:             self._traversal.add('subtiers/%s_finish' % tier)
162878:             self._traversal.add('', dirs=['subtiers/%s' % tier])
134431: 
130924:         if obj.dirs:
130924:             fh.write('DIRS := %s\n' % ' '.join(obj.dirs))
162878:             self._traversal.add(backend_file.relobjdir, dirs=relativize(obj.dirs))
130924: 
130924:         if obj.parallel_dirs:
130924:             fh.write('PARALLEL_DIRS := %s\n' % ' '.join(obj.parallel_dirs))
162878:             self._traversal.add(backend_file.relobjdir,
162878:                                 parallel=relativize(obj.parallel_dirs))
130924: 
130924:         if obj.tool_dirs:
130924:             fh.write('TOOL_DIRS := %s\n' % ' '.join(obj.tool_dirs))
162878:             self._traversal.add(backend_file.relobjdir,
162878:                                 tools=relativize(obj.tool_dirs))
130924: 
130924:         if obj.test_dirs:
130924:             fh.write('TEST_DIRS := %s\n' % ' '.join(obj.test_dirs))
163154:             if self.environment.substs.get('ENABLE_TESTS', False):
162878:                 self._traversal.add(backend_file.relobjdir,
162878:                                     tests=relativize(obj.test_dirs))
130924: 
136340:         if obj.test_tool_dirs and \
136340:             self.environment.substs.get('ENABLE_TESTS', False):
136340: 
130924:             fh.write('TOOL_DIRS += %s\n' % ' '.join(obj.test_tool_dirs))
162878:             self._traversal.add(backend_file.relobjdir,
162878:                                 tools=relativize(obj.test_tool_dirs))
130924: 
130926:         if len(obj.external_make_dirs):
130926:             fh.write('DIRS += %s\n' % ' '.join(obj.external_make_dirs))
162878:             self._traversal.add(backend_file.relobjdir,
162878:                                 dirs=relativize(obj.external_make_dirs))
130926: 
130926:         if len(obj.parallel_external_make_dirs):
130926:             fh.write('PARALLEL_DIRS += %s\n' %
130926:                 ' '.join(obj.parallel_external_make_dirs))
162878:             self._traversal.add(backend_file.relobjdir,
162878:                                 parallel=relativize(obj.parallel_external_make_dirs))
162878: 
162878:         # The directory needs to be registered whether subdirectories have been
162878:         # registered or not.
162878:         self._traversal.add(backend_file.relobjdir)
130926: 
162580:         if obj.is_tool_dir:
162580:             fh.write('IS_TOOL_DIR := 1\n')
162580: 
164351:         if self._no_skip:
164351:             return
164351: 
164351:         affected_tiers = set(obj.affected_tiers)
169435:         # Until all SOURCES are really in moz.build, consider all directories
169435:         # building binaries to require a pass at compile, too.
169435:         if 'binaries' in affected_tiers:
169435:             affected_tiers.add('compile')
164351:         if 'compile' in affected_tiers or 'binaries' in affected_tiers:
164351:             affected_tiers.add('libs')
164351:         if obj.is_tool_dir and 'libs' in affected_tiers:
164351:             affected_tiers.remove('libs')
164351:             affected_tiers.add('tools')
164351: 
164351:         for tier in set(self._may_skip.keys()) - affected_tiers:
164351:             self._may_skip[tier].add(backend_file.relobjdir)
164351: 
159250:     def _process_exports(self, obj, exports, backend_file, namespace=""):
159381:         # This may not be needed, but is present for backwards compatibility
159381:         # with the old make rules, just in case.
159381:         if not obj.dist_install:
159381:             return
159381: 
139977:         strings = exports.get_strings()
139977:         if namespace:
139977:             namespace += '/'
139977: 
150022:         for s in strings:
159250:             source = os.path.normpath(os.path.join(obj.srcdir, s))
159381:             dest = '%s%s' % (namespace, os.path.basename(s))
159381:             self._install_manifests['dist_include'].add_symlink(source, dest)
159381: 
159381:             if not os.path.exists(source):
159270:                 raise Exception('File listed in EXPORTS does not exist: %s' % source)
159251: 
139977:         children = exports.get_children()
139977:         for subdir in sorted(children):
159250:             self._process_exports(obj, children[subdir], backend_file,
139977:                 namespace=namespace + subdir)
142045: 
166365:     def _process_installation_target(self, obj, backend_file):
166365:         # A few makefiles need to be able to override the following rules via
166365:         # make XPI_NAME=blah commands, so we default to the lazy evaluation as
166365:         # much as possible here to avoid breaking things.
166365:         if obj.xpiname:
166365:             backend_file.write('XPI_NAME = %s\n' % (obj.xpiname))
166365:         if obj.subdir:
166365:             backend_file.write('DIST_SUBDIR = %s\n' % (obj.subdir))
166365:         if obj.target and not obj.is_custom():
166365:             backend_file.write('FINAL_TARGET = $(DEPTH)/%s\n' % (obj.target))
166365:         else:
166365:             backend_file.write('FINAL_TARGET = $(if $(XPI_NAME),$(DIST)/xpi-stage/$(XPI_NAME),$(DIST)/bin)$(DIST_SUBDIR:%=/%)\n')
166365: 
166365:         if not obj.enabled:
166365:             backend_file.write('NO_DIST_INSTALL := 1\n')
166365: 
156517:     def _handle_idl_manager(self, manager):
162784:         build_files = self._install_manifests['xpidl']
156517: 
156517:         for p in ('Makefile', 'backend.mk', '.deps/.mkdir.done',
159429:             'xpt/.mkdir.done'):
162784:             build_files.add_optional_exists(p)
156517: 
156517:         for idl in manager.idls.values():
156517:             self._install_manifests['dist_idl'].add_symlink(idl['source'],
156517:                 idl['basename'])
159381:             self._install_manifests['dist_include'].add_optional_exists('%s.h'
159381:                 % idl['root'])
156517: 
156517:         for module in manager.modules:
162784:             build_files.add_optional_exists(mozpath.join('xpt',
162784:                 '%s.xpt' % module))
162784:             build_files.add_optional_exists(mozpath.join('.deps',
162784:                 '%s.pp' % module))
156517: 
156517:         modules = manager.modules
156517:         xpt_modules = sorted(modules.keys())
156517:         rules = []
156517: 
156517:         for module in xpt_modules:
156517:             deps = sorted(modules[module])
158112:             idl_deps = ['$(dist_idl_dir)/%s.idl' % dep for dep in deps]
156517:             rules.extend([
158112:                 # It may seem strange to have the .idl files listed as
158112:                 # prerequisites both here and in the auto-generated .pp files.
158112:                 # It is necessary to list them here to handle the case where a
158112:                 # new .idl is added to an xpt. If we add a new .idl and nothing
158112:                 # else has changed, the new .idl won't be referenced anywhere
158112:                 # except in the command invocation. Therefore, the .xpt won't
158112:                 # be rebuilt because the dependencies say it is up to date. By
158112:                 # listing the .idls here, we ensure the make file has a
158112:                 # reference to the new .idl. Since the new .idl presumably has
158112:                 # an mtime newer than the .xpt, it will trigger xpt generation.
158112:                 '$(idl_xpt_dir)/%s.xpt: %s' % (module, ' '.join(idl_deps)),
156517:                 '\t@echo "$(notdir $@)"',
156517:                 '\t$(idlprocess) $(basename $(notdir $@)) %s' % ' '.join(deps),
156517:                 '',
156517:             ])
156517: 
156517:         # Create dependency for output header so we force regeneration if the
156517:         # header was deleted. This ideally should not be necessary. However,
156517:         # some processes (such as PGO at the time this was implemented) wipe
162784:         # out dist/include without regard to our install manifests.
156517: 
156517:         out_path = os.path.join(self.environment.topobjdir, 'config',
156517:             'makefiles', 'xpidl', 'Makefile')
168106:         with self._write_file(out_path) as fh:
168106:             self.environment.create_config_file(fh, extra=dict(
156517:                 xpidl_rules='\n'.join(rules),
156517:                 xpidl_modules=' '.join(xpt_modules),
156517:             ))
156517: 
156523:         # The Makefile can't regenerate itself because of custom substitution.
156523:         # We need to list it here to ensure changes cause regeneration.
156523:         self.backend_input_files.add(os.path.join(self.environment.topsrcdir,
156523:             'config', 'makefiles', 'xpidl', 'Makefile.in'))
156523: 
143209:     def _process_program(self, program, backend_file):
143209:         backend_file.write('PROGRAM = %s\n' % program)
143209: 
168744:     def _process_host_program(self, program, backend_file):
168744:         backend_file.write('HOST_PROGRAM = %s\n' % program)
168744: 
168743:     def _process_simple_program(self, program, backend_file):
168743:         backend_file.write('SIMPLE_PROGRAMS += %s\n' % program)
168743: 
168744:     def _process_host_simple_program(self, program, backend_file):
168744:         backend_file.write('HOST_SIMPLE_PROGRAMS += %s\n' % program)
168744: 
171969:     def _process_webidl_basename(self, basename):
171969:         header = 'mozilla/dom/%sBinding.h' % os.path.splitext(basename)[0]
171969:         self._install_manifests['dist_include'].add_optional_exists(header)
171969: 
163244:     def _process_test_manifest(self, obj, backend_file):
166496:         # Much of the logic in this function could be moved to CommonBackend.
163244:         self.backend_input_files.add(os.path.join(obj.topsrcdir,
163244:             obj.manifest_relpath))
163244: 
172472:         # Duplicate manifests may define the same file. That's OK.
172472:         for source, dest in obj.installs.items():
163244:             try:
163244:                 self._install_manifests['tests'].add_symlink(source, dest)
163244:             except ValueError:
172472:                 if not obj.dupe_manifest:
163244:                     raise
163244: 
163244:         for dest in obj.external_installs:
163244:             try:
163244:                 self._install_manifests['tests'].add_optional_exists(dest)
163244:             except ValueError:
163244:                 if not obj.dupe_manifest:
163244:                     raise
163244: 
163244:         m = self._test_manifests.setdefault(obj.flavor,
163244:             (obj.install_prefix, set()))
163244:         m[1].add(obj.manifest_relpath)
150021: 
159584:     def _process_local_include(self, local_include, backend_file):
159584:         if local_include.startswith('/'):
159584:             path = '$(topsrcdir)'
159584:         else:
159584:             path = '$(srcdir)/'
159584:         backend_file.write('LOCAL_INCLUDES += -I%s%s\n' % (path, local_include))
159584: 
166712:     def _process_generated_include(self, generated_include, backend_file):
166712:         if generated_include.startswith('/'):
166712:             path = self.environment.topobjdir.replace('\\', '/')
166712:         else:
166712:             path = ''
166712:         backend_file.write('LOCAL_INCLUDES += -I%s%s\n' % (path, generated_include))
166712: 
167037:     def _process_java_jar_data(self, jar, backend_file):
167037:         target = jar.name
167037:         backend_file.write('JAVA_JAR_TARGETS += %s\n' % target)
167037:         backend_file.write('%s_DEST := %s.jar\n' % (target, jar.name))
167037:         if jar.sources:
167037:             backend_file.write('%s_JAVAFILES := %s\n' %
167037:                 (target, ' '.join(jar.sources)))
167037:         if jar.generated_sources:
167037:             backend_file.write('%s_PP_JAVAFILES := %s\n' %
170279:                 (target, ' '.join(os.path.join('generated', f) for f in jar.generated_sources)))
167037:         if jar.extra_jars:
167037:             backend_file.write('%s_EXTRA_JARS := %s\n' %
167037:                 (target, ' '.join(jar.extra_jars)))
167037:         if jar.javac_flags:
167037:             backend_file.write('%s_JAVAC_FLAGS := %s\n' %
170279:                 (target, ' '.join(jar.javac_flags)))
167037: 
171741:     def _process_library_definition(self, libdef, backend_file):
171741:         backend_file.write('LIBRARY_NAME = %s\n' % libdef.basename)
171789:         thisobjdir = libdef.objdir
171794:         topobjdir = libdef.topobjdir.replace(os.sep, '/')
171789:         for objdir, basename in libdef.static_libraries:
171789:             # If this is an external objdir (i.e., comm-central), use the other
171789:             # directory instead of $(DEPTH).
171789:             if objdir.startswith(topobjdir + '/'):
171789:                 relpath = '$(DEPTH)/%s' % mozpath.relpath(objdir, topobjdir)
171789:             else:
171790:                 relpath = mozpath.relpath(objdir, thisobjdir)
171789:             backend_file.write('SHARED_LIBRARY_LIBS += %s/$(LIB_PREFIX)%s.$(LIB_SUFFIX)\n'
171789:                                % (relpath, basename))
171741: 
154196:     def _write_manifests(self, dest, manifests):
150021:         man_dir = os.path.join(self.environment.topobjdir, '_build_manifests',
154196:             dest)
150021: 
154196:         # We have a purger for the manifests themselves to ensure legacy
154196:         # manifests are deleted.
150021:         purger = FilePurger()
150021: 
154196:         for k, manifest in manifests.items():
150021:             purger.add(k)
150021: 
168106:             with self._write_file(os.path.join(man_dir, k)) as fh:
152884:                 manifest.write(fileobj=fh)
150021: 
150021:         purger.purge(man_dir)
163244: 
163244:     def _write_master_test_manifest(self, path, manifests):
168106:         with self._write_file(path) as master:
163244:             master.write(
163244:                 '; THIS FILE WAS AUTOMATICALLY GENERATED. DO NOT MODIFY BY HAND.\n\n')
163244: 
163244:             for manifest in sorted(manifests):
163244:                 master.write('[include:%s]\n' % manifest)
