37355: /*
37355:  * Copyright Â© 2009 Nokia Corporation
37355:  *
37355:  * Permission is hereby granted, free of charge, to any person obtaining a
37355:  * copy of this software and associated documentation files (the "Software"),
37355:  * to deal in the Software without restriction, including without limitation
37355:  * the rights to use, copy, modify, merge, publish, distribute, sublicense,
37355:  * and/or sell copies of the Software, and to permit persons to whom the
37355:  * Software is furnished to do so, subject to the following conditions:
37355:  *
37355:  * The above copyright notice and this permission notice (including the next
37355:  * paragraph) shall be included in all copies or substantial portions of the
37355:  * Software.
37355:  *
37355:  * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
37355:  * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
37355:  * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
37355:  * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
37355:  * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
37355:  * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
37355:  * DEALINGS IN THE SOFTWARE.
37355:  *
37355:  * Author:  Siarhei Siamashka (siarhei.siamashka@nokia.com)
37355:  */
37355: 
37355: /*
37355:  * This file contains implementations of NEON optimized pixel processing
37355:  * functions. There is no full and detailed tutorial, but some functions
37355:  * (those which are exposing some new or interesting features) are
37355:  * extensively commented and can be used as examples.
37355:  *
37355:  * You may want to have a look at the comments for following functions:
37355:  *  - pixman_composite_over_8888_0565_asm_neon
37355:  *  - pixman_composite_over_n_8_0565_asm_neon
37355:  */
37355: 
37355: /* Prevent the stack from becoming executable for no reason... */
37355: #if defined(__linux__) && defined(__ELF__)
37355: .section .note.GNU-stack,"",%progbits
37355: #endif
37355: 
37355:     .text
37355:     .fpu neon
37355:     .arch armv7a
41920:     .object_arch armv4
41920:     .eabi_attribute 10, 0 /* suppress Tag_FP_arch */
41920:     .eabi_attribute 12, 0 /* suppress Tag_Advanced_SIMD_arch */
41920:     .arm
37355:     .altmacro
78578:     .p2align 2
37355: 
37355: #include "pixman-arm-neon-asm.h"
37355: 
37355: /* Global configuration options and preferences */
37355: 
37355: /*
37355:  * The code can optionally make use of unaligned memory accesses to improve
37355:  * performance of handling leading/trailing pixels for each scanline.
37355:  * Configuration variable RESPECT_STRICT_ALIGNMENT can be set to 0 for
37355:  * example in linux if unaligned memory accesses are not configured to
37355:  * generate.exceptions.
37355:  */
37355: .set RESPECT_STRICT_ALIGNMENT, 1
37355: 
37355: /*
37355:  * Set default prefetch type. There is a choice between the following options:
37355:  *
37355:  * PREFETCH_TYPE_NONE (may be useful for the ARM cores where PLD is set to work
37355:  * as NOP to workaround some HW bugs or for whatever other reason)
37355:  *
37355:  * PREFETCH_TYPE_SIMPLE (may be useful for simple single-issue ARM cores where
37355:  * advanced prefetch intruduces heavy overhead)
37355:  *
37355:  * PREFETCH_TYPE_ADVANCED (useful for superscalar cores such as ARM Cortex-A8
37355:  * which can run ARM and NEON instructions simultaneously so that extra ARM
37355:  * instructions do not add (many) extra cycles, but improve prefetch efficiency)
37355:  *
37355:  * Note: some types of function can't support advanced prefetch and fallback
37355:  *       to simple one (those which handle 24bpp pixels)
37355:  */
37355: .set PREFETCH_TYPE_DEFAULT, PREFETCH_TYPE_ADVANCED
37355: 
37355: /* Prefetch distance in pixels for simple prefetch */
37355: .set PREFETCH_DISTANCE_SIMPLE, 64
37355: 
37355: /*
37355:  * Implementation of pixman_composite_over_8888_0565_asm_neon
37355:  *
37355:  * This function takes a8r8g8b8 source buffer, r5g6b5 destination buffer and
37355:  * performs OVER compositing operation. Function fast_composite_over_8888_0565
37355:  * from pixman-fast-path.c does the same in C and can be used as a reference.
37355:  *
37355:  * First we need to have some NEON assembly code which can do the actual
37355:  * operation on the pixels and provide it to the template macro.
37355:  *
37355:  * Template macro quite conveniently takes care of emitting all the necessary
37355:  * code for memory reading and writing (including quite tricky cases of
37355:  * handling unaligned leading/trailing pixels), so we only need to deal with
37355:  * the data in NEON registers.
37355:  *
37355:  * NEON registers allocation in general is recommented to be the following:
37355:  * d0,  d1,  d2,  d3  - contain loaded source pixel data
37355:  * d4,  d5,  d6,  d7  - contain loaded destination pixels (if they are needed)
37355:  * d24, d25, d26, d27 - contain loading mask pixel data (if mask is used)
37355:  * d28, d29, d30, d31 - place for storing the result (destination pixels)
37355:  *
37355:  * As can be seen above, four 64-bit NEON registers are used for keeping
37355:  * intermediate pixel data and up to 8 pixels can be processed in one step
37355:  * for 32bpp formats (16 pixels for 16bpp, 32 pixels for 8bpp).
37355:  *
37355:  * This particular function uses the following registers allocation:
37355:  * d0,  d1,  d2,  d3  - contain loaded source pixel data
37355:  * d4,  d5            - contain loaded destination pixels (they are needed)
37355:  * d28, d29           - place for storing the result (destination pixels)
37355:  */
37355: 
37355: /*
37355:  * Step one. We need to have some code to do some arithmetics on pixel data.
37355:  * This is implemented as a pair of macros: '*_head' and '*_tail'. When used
37355:  * back-to-back, they take pixel data from {d0, d1, d2, d3} and {d4, d5},
37355:  * perform all the needed calculations and write the result to {d28, d29}.
37355:  * The rationale for having two macros and not just one will be explained
37355:  * later. In practice, any single monolitic function which does the work can
37355:  * be split into two parts in any arbitrary way without affecting correctness.
37355:  *
37355:  * There is one special trick here too. Common template macro can optionally
37355:  * make our life a bit easier by doing R, G, B, A color components
37355:  * deinterleaving for 32bpp pixel formats (and this feature is used in
37355:  * 'pixman_composite_over_8888_0565_asm_neon' function). So it means that
37355:  * instead of having 8 packed pixels in {d0, d1, d2, d3} registers, we
37355:  * actually use d0 register for blue channel (a vector of eight 8-bit
37355:  * values), d1 register for green, d2 for red and d3 for alpha. This
37355:  * simple conversion can be also done with a few NEON instructions:
37355:  *
37355:  * Packed to planar conversion:
37355:  *  vuzp.8 d0, d1
37355:  *  vuzp.8 d2, d3
37355:  *  vuzp.8 d1, d3
37355:  *  vuzp.8 d0, d2
37355:  *
37355:  * Planar to packed conversion:
37355:  *  vzip.8 d0, d2
37355:  *  vzip.8 d1, d3
37355:  *  vzip.8 d2, d3
37355:  *  vzip.8 d0, d1
37355:  *
37355:  * But pixel can be loaded directly in planar format using VLD4.8 NEON
37355:  * instruction. It is 1 cycle slower than VLD1.32, so this is not always
37355:  * desirable, that's why deinterleaving is optional.
37355:  *
37355:  * But anyway, here is the code:
37355:  */
37355: .macro pixman_composite_over_8888_0565_process_pixblock_head
37355:     /* convert 8 r5g6b5 pixel data from {d4, d5} to planar 8-bit format
37355:        and put data into d6 - red, d7 - green, d30 - blue */
37355:     vshrn.u16   d6, q2, #8
37355:     vshrn.u16   d7, q2, #3
37355:     vsli.u16    q2, q2, #5
37355:     vsri.u8     d6, d6, #5
37355:     vmvn.8      d3, d3      /* invert source alpha */
37355:     vsri.u8     d7, d7, #6
37355:     vshrn.u16   d30, q2, #2
37355:     /* now do alpha blending, storing results in 8-bit planar format
37355:        into d16 - red, d19 - green, d18 - blue */
37355:     vmull.u8    q10, d3, d6
37355:     vmull.u8    q11, d3, d7
37355:     vmull.u8    q12, d3, d30
37355:     vrshr.u16   q13, q10, #8
37355:     vrshr.u16   q3, q11, #8
37355:     vrshr.u16   q15, q12, #8
37355:     vraddhn.u16 d20, q10, q13
37355:     vraddhn.u16 d23, q11, q3
37355:     vraddhn.u16 d22, q12, q15
37355: .endm
37355: 
37355: .macro pixman_composite_over_8888_0565_process_pixblock_tail
37355:     /* ... continue alpha blending */
37355:     vqadd.u8    d16, d2, d20
37355:     vqadd.u8    q9, q0, q11
37355:     /* convert the result to r5g6b5 and store it into {d28, d29} */
37355:     vshll.u8    q14, d16, #8
37355:     vshll.u8    q8, d19, #8
37355:     vshll.u8    q9, d18, #8
37355:     vsri.u16    q14, q8, #5
37355:     vsri.u16    q14, q9, #11
37355: .endm
37355: 
37355: /*
37355:  * OK, now we got almost everything that we need. Using the above two
37355:  * macros, the work can be done right. But now we want to optimize
37355:  * it a bit. ARM Cortex-A8 is an in-order core, and benefits really
37355:  * a lot from good code scheduling and software pipelining.
37355:  *
37355:  * Let's construct some code, which will run in the core main loop.
37355:  * Some pseudo-code of the main loop will look like this:
37355:  *   head
37355:  *   while (...) {
37355:  *     tail
37355:  *     head
37355:  *   }
37355:  *   tail
37355:  *
37355:  * It may look a bit weird, but this setup allows to hide instruction
37355:  * latencies better and also utilize dual-issue capability more
37355:  * efficiently (make pairs of load-store and ALU instructions).
37355:  *
37355:  * So what we need now is a '*_tail_head' macro, which will be used
37355:  * in the core main loop. A trivial straightforward implementation
37355:  * of this macro would look like this:
37355:  *
37355:  *   pixman_composite_over_8888_0565_process_pixblock_tail
37355:  *   vst1.16     {d28, d29}, [DST_W, :128]!
37355:  *   vld1.16     {d4, d5}, [DST_R, :128]!
37355:  *   vld4.32     {d0, d1, d2, d3}, [SRC]!
37355:  *   pixman_composite_over_8888_0565_process_pixblock_head
37355:  *   cache_preload 8, 8
37355:  *
37355:  * Now it also got some VLD/VST instructions. We simply can't move from
37355:  * processing one block of pixels to the other one with just arithmetics.
37355:  * The previously processed data needs to be written to memory and new
37355:  * data needs to be fetched. Fortunately, this main loop does not deal
37355:  * with partial leading/trailing pixels and can load/store a full block
37355:  * of pixels in a bulk. Additionally, destination buffer is already
37355:  * 16 bytes aligned here (which is good for performance).
37355:  *
37355:  * New things here are DST_R, DST_W, SRC and MASK identifiers. These
37355:  * are the aliases for ARM registers which are used as pointers for
37355:  * accessing data. We maintain separate pointers for reading and writing
37355:  * destination buffer (DST_R and DST_W).
37355:  *
37355:  * Another new thing is 'cache_preload' macro. It is used for prefetching
37355:  * data into CPU L2 cache and improve performance when dealing with large
37355:  * images which are far larger than cache size. It uses one argument
37355:  * (actually two, but they need to be the same here) - number of pixels
37355:  * in a block. Looking into 'pixman-arm-neon-asm.h' can provide some
37355:  * details about this macro. Moreover, if good performance is needed
37355:  * the code from this macro needs to be copied into '*_tail_head' macro
37355:  * and mixed with the rest of code for optimal instructions scheduling.
37355:  * We are actually doing it below.
37355:  *
37355:  * Now after all the explanations, here is the optimized code.
37355:  * Different instruction streams (originaling from '*_head', '*_tail'
37355:  * and 'cache_preload' macro) use different indentation levels for
37355:  * better readability. Actually taking the code from one of these
37355:  * indentation levels and ignoring a few VLD/VST instructions would
37355:  * result in exactly the code from '*_head', '*_tail' or 'cache_preload'
37355:  * macro!
37355:  */
37355: 
37355: #if 1
37355: 
37355: .macro pixman_composite_over_8888_0565_process_pixblock_tail_head
37355:         vqadd.u8    d16, d2, d20
37355:     vld1.16     {d4, d5}, [DST_R, :128]!
37355:         vqadd.u8    q9, q0, q11
37355:     vshrn.u16   d6, q2, #8
64161:     fetch_src_pixblock
37355:     vshrn.u16   d7, q2, #3
37355:     vsli.u16    q2, q2, #5
37355:         vshll.u8    q14, d16, #8
37355:                                     PF add PF_X, PF_X, #8
37355:         vshll.u8    q8, d19, #8
37355:                                     PF tst PF_CTL, #0xF
37355:     vsri.u8     d6, d6, #5
37355:                                     PF addne PF_X, PF_X, #8
37355:     vmvn.8      d3, d3
37355:                                     PF subne PF_CTL, PF_CTL, #1
37355:     vsri.u8     d7, d7, #6
37355:     vshrn.u16   d30, q2, #2
37355:     vmull.u8    q10, d3, d6
37355:                                     PF pld, [PF_SRC, PF_X, lsl #src_bpp_shift]
37355:     vmull.u8    q11, d3, d7
37355:     vmull.u8    q12, d3, d30
37355:                                     PF pld, [PF_DST, PF_X, lsl #dst_bpp_shift]
37355:         vsri.u16    q14, q8, #5
37355:                                     PF cmp PF_X, ORIG_W
37355:         vshll.u8    q9, d18, #8
37355:     vrshr.u16   q13, q10, #8
37355:                                     PF subge PF_X, PF_X, ORIG_W
37355:     vrshr.u16   q3, q11, #8
37355:     vrshr.u16   q15, q12, #8
37355:                                     PF subges PF_CTL, PF_CTL, #0x10
37355:         vsri.u16    q14, q9, #11
37355:                                     PF ldrgeb DUMMY, [PF_SRC, SRC_STRIDE, lsl #src_bpp_shift]!
37355:     vraddhn.u16 d20, q10, q13
37355:     vraddhn.u16 d23, q11, q3
37355:                                     PF ldrgeb DUMMY, [PF_DST, DST_STRIDE, lsl #dst_bpp_shift]!
37355:     vraddhn.u16 d22, q12, q15
37355:         vst1.16     {d28, d29}, [DST_W, :128]!
37355: .endm
37355: 
37355: #else
37355: 
37355: /* If we did not care much about the performance, we would just use this... */
37355: .macro pixman_composite_over_8888_0565_process_pixblock_tail_head
37355:     pixman_composite_over_8888_0565_process_pixblock_tail
37355:     vst1.16     {d28, d29}, [DST_W, :128]!
37355:     vld1.16     {d4, d5}, [DST_R, :128]!
64161:     fetch_src_pixblock
37355:     pixman_composite_over_8888_0565_process_pixblock_head
37355:     cache_preload 8, 8
37355: .endm
37355: 
37355: #endif
37355: 
37355: /*
37355:  * And now the final part. We are using 'generate_composite_function' macro
37355:  * to put all the stuff together. We are specifying the name of the function
37355:  * which we want to get, number of bits per pixel for the source, mask and
37355:  * destination (0 if unused, like mask in this case). Next come some bit
37355:  * flags:
37355:  *   FLAG_DST_READWRITE      - tells that the destination buffer is both read
37355:  *                             and written, for write-only buffer we would use
37355:  *                             FLAG_DST_WRITEONLY flag instead
37355:  *   FLAG_DEINTERLEAVE_32BPP - tells that we prefer to work with planar data
37355:  *                             and separate color channels for 32bpp format.
37355:  * The next things are:
37355:  *  - the number of pixels processed per iteration (8 in this case, because
37355:  *    that's the maximum what can fit into four 64-bit NEON registers).
37355:  *  - prefetch distance, measured in pixel blocks. In this case it is 5 times
37355:  *    by 8 pixels. That would be 40 pixels, or up to 160 bytes. Optimal
37355:  *    prefetch distance can be selected by running some benchmarks.
37355:  *
37355:  * After that we specify some macros, these are 'default_init',
37355:  * 'default_cleanup' here which are empty (but it is possible to have custom
37355:  * init/cleanup macros to be able to save/restore some extra NEON registers
37355:  * like d8-d15 or do anything else) followed by
37355:  * 'pixman_composite_over_8888_0565_process_pixblock_head',
37355:  * 'pixman_composite_over_8888_0565_process_pixblock_tail' and
37355:  * 'pixman_composite_over_8888_0565_process_pixblock_tail_head'
37355:  * which we got implemented above.
37355:  *
37355:  * The last part is the NEON registers allocation scheme.
37355:  */
37355: generate_composite_function \
37355:     pixman_composite_over_8888_0565_asm_neon, 32, 0, 16, \
37355:     FLAG_DST_READWRITE | FLAG_DEINTERLEAVE_32BPP, \
37355:     8, /* number of pixels, processed in a single block */ \
37355:     5, /* prefetch distance */ \
37355:     default_init, \
37355:     default_cleanup, \
37355:     pixman_composite_over_8888_0565_process_pixblock_head, \
37355:     pixman_composite_over_8888_0565_process_pixblock_tail, \
37355:     pixman_composite_over_8888_0565_process_pixblock_tail_head, \
37355:     28, /* dst_w_basereg */ \
37355:     4,  /* dst_r_basereg */ \
37355:     0,  /* src_basereg   */ \
37355:     24  /* mask_basereg  */
37355: 
37355: /******************************************************************************/
37355: 
37355: .macro pixman_composite_over_n_0565_process_pixblock_head
37355:     /* convert 8 r5g6b5 pixel data from {d4, d5} to planar 8-bit format
37355:        and put data into d6 - red, d7 - green, d30 - blue */
37355:     vshrn.u16   d6, q2, #8
37355:     vshrn.u16   d7, q2, #3
37355:     vsli.u16    q2, q2, #5
37355:     vsri.u8     d6, d6, #5
37355:     vsri.u8     d7, d7, #6
37355:     vshrn.u16   d30, q2, #2
37355:     /* now do alpha blending, storing results in 8-bit planar format
37355:        into d16 - red, d19 - green, d18 - blue */
37355:     vmull.u8    q10, d3, d6
37355:     vmull.u8    q11, d3, d7
37355:     vmull.u8    q12, d3, d30
37355:     vrshr.u16   q13, q10, #8
37355:     vrshr.u16   q3, q11, #8
37355:     vrshr.u16   q15, q12, #8
37355:     vraddhn.u16 d20, q10, q13
37355:     vraddhn.u16 d23, q11, q3
37355:     vraddhn.u16 d22, q12, q15
37355: .endm
37355: 
37355: .macro pixman_composite_over_n_0565_process_pixblock_tail
37355:     /* ... continue alpha blending */
37355:     vqadd.u8    d16, d2, d20
37355:     vqadd.u8    q9, q0, q11
37355:     /* convert the result to r5g6b5 and store it into {d28, d29} */
37355:     vshll.u8    q14, d16, #8
37355:     vshll.u8    q8, d19, #8
37355:     vshll.u8    q9, d18, #8
37355:     vsri.u16    q14, q8, #5
37355:     vsri.u16    q14, q9, #11
37355: .endm
37355: 
37355: /* TODO: expand macros and do better instructions scheduling */
37355: .macro pixman_composite_over_n_0565_process_pixblock_tail_head
37355:     pixman_composite_over_n_0565_process_pixblock_tail
37355:     vld1.16     {d4, d5}, [DST_R, :128]!
37355:     vst1.16     {d28, d29}, [DST_W, :128]!
37355:     pixman_composite_over_n_0565_process_pixblock_head
56260:     cache_preload 8, 8
37355: .endm
37355: 
37355: .macro pixman_composite_over_n_0565_init
37355:     add         DUMMY, sp, #ARGS_STACK_OFFSET
37355:     vld1.32     {d3[0]}, [DUMMY]
37355:     vdup.8      d0, d3[0]
37355:     vdup.8      d1, d3[1]
37355:     vdup.8      d2, d3[2]
37355:     vdup.8      d3, d3[3]
37355:     vmvn.8      d3, d3      /* invert source alpha */
37355: .endm
37355: 
37355: generate_composite_function \
37355:     pixman_composite_over_n_0565_asm_neon, 0, 0, 16, \
37355:     FLAG_DST_READWRITE, \
37355:     8, /* number of pixels, processed in a single block */ \
37355:     5, /* prefetch distance */ \
37355:     pixman_composite_over_n_0565_init, \
37355:     default_cleanup, \
37355:     pixman_composite_over_n_0565_process_pixblock_head, \
37355:     pixman_composite_over_n_0565_process_pixblock_tail, \
37355:     pixman_composite_over_n_0565_process_pixblock_tail_head, \
37355:     28, /* dst_w_basereg */ \
37355:     4,  /* dst_r_basereg */ \
37355:     0,  /* src_basereg   */ \
37355:     24  /* mask_basereg  */
37355: 
37355: /******************************************************************************/
37355: 
37355: .macro pixman_composite_src_8888_0565_process_pixblock_head
37355:     vshll.u8    q8, d1, #8
37355:     vshll.u8    q14, d2, #8
37355:     vshll.u8    q9, d0, #8
37355: .endm
37355: 
37355: .macro pixman_composite_src_8888_0565_process_pixblock_tail
37355:     vsri.u16    q14, q8, #5
37355:     vsri.u16    q14, q9, #11
37355: .endm
37355: 
37355: .macro pixman_composite_src_8888_0565_process_pixblock_tail_head
37355:         vsri.u16    q14, q8, #5
37355:                                     PF add PF_X, PF_X, #8
37355:                                     PF tst PF_CTL, #0xF
64161:     fetch_src_pixblock
37355:                                     PF addne PF_X, PF_X, #8
37355:                                     PF subne PF_CTL, PF_CTL, #1
37355:         vsri.u16    q14, q9, #11
37355:                                     PF cmp PF_X, ORIG_W
37355:                                     PF pld, [PF_SRC, PF_X, lsl #src_bpp_shift]
37355:     vshll.u8    q8, d1, #8
37355:         vst1.16     {d28, d29}, [DST_W, :128]!
37355:                                     PF subge PF_X, PF_X, ORIG_W
37355:                                     PF subges PF_CTL, PF_CTL, #0x10
37355:     vshll.u8    q14, d2, #8
37355:                                     PF ldrgeb DUMMY, [PF_SRC, SRC_STRIDE, lsl #src_bpp_shift]!
37355:     vshll.u8    q9, d0, #8
37355: .endm
37355: 
37355: generate_composite_function \
37355:     pixman_composite_src_8888_0565_asm_neon, 32, 0, 16, \
37355:     FLAG_DST_WRITEONLY | FLAG_DEINTERLEAVE_32BPP, \
37355:     8, /* number of pixels, processed in a single block */ \
37355:     10, /* prefetch distance */ \
37355:     default_init, \
37355:     default_cleanup, \
37355:     pixman_composite_src_8888_0565_process_pixblock_head, \
37355:     pixman_composite_src_8888_0565_process_pixblock_tail, \
37355:     pixman_composite_src_8888_0565_process_pixblock_tail_head
37355: 
37355: /******************************************************************************/
37355: 
37355: .macro pixman_composite_src_0565_8888_process_pixblock_head
37355:     vshrn.u16   d30, q0, #8
37355:     vshrn.u16   d29, q0, #3
37355:     vsli.u16    q0, q0, #5
37355:     vmov.u8     d31, #255
37355:     vsri.u8     d30, d30, #5
37355:     vsri.u8     d29, d29, #6
37355:     vshrn.u16   d28, q0, #2
37355: .endm
37355: 
37355: .macro pixman_composite_src_0565_8888_process_pixblock_tail
37355: .endm
37355: 
37355: /* TODO: expand macros and do better instructions scheduling */
37355: .macro pixman_composite_src_0565_8888_process_pixblock_tail_head
37355:     pixman_composite_src_0565_8888_process_pixblock_tail
37355:     vst4.8     {d28, d29, d30, d31}, [DST_W, :128]!
64161:     fetch_src_pixblock
37355:     pixman_composite_src_0565_8888_process_pixblock_head
37355:     cache_preload 8, 8
37355: .endm
37355: 
37355: generate_composite_function \
37355:     pixman_composite_src_0565_8888_asm_neon, 16, 0, 32, \
37355:     FLAG_DST_WRITEONLY | FLAG_DEINTERLEAVE_32BPP, \
37355:     8, /* number of pixels, processed in a single block */ \
37355:     10, /* prefetch distance */ \
37355:     default_init, \
37355:     default_cleanup, \
37355:     pixman_composite_src_0565_8888_process_pixblock_head, \
37355:     pixman_composite_src_0565_8888_process_pixblock_tail, \
37355:     pixman_composite_src_0565_8888_process_pixblock_tail_head
37355: 
37355: /******************************************************************************/
37355: 
56260: .macro pixman_composite_add_8_8_process_pixblock_head
37355:     vqadd.u8    q14, q0, q2
37355:     vqadd.u8    q15, q1, q3
37355: .endm
37355: 
56260: .macro pixman_composite_add_8_8_process_pixblock_tail
37355: .endm
37355: 
56260: .macro pixman_composite_add_8_8_process_pixblock_tail_head
64161:     fetch_src_pixblock
37355:                                     PF add PF_X, PF_X, #32
37355:                                     PF tst PF_CTL, #0xF
37355:     vld1.8      {d4, d5, d6, d7}, [DST_R, :128]!
37355:                                     PF addne PF_X, PF_X, #32
37355:                                     PF subne PF_CTL, PF_CTL, #1
37355:         vst1.8      {d28, d29, d30, d31}, [DST_W, :128]!
37355:                                     PF cmp PF_X, ORIG_W
37355:                                     PF pld, [PF_SRC, PF_X, lsl #src_bpp_shift]
37355:                                     PF pld, [PF_DST, PF_X, lsl #dst_bpp_shift]
37355:                                     PF subge PF_X, PF_X, ORIG_W
37355:                                     PF subges PF_CTL, PF_CTL, #0x10
37355:     vqadd.u8    q14, q0, q2
37355:                                     PF ldrgeb DUMMY, [PF_SRC, SRC_STRIDE, lsl #src_bpp_shift]!
37355:                                     PF ldrgeb DUMMY, [PF_DST, DST_STRIDE, lsl #dst_bpp_shift]!
37355:     vqadd.u8    q15, q1, q3
37355: .endm
37355: 
37355: generate_composite_function \
56260:     pixman_composite_add_8_8_asm_neon, 8, 0, 8, \
37355:     FLAG_DST_READWRITE, \
37355:     32, /* number of pixels, processed in a single block */ \
37355:     10, /* prefetch distance */ \
37355:     default_init, \
37355:     default_cleanup, \
56260:     pixman_composite_add_8_8_process_pixblock_head, \
56260:     pixman_composite_add_8_8_process_pixblock_tail, \
56260:     pixman_composite_add_8_8_process_pixblock_tail_head
37355: 
37355: /******************************************************************************/
37355: 
37355: .macro pixman_composite_add_8888_8888_process_pixblock_tail_head
64161:     fetch_src_pixblock
37355:                                     PF add PF_X, PF_X, #8
37355:                                     PF tst PF_CTL, #0xF
64161:     vld1.32     {d4, d5, d6, d7}, [DST_R, :128]!
37355:                                     PF addne PF_X, PF_X, #8
37355:                                     PF subne PF_CTL, PF_CTL, #1
64161:         vst1.32     {d28, d29, d30, d31}, [DST_W, :128]!
37355:                                     PF cmp PF_X, ORIG_W
37355:                                     PF pld, [PF_SRC, PF_X, lsl #src_bpp_shift]
37355:                                     PF pld, [PF_DST, PF_X, lsl #dst_bpp_shift]
37355:                                     PF subge PF_X, PF_X, ORIG_W
37355:                                     PF subges PF_CTL, PF_CTL, #0x10
37355:     vqadd.u8    q14, q0, q2
37355:                                     PF ldrgeb DUMMY, [PF_SRC, SRC_STRIDE, lsl #src_bpp_shift]!
37355:                                     PF ldrgeb DUMMY, [PF_DST, DST_STRIDE, lsl #dst_bpp_shift]!
37355:     vqadd.u8    q15, q1, q3
37355: .endm
37355: 
37355: generate_composite_function \
37355:     pixman_composite_add_8888_8888_asm_neon, 32, 0, 32, \
37355:     FLAG_DST_READWRITE, \
37355:     8, /* number of pixels, processed in a single block */ \
37355:     10, /* prefetch distance */ \
37355:     default_init, \
37355:     default_cleanup, \
56260:     pixman_composite_add_8_8_process_pixblock_head, \
56260:     pixman_composite_add_8_8_process_pixblock_tail, \
37355:     pixman_composite_add_8888_8888_process_pixblock_tail_head
37355: 
37355: generate_composite_function_single_scanline \
37355:     pixman_composite_scanline_add_asm_neon, 32, 0, 32, \
37355:     FLAG_DST_READWRITE, \
37355:     8, /* number of pixels, processed in a single block */ \
37355:     default_init, \
37355:     default_cleanup, \
56260:     pixman_composite_add_8_8_process_pixblock_head, \
56260:     pixman_composite_add_8_8_process_pixblock_tail, \
37355:     pixman_composite_add_8888_8888_process_pixblock_tail_head
37355: 
37355: /******************************************************************************/
37355: 
56260: .macro pixman_composite_out_reverse_8888_8888_process_pixblock_head
37355:     vmvn.8      d24, d3  /* get inverted alpha */
37355:     /* do alpha blending */
37355:     vmull.u8    q8, d24, d4
37355:     vmull.u8    q9, d24, d5
37355:     vmull.u8    q10, d24, d6
37355:     vmull.u8    q11, d24, d7
37355: .endm
37355: 
56260: .macro pixman_composite_out_reverse_8888_8888_process_pixblock_tail
37355:     vrshr.u16   q14, q8, #8
37355:     vrshr.u16   q15, q9, #8
37355:     vrshr.u16   q12, q10, #8
37355:     vrshr.u16   q13, q11, #8
37355:     vraddhn.u16 d28, q14, q8
37355:     vraddhn.u16 d29, q15, q9
37355:     vraddhn.u16 d30, q12, q10
37355:     vraddhn.u16 d31, q13, q11
56260: .endm
56260: 
56260: .macro pixman_composite_out_reverse_8888_8888_process_pixblock_tail_head
56260:     vld4.8      {d4, d5, d6, d7}, [DST_R, :128]!
56260:         vrshr.u16   q14, q8, #8
56260:                                     PF add PF_X, PF_X, #8
56260:                                     PF tst PF_CTL, #0xF
56260:         vrshr.u16   q15, q9, #8
56260:         vrshr.u16   q12, q10, #8
56260:         vrshr.u16   q13, q11, #8
56260:                                     PF addne PF_X, PF_X, #8
56260:                                     PF subne PF_CTL, PF_CTL, #1
56260:         vraddhn.u16 d28, q14, q8
56260:         vraddhn.u16 d29, q15, q9
56260:                                     PF cmp PF_X, ORIG_W
56260:         vraddhn.u16 d30, q12, q10
56260:         vraddhn.u16 d31, q13, q11
64161:     fetch_src_pixblock
56260:                                     PF pld, [PF_SRC, PF_X, lsl #src_bpp_shift]
56260:     vmvn.8      d22, d3
56260:                                     PF pld, [PF_DST, PF_X, lsl #dst_bpp_shift]
56260:         vst4.8      {d28, d29, d30, d31}, [DST_W, :128]!
56260:                                     PF subge PF_X, PF_X, ORIG_W
56260:     vmull.u8    q8, d22, d4
56260:                                     PF subges PF_CTL, PF_CTL, #0x10
56260:     vmull.u8    q9, d22, d5
56260:                                     PF ldrgeb DUMMY, [PF_SRC, SRC_STRIDE, lsl #src_bpp_shift]!
56260:     vmull.u8    q10, d22, d6
56260:                                     PF ldrgeb DUMMY, [PF_DST, DST_STRIDE, lsl #dst_bpp_shift]!
56260:     vmull.u8    q11, d22, d7
56260: .endm
56260: 
56260: generate_composite_function_single_scanline \
56260:     pixman_composite_scanline_out_reverse_asm_neon, 32, 0, 32, \
56260:     FLAG_DST_READWRITE | FLAG_DEINTERLEAVE_32BPP, \
56260:     8, /* number of pixels, processed in a single block */ \
56260:     default_init, \
56260:     default_cleanup, \
56260:     pixman_composite_out_reverse_8888_8888_process_pixblock_head, \
56260:     pixman_composite_out_reverse_8888_8888_process_pixblock_tail, \
56260:     pixman_composite_out_reverse_8888_8888_process_pixblock_tail_head
56260: 
56260: /******************************************************************************/
56260: 
56260: .macro pixman_composite_over_8888_8888_process_pixblock_head
56260:     pixman_composite_out_reverse_8888_8888_process_pixblock_head
56260: .endm
56260: 
56260: .macro pixman_composite_over_8888_8888_process_pixblock_tail
56260:     pixman_composite_out_reverse_8888_8888_process_pixblock_tail
37355:     vqadd.u8    q14, q0, q14
37355:     vqadd.u8    q15, q1, q15
37355: .endm
37355: 
37355: .macro pixman_composite_over_8888_8888_process_pixblock_tail_head
37355:     vld4.8      {d4, d5, d6, d7}, [DST_R, :128]!
37355:         vrshr.u16   q14, q8, #8
37355:                                     PF add PF_X, PF_X, #8
37355:                                     PF tst PF_CTL, #0xF
37355:         vrshr.u16   q15, q9, #8
37355:         vrshr.u16   q12, q10, #8
37355:         vrshr.u16   q13, q11, #8
37355:                                     PF addne PF_X, PF_X, #8
37355:                                     PF subne PF_CTL, PF_CTL, #1
37355:         vraddhn.u16 d28, q14, q8
37355:         vraddhn.u16 d29, q15, q9
37355:                                     PF cmp PF_X, ORIG_W
37355:         vraddhn.u16 d30, q12, q10
37355:         vraddhn.u16 d31, q13, q11
37355:         vqadd.u8    q14, q0, q14
37355:         vqadd.u8    q15, q1, q15
64161:     fetch_src_pixblock
37355:                                     PF pld, [PF_SRC, PF_X, lsl #src_bpp_shift]
37355:     vmvn.8      d22, d3
37355:                                     PF pld, [PF_DST, PF_X, lsl #dst_bpp_shift]
37355:         vst4.8      {d28, d29, d30, d31}, [DST_W, :128]!
37355:                                     PF subge PF_X, PF_X, ORIG_W
37355:     vmull.u8    q8, d22, d4
37355:                                     PF subges PF_CTL, PF_CTL, #0x10
37355:     vmull.u8    q9, d22, d5
37355:                                     PF ldrgeb DUMMY, [PF_SRC, SRC_STRIDE, lsl #src_bpp_shift]!
37355:     vmull.u8    q10, d22, d6
37355:                                     PF ldrgeb DUMMY, [PF_DST, DST_STRIDE, lsl #dst_bpp_shift]!
37355:     vmull.u8    q11, d22, d7
37355: .endm
37355: 
37355: generate_composite_function \
37355:     pixman_composite_over_8888_8888_asm_neon, 32, 0, 32, \
37355:     FLAG_DST_READWRITE | FLAG_DEINTERLEAVE_32BPP, \
37355:     8, /* number of pixels, processed in a single block */ \
37355:     5, /* prefetch distance */ \
37355:     default_init, \
37355:     default_cleanup, \
37355:     pixman_composite_over_8888_8888_process_pixblock_head, \
37355:     pixman_composite_over_8888_8888_process_pixblock_tail, \
37355:     pixman_composite_over_8888_8888_process_pixblock_tail_head
37355: 
37355: generate_composite_function_single_scanline \
37355:     pixman_composite_scanline_over_asm_neon, 32, 0, 32, \
37355:     FLAG_DST_READWRITE | FLAG_DEINTERLEAVE_32BPP, \
37355:     8, /* number of pixels, processed in a single block */ \
37355:     default_init, \
37355:     default_cleanup, \
37355:     pixman_composite_over_8888_8888_process_pixblock_head, \
37355:     pixman_composite_over_8888_8888_process_pixblock_tail, \
37355:     pixman_composite_over_8888_8888_process_pixblock_tail_head
37355: 
37355: /******************************************************************************/
37355: 
37355: /* TODO: expand macros and do better instructions scheduling */
37355: .macro pixman_composite_over_n_8888_process_pixblock_tail_head
37355:     pixman_composite_over_8888_8888_process_pixblock_tail
37355:     vld4.8      {d4, d5, d6, d7}, [DST_R, :128]!
37355:     vst4.8      {d28, d29, d30, d31}, [DST_W, :128]!
37355:     pixman_composite_over_8888_8888_process_pixblock_head
56260:     cache_preload 8, 8
37355: .endm
37355: 
37355: .macro pixman_composite_over_n_8888_init
37355:     add         DUMMY, sp, #ARGS_STACK_OFFSET
37355:     vld1.32     {d3[0]}, [DUMMY]
37355:     vdup.8      d0, d3[0]
37355:     vdup.8      d1, d3[1]
37355:     vdup.8      d2, d3[2]
37355:     vdup.8      d3, d3[3]
37355: .endm
37355: 
37355: generate_composite_function \
37355:     pixman_composite_over_n_8888_asm_neon, 0, 0, 32, \
37355:     FLAG_DST_READWRITE | FLAG_DEINTERLEAVE_32BPP, \
37355:     8, /* number of pixels, processed in a single block */ \
37355:     5, /* prefetch distance */ \
37355:     pixman_composite_over_n_8888_init, \
37355:     default_cleanup, \
37355:     pixman_composite_over_8888_8888_process_pixblock_head, \
37355:     pixman_composite_over_8888_8888_process_pixblock_tail, \
37355:     pixman_composite_over_n_8888_process_pixblock_tail_head
37355: 
37355: /******************************************************************************/
37355: 
41920: .macro pixman_composite_over_reverse_n_8888_process_pixblock_tail_head
41920:         vrshr.u16   q14, q8, #8
41920:                                     PF add PF_X, PF_X, #8
41920:                                     PF tst PF_CTL, #0xF
41920:         vrshr.u16   q15, q9, #8
41920:         vrshr.u16   q12, q10, #8
41920:         vrshr.u16   q13, q11, #8
41920:                                     PF addne PF_X, PF_X, #8
41920:                                     PF subne PF_CTL, PF_CTL, #1
41920:         vraddhn.u16 d28, q14, q8
41920:         vraddhn.u16 d29, q15, q9
41920:                                     PF cmp PF_X, ORIG_W
41920:         vraddhn.u16 d30, q12, q10
41920:         vraddhn.u16 d31, q13, q11
41920:         vqadd.u8    q14, q0, q14
41920:         vqadd.u8    q15, q1, q15
41920:     vld4.8      {d0, d1, d2, d3}, [DST_R, :128]!
41920:     vmvn.8      d22, d3
41920:                                     PF pld, [PF_DST, PF_X, lsl #dst_bpp_shift]
41920:         vst4.8      {d28, d29, d30, d31}, [DST_W, :128]!
41920:                                     PF subge PF_X, PF_X, ORIG_W
41920:     vmull.u8    q8, d22, d4
41920:                                     PF subges PF_CTL, PF_CTL, #0x10
41920:     vmull.u8    q9, d22, d5
41920:     vmull.u8    q10, d22, d6
41920:                                     PF ldrgeb DUMMY, [PF_DST, DST_STRIDE, lsl #dst_bpp_shift]!
41920:     vmull.u8    q11, d22, d7
41920: .endm
41920: 
41920: .macro pixman_composite_over_reverse_n_8888_init
41920:     add         DUMMY, sp, #ARGS_STACK_OFFSET
41920:     vld1.32     {d7[0]}, [DUMMY]
41920:     vdup.8      d4, d7[0]
41920:     vdup.8      d5, d7[1]
41920:     vdup.8      d6, d7[2]
41920:     vdup.8      d7, d7[3]
41920: .endm
41920: 
41920: generate_composite_function \
41920:     pixman_composite_over_reverse_n_8888_asm_neon, 0, 0, 32, \
41920:     FLAG_DST_READWRITE | FLAG_DEINTERLEAVE_32BPP, \
41920:     8, /* number of pixels, processed in a single block */ \
41920:     5, /* prefetch distance */ \
41920:     pixman_composite_over_reverse_n_8888_init, \
41920:     default_cleanup, \
41920:     pixman_composite_over_8888_8888_process_pixblock_head, \
41920:     pixman_composite_over_8888_8888_process_pixblock_tail, \
41920:     pixman_composite_over_reverse_n_8888_process_pixblock_tail_head, \
41920:     28, /* dst_w_basereg */ \
41920:     0,  /* dst_r_basereg */ \
41920:     4,  /* src_basereg   */ \
41920:     24  /* mask_basereg  */
41920: 
41920: /******************************************************************************/
41920: 
64161: .macro pixman_composite_over_8888_8_0565_process_pixblock_head
64161:     vmull.u8    q0,  d24, d8    /* IN for SRC pixels (part1) */
37355:     vmull.u8    q1,  d24, d9
37355:     vmull.u8    q6,  d24, d10
37355:     vmull.u8    q7,  d24, d11
64161:         vshrn.u16   d6,  q2, #8 /* convert DST_R data to 32-bpp (part1) */
37355:         vshrn.u16   d7,  q2, #3
37355:         vsli.u16    q2,  q2, #5
64161:     vrshr.u16   q8,  q0,  #8    /* IN for SRC pixels (part2) */
64161:     vrshr.u16   q9,  q1,  #8
64161:     vrshr.u16   q10, q6,  #8
64161:     vrshr.u16   q11, q7,  #8
64161:     vraddhn.u16 d0,  q0,  q8
64161:     vraddhn.u16 d1,  q1,  q9
64161:     vraddhn.u16 d2,  q6,  q10
64161:     vraddhn.u16 d3,  q7,  q11
64161:         vsri.u8     d6,  d6, #5 /* convert DST_R data to 32-bpp (part2) */
64161:         vsri.u8     d7,  d7, #6
37355:     vmvn.8      d3,  d3
37355:         vshrn.u16   d30, q2, #2
64161:     vmull.u8    q8,  d3, d6     /* now do alpha blending */
64161:     vmull.u8    q9,  d3, d7
64161:     vmull.u8    q10, d3, d30
37355: .endm
37355: 
64161: .macro pixman_composite_over_8888_8_0565_process_pixblock_tail
64161:     /* 3 cycle bubble (after vmull.u8) */
64161:     vrshr.u16   q13, q8,  #8
64161:     vrshr.u16   q11, q9,  #8
64161:     vrshr.u16   q15, q10, #8
64161:     vraddhn.u16 d16, q8,  q13
64161:     vraddhn.u16 d27, q9,  q11
64161:     vraddhn.u16 d26, q10, q15
64161:     vqadd.u8    d16, d2,  d16
64161:     /* 1 cycle bubble */
64161:     vqadd.u8    q9,  q0,  q13
64161:     vshll.u8    q14, d16, #8    /* convert to 16bpp */
37355:     vshll.u8    q8,  d19, #8
37355:     vshll.u8    q9,  d18, #8
37355:     vsri.u16    q14, q8,  #5
64161:     /* 1 cycle bubble */
37355:     vsri.u16    q14, q9,  #11
37355: .endm
37355: 
64161: .macro pixman_composite_over_8888_8_0565_process_pixblock_tail_head
64161:     vld1.16     {d4, d5}, [DST_R, :128]!
64161:     vshrn.u16   d6,  q2,  #8
64161:     fetch_mask_pixblock
64161:     vshrn.u16   d7,  q2,  #3
64161:     fetch_src_pixblock
64161:     vmull.u8    q6,  d24, d10
64161:         vrshr.u16   q13, q8,  #8
64161:         vrshr.u16   q11, q9,  #8
64161:         vrshr.u16   q15, q10, #8
64161:         vraddhn.u16 d16, q8,  q13
64161:         vraddhn.u16 d27, q9,  q11
64161:         vraddhn.u16 d26, q10, q15
64161:         vqadd.u8    d16, d2,  d16
64161:     vmull.u8    q1,  d24, d9
64161:         vqadd.u8    q9,  q0,  q13
64161:         vshll.u8    q14, d16, #8
64161:     vmull.u8    q0,  d24, d8
64161:         vshll.u8    q8,  d19, #8
64161:         vshll.u8    q9,  d18, #8
64161:         vsri.u16    q14, q8,  #5
64161:     vmull.u8    q7,  d24, d11
64161:         vsri.u16    q14, q9,  #11
64161: 
64161:     cache_preload 8, 8
64161: 
64161:     vsli.u16    q2,  q2,  #5
64161:     vrshr.u16   q8,  q0,  #8
64161:     vrshr.u16   q9,  q1,  #8
64161:     vrshr.u16   q10, q6,  #8
64161:     vrshr.u16   q11, q7,  #8
64161:     vraddhn.u16 d0,  q0,  q8
64161:     vraddhn.u16 d1,  q1,  q9
64161:     vraddhn.u16 d2,  q6,  q10
64161:     vraddhn.u16 d3,  q7,  q11
64161:     vsri.u8     d6,  d6,  #5
64161:     vsri.u8     d7,  d7,  #6
64161:     vmvn.8      d3,  d3
64161:     vshrn.u16   d30, q2,  #2
37355:     vst1.16     {d28, d29}, [DST_W, :128]!
64161:     vmull.u8    q8,  d3,  d6
64161:     vmull.u8    q9,  d3,  d7
64161:     vmull.u8    q10, d3,  d30
37355: .endm
37355: 
64161: generate_composite_function \
64161:     pixman_composite_over_8888_8_0565_asm_neon, 32, 8, 16, \
64161:     FLAG_DST_READWRITE | FLAG_DEINTERLEAVE_32BPP, \
64161:     8, /* number of pixels, processed in a single block */ \
64161:     5, /* prefetch distance */ \
64161:     default_init_need_all_regs, \
64161:     default_cleanup_need_all_regs, \
64161:     pixman_composite_over_8888_8_0565_process_pixblock_head, \
64161:     pixman_composite_over_8888_8_0565_process_pixblock_tail, \
64161:     pixman_composite_over_8888_8_0565_process_pixblock_tail_head, \
64161:     28, /* dst_w_basereg */ \
64161:     4,  /* dst_r_basereg */ \
64161:     8,  /* src_basereg   */ \
64161:     24  /* mask_basereg  */
64161: 
64161: /******************************************************************************/
64161: 
37355: /*
37355:  * This function needs a special initialization of solid mask.
37355:  * Solid source pixel data is fetched from stack at ARGS_STACK_OFFSET
37355:  * offset, split into color components and replicated in d8-d11
37355:  * registers. Additionally, this function needs all the NEON registers,
37355:  * so it has to save d8-d15 registers which are callee saved according
37355:  * to ABI. These registers are restored from 'cleanup' macro. All the
37355:  * other NEON registers are caller saved, so can be clobbered freely
37355:  * without introducing any problems.
37355:  */
37355: .macro pixman_composite_over_n_8_0565_init
37355:     add         DUMMY, sp, #ARGS_STACK_OFFSET
37355:     vpush       {d8-d15}
37355:     vld1.32     {d11[0]}, [DUMMY]
37355:     vdup.8      d8, d11[0]
37355:     vdup.8      d9, d11[1]
37355:     vdup.8      d10, d11[2]
37355:     vdup.8      d11, d11[3]
37355: .endm
37355: 
37355: .macro pixman_composite_over_n_8_0565_cleanup
37355:     vpop        {d8-d15}
37355: .endm
37355: 
37355: generate_composite_function \
37355:     pixman_composite_over_n_8_0565_asm_neon, 0, 8, 16, \
37355:     FLAG_DST_READWRITE, \
37355:     8, /* number of pixels, processed in a single block */ \
37355:     5, /* prefetch distance */ \
37355:     pixman_composite_over_n_8_0565_init, \
37355:     pixman_composite_over_n_8_0565_cleanup, \
64161:     pixman_composite_over_8888_8_0565_process_pixblock_head, \
64161:     pixman_composite_over_8888_8_0565_process_pixblock_tail, \
64161:     pixman_composite_over_8888_8_0565_process_pixblock_tail_head
37355: 
37355: /******************************************************************************/
37355: 
64161: .macro pixman_composite_over_8888_n_0565_init
64161:     add         DUMMY, sp, #(ARGS_STACK_OFFSET + 8)
64161:     vpush       {d8-d15}
64161:     vld1.32     {d24[0]}, [DUMMY]
64161:     vdup.8      d24, d24[3]
64161: .endm
64161: 
64161: .macro pixman_composite_over_8888_n_0565_cleanup
64161:     vpop        {d8-d15}
56260: .endm
56260: 
56260: generate_composite_function \
64161:     pixman_composite_over_8888_n_0565_asm_neon, 32, 0, 16, \
56260:     FLAG_DST_READWRITE | FLAG_DEINTERLEAVE_32BPP, \
56260:     8, /* number of pixels, processed in a single block */ \
56260:     5, /* prefetch distance */ \
64161:     pixman_composite_over_8888_n_0565_init, \
64161:     pixman_composite_over_8888_n_0565_cleanup, \
64161:     pixman_composite_over_8888_8_0565_process_pixblock_head, \
64161:     pixman_composite_over_8888_8_0565_process_pixblock_tail, \
56260:     pixman_composite_over_8888_8_0565_process_pixblock_tail_head, \
56260:     28, /* dst_w_basereg */ \
56260:     4,  /* dst_r_basereg */ \
56260:     8,  /* src_basereg   */ \
56260:     24  /* mask_basereg  */
56260: 
56260: /******************************************************************************/
56260: 
37355: .macro pixman_composite_src_0565_0565_process_pixblock_head
37355: .endm
37355: 
37355: .macro pixman_composite_src_0565_0565_process_pixblock_tail
37355: .endm
37355: 
37355: .macro pixman_composite_src_0565_0565_process_pixblock_tail_head
37355:     vst1.16 {d0, d1, d2, d3}, [DST_W, :128]!
64161:     fetch_src_pixblock
37355:     cache_preload 16, 16
37355: .endm
37355: 
37355: generate_composite_function \
37355:     pixman_composite_src_0565_0565_asm_neon, 16, 0, 16, \
37355:     FLAG_DST_WRITEONLY, \
37355:     16, /* number of pixels, processed in a single block */ \
37355:     10, /* prefetch distance */ \
37355:     default_init, \
37355:     default_cleanup, \
37355:     pixman_composite_src_0565_0565_process_pixblock_head, \
37355:     pixman_composite_src_0565_0565_process_pixblock_tail, \
37355:     pixman_composite_src_0565_0565_process_pixblock_tail_head, \
37355:     0, /* dst_w_basereg */ \
37355:     0, /* dst_r_basereg */ \
37355:     0, /* src_basereg   */ \
37355:     0  /* mask_basereg  */
37355: 
37355: /******************************************************************************/
37355: 
37355: .macro pixman_composite_src_n_8_process_pixblock_head
37355: .endm
37355: 
37355: .macro pixman_composite_src_n_8_process_pixblock_tail
37355: .endm
37355: 
37355: .macro pixman_composite_src_n_8_process_pixblock_tail_head
37355:     vst1.8  {d0, d1, d2, d3}, [DST_W, :128]!
37355: .endm
37355: 
37355: .macro pixman_composite_src_n_8_init
37355:     add         DUMMY, sp, #ARGS_STACK_OFFSET
37355:     vld1.32     {d0[0]}, [DUMMY]
37355:     vsli.u64    d0, d0, #8
37355:     vsli.u64    d0, d0, #16
37355:     vsli.u64    d0, d0, #32
38016:     vorr        d1, d0, d0
38016:     vorr        q1, q0, q0
37355: .endm
37355: 
37355: .macro pixman_composite_src_n_8_cleanup
37355: .endm
37355: 
37355: generate_composite_function \
37355:     pixman_composite_src_n_8_asm_neon, 0, 0, 8, \
37355:     FLAG_DST_WRITEONLY, \
37355:     32, /* number of pixels, processed in a single block */ \
37355:     0,  /* prefetch distance */ \
37355:     pixman_composite_src_n_8_init, \
37355:     pixman_composite_src_n_8_cleanup, \
37355:     pixman_composite_src_n_8_process_pixblock_head, \
37355:     pixman_composite_src_n_8_process_pixblock_tail, \
37355:     pixman_composite_src_n_8_process_pixblock_tail_head, \
37355:     0, /* dst_w_basereg */ \
37355:     0, /* dst_r_basereg */ \
37355:     0, /* src_basereg   */ \
37355:     0  /* mask_basereg  */
37355: 
37355: /******************************************************************************/
37355: 
37355: .macro pixman_composite_src_n_0565_process_pixblock_head
37355: .endm
37355: 
37355: .macro pixman_composite_src_n_0565_process_pixblock_tail
37355: .endm
37355: 
37355: .macro pixman_composite_src_n_0565_process_pixblock_tail_head
37355:     vst1.16 {d0, d1, d2, d3}, [DST_W, :128]!
37355: .endm
37355: 
37355: .macro pixman_composite_src_n_0565_init
37355:     add         DUMMY, sp, #ARGS_STACK_OFFSET
37355:     vld1.32     {d0[0]}, [DUMMY]
37355:     vsli.u64    d0, d0, #16
37355:     vsli.u64    d0, d0, #32
38016:     vorr        d1, d0, d0
38016:     vorr        q1, q0, q0
37355: .endm
37355: 
37355: .macro pixman_composite_src_n_0565_cleanup
37355: .endm
37355: 
37355: generate_composite_function \
37355:     pixman_composite_src_n_0565_asm_neon, 0, 0, 16, \
37355:     FLAG_DST_WRITEONLY, \
37355:     16, /* number of pixels, processed in a single block */ \
37355:     0,  /* prefetch distance */ \
37355:     pixman_composite_src_n_0565_init, \
37355:     pixman_composite_src_n_0565_cleanup, \
37355:     pixman_composite_src_n_0565_process_pixblock_head, \
37355:     pixman_composite_src_n_0565_process_pixblock_tail, \
37355:     pixman_composite_src_n_0565_process_pixblock_tail_head, \
37355:     0, /* dst_w_basereg */ \
37355:     0, /* dst_r_basereg */ \
37355:     0, /* src_basereg   */ \
37355:     0  /* mask_basereg  */
37355: 
37355: /******************************************************************************/
37355: 
37355: .macro pixman_composite_src_n_8888_process_pixblock_head
37355: .endm
37355: 
37355: .macro pixman_composite_src_n_8888_process_pixblock_tail
37355: .endm
37355: 
37355: .macro pixman_composite_src_n_8888_process_pixblock_tail_head
37355:     vst1.32 {d0, d1, d2, d3}, [DST_W, :128]!
37355: .endm
37355: 
37355: .macro pixman_composite_src_n_8888_init
37355:     add         DUMMY, sp, #ARGS_STACK_OFFSET
37355:     vld1.32     {d0[0]}, [DUMMY]
37355:     vsli.u64    d0, d0, #32
38016:     vorr        d1, d0, d0
38016:     vorr        q1, q0, q0
37355: .endm
37355: 
37355: .macro pixman_composite_src_n_8888_cleanup
37355: .endm
37355: 
37355: generate_composite_function \
37355:     pixman_composite_src_n_8888_asm_neon, 0, 0, 32, \
37355:     FLAG_DST_WRITEONLY, \
37355:     8, /* number of pixels, processed in a single block */ \
37355:     0, /* prefetch distance */ \
37355:     pixman_composite_src_n_8888_init, \
37355:     pixman_composite_src_n_8888_cleanup, \
37355:     pixman_composite_src_n_8888_process_pixblock_head, \
37355:     pixman_composite_src_n_8888_process_pixblock_tail, \
37355:     pixman_composite_src_n_8888_process_pixblock_tail_head, \
37355:     0, /* dst_w_basereg */ \
37355:     0, /* dst_r_basereg */ \
37355:     0, /* src_basereg   */ \
37355:     0  /* mask_basereg  */
37355: 
37355: /******************************************************************************/
37355: 
37355: .macro pixman_composite_src_8888_8888_process_pixblock_head
37355: .endm
37355: 
37355: .macro pixman_composite_src_8888_8888_process_pixblock_tail
37355: .endm
37355: 
37355: .macro pixman_composite_src_8888_8888_process_pixblock_tail_head
37355:     vst1.32 {d0, d1, d2, d3}, [DST_W, :128]!
64161:     fetch_src_pixblock
37355:     cache_preload 8, 8
37355: .endm
37355: 
37355: generate_composite_function \
37355:     pixman_composite_src_8888_8888_asm_neon, 32, 0, 32, \
37355:     FLAG_DST_WRITEONLY, \
37355:     8, /* number of pixels, processed in a single block */ \
37355:     10, /* prefetch distance */ \
37355:     default_init, \
37355:     default_cleanup, \
37355:     pixman_composite_src_8888_8888_process_pixblock_head, \
37355:     pixman_composite_src_8888_8888_process_pixblock_tail, \
37355:     pixman_composite_src_8888_8888_process_pixblock_tail_head, \
37355:     0, /* dst_w_basereg */ \
37355:     0, /* dst_r_basereg */ \
37355:     0, /* src_basereg   */ \
37355:     0  /* mask_basereg  */
37355: 
37355: /******************************************************************************/
37355: 
41920: .macro pixman_composite_src_x888_8888_process_pixblock_head
41920:     vorr     q0, q0, q2
41920:     vorr     q1, q1, q2
41920: .endm
41920: 
41920: .macro pixman_composite_src_x888_8888_process_pixblock_tail
41920: .endm
41920: 
41920: .macro pixman_composite_src_x888_8888_process_pixblock_tail_head
41920:     vst1.32 {d0, d1, d2, d3}, [DST_W, :128]!
64161:     fetch_src_pixblock
41920:     vorr     q0, q0, q2
41920:     vorr     q1, q1, q2
41920:     cache_preload 8, 8
41920: .endm
41920: 
41920: .macro pixman_composite_src_x888_8888_init
41920:     vmov.u8  q2, #0xFF
41920:     vshl.u32 q2, q2, #24
41920: .endm
41920: 
41920: generate_composite_function \
41920:     pixman_composite_src_x888_8888_asm_neon, 32, 0, 32, \
41920:     FLAG_DST_WRITEONLY, \
41920:     8, /* number of pixels, processed in a single block */ \
41920:     10, /* prefetch distance */ \
41920:     pixman_composite_src_x888_8888_init, \
41920:     default_cleanup, \
41920:     pixman_composite_src_x888_8888_process_pixblock_head, \
41920:     pixman_composite_src_x888_8888_process_pixblock_tail, \
41920:     pixman_composite_src_x888_8888_process_pixblock_tail_head, \
41920:     0, /* dst_w_basereg */ \
41920:     0, /* dst_r_basereg */ \
41920:     0, /* src_basereg   */ \
41920:     0  /* mask_basereg  */
41920: 
41920: /******************************************************************************/
41920: 
37355: .macro pixman_composite_over_n_8_8888_process_pixblock_head
37355:     /* expecting deinterleaved source data in {d8, d9, d10, d11} */
37355:     /* d8 - blue, d9 - green, d10 - red, d11 - alpha */
37355:     /* and destination data in {d4, d5, d6, d7} */
37355:     /* mask is in d24 (d25, d26, d27 are unused) */
37355: 
37355:     /* in */
37355:     vmull.u8    q0, d24, d8
37355:     vmull.u8    q1, d24, d9
37355:     vmull.u8    q6, d24, d10
37355:     vmull.u8    q7, d24, d11
37355:     vrshr.u16   q10, q0, #8
37355:     vrshr.u16   q11, q1, #8
37355:     vrshr.u16   q12, q6, #8
37355:     vrshr.u16   q13, q7, #8
37355:     vraddhn.u16 d0, q0, q10
37355:     vraddhn.u16 d1, q1, q11
37355:     vraddhn.u16 d2, q6, q12
37355:     vraddhn.u16 d3, q7, q13
37355:     vmvn.8      d24, d3  /* get inverted alpha */
37355:     /* source:      d0 - blue, d1 - green, d2 - red, d3 - alpha */
37355:     /* destination: d4 - blue, d5 - green, d6 - red, d7 - alpha */
37355:     /* now do alpha blending */
37355:     vmull.u8    q8, d24, d4
37355:     vmull.u8    q9, d24, d5
37355:     vmull.u8    q10, d24, d6
37355:     vmull.u8    q11, d24, d7
37355: .endm
37355: 
37355: .macro pixman_composite_over_n_8_8888_process_pixblock_tail
37355:     vrshr.u16   q14, q8, #8
37355:     vrshr.u16   q15, q9, #8
37355:     vrshr.u16   q12, q10, #8
37355:     vrshr.u16   q13, q11, #8
37355:     vraddhn.u16 d28, q14, q8
37355:     vraddhn.u16 d29, q15, q9
37355:     vraddhn.u16 d30, q12, q10
37355:     vraddhn.u16 d31, q13, q11
37355:     vqadd.u8    q14, q0, q14
37355:     vqadd.u8    q15, q1, q15
37355: .endm
37355: 
37355: /* TODO: expand macros and do better instructions scheduling */
37355: .macro pixman_composite_over_n_8_8888_process_pixblock_tail_head
37355:     pixman_composite_over_n_8_8888_process_pixblock_tail
37355:     vst4.8      {d28, d29, d30, d31}, [DST_W, :128]!
37355:     vld4.8      {d4, d5, d6, d7}, [DST_R, :128]!
64161:     fetch_mask_pixblock
37355:     cache_preload 8, 8
37355:     pixman_composite_over_n_8_8888_process_pixblock_head
37355: .endm
37355: 
37355: .macro pixman_composite_over_n_8_8888_init
37355:     add         DUMMY, sp, #ARGS_STACK_OFFSET
37355:     vpush       {d8-d15}
37355:     vld1.32     {d11[0]}, [DUMMY]
37355:     vdup.8      d8, d11[0]
37355:     vdup.8      d9, d11[1]
37355:     vdup.8      d10, d11[2]
37355:     vdup.8      d11, d11[3]
37355: .endm
37355: 
37355: .macro pixman_composite_over_n_8_8888_cleanup
37355:     vpop        {d8-d15}
37355: .endm
37355: 
37355: generate_composite_function \
37355:     pixman_composite_over_n_8_8888_asm_neon, 0, 8, 32, \
37355:     FLAG_DST_READWRITE | FLAG_DEINTERLEAVE_32BPP, \
37355:     8, /* number of pixels, processed in a single block */ \
37355:     5, /* prefetch distance */ \
37355:     pixman_composite_over_n_8_8888_init, \
37355:     pixman_composite_over_n_8_8888_cleanup, \
37355:     pixman_composite_over_n_8_8888_process_pixblock_head, \
37355:     pixman_composite_over_n_8_8888_process_pixblock_tail, \
37355:     pixman_composite_over_n_8_8888_process_pixblock_tail_head
37355: 
37355: /******************************************************************************/
37355: 
64161: .macro pixman_composite_over_n_8_8_process_pixblock_head
64161:     vmull.u8    q0,  d24, d8
64161:     vmull.u8    q1,  d25, d8
64161:     vmull.u8    q6,  d26, d8
64161:     vmull.u8    q7,  d27, d8
64161:     vrshr.u16   q10, q0,  #8
64161:     vrshr.u16   q11, q1,  #8
64161:     vrshr.u16   q12, q6,  #8
64161:     vrshr.u16   q13, q7,  #8
64161:     vraddhn.u16 d0,  q0,  q10
64161:     vraddhn.u16 d1,  q1,  q11
64161:     vraddhn.u16 d2,  q6,  q12
64161:     vraddhn.u16 d3,  q7,  q13
64161:     vmvn.8      q12, q0
64161:     vmvn.8      q13, q1
64161:     vmull.u8    q8,  d24, d4
64161:     vmull.u8    q9,  d25, d5
64161:     vmull.u8    q10, d26, d6
64161:     vmull.u8    q11, d27, d7
64161: .endm
64161: 
64161: .macro pixman_composite_over_n_8_8_process_pixblock_tail
64161:     vrshr.u16   q14, q8,  #8
64161:     vrshr.u16   q15, q9,  #8
64161:     vrshr.u16   q12, q10, #8
64161:     vrshr.u16   q13, q11, #8
64161:     vraddhn.u16 d28, q14, q8
64161:     vraddhn.u16 d29, q15, q9
64161:     vraddhn.u16 d30, q12, q10
64161:     vraddhn.u16 d31, q13, q11
64161:     vqadd.u8    q14, q0,  q14
64161:     vqadd.u8    q15, q1,  q15
64161: .endm
64161: 
64161: /* TODO: expand macros and do better instructions scheduling */
64161: .macro pixman_composite_over_n_8_8_process_pixblock_tail_head
64161:     vld1.8      {d4, d5, d6, d7}, [DST_R, :128]!
64161:     pixman_composite_over_n_8_8_process_pixblock_tail
64161:     fetch_mask_pixblock
64161:     cache_preload 32, 32
64161:     vst1.8      {d28, d29, d30, d31}, [DST_W, :128]!
64161:     pixman_composite_over_n_8_8_process_pixblock_head
64161: .endm
64161: 
64161: .macro pixman_composite_over_n_8_8_init
64161:     add         DUMMY, sp, #ARGS_STACK_OFFSET
64161:     vpush       {d8-d15}
64161:     vld1.32     {d8[0]}, [DUMMY]
64161:     vdup.8      d8, d8[3]
64161: .endm
64161: 
64161: .macro pixman_composite_over_n_8_8_cleanup
64161:     vpop        {d8-d15}
64161: .endm
64161: 
64161: generate_composite_function \
64161:     pixman_composite_over_n_8_8_asm_neon, 0, 8, 8, \
64161:     FLAG_DST_READWRITE, \
64161:     32, /* number of pixels, processed in a single block */ \
64161:     5, /* prefetch distance */ \
64161:     pixman_composite_over_n_8_8_init, \
64161:     pixman_composite_over_n_8_8_cleanup, \
64161:     pixman_composite_over_n_8_8_process_pixblock_head, \
64161:     pixman_composite_over_n_8_8_process_pixblock_tail, \
64161:     pixman_composite_over_n_8_8_process_pixblock_tail_head
64161: 
64161: /******************************************************************************/
64161: 
41920: .macro pixman_composite_over_n_8888_8888_ca_process_pixblock_head
41920:     /*
41920:      * 'combine_mask_ca' replacement
41920:      *
41920:      * input:  solid src (n) in {d8,  d9,  d10, d11}
41920:      *         dest in          {d4,  d5,  d6,  d7 }
41920:      *         mask in          {d24, d25, d26, d27}
41920:      * output: updated src in   {d0,  d1,  d2,  d3 }
41920:      *         updated mask in  {d24, d25, d26, d3 }
41920:      */
41920:     vmull.u8    q0,  d24, d8
41920:     vmull.u8    q1,  d25, d9
41920:     vmull.u8    q6,  d26, d10
41920:     vmull.u8    q7,  d27, d11
41920:     vmull.u8    q9,  d11, d25
41920:     vmull.u8    q12, d11, d24
41920:     vmull.u8    q13, d11, d26
41920:     vrshr.u16   q8,  q0,  #8
41920:     vrshr.u16   q10, q1,  #8
41920:     vrshr.u16   q11, q6,  #8
41920:     vraddhn.u16 d0,  q0,  q8
41920:     vraddhn.u16 d1,  q1,  q10
41920:     vraddhn.u16 d2,  q6,  q11
41920:     vrshr.u16   q11, q12, #8
41920:     vrshr.u16   q8,  q9,  #8
41920:     vrshr.u16   q6,  q13, #8
41920:     vrshr.u16   q10, q7,  #8
41920:     vraddhn.u16 d24, q12, q11
41920:     vraddhn.u16 d25, q9,  q8
41920:     vraddhn.u16 d26, q13, q6
41920:     vraddhn.u16 d3,  q7,  q10
41920:     /*
41920:      * 'combine_over_ca' replacement
41920:      *
41920:      * output: updated dest in {d28, d29, d30, d31}
41920:      */
70077:     vmvn.8      q12, q12
70077:     vmvn.8      d26, d26
41920:     vmull.u8    q8,  d24, d4
41920:     vmull.u8    q9,  d25, d5
41920:     vmvn.8      d27, d3
41920:     vmull.u8    q10, d26, d6
41920:     vmull.u8    q11, d27, d7
41920: .endm
41920: 
41920: .macro pixman_composite_over_n_8888_8888_ca_process_pixblock_tail
41920:     /* ... continue 'combine_over_ca' replacement */
41920:     vrshr.u16   q14, q8,  #8
41920:     vrshr.u16   q15, q9,  #8
41920:     vrshr.u16   q6,  q10, #8
41920:     vrshr.u16   q7,  q11, #8
41920:     vraddhn.u16 d28, q14, q8
41920:     vraddhn.u16 d29, q15, q9
41920:     vraddhn.u16 d30, q6,  q10
41920:     vraddhn.u16 d31, q7,  q11
41920:     vqadd.u8    q14, q0,  q14
41920:     vqadd.u8    q15, q1,  q15
41920: .endm
41920: 
41920: .macro pixman_composite_over_n_8888_8888_ca_process_pixblock_tail_head
41920:         vrshr.u16   q14, q8, #8
41920:         vrshr.u16   q15, q9, #8
41920:     vld4.8      {d4, d5, d6, d7}, [DST_R, :128]!
41920:         vrshr.u16   q6, q10, #8
41920:         vrshr.u16   q7, q11, #8
41920:         vraddhn.u16 d28, q14, q8
41920:         vraddhn.u16 d29, q15, q9
41920:         vraddhn.u16 d30, q6, q10
41920:         vraddhn.u16 d31, q7, q11
64161:     fetch_mask_pixblock
41920:         vqadd.u8    q14, q0, q14
41920:         vqadd.u8    q15, q1, q15
41920:     cache_preload 8, 8
41920:     pixman_composite_over_n_8888_8888_ca_process_pixblock_head
41920:     vst4.8      {d28, d29, d30, d31}, [DST_W, :128]!
41920: .endm
41920: 
41920: .macro pixman_composite_over_n_8888_8888_ca_init
41920:     add         DUMMY, sp, #ARGS_STACK_OFFSET
41920:     vpush       {d8-d15}
41920:     vld1.32     {d11[0]}, [DUMMY]
41920:     vdup.8      d8, d11[0]
41920:     vdup.8      d9, d11[1]
41920:     vdup.8      d10, d11[2]
41920:     vdup.8      d11, d11[3]
41920: .endm
41920: 
41920: .macro pixman_composite_over_n_8888_8888_ca_cleanup
41920:     vpop        {d8-d15}
41920: .endm
41920: 
41920: generate_composite_function \
41920:     pixman_composite_over_n_8888_8888_ca_asm_neon, 0, 32, 32, \
41920:     FLAG_DST_READWRITE | FLAG_DEINTERLEAVE_32BPP, \
41920:     8, /* number of pixels, processed in a single block */ \
41920:     5, /* prefetch distance */ \
41920:     pixman_composite_over_n_8888_8888_ca_init, \
41920:     pixman_composite_over_n_8888_8888_ca_cleanup, \
41920:     pixman_composite_over_n_8888_8888_ca_process_pixblock_head, \
41920:     pixman_composite_over_n_8888_8888_ca_process_pixblock_tail, \
41920:     pixman_composite_over_n_8888_8888_ca_process_pixblock_tail_head
41920: 
41920: /******************************************************************************/
41920: 
70077: .macro pixman_composite_over_n_8888_0565_ca_process_pixblock_head
70077:     /*
70077:      * 'combine_mask_ca' replacement
70077:      *
70077:      * input:  solid src (n) in {d8,  d9,  d10, d11}  [B, G, R, A]
70077:      *         mask in          {d24, d25, d26}       [B, G, R]
70077:      * output: updated src in   {d0,  d1,  d2 }       [B, G, R]
70077:      *         updated mask in  {d24, d25, d26}       [B, G, R]
70077:      */
70077:     vmull.u8    q0,  d24, d8
70077:     vmull.u8    q1,  d25, d9
70077:     vmull.u8    q6,  d26, d10
70077:     vmull.u8    q9,  d11, d25
70077:     vmull.u8    q12, d11, d24
70077:     vmull.u8    q13, d11, d26
70077:     vrshr.u16   q8,  q0,  #8
70077:     vrshr.u16   q10, q1,  #8
70077:     vrshr.u16   q11, q6,  #8
70077:     vraddhn.u16 d0,  q0,  q8
70077:     vraddhn.u16 d1,  q1,  q10
70077:     vraddhn.u16 d2,  q6,  q11
70077:     vrshr.u16   q11, q12, #8
70077:     vrshr.u16   q8,  q9,  #8
70077:     vrshr.u16   q6,  q13, #8
70077:     vraddhn.u16 d24, q12, q11
70077:     vraddhn.u16 d25, q9,  q8
70077:     /*
70077:      * convert 8 r5g6b5 pixel data from {d4, d5} to planar 8-bit format
70077:      * and put data into d16 - blue, d17 - green, d18 - red
70077:      */
70077:        vshrn.u16   d17, q2,  #3
70077:        vshrn.u16   d18, q2,  #8
70077:     vraddhn.u16 d26, q13, q6
70077:        vsli.u16    q2,  q2,  #5
70077:        vsri.u8     d18, d18, #5
70077:        vsri.u8     d17, d17, #6
70077:     /*
70077:      * 'combine_over_ca' replacement
70077:      *
70077:      * output: updated dest in d16 - blue, d17 - green, d18 - red
70077:      */
70077:     vmvn.8      q12, q12
70077:        vshrn.u16   d16, q2,  #2
70077:     vmvn.8      d26, d26
70077:     vmull.u8    q6,  d16, d24
70077:     vmull.u8    q7,  d17, d25
70077:     vmull.u8    q11, d18, d26
70077: .endm
70077: 
70077: .macro pixman_composite_over_n_8888_0565_ca_process_pixblock_tail
70077:     /* ... continue 'combine_over_ca' replacement */
70077:     vrshr.u16   q10, q6,  #8
70077:     vrshr.u16   q14, q7,  #8
70077:     vrshr.u16   q15, q11, #8
70077:     vraddhn.u16 d16, q10, q6
70077:     vraddhn.u16 d17, q14, q7
70077:     vraddhn.u16 d18, q15, q11
70077:     vqadd.u8    q8,  q0,  q8
70077:     vqadd.u8    d18, d2,  d18
70077:     /*
70077:      * convert the results in d16, d17, d18 to r5g6b5 and store
70077:      * them into {d28, d29}
70077:      */
70077:     vshll.u8    q14, d18, #8
70077:     vshll.u8    q10, d17, #8
70077:     vshll.u8    q15, d16, #8
70077:     vsri.u16    q14, q10, #5
70077:     vsri.u16    q14, q15, #11
70077: .endm
70077: 
70077: .macro pixman_composite_over_n_8888_0565_ca_process_pixblock_tail_head
70077:     fetch_mask_pixblock
70077:         vrshr.u16   q10, q6, #8
70077:         vrshr.u16   q14, q7, #8
70077:     vld1.16     {d4, d5}, [DST_R, :128]!
70077:         vrshr.u16   q15, q11, #8
70077:         vraddhn.u16 d16, q10, q6
70077:         vraddhn.u16 d17, q14, q7
70077:         vraddhn.u16 d22, q15, q11
70077:             /* process_pixblock_head */
70077:             /*
70077:              * 'combine_mask_ca' replacement
70077:              *
70077:              * input:  solid src (n) in {d8,  d9,  d10, d11}  [B, G, R, A]
70077:              *         mask in          {d24, d25, d26}       [B, G, R]
70077:              * output: updated src in   {d0,  d1,  d2 }       [B, G, R]
70077:              *         updated mask in  {d24, d25, d26}       [B, G, R]
70077:              */
70077:             vmull.u8    q1,  d25, d9
70077:         vqadd.u8    q8,  q0, q8
70077:             vmull.u8    q0,  d24, d8
70077:         vqadd.u8    d22, d2, d22
70077:             vmull.u8    q6,  d26, d10
70077:         /*
70077:          * convert the result in d16, d17, d22 to r5g6b5 and store
70077:          * it into {d28, d29}
70077:          */
70077:         vshll.u8    q14, d22, #8
70077:         vshll.u8    q10, d17, #8
70077:         vshll.u8    q15, d16, #8
70077:             vmull.u8    q9,  d11, d25
70077:         vsri.u16    q14, q10, #5
70077:             vmull.u8    q12, d11, d24
70077:             vmull.u8    q13, d11, d26
70077:         vsri.u16    q14, q15, #11
70077:     cache_preload 8, 8
70077:             vrshr.u16   q8,  q0,  #8
70077:             vrshr.u16   q10, q1,  #8
70077:             vrshr.u16   q11, q6,  #8
70077:             vraddhn.u16 d0,  q0,  q8
70077:             vraddhn.u16 d1,  q1,  q10
70077:             vraddhn.u16 d2,  q6,  q11
70077:             vrshr.u16   q11, q12, #8
70077:             vrshr.u16   q8,  q9,  #8
70077:             vrshr.u16   q6,  q13, #8
70077:             vraddhn.u16 d25, q9,  q8
70077:                 /*
70077:                  * convert 8 r5g6b5 pixel data from {d4, d5} to planar
70077: 	         * 8-bit format and put data into d16 - blue, d17 - green,
70077: 	         * d18 - red
70077:                  */
70077:                 vshrn.u16   d17, q2,  #3
70077:                 vshrn.u16   d18, q2,  #8
70077:             vraddhn.u16 d24, q12, q11
70077:             vraddhn.u16 d26, q13, q6
70077:                 vsli.u16    q2,  q2,  #5
70077:                 vsri.u8     d18, d18, #5
70077:                 vsri.u8     d17, d17, #6
70077:             /*
70077:              * 'combine_over_ca' replacement
70077:              *
70077:              * output: updated dest in d16 - blue, d17 - green, d18 - red
70077:              */
70077:             vmvn.8      q12, q12
70077:                 vshrn.u16   d16, q2,  #2
70077:             vmvn.8      d26, d26
70077:             vmull.u8    q7,  d17, d25
70077:             vmull.u8    q6,  d16, d24
70077:             vmull.u8    q11, d18, d26
70077:     vst1.16     {d28, d29}, [DST_W, :128]!
70077: .endm
70077: 
70077: .macro pixman_composite_over_n_8888_0565_ca_init
70077:     add         DUMMY, sp, #ARGS_STACK_OFFSET
70077:     vpush       {d8-d15}
70077:     vld1.32     {d11[0]}, [DUMMY]
70077:     vdup.8      d8, d11[0]
70077:     vdup.8      d9, d11[1]
70077:     vdup.8      d10, d11[2]
70077:     vdup.8      d11, d11[3]
70077: .endm
70077: 
70077: .macro pixman_composite_over_n_8888_0565_ca_cleanup
70077:     vpop        {d8-d15}
70077: .endm
70077: 
70077: generate_composite_function \
70077:     pixman_composite_over_n_8888_0565_ca_asm_neon, 0, 32, 16, \
70077:     FLAG_DST_READWRITE | FLAG_DEINTERLEAVE_32BPP, \
70077:     8, /* number of pixels, processed in a single block */ \
70077:     5, /* prefetch distance */ \
70077:     pixman_composite_over_n_8888_0565_ca_init, \
70077:     pixman_composite_over_n_8888_0565_ca_cleanup, \
70077:     pixman_composite_over_n_8888_0565_ca_process_pixblock_head, \
70077:     pixman_composite_over_n_8888_0565_ca_process_pixblock_tail, \
70077:     pixman_composite_over_n_8888_0565_ca_process_pixblock_tail_head
70077: 
70077: /******************************************************************************/
70077: 
64161: .macro pixman_composite_in_n_8_process_pixblock_head
64161:     /* expecting source data in {d0, d1, d2, d3} */
64161:     /* and destination data in {d4, d5, d6, d7} */
64161:     vmull.u8    q8,  d4,  d3
64161:     vmull.u8    q9,  d5,  d3
64161:     vmull.u8    q10, d6,  d3
64161:     vmull.u8    q11, d7,  d3
64161: .endm
64161: 
64161: .macro pixman_composite_in_n_8_process_pixblock_tail
64161:     vrshr.u16   q14, q8,  #8
64161:     vrshr.u16   q15, q9,  #8
64161:     vrshr.u16   q12, q10, #8
64161:     vrshr.u16   q13, q11, #8
64161:     vraddhn.u16 d28, q8,  q14
64161:     vraddhn.u16 d29, q9,  q15
64161:     vraddhn.u16 d30, q10, q12
64161:     vraddhn.u16 d31, q11, q13
64161: .endm
64161: 
64161: .macro pixman_composite_in_n_8_process_pixblock_tail_head
64161:     pixman_composite_in_n_8_process_pixblock_tail
64161:     vld1.8      {d4, d5, d6, d7}, [DST_R, :128]!
64161:     cache_preload 32, 32
64161:     pixman_composite_in_n_8_process_pixblock_head
64161:     vst1.8      {d28, d29, d30, d31}, [DST_W, :128]!
64161: .endm
64161: 
64161: .macro pixman_composite_in_n_8_init
64161:     add         DUMMY, sp, #ARGS_STACK_OFFSET
64161:     vld1.32     {d3[0]}, [DUMMY]
64161:     vdup.8      d3, d3[3]
64161: .endm
64161: 
64161: .macro pixman_composite_in_n_8_cleanup
64161: .endm
64161: 
64161: generate_composite_function \
64161:     pixman_composite_in_n_8_asm_neon, 0, 0, 8, \
64161:     FLAG_DST_READWRITE, \
64161:     32, /* number of pixels, processed in a single block */ \
64161:     5, /* prefetch distance */ \
64161:     pixman_composite_in_n_8_init, \
64161:     pixman_composite_in_n_8_cleanup, \
64161:     pixman_composite_in_n_8_process_pixblock_head, \
64161:     pixman_composite_in_n_8_process_pixblock_tail, \
64161:     pixman_composite_in_n_8_process_pixblock_tail_head, \
64161:     28, /* dst_w_basereg */ \
64161:     4,  /* dst_r_basereg */ \
64161:     0,  /* src_basereg   */ \
64161:     24  /* mask_basereg  */
64161: 
37355: .macro pixman_composite_add_n_8_8_process_pixblock_head
37355:     /* expecting source data in {d8, d9, d10, d11} */
37355:     /* d8 - blue, d9 - green, d10 - red, d11 - alpha */
37355:     /* and destination data in {d4, d5, d6, d7} */
37355:     /* mask is in d24, d25, d26, d27 */
37355:     vmull.u8    q0, d24, d11
37355:     vmull.u8    q1, d25, d11
37355:     vmull.u8    q6, d26, d11
37355:     vmull.u8    q7, d27, d11
37355:     vrshr.u16   q10, q0, #8
37355:     vrshr.u16   q11, q1, #8
37355:     vrshr.u16   q12, q6, #8
37355:     vrshr.u16   q13, q7, #8
37355:     vraddhn.u16 d0, q0, q10
37355:     vraddhn.u16 d1, q1, q11
37355:     vraddhn.u16 d2, q6, q12
37355:     vraddhn.u16 d3, q7, q13
37355:     vqadd.u8    q14, q0, q2
37355:     vqadd.u8    q15, q1, q3
37355: .endm
37355: 
37355: .macro pixman_composite_add_n_8_8_process_pixblock_tail
37355: .endm
37355: 
37355: /* TODO: expand macros and do better instructions scheduling */
37355: .macro pixman_composite_add_n_8_8_process_pixblock_tail_head
37355:     pixman_composite_add_n_8_8_process_pixblock_tail
37355:     vst1.8      {d28, d29, d30, d31}, [DST_W, :128]!
37355:     vld1.8      {d4, d5, d6, d7}, [DST_R, :128]!
64161:     fetch_mask_pixblock
37355:     cache_preload 32, 32
37355:     pixman_composite_add_n_8_8_process_pixblock_head
37355: .endm
37355: 
37355: .macro pixman_composite_add_n_8_8_init
37355:     add         DUMMY, sp, #ARGS_STACK_OFFSET
37355:     vpush       {d8-d15}
37355:     vld1.32     {d11[0]}, [DUMMY]
37355:     vdup.8      d11, d11[3]
37355: .endm
37355: 
37355: .macro pixman_composite_add_n_8_8_cleanup
37355:     vpop        {d8-d15}
37355: .endm
37355: 
37355: generate_composite_function \
37355:     pixman_composite_add_n_8_8_asm_neon, 0, 8, 8, \
37355:     FLAG_DST_READWRITE, \
37355:     32, /* number of pixels, processed in a single block */ \
37355:     5, /* prefetch distance */ \
37355:     pixman_composite_add_n_8_8_init, \
37355:     pixman_composite_add_n_8_8_cleanup, \
37355:     pixman_composite_add_n_8_8_process_pixblock_head, \
37355:     pixman_composite_add_n_8_8_process_pixblock_tail, \
37355:     pixman_composite_add_n_8_8_process_pixblock_tail_head
37355: 
37355: /******************************************************************************/
37355: 
37355: .macro pixman_composite_add_8_8_8_process_pixblock_head
37355:     /* expecting source data in {d0, d1, d2, d3} */
37355:     /* destination data in {d4, d5, d6, d7} */
37355:     /* mask in {d24, d25, d26, d27} */
37355:     vmull.u8    q8, d24, d0
37355:     vmull.u8    q9, d25, d1
37355:     vmull.u8    q10, d26, d2
37355:     vmull.u8    q11, d27, d3
37355:     vrshr.u16   q0, q8, #8
37355:     vrshr.u16   q1, q9, #8
37355:     vrshr.u16   q12, q10, #8
37355:     vrshr.u16   q13, q11, #8
37355:     vraddhn.u16 d0, q0, q8
37355:     vraddhn.u16 d1, q1, q9
37355:     vraddhn.u16 d2, q12, q10
37355:     vraddhn.u16 d3, q13, q11
37355:     vqadd.u8    q14, q0, q2
37355:     vqadd.u8    q15, q1, q3
37355: .endm
37355: 
37355: .macro pixman_composite_add_8_8_8_process_pixblock_tail
37355: .endm
37355: 
37355: /* TODO: expand macros and do better instructions scheduling */
37355: .macro pixman_composite_add_8_8_8_process_pixblock_tail_head
37355:     pixman_composite_add_8_8_8_process_pixblock_tail
37355:     vst1.8      {d28, d29, d30, d31}, [DST_W, :128]!
37355:     vld1.8      {d4, d5, d6, d7}, [DST_R, :128]!
64161:     fetch_mask_pixblock
64161:     fetch_src_pixblock
37355:     cache_preload 32, 32
37355:     pixman_composite_add_8_8_8_process_pixblock_head
37355: .endm
37355: 
37355: .macro pixman_composite_add_8_8_8_init
37355: .endm
37355: 
37355: .macro pixman_composite_add_8_8_8_cleanup
37355: .endm
37355: 
37355: generate_composite_function \
37355:     pixman_composite_add_8_8_8_asm_neon, 8, 8, 8, \
37355:     FLAG_DST_READWRITE, \
37355:     32, /* number of pixels, processed in a single block */ \
37355:     5, /* prefetch distance */ \
37355:     pixman_composite_add_8_8_8_init, \
37355:     pixman_composite_add_8_8_8_cleanup, \
37355:     pixman_composite_add_8_8_8_process_pixblock_head, \
37355:     pixman_composite_add_8_8_8_process_pixblock_tail, \
37355:     pixman_composite_add_8_8_8_process_pixblock_tail_head
37355: 
37355: /******************************************************************************/
37355: 
37355: .macro pixman_composite_add_8888_8888_8888_process_pixblock_head
37355:     /* expecting source data in {d0, d1, d2, d3} */
37355:     /* destination data in {d4, d5, d6, d7} */
37355:     /* mask in {d24, d25, d26, d27} */
37355:     vmull.u8    q8,  d27, d0
37355:     vmull.u8    q9,  d27, d1
37355:     vmull.u8    q10, d27, d2
37355:     vmull.u8    q11, d27, d3
64161:     /* 1 cycle bubble */
64161:     vrsra.u16   q8,  q8,  #8
64161:     vrsra.u16   q9,  q9,  #8
64161:     vrsra.u16   q10, q10, #8
64161:     vrsra.u16   q11, q11, #8
37355: .endm
37355: 
37355: .macro pixman_composite_add_8888_8888_8888_process_pixblock_tail
64161:     /* 2 cycle bubble */
64161:     vrshrn.u16  d28, q8,  #8
64161:     vrshrn.u16  d29, q9,  #8
64161:     vrshrn.u16  d30, q10, #8
64161:     vrshrn.u16  d31, q11, #8
64161:     vqadd.u8    q14, q2,  q14
64161:     /* 1 cycle bubble */
64161:     vqadd.u8    q15, q3,  q15
37355: .endm
37355: 
37355: .macro pixman_composite_add_8888_8888_8888_process_pixblock_tail_head
64161:     fetch_src_pixblock
64161:         vrshrn.u16  d28, q8,  #8
64161:     fetch_mask_pixblock
64161:         vrshrn.u16  d29, q9,  #8
64161:     vmull.u8    q8,  d27, d0
64161:         vrshrn.u16  d30, q10, #8
64161:     vmull.u8    q9,  d27, d1
64161:         vrshrn.u16  d31, q11, #8
64161:     vmull.u8    q10, d27, d2
64161:         vqadd.u8    q14, q2,  q14
64161:     vmull.u8    q11, d27, d3
64161:         vqadd.u8    q15, q3,  q15
64161:     vrsra.u16   q8,  q8,  #8
64161:     vld4.8      {d4, d5, d6, d7}, [DST_R, :128]!
64161:     vrsra.u16   q9,  q9,  #8
37355:         vst4.8      {d28, d29, d30, d31}, [DST_W, :128]!
64161:     vrsra.u16   q10, q10, #8
64161: 
37355:     cache_preload 8, 8
64161: 
64161:     vrsra.u16   q11, q11, #8
37355: .endm
37355: 
37355: generate_composite_function \
37355:     pixman_composite_add_8888_8888_8888_asm_neon, 32, 32, 32, \
37355:     FLAG_DST_READWRITE | FLAG_DEINTERLEAVE_32BPP, \
37355:     8, /* number of pixels, processed in a single block */ \
37355:     10, /* prefetch distance */ \
37355:     default_init, \
37355:     default_cleanup, \
37355:     pixman_composite_add_8888_8888_8888_process_pixblock_head, \
37355:     pixman_composite_add_8888_8888_8888_process_pixblock_tail, \
37355:     pixman_composite_add_8888_8888_8888_process_pixblock_tail_head
37355: 
37355: generate_composite_function_single_scanline \
37355:     pixman_composite_scanline_add_mask_asm_neon, 32, 32, 32, \
37355:     FLAG_DST_READWRITE | FLAG_DEINTERLEAVE_32BPP, \
37355:     8, /* number of pixels, processed in a single block */ \
37355:     default_init, \
37355:     default_cleanup, \
37355:     pixman_composite_add_8888_8888_8888_process_pixblock_head, \
37355:     pixman_composite_add_8888_8888_8888_process_pixblock_tail, \
37355:     pixman_composite_add_8888_8888_8888_process_pixblock_tail_head
37355: 
37355: /******************************************************************************/
37355: 
64161: generate_composite_function \
64161:     pixman_composite_add_8888_8_8888_asm_neon, 32, 8, 32, \
64161:     FLAG_DST_READWRITE | FLAG_DEINTERLEAVE_32BPP, \
64161:     8, /* number of pixels, processed in a single block */ \
64161:     5, /* prefetch distance */ \
64161:     default_init, \
64161:     default_cleanup, \
64161:     pixman_composite_add_8888_8888_8888_process_pixblock_head, \
64161:     pixman_composite_add_8888_8888_8888_process_pixblock_tail, \
64161:     pixman_composite_add_8888_8888_8888_process_pixblock_tail_head, \
64161:     28, /* dst_w_basereg */ \
64161:     4,  /* dst_r_basereg */ \
64161:     0,  /* src_basereg   */ \
64161:     27  /* mask_basereg  */
64161: 
64161: /******************************************************************************/
64161: 
64161: .macro pixman_composite_add_n_8_8888_init
64161:     add         DUMMY, sp, #ARGS_STACK_OFFSET
64161:     vld1.32     {d3[0]}, [DUMMY]
64161:     vdup.8      d0, d3[0]
64161:     vdup.8      d1, d3[1]
64161:     vdup.8      d2, d3[2]
64161:     vdup.8      d3, d3[3]
64161: .endm
64161: 
64161: .macro pixman_composite_add_n_8_8888_cleanup
64161: .endm
64161: 
64161: generate_composite_function \
64161:     pixman_composite_add_n_8_8888_asm_neon, 0, 8, 32, \
64161:     FLAG_DST_READWRITE | FLAG_DEINTERLEAVE_32BPP, \
64161:     8, /* number of pixels, processed in a single block */ \
64161:     5, /* prefetch distance */ \
64161:     pixman_composite_add_n_8_8888_init, \
64161:     pixman_composite_add_n_8_8888_cleanup, \
64161:     pixman_composite_add_8888_8888_8888_process_pixblock_head, \
64161:     pixman_composite_add_8888_8888_8888_process_pixblock_tail, \
64161:     pixman_composite_add_8888_8888_8888_process_pixblock_tail_head, \
64161:     28, /* dst_w_basereg */ \
64161:     4,  /* dst_r_basereg */ \
64161:     0,  /* src_basereg   */ \
64161:     27  /* mask_basereg  */
64161: 
64161: /******************************************************************************/
64161: 
64161: .macro pixman_composite_add_8888_n_8888_init
64161:     add         DUMMY, sp, #(ARGS_STACK_OFFSET + 8)
64161:     vld1.32     {d27[0]}, [DUMMY]
64161:     vdup.8      d27, d27[3]
64161: .endm
64161: 
64161: .macro pixman_composite_add_8888_n_8888_cleanup
64161: .endm
64161: 
64161: generate_composite_function \
64161:     pixman_composite_add_8888_n_8888_asm_neon, 32, 0, 32, \
64161:     FLAG_DST_READWRITE | FLAG_DEINTERLEAVE_32BPP, \
64161:     8, /* number of pixels, processed in a single block */ \
64161:     5, /* prefetch distance */ \
64161:     pixman_composite_add_8888_n_8888_init, \
64161:     pixman_composite_add_8888_n_8888_cleanup, \
64161:     pixman_composite_add_8888_8888_8888_process_pixblock_head, \
64161:     pixman_composite_add_8888_8888_8888_process_pixblock_tail, \
64161:     pixman_composite_add_8888_8888_8888_process_pixblock_tail_head, \
64161:     28, /* dst_w_basereg */ \
64161:     4,  /* dst_r_basereg */ \
64161:     0,  /* src_basereg   */ \
64161:     27  /* mask_basereg  */
64161: 
64161: /******************************************************************************/
64161: 
56260: .macro pixman_composite_out_reverse_8888_n_8888_process_pixblock_head
37355:     /* expecting source data in {d0, d1, d2, d3} */
37355:     /* destination data in {d4, d5, d6, d7} */
37355:     /* solid mask is in d15 */
37355: 
37355:     /* 'in' */
37355:     vmull.u8    q8, d15, d3
37355:     vmull.u8    q6, d15, d2
37355:     vmull.u8    q5, d15, d1
37355:     vmull.u8    q4, d15, d0
37355:     vrshr.u16   q13, q8, #8
37355:     vrshr.u16   q12, q6, #8
37355:     vrshr.u16   q11, q5, #8
37355:     vrshr.u16   q10, q4, #8
37355:     vraddhn.u16 d3, q8, q13
37355:     vraddhn.u16 d2, q6, q12
37355:     vraddhn.u16 d1, q5, q11
37355:     vraddhn.u16 d0, q4, q10
37355:     vmvn.8      d24, d3  /* get inverted alpha */
37355:     /* now do alpha blending */
37355:     vmull.u8    q8, d24, d4
37355:     vmull.u8    q9, d24, d5
37355:     vmull.u8    q10, d24, d6
37355:     vmull.u8    q11, d24, d7
37355: .endm
37355: 
56260: .macro pixman_composite_out_reverse_8888_n_8888_process_pixblock_tail
37355:     vrshr.u16   q14, q8, #8
37355:     vrshr.u16   q15, q9, #8
37355:     vrshr.u16   q12, q10, #8
37355:     vrshr.u16   q13, q11, #8
37355:     vraddhn.u16 d28, q14, q8
37355:     vraddhn.u16 d29, q15, q9
37355:     vraddhn.u16 d30, q12, q10
37355:     vraddhn.u16 d31, q13, q11
56260: .endm
56260: 
56260: /* TODO: expand macros and do better instructions scheduling */
56260: .macro pixman_composite_out_reverse_8888_8888_8888_process_pixblock_tail_head
56260:     vld4.8     {d4, d5, d6, d7}, [DST_R, :128]!
56260:     pixman_composite_out_reverse_8888_n_8888_process_pixblock_tail
64161:     fetch_src_pixblock
56260:     cache_preload 8, 8
64161:     fetch_mask_pixblock
56260:     pixman_composite_out_reverse_8888_n_8888_process_pixblock_head
56260:     vst4.8     {d28, d29, d30, d31}, [DST_W, :128]!
56260: .endm
56260: 
56260: generate_composite_function_single_scanline \
56260:     pixman_composite_scanline_out_reverse_mask_asm_neon, 32, 32, 32, \
56260:     FLAG_DST_READWRITE | FLAG_DEINTERLEAVE_32BPP, \
56260:     8, /* number of pixels, processed in a single block */ \
56260:     default_init_need_all_regs, \
56260:     default_cleanup_need_all_regs, \
56260:     pixman_composite_out_reverse_8888_n_8888_process_pixblock_head, \
56260:     pixman_composite_out_reverse_8888_n_8888_process_pixblock_tail, \
56260:     pixman_composite_out_reverse_8888_8888_8888_process_pixblock_tail_head \
56260:     28, /* dst_w_basereg */ \
56260:     4,  /* dst_r_basereg */ \
56260:     0,  /* src_basereg   */ \
56260:     12  /* mask_basereg  */
56260: 
56260: /******************************************************************************/
56260: 
56260: .macro pixman_composite_over_8888_n_8888_process_pixblock_head
56260:     pixman_composite_out_reverse_8888_n_8888_process_pixblock_head
56260: .endm
56260: 
56260: .macro pixman_composite_over_8888_n_8888_process_pixblock_tail
56260:     pixman_composite_out_reverse_8888_n_8888_process_pixblock_tail
37355:     vqadd.u8    q14, q0, q14
37355:     vqadd.u8    q15, q1, q15
37355: .endm
37355: 
37355: /* TODO: expand macros and do better instructions scheduling */
37355: .macro pixman_composite_over_8888_n_8888_process_pixblock_tail_head
37355:     vld4.8     {d4, d5, d6, d7}, [DST_R, :128]!
37355:     pixman_composite_over_8888_n_8888_process_pixblock_tail
64161:     fetch_src_pixblock
37355:     cache_preload 8, 8
37355:     pixman_composite_over_8888_n_8888_process_pixblock_head
37355:     vst4.8     {d28, d29, d30, d31}, [DST_W, :128]!
37355: .endm
37355: 
37355: .macro pixman_composite_over_8888_n_8888_init
37355:     add         DUMMY, sp, #48
37355:     vpush       {d8-d15}
37355:     vld1.32     {d15[0]}, [DUMMY]
37355:     vdup.8      d15, d15[3]
37355: .endm
37355: 
37355: .macro pixman_composite_over_8888_n_8888_cleanup
37355:     vpop        {d8-d15}
37355: .endm
37355: 
37355: generate_composite_function \
37355:     pixman_composite_over_8888_n_8888_asm_neon, 32, 0, 32, \
37355:     FLAG_DST_READWRITE | FLAG_DEINTERLEAVE_32BPP, \
37355:     8, /* number of pixels, processed in a single block */ \
37355:     5, /* prefetch distance */ \
37355:     pixman_composite_over_8888_n_8888_init, \
37355:     pixman_composite_over_8888_n_8888_cleanup, \
37355:     pixman_composite_over_8888_n_8888_process_pixblock_head, \
37355:     pixman_composite_over_8888_n_8888_process_pixblock_tail, \
37355:     pixman_composite_over_8888_n_8888_process_pixblock_tail_head
37355: 
37355: /******************************************************************************/
37355: 
37355: /* TODO: expand macros and do better instructions scheduling */
37355: .macro pixman_composite_over_8888_8888_8888_process_pixblock_tail_head
37355:     vld4.8     {d4, d5, d6, d7}, [DST_R, :128]!
37355:     pixman_composite_over_8888_n_8888_process_pixblock_tail
64161:     fetch_src_pixblock
37355:     cache_preload 8, 8
64161:     fetch_mask_pixblock
37355:     pixman_composite_over_8888_n_8888_process_pixblock_head
37355:     vst4.8     {d28, d29, d30, d31}, [DST_W, :128]!
37355: .endm
37355: 
37355: generate_composite_function \
37355:     pixman_composite_over_8888_8888_8888_asm_neon, 32, 32, 32, \
37355:     FLAG_DST_READWRITE | FLAG_DEINTERLEAVE_32BPP, \
37355:     8, /* number of pixels, processed in a single block */ \
37355:     5, /* prefetch distance */ \
56260:     default_init_need_all_regs, \
56260:     default_cleanup_need_all_regs, \
37355:     pixman_composite_over_8888_n_8888_process_pixblock_head, \
37355:     pixman_composite_over_8888_n_8888_process_pixblock_tail, \
37355:     pixman_composite_over_8888_8888_8888_process_pixblock_tail_head \
37355:     28, /* dst_w_basereg */ \
37355:     4,  /* dst_r_basereg */ \
37355:     0,  /* src_basereg   */ \
37355:     12  /* mask_basereg  */
37355: 
37355: generate_composite_function_single_scanline \
37355:     pixman_composite_scanline_over_mask_asm_neon, 32, 32, 32, \
37355:     FLAG_DST_READWRITE | FLAG_DEINTERLEAVE_32BPP, \
37355:     8, /* number of pixels, processed in a single block */ \
56260:     default_init_need_all_regs, \
56260:     default_cleanup_need_all_regs, \
37355:     pixman_composite_over_8888_n_8888_process_pixblock_head, \
37355:     pixman_composite_over_8888_n_8888_process_pixblock_tail, \
37355:     pixman_composite_over_8888_8888_8888_process_pixblock_tail_head \
37355:     28, /* dst_w_basereg */ \
37355:     4,  /* dst_r_basereg */ \
37355:     0,  /* src_basereg   */ \
37355:     12  /* mask_basereg  */
37355: 
37355: /******************************************************************************/
37355: 
37355: /* TODO: expand macros and do better instructions scheduling */
37355: .macro pixman_composite_over_8888_8_8888_process_pixblock_tail_head
37355:     vld4.8     {d4, d5, d6, d7}, [DST_R, :128]!
37355:     pixman_composite_over_8888_n_8888_process_pixblock_tail
64161:     fetch_src_pixblock
37355:     cache_preload 8, 8
64161:     fetch_mask_pixblock
37355:     pixman_composite_over_8888_n_8888_process_pixblock_head
37355:     vst4.8     {d28, d29, d30, d31}, [DST_W, :128]!
37355: .endm
37355: 
37355: generate_composite_function \
37355:     pixman_composite_over_8888_8_8888_asm_neon, 32, 8, 32, \
37355:     FLAG_DST_READWRITE | FLAG_DEINTERLEAVE_32BPP, \
37355:     8, /* number of pixels, processed in a single block */ \
37355:     5, /* prefetch distance */ \
56260:     default_init_need_all_regs, \
56260:     default_cleanup_need_all_regs, \
37355:     pixman_composite_over_8888_n_8888_process_pixblock_head, \
37355:     pixman_composite_over_8888_n_8888_process_pixblock_tail, \
37355:     pixman_composite_over_8888_8_8888_process_pixblock_tail_head \
37355:     28, /* dst_w_basereg */ \
37355:     4,  /* dst_r_basereg */ \
37355:     0,  /* src_basereg   */ \
37355:     15  /* mask_basereg  */
37355: 
37355: /******************************************************************************/
37355: 
37355: .macro pixman_composite_src_0888_0888_process_pixblock_head
37355: .endm
37355: 
37355: .macro pixman_composite_src_0888_0888_process_pixblock_tail
37355: .endm
37355: 
37355: .macro pixman_composite_src_0888_0888_process_pixblock_tail_head
37355:     vst3.8 {d0, d1, d2}, [DST_W]!
64161:     fetch_src_pixblock
37355:     cache_preload 8, 8
37355: .endm
37355: 
37355: generate_composite_function \
37355:     pixman_composite_src_0888_0888_asm_neon, 24, 0, 24, \
37355:     FLAG_DST_WRITEONLY, \
37355:     8, /* number of pixels, processed in a single block */ \
37355:     10, /* prefetch distance */ \
37355:     default_init, \
37355:     default_cleanup, \
37355:     pixman_composite_src_0888_0888_process_pixblock_head, \
37355:     pixman_composite_src_0888_0888_process_pixblock_tail, \
37355:     pixman_composite_src_0888_0888_process_pixblock_tail_head, \
37355:     0, /* dst_w_basereg */ \
37355:     0, /* dst_r_basereg */ \
37355:     0, /* src_basereg   */ \
37355:     0  /* mask_basereg  */
37355: 
37355: /******************************************************************************/
37355: 
37355: .macro pixman_composite_src_0888_8888_rev_process_pixblock_head
37355:     vswp   d0, d2
37355: .endm
37355: 
37355: .macro pixman_composite_src_0888_8888_rev_process_pixblock_tail
37355: .endm
37355: 
37355: .macro pixman_composite_src_0888_8888_rev_process_pixblock_tail_head
37355:     vst4.8 {d0, d1, d2, d3}, [DST_W]!
64161:     fetch_src_pixblock
37355:     vswp   d0, d2
37355:     cache_preload 8, 8
37355: .endm
37355: 
37355: .macro pixman_composite_src_0888_8888_rev_init
37355:     veor   d3, d3, d3
37355: .endm
37355: 
37355: generate_composite_function \
37355:     pixman_composite_src_0888_8888_rev_asm_neon, 24, 0, 32, \
37355:     FLAG_DST_WRITEONLY | FLAG_DEINTERLEAVE_32BPP, \
37355:     8, /* number of pixels, processed in a single block */ \
37355:     10, /* prefetch distance */ \
37355:     pixman_composite_src_0888_8888_rev_init, \
37355:     default_cleanup, \
37355:     pixman_composite_src_0888_8888_rev_process_pixblock_head, \
37355:     pixman_composite_src_0888_8888_rev_process_pixblock_tail, \
37355:     pixman_composite_src_0888_8888_rev_process_pixblock_tail_head, \
37355:     0, /* dst_w_basereg */ \
37355:     0, /* dst_r_basereg */ \
37355:     0, /* src_basereg   */ \
37355:     0  /* mask_basereg  */
37355: 
37355: /******************************************************************************/
37355: 
37355: .macro pixman_composite_src_0888_0565_rev_process_pixblock_head
37355:     vshll.u8    q8, d1, #8
37355:     vshll.u8    q9, d2, #8
37355: .endm
37355: 
37355: .macro pixman_composite_src_0888_0565_rev_process_pixblock_tail
37355:     vshll.u8    q14, d0, #8
37355:     vsri.u16    q14, q8, #5
37355:     vsri.u16    q14, q9, #11
37355: .endm
37355: 
37355: .macro pixman_composite_src_0888_0565_rev_process_pixblock_tail_head
37355:         vshll.u8    q14, d0, #8
64161:     fetch_src_pixblock
37355:         vsri.u16    q14, q8, #5
37355:         vsri.u16    q14, q9, #11
37355:     vshll.u8    q8, d1, #8
37355:         vst1.16 {d28, d29}, [DST_W, :128]!
37355:     vshll.u8    q9, d2, #8
37355: .endm
37355: 
37355: generate_composite_function \
37355:     pixman_composite_src_0888_0565_rev_asm_neon, 24, 0, 16, \
37355:     FLAG_DST_WRITEONLY, \
37355:     8, /* number of pixels, processed in a single block */ \
37355:     10, /* prefetch distance */ \
37355:     default_init, \
37355:     default_cleanup, \
37355:     pixman_composite_src_0888_0565_rev_process_pixblock_head, \
37355:     pixman_composite_src_0888_0565_rev_process_pixblock_tail, \
37355:     pixman_composite_src_0888_0565_rev_process_pixblock_tail_head, \
37355:     28, /* dst_w_basereg */ \
37355:     0, /* dst_r_basereg */ \
37355:     0, /* src_basereg   */ \
37355:     0  /* mask_basereg  */
37355: 
37355: /******************************************************************************/
37355: 
37355: .macro pixman_composite_src_pixbuf_8888_process_pixblock_head
37355:     vmull.u8    q8, d3, d0
37355:     vmull.u8    q9, d3, d1
37355:     vmull.u8    q10, d3, d2
37355: .endm
37355: 
37355: .macro pixman_composite_src_pixbuf_8888_process_pixblock_tail
37355:     vrshr.u16   q11, q8, #8
37355:     vswp        d3, d31
37355:     vrshr.u16   q12, q9, #8
37355:     vrshr.u16   q13, q10, #8
37355:     vraddhn.u16 d30, q11, q8
37355:     vraddhn.u16 d29, q12, q9
37355:     vraddhn.u16 d28, q13, q10
37355: .endm
37355: 
37355: .macro pixman_composite_src_pixbuf_8888_process_pixblock_tail_head
37355:         vrshr.u16   q11, q8, #8
37355:         vswp        d3, d31
37355:         vrshr.u16   q12, q9, #8
37355:         vrshr.u16   q13, q10, #8
64161:     fetch_src_pixblock
37355:         vraddhn.u16 d30, q11, q8
37355:                                     PF add PF_X, PF_X, #8
37355:                                     PF tst PF_CTL, #0xF
37355:                                     PF addne PF_X, PF_X, #8
37355:                                     PF subne PF_CTL, PF_CTL, #1
37355:         vraddhn.u16 d29, q12, q9
37355:         vraddhn.u16 d28, q13, q10
37355:     vmull.u8    q8, d3, d0
37355:     vmull.u8    q9, d3, d1
37355:     vmull.u8    q10, d3, d2
37355:         vst4.8 {d28, d29, d30, d31}, [DST_W, :128]!
37355:                                     PF cmp PF_X, ORIG_W
37355:                                     PF pld, [PF_SRC, PF_X, lsl #src_bpp_shift]
37355:                                     PF subge PF_X, PF_X, ORIG_W
37355:                                     PF subges PF_CTL, PF_CTL, #0x10
37355:                                     PF ldrgeb DUMMY, [PF_SRC, SRC_STRIDE, lsl #src_bpp_shift]!
37355: .endm
37355: 
37355: generate_composite_function \
37355:     pixman_composite_src_pixbuf_8888_asm_neon, 32, 0, 32, \
37355:     FLAG_DST_WRITEONLY | FLAG_DEINTERLEAVE_32BPP, \
37355:     8, /* number of pixels, processed in a single block */ \
37355:     10, /* prefetch distance */ \
37355:     default_init, \
37355:     default_cleanup, \
37355:     pixman_composite_src_pixbuf_8888_process_pixblock_head, \
37355:     pixman_composite_src_pixbuf_8888_process_pixblock_tail, \
37355:     pixman_composite_src_pixbuf_8888_process_pixblock_tail_head, \
37355:     28, /* dst_w_basereg */ \
37355:     0, /* dst_r_basereg */ \
37355:     0, /* src_basereg   */ \
37355:     0  /* mask_basereg  */
56260: 
56260: /******************************************************************************/
56260: 
64161: .macro pixman_composite_src_rpixbuf_8888_process_pixblock_head
64161:     vmull.u8    q8, d3, d0
64161:     vmull.u8    q9, d3, d1
64161:     vmull.u8    q10, d3, d2
64161: .endm
64161: 
64161: .macro pixman_composite_src_rpixbuf_8888_process_pixblock_tail
64161:     vrshr.u16   q11, q8, #8
64161:     vswp        d3, d31
64161:     vrshr.u16   q12, q9, #8
64161:     vrshr.u16   q13, q10, #8
64161:     vraddhn.u16 d28, q11, q8
64161:     vraddhn.u16 d29, q12, q9
64161:     vraddhn.u16 d30, q13, q10
64161: .endm
64161: 
64161: .macro pixman_composite_src_rpixbuf_8888_process_pixblock_tail_head
64161:         vrshr.u16   q11, q8, #8
64161:         vswp        d3, d31
64161:         vrshr.u16   q12, q9, #8
64161:         vrshr.u16   q13, q10, #8
64161:     fetch_src_pixblock
64161:         vraddhn.u16 d28, q11, q8
64161:                                     PF add PF_X, PF_X, #8
64161:                                     PF tst PF_CTL, #0xF
64161:                                     PF addne PF_X, PF_X, #8
64161:                                     PF subne PF_CTL, PF_CTL, #1
64161:         vraddhn.u16 d29, q12, q9
64161:         vraddhn.u16 d30, q13, q10
64161:     vmull.u8    q8, d3, d0
64161:     vmull.u8    q9, d3, d1
64161:     vmull.u8    q10, d3, d2
64161:         vst4.8 {d28, d29, d30, d31}, [DST_W, :128]!
64161:                                     PF cmp PF_X, ORIG_W
64161:                                     PF pld, [PF_SRC, PF_X, lsl #src_bpp_shift]
64161:                                     PF subge PF_X, PF_X, ORIG_W
64161:                                     PF subges PF_CTL, PF_CTL, #0x10
64161:                                     PF ldrgeb DUMMY, [PF_SRC, SRC_STRIDE, lsl #src_bpp_shift]!
64161: .endm
64161: 
64161: generate_composite_function \
64161:     pixman_composite_src_rpixbuf_8888_asm_neon, 32, 0, 32, \
64161:     FLAG_DST_WRITEONLY | FLAG_DEINTERLEAVE_32BPP, \
64161:     8, /* number of pixels, processed in a single block */ \
64161:     10, /* prefetch distance */ \
64161:     default_init, \
64161:     default_cleanup, \
64161:     pixman_composite_src_rpixbuf_8888_process_pixblock_head, \
64161:     pixman_composite_src_rpixbuf_8888_process_pixblock_tail, \
64161:     pixman_composite_src_rpixbuf_8888_process_pixblock_tail_head, \
64161:     28, /* dst_w_basereg */ \
64161:     0, /* dst_r_basereg */ \
64161:     0, /* src_basereg   */ \
64161:     0  /* mask_basereg  */
64161: 
64161: /******************************************************************************/
64161: 
56260: .macro pixman_composite_over_0565_8_0565_process_pixblock_head
56260:     /* mask is in d15 */
56260:     convert_0565_to_x888 q4, d2, d1, d0
56260:     convert_0565_to_x888 q5, d6, d5, d4
56260:     /* source pixel data is in      {d0, d1, d2, XX} */
56260:     /* destination pixel data is in {d4, d5, d6, XX} */
56260:     vmvn.8      d7,  d15
56260:     vmull.u8    q6,  d15, d2
56260:     vmull.u8    q5,  d15, d1
56260:     vmull.u8    q4,  d15, d0
56260:     vmull.u8    q8,  d7,  d4
56260:     vmull.u8    q9,  d7,  d5
56260:     vmull.u8    q13, d7,  d6
56260:     vrshr.u16   q12, q6,  #8
56260:     vrshr.u16   q11, q5,  #8
56260:     vrshr.u16   q10, q4,  #8
56260:     vraddhn.u16 d2,  q6,  q12
56260:     vraddhn.u16 d1,  q5,  q11
56260:     vraddhn.u16 d0,  q4,  q10
56260: .endm
56260: 
56260: .macro pixman_composite_over_0565_8_0565_process_pixblock_tail
56260:     vrshr.u16   q14, q8,  #8
56260:     vrshr.u16   q15, q9,  #8
56260:     vrshr.u16   q12, q13, #8
56260:     vraddhn.u16 d28, q14, q8
56260:     vraddhn.u16 d29, q15, q9
56260:     vraddhn.u16 d30, q12, q13
56260:     vqadd.u8    q0,  q0,  q14
56260:     vqadd.u8    q1,  q1,  q15
56260:     /* 32bpp result is in {d0, d1, d2, XX} */
56260:     convert_8888_to_0565 d2, d1, d0, q14, q15, q3
56260: .endm
56260: 
56260: /* TODO: expand macros and do better instructions scheduling */
56260: .macro pixman_composite_over_0565_8_0565_process_pixblock_tail_head
64161:     fetch_mask_pixblock
56260:     pixman_composite_over_0565_8_0565_process_pixblock_tail
64161:     fetch_src_pixblock
56260:     vld1.16    {d10, d11}, [DST_R, :128]!
56260:     cache_preload 8, 8
56260:     pixman_composite_over_0565_8_0565_process_pixblock_head
56260:     vst1.16    {d28, d29}, [DST_W, :128]!
56260: .endm
56260: 
56260: generate_composite_function \
56260:     pixman_composite_over_0565_8_0565_asm_neon, 16, 8, 16, \
56260:     FLAG_DST_READWRITE, \
56260:     8, /* number of pixels, processed in a single block */ \
56260:     5, /* prefetch distance */ \
56260:     default_init_need_all_regs, \
56260:     default_cleanup_need_all_regs, \
56260:     pixman_composite_over_0565_8_0565_process_pixblock_head, \
56260:     pixman_composite_over_0565_8_0565_process_pixblock_tail, \
56260:     pixman_composite_over_0565_8_0565_process_pixblock_tail_head, \
56260:     28, /* dst_w_basereg */ \
56260:     10,  /* dst_r_basereg */ \
56260:     8,  /* src_basereg   */ \
56260:     15  /* mask_basereg  */
56260: 
56260: /******************************************************************************/
56260: 
64161: .macro pixman_composite_over_0565_n_0565_init
64161:     add         DUMMY, sp, #(ARGS_STACK_OFFSET + 8)
64161:     vpush       {d8-d15}
64161:     vld1.32     {d15[0]}, [DUMMY]
64161:     vdup.8      d15, d15[3]
64161: .endm
64161: 
64161: .macro pixman_composite_over_0565_n_0565_cleanup
64161:     vpop        {d8-d15}
64161: .endm
64161: 
64161: generate_composite_function \
64161:     pixman_composite_over_0565_n_0565_asm_neon, 16, 0, 16, \
64161:     FLAG_DST_READWRITE, \
64161:     8, /* number of pixels, processed in a single block */ \
64161:     5, /* prefetch distance */ \
64161:     pixman_composite_over_0565_n_0565_init, \
64161:     pixman_composite_over_0565_n_0565_cleanup, \
64161:     pixman_composite_over_0565_8_0565_process_pixblock_head, \
64161:     pixman_composite_over_0565_8_0565_process_pixblock_tail, \
64161:     pixman_composite_over_0565_8_0565_process_pixblock_tail_head, \
64161:     28, /* dst_w_basereg */ \
64161:     10, /* dst_r_basereg */ \
64161:     8,  /* src_basereg   */ \
64161:     15  /* mask_basereg  */
64161: 
64161: /******************************************************************************/
64161: 
56260: .macro pixman_composite_add_0565_8_0565_process_pixblock_head
56260:     /* mask is in d15 */
56260:     convert_0565_to_x888 q4, d2, d1, d0
56260:     convert_0565_to_x888 q5, d6, d5, d4
56260:     /* source pixel data is in      {d0, d1, d2, XX} */
56260:     /* destination pixel data is in {d4, d5, d6, XX} */
56260:     vmull.u8    q6,  d15, d2
56260:     vmull.u8    q5,  d15, d1
56260:     vmull.u8    q4,  d15, d0
56260:     vrshr.u16   q12, q6,  #8
56260:     vrshr.u16   q11, q5,  #8
56260:     vrshr.u16   q10, q4,  #8
56260:     vraddhn.u16 d2,  q6,  q12
56260:     vraddhn.u16 d1,  q5,  q11
56260:     vraddhn.u16 d0,  q4,  q10
56260: .endm
56260: 
56260: .macro pixman_composite_add_0565_8_0565_process_pixblock_tail
56260:     vqadd.u8    q0,  q0,  q2
56260:     vqadd.u8    q1,  q1,  q3
56260:     /* 32bpp result is in {d0, d1, d2, XX} */
56260:     convert_8888_to_0565 d2, d1, d0, q14, q15, q3
56260: .endm
56260: 
56260: /* TODO: expand macros and do better instructions scheduling */
56260: .macro pixman_composite_add_0565_8_0565_process_pixblock_tail_head
64161:     fetch_mask_pixblock
56260:     pixman_composite_add_0565_8_0565_process_pixblock_tail
64161:     fetch_src_pixblock
56260:     vld1.16    {d10, d11}, [DST_R, :128]!
56260:     cache_preload 8, 8
56260:     pixman_composite_add_0565_8_0565_process_pixblock_head
56260:     vst1.16    {d28, d29}, [DST_W, :128]!
56260: .endm
56260: 
56260: generate_composite_function \
56260:     pixman_composite_add_0565_8_0565_asm_neon, 16, 8, 16, \
56260:     FLAG_DST_READWRITE, \
56260:     8, /* number of pixels, processed in a single block */ \
56260:     5, /* prefetch distance */ \
56260:     default_init_need_all_regs, \
56260:     default_cleanup_need_all_regs, \
56260:     pixman_composite_add_0565_8_0565_process_pixblock_head, \
56260:     pixman_composite_add_0565_8_0565_process_pixblock_tail, \
56260:     pixman_composite_add_0565_8_0565_process_pixblock_tail_head, \
56260:     28, /* dst_w_basereg */ \
56260:     10, /* dst_r_basereg */ \
56260:     8,  /* src_basereg   */ \
56260:     15  /* mask_basereg  */
56260: 
56260: /******************************************************************************/
56260: 
56260: .macro pixman_composite_out_reverse_8_0565_process_pixblock_head
56260:     /* mask is in d15 */
56260:     convert_0565_to_x888 q5, d6, d5, d4
56260:     /* destination pixel data is in {d4, d5, d6, xx} */
56260:     vmvn.8      d24, d15 /* get inverted alpha */
56260:     /* now do alpha blending */
56260:     vmull.u8    q8, d24, d4
56260:     vmull.u8    q9, d24, d5
56260:     vmull.u8    q10, d24, d6
56260: .endm
56260: 
56260: .macro pixman_composite_out_reverse_8_0565_process_pixblock_tail
56260:     vrshr.u16   q14, q8, #8
56260:     vrshr.u16   q15, q9, #8
56260:     vrshr.u16   q12, q10, #8
56260:     vraddhn.u16 d0, q14, q8
56260:     vraddhn.u16 d1, q15, q9
56260:     vraddhn.u16 d2, q12, q10
56260:     /* 32bpp result is in {d0, d1, d2, XX} */
56260:     convert_8888_to_0565 d2, d1, d0, q14, q15, q3
56260: .endm
56260: 
56260: /* TODO: expand macros and do better instructions scheduling */
56260: .macro pixman_composite_out_reverse_8_0565_process_pixblock_tail_head
64161:     fetch_src_pixblock
56260:     pixman_composite_out_reverse_8_0565_process_pixblock_tail
56260:     vld1.16    {d10, d11}, [DST_R, :128]!
56260:     cache_preload 8, 8
56260:     pixman_composite_out_reverse_8_0565_process_pixblock_head
56260:     vst1.16    {d28, d29}, [DST_W, :128]!
56260: .endm
56260: 
56260: generate_composite_function \
56260:     pixman_composite_out_reverse_8_0565_asm_neon, 8, 0, 16, \
56260:     FLAG_DST_READWRITE, \
56260:     8, /* number of pixels, processed in a single block */ \
56260:     5, /* prefetch distance */ \
56260:     default_init_need_all_regs, \
56260:     default_cleanup_need_all_regs, \
56260:     pixman_composite_out_reverse_8_0565_process_pixblock_head, \
56260:     pixman_composite_out_reverse_8_0565_process_pixblock_tail, \
56260:     pixman_composite_out_reverse_8_0565_process_pixblock_tail_head, \
56260:     28, /* dst_w_basereg */ \
56260:     10, /* dst_r_basereg */ \
56260:     15, /* src_basereg   */ \
56260:     0   /* mask_basereg  */
64161: 
64161: /******************************************************************************/
64161: 
64161: generate_composite_function_nearest_scanline \
64161:     pixman_scaled_nearest_scanline_8888_8888_OVER_asm_neon, 32, 0, 32, \
64161:     FLAG_DST_READWRITE | FLAG_DEINTERLEAVE_32BPP, \
64161:     8, /* number of pixels, processed in a single block */ \
64161:     default_init, \
64161:     default_cleanup, \
64161:     pixman_composite_over_8888_8888_process_pixblock_head, \
64161:     pixman_composite_over_8888_8888_process_pixblock_tail, \
64161:     pixman_composite_over_8888_8888_process_pixblock_tail_head
64161: 
64161: generate_composite_function_nearest_scanline \
64161:     pixman_scaled_nearest_scanline_8888_0565_OVER_asm_neon, 32, 0, 16, \
64161:     FLAG_DST_READWRITE | FLAG_DEINTERLEAVE_32BPP, \
64161:     8, /* number of pixels, processed in a single block */ \
64161:     default_init, \
64161:     default_cleanup, \
64161:     pixman_composite_over_8888_0565_process_pixblock_head, \
64161:     pixman_composite_over_8888_0565_process_pixblock_tail, \
64161:     pixman_composite_over_8888_0565_process_pixblock_tail_head, \
64161:     28, /* dst_w_basereg */ \
64161:     4,  /* dst_r_basereg */ \
64161:     0,  /* src_basereg   */ \
64161:     24  /* mask_basereg  */
64161: 
64161: generate_composite_function_nearest_scanline \
64161:     pixman_scaled_nearest_scanline_8888_0565_SRC_asm_neon, 32, 0, 16, \
64161:     FLAG_DST_WRITEONLY | FLAG_DEINTERLEAVE_32BPP, \
64161:     8, /* number of pixels, processed in a single block */ \
64161:     default_init, \
64161:     default_cleanup, \
64161:     pixman_composite_src_8888_0565_process_pixblock_head, \
64161:     pixman_composite_src_8888_0565_process_pixblock_tail, \
64161:     pixman_composite_src_8888_0565_process_pixblock_tail_head
64161: 
64161: generate_composite_function_nearest_scanline \
64161:     pixman_scaled_nearest_scanline_0565_8888_SRC_asm_neon, 16, 0, 32, \
64161:     FLAG_DST_WRITEONLY | FLAG_DEINTERLEAVE_32BPP, \
64161:     8, /* number of pixels, processed in a single block */ \
64161:     default_init, \
64161:     default_cleanup, \
64161:     pixman_composite_src_0565_8888_process_pixblock_head, \
64161:     pixman_composite_src_0565_8888_process_pixblock_tail, \
64161:     pixman_composite_src_0565_8888_process_pixblock_tail_head
64161: 
64161: generate_composite_function_nearest_scanline \
64161:     pixman_scaled_nearest_scanline_8888_8_0565_OVER_asm_neon, 32, 8, 16, \
64161:     FLAG_DST_READWRITE | FLAG_DEINTERLEAVE_32BPP, \
64161:     8, /* number of pixels, processed in a single block */ \
64161:     default_init_need_all_regs, \
64161:     default_cleanup_need_all_regs, \
64161:     pixman_composite_over_8888_8_0565_process_pixblock_head, \
64161:     pixman_composite_over_8888_8_0565_process_pixblock_tail, \
64161:     pixman_composite_over_8888_8_0565_process_pixblock_tail_head, \
64161:     28, /* dst_w_basereg */ \
64161:     4,  /* dst_r_basereg */ \
64161:     8,  /* src_basereg   */ \
64161:     24  /* mask_basereg  */
64161: 
64161: generate_composite_function_nearest_scanline \
64161:     pixman_scaled_nearest_scanline_0565_8_0565_OVER_asm_neon, 16, 8, 16, \
64161:     FLAG_DST_READWRITE, \
64161:     8, /* number of pixels, processed in a single block */ \
64161:     default_init_need_all_regs, \
64161:     default_cleanup_need_all_regs, \
64161:     pixman_composite_over_0565_8_0565_process_pixblock_head, \
64161:     pixman_composite_over_0565_8_0565_process_pixblock_tail, \
64161:     pixman_composite_over_0565_8_0565_process_pixblock_tail_head, \
64161:     28, /* dst_w_basereg */ \
64161:     10,  /* dst_r_basereg */ \
64161:     8,  /* src_basereg   */ \
64161:     15  /* mask_basereg  */
64161: 
64161: /******************************************************************************/
64161: 
64161: /* Supplementary macro for setting function attributes */
64161: .macro pixman_asm_function fname
64161:     .func fname
64161:     .global fname
64161: #ifdef __ELF__
64161:     .hidden fname
64161:     .type fname, %function
64161: #endif
64161: fname:
64161: .endm
64161: 
64161: /*
64161:  * Bilinear scaling support code which tries to provide pixel fetching, color
64161:  * format conversion, and interpolation as separate macros which can be used
64161:  * as the basic building blocks for constructing bilinear scanline functions.
64161:  */
64161: 
64161: .macro bilinear_load_8888 reg1, reg2, tmp
70077:     mov       TMP1, X, asr #16
64161:     add       X, X, UX
70077:     add       TMP1, TOP, TMP1, asl #2
70077:     vld1.32   {reg1}, [TMP1], STRIDE
70077:     vld1.32   {reg2}, [TMP1]
64161: .endm
64161: 
64161: .macro bilinear_load_0565 reg1, reg2, tmp
70077:     mov       TMP1, X, asr #16
64161:     add       X, X, UX
70077:     add       TMP1, TOP, TMP1, asl #1
70077:     vld1.32   {reg2[0]}, [TMP1], STRIDE
70077:     vld1.32   {reg2[1]}, [TMP1]
64161:     convert_four_0565_to_x888_packed reg2, reg1, reg2, tmp
64161: .endm
64161: 
64161: .macro bilinear_load_and_vertical_interpolate_two_8888 \
64161:                     acc1, acc2, reg1, reg2, reg3, reg4, tmp1, tmp2
64161: 
64161:     bilinear_load_8888 reg1, reg2, tmp1
64161:     vmull.u8  acc1, reg1, d28
64161:     vmlal.u8  acc1, reg2, d29
64161:     bilinear_load_8888 reg3, reg4, tmp2
64161:     vmull.u8  acc2, reg3, d28
64161:     vmlal.u8  acc2, reg4, d29
64161: .endm
64161: 
64161: .macro bilinear_load_and_vertical_interpolate_four_8888 \
64161:                 xacc1, xacc2, xreg1, xreg2, xreg3, xreg4, xacc2lo, xacc2hi \
64161:                 yacc1, yacc2, yreg1, yreg2, yreg3, yreg4, yacc2lo, yacc2hi
64161: 
64161:     bilinear_load_and_vertical_interpolate_two_8888 \
64161:                 xacc1, xacc2, xreg1, xreg2, xreg3, xreg4, xacc2lo, xacc2hi
64161:     bilinear_load_and_vertical_interpolate_two_8888 \
64161:                 yacc1, yacc2, yreg1, yreg2, yreg3, yreg4, yacc2lo, yacc2hi
64161: .endm
64161: 
64161: .macro bilinear_load_and_vertical_interpolate_two_0565 \
64161:                 acc1, acc2, reg1, reg2, reg3, reg4, acc2lo, acc2hi
64161: 
70077:     mov       TMP1, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP1, TOP, TMP1, asl #1
64161:     mov       TMP2, X, asr #16
64161:     add       X, X, UX
70077:     add       TMP2, TOP, TMP2, asl #1
70077:     vld1.32   {acc2lo[0]}, [TMP1], STRIDE
70077:     vld1.32   {acc2hi[0]}, [TMP2], STRIDE
70077:     vld1.32   {acc2lo[1]}, [TMP1]
70077:     vld1.32   {acc2hi[1]}, [TMP2]
64161:     convert_0565_to_x888 acc2, reg3, reg2, reg1
64161:     vzip.u8   reg1, reg3
64161:     vzip.u8   reg2, reg4
64161:     vzip.u8   reg3, reg4
64161:     vzip.u8   reg1, reg2
64161:     vmull.u8  acc1, reg1, d28
64161:     vmlal.u8  acc1, reg2, d29
64161:     vmull.u8  acc2, reg3, d28
64161:     vmlal.u8  acc2, reg4, d29
64161: .endm
64161: 
64161: .macro bilinear_load_and_vertical_interpolate_four_0565 \
64161:                 xacc1, xacc2, xreg1, xreg2, xreg3, xreg4, xacc2lo, xacc2hi \
64161:                 yacc1, yacc2, yreg1, yreg2, yreg3, yreg4, yacc2lo, yacc2hi
64161: 
70077:     mov       TMP1, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP1, TOP, TMP1, asl #1
64161:     mov       TMP2, X, asr #16
64161:     add       X, X, UX
70077:     add       TMP2, TOP, TMP2, asl #1
70077:     vld1.32   {xacc2lo[0]}, [TMP1], STRIDE
70077:     vld1.32   {xacc2hi[0]}, [TMP2], STRIDE
70077:     vld1.32   {xacc2lo[1]}, [TMP1]
70077:     vld1.32   {xacc2hi[1]}, [TMP2]
70077:     convert_0565_to_x888 xacc2, xreg3, xreg2, xreg1
70077:     mov       TMP1, X, asr #16
64161:     add       X, X, UX
70077:     add       TMP1, TOP, TMP1, asl #1
64161:     mov       TMP2, X, asr #16
64161:     add       X, X, UX
70077:     add       TMP2, TOP, TMP2, asl #1
70077:     vld1.32   {yacc2lo[0]}, [TMP1], STRIDE
64161:     vzip.u8   xreg1, xreg3
70077:     vld1.32   {yacc2hi[0]}, [TMP2], STRIDE
64161:     vzip.u8   xreg2, xreg4
70077:     vld1.32   {yacc2lo[1]}, [TMP1]
64161:     vzip.u8   xreg3, xreg4
70077:     vld1.32   {yacc2hi[1]}, [TMP2]
64161:     vzip.u8   xreg1, xreg2
64161:     convert_0565_to_x888 yacc2, yreg3, yreg2, yreg1
64161:     vmull.u8  xacc1, xreg1, d28
64161:     vzip.u8   yreg1, yreg3
64161:     vmlal.u8  xacc1, xreg2, d29
64161:     vzip.u8   yreg2, yreg4
64161:     vmull.u8  xacc2, xreg3, d28
64161:     vzip.u8   yreg3, yreg4
64161:     vmlal.u8  xacc2, xreg4, d29
64161:     vzip.u8   yreg1, yreg2
64161:     vmull.u8  yacc1, yreg1, d28
64161:     vmlal.u8  yacc1, yreg2, d29
64161:     vmull.u8  yacc2, yreg3, d28
64161:     vmlal.u8  yacc2, yreg4, d29
64161: .endm
64161: 
64161: .macro bilinear_store_8888 numpix, tmp1, tmp2
64161: .if numpix == 4
70077:     vst1.32   {d0, d1}, [OUT, :128]!
64161: .elseif numpix == 2
70077:     vst1.32   {d0}, [OUT, :64]!
64161: .elseif numpix == 1
64161:     vst1.32   {d0[0]}, [OUT, :32]!
64161: .else
64161:     .error bilinear_store_8888 numpix is unsupported
64161: .endif
64161: .endm
64161: 
64161: .macro bilinear_store_0565 numpix, tmp1, tmp2
64161:     vuzp.u8 d0, d1
64161:     vuzp.u8 d2, d3
64161:     vuzp.u8 d1, d3
64161:     vuzp.u8 d0, d2
64161:     convert_8888_to_0565 d2, d1, d0, q1, tmp1, tmp2
64161: .if numpix == 4
70077:     vst1.16   {d2}, [OUT, :64]!
64161: .elseif numpix == 2
70077:     vst1.32   {d2[0]}, [OUT, :32]!
64161: .elseif numpix == 1
70077:     vst1.16   {d2[0]}, [OUT, :16]!
64161: .else
64161:     .error bilinear_store_0565 numpix is unsupported
64161: .endif
64161: .endm
64161: 
64161: .macro bilinear_interpolate_last_pixel src_fmt, dst_fmt
64161:     bilinear_load_&src_fmt d0, d1, d2
64161:     vmull.u8  q1, d0, d28
64161:     vmlal.u8  q1, d1, d29
70077:     /* 5 cycles bubble */
64161:     vshll.u16 q0, d2, #8
64161:     vmlsl.u16 q0, d2, d30
64161:     vmlal.u16 q0, d3, d30
64161:     /* 5 cycles bubble */
64161:     vshrn.u32 d0, q0, #16
64161:     /* 3 cycles bubble */
64161:     vmovn.u16 d0, q0
64161:     /* 1 cycle bubble */
64161:     bilinear_store_&dst_fmt 1, q2, q3
64161: .endm
64161: 
64161: .macro bilinear_interpolate_two_pixels src_fmt, dst_fmt
64161:     bilinear_load_and_vertical_interpolate_two_&src_fmt \
64161:                 q1, q11, d0, d1, d20, d21, d22, d23
64161:     vshll.u16 q0, d2, #8
64161:     vmlsl.u16 q0, d2, d30
64161:     vmlal.u16 q0, d3, d30
64161:     vshll.u16 q10, d22, #8
64161:     vmlsl.u16 q10, d22, d31
64161:     vmlal.u16 q10, d23, d31
70077:     vshrn.u32 d0, q0, #16
70077:     vshrn.u32 d1, q10, #16
70077:     vshr.u16  q15, q12, #8
70077:     vadd.u16  q12, q12, q13
70077:     vmovn.u16 d0, q0
64161:     bilinear_store_&dst_fmt 2, q2, q3
64161: .endm
64161: 
64161: .macro bilinear_interpolate_four_pixels src_fmt, dst_fmt
64161:     bilinear_load_and_vertical_interpolate_four_&src_fmt \
64161:                 q1, q11, d0, d1, d20, d21, d22, d23 \
64161:                 q3, q9,  d4, d5, d16, d17, d18, d19
64161:     pld       [TMP1, PF_OFFS]
70077:     sub       TMP1, TMP1, STRIDE
64161:     vshll.u16 q0, d2, #8
64161:     vmlsl.u16 q0, d2, d30
64161:     vmlal.u16 q0, d3, d30
64161:     vshll.u16 q10, d22, #8
64161:     vmlsl.u16 q10, d22, d31
64161:     vmlal.u16 q10, d23, d31
64161:     vshr.u16  q15, q12, #8
64161:     vshll.u16 q2, d6, #8
64161:     vmlsl.u16 q2, d6, d30
64161:     vmlal.u16 q2, d7, d30
64161:     vshll.u16 q8, d18, #8
64161:     pld       [TMP2, PF_OFFS]
64161:     vmlsl.u16 q8, d18, d31
64161:     vmlal.u16 q8, d19, d31
64161:     vadd.u16  q12, q12, q13
64161:     vshrn.u32 d0, q0, #16
64161:     vshrn.u32 d1, q10, #16
64161:     vshrn.u32 d4, q2, #16
64161:     vshrn.u32 d5, q8, #16
70077:     vshr.u16  q15, q12, #8
64161:     vmovn.u16 d0, q0
64161:     vmovn.u16 d1, q2
70077:     vadd.u16  q12, q12, q13
64161:     bilinear_store_&dst_fmt 4, q2, q3
64161: .endm
64161: 
70077: .macro bilinear_interpolate_four_pixels_head src_fmt, dst_fmt
70077: .ifdef have_bilinear_interpolate_four_pixels_&src_fmt&_&dst_fmt
70077:     bilinear_interpolate_four_pixels_&src_fmt&_&dst_fmt&_head
70077: .else
70077:     bilinear_interpolate_four_pixels src_fmt, dst_fmt
70077: .endif
70077: .endm
70077: 
70077: .macro bilinear_interpolate_four_pixels_tail src_fmt, dst_fmt
70077: .ifdef have_bilinear_interpolate_four_pixels_&src_fmt&_&dst_fmt
70077:     bilinear_interpolate_four_pixels_&src_fmt&_&dst_fmt&_tail
70077: .endif
70077: .endm
70077: 
70077: .macro bilinear_interpolate_four_pixels_tail_head src_fmt, dst_fmt
70077: .ifdef have_bilinear_interpolate_four_pixels_&src_fmt&_&dst_fmt
70077:     bilinear_interpolate_four_pixels_&src_fmt&_&dst_fmt&_tail_head
70077: .else
70077:     bilinear_interpolate_four_pixels src_fmt, dst_fmt
70077: .endif
70077: .endm
70077: 
70077: .macro bilinear_interpolate_eight_pixels_head src_fmt, dst_fmt
70077: .ifdef have_bilinear_interpolate_eight_pixels_&src_fmt&_&dst_fmt
70077:     bilinear_interpolate_eight_pixels_&src_fmt&_&dst_fmt&_head
70077: .else
70077:     bilinear_interpolate_four_pixels_head src_fmt, dst_fmt
70077:     bilinear_interpolate_four_pixels_tail_head src_fmt, dst_fmt
70077: .endif
70077: .endm
70077: 
70077: .macro bilinear_interpolate_eight_pixels_tail src_fmt, dst_fmt
70077: .ifdef have_bilinear_interpolate_eight_pixels_&src_fmt&_&dst_fmt
70077:     bilinear_interpolate_eight_pixels_&src_fmt&_&dst_fmt&_tail
70077: .else
70077:     bilinear_interpolate_four_pixels_tail src_fmt, dst_fmt
70077: .endif
70077: .endm
70077: 
70077: .macro bilinear_interpolate_eight_pixels_tail_head src_fmt, dst_fmt
70077: .ifdef have_bilinear_interpolate_eight_pixels_&src_fmt&_&dst_fmt
70077:     bilinear_interpolate_eight_pixels_&src_fmt&_&dst_fmt&_tail_head
70077: .else
70077:     bilinear_interpolate_four_pixels_tail_head src_fmt, dst_fmt
70077:     bilinear_interpolate_four_pixels_tail_head src_fmt, dst_fmt
70077: .endif
70077: .endm
70077: 
70077: .set BILINEAR_FLAG_UNROLL_4,          0
70077: .set BILINEAR_FLAG_UNROLL_8,          1
70077: .set BILINEAR_FLAG_USE_ALL_NEON_REGS, 2
70077: 
64161: /*
64161:  * Main template macro for generating NEON optimized bilinear scanline
64161:  * functions.
64161:  *
64161:  * Bilinear scanline scaler macro template uses the following arguments:
64161:  *  fname             - name of the function to generate
64161:  *  src_fmt           - source color format (8888 or 0565)
64161:  *  dst_fmt           - destination color format (8888 or 0565)
64161:  *  bpp_shift         - (1 << bpp_shift) is the size of source pixel in bytes
64161:  *  prefetch_distance - prefetch in the source image by that many
64161:  *                      pixels ahead
64161:  */
64161: 
64161: .macro generate_bilinear_scanline_func fname, src_fmt, dst_fmt, \
70077:                                        src_bpp_shift, dst_bpp_shift, \
70077:                                        prefetch_distance, flags
64161: 
64161: pixman_asm_function fname
64161:     OUT       .req      r0
64161:     TOP       .req      r1
64161:     BOTTOM    .req      r2
64161:     WT        .req      r3
64161:     WB        .req      r4
64161:     X         .req      r5
64161:     UX        .req      r6
64161:     WIDTH     .req      ip
64161:     TMP1      .req      r3
64161:     TMP2      .req      r4
64161:     PF_OFFS   .req      r7
64161:     TMP3      .req      r8
64161:     TMP4      .req      r9
70077:     STRIDE    .req      r2
64161: 
64161:     mov       ip, sp
64161:     push      {r4, r5, r6, r7, r8, r9}
64161:     mov       PF_OFFS, #prefetch_distance
64161:     ldmia     ip, {WB, X, UX, WIDTH}
64161:     mul       PF_OFFS, PF_OFFS, UX
64161: 
70077: .if ((flags) & BILINEAR_FLAG_USE_ALL_NEON_REGS) != 0
70077:     vpush     {d8-d15}
70077: .endif
70077: 
70077:     sub       STRIDE, BOTTOM, TOP
70077:     .unreq    BOTTOM
70077: 
64161:     cmp       WIDTH, #0
64161:     ble       3f
64161: 
64161:     vdup.u16  q12, X
64161:     vdup.u16  q13, UX
64161:     vdup.u8   d28, WT
64161:     vdup.u8   d29, WB
64161:     vadd.u16  d25, d25, d26
70077: 
70077:     /* ensure good destination alignment  */
70077:     cmp       WIDTH, #1
70077:     blt       0f
70077:     tst       OUT, #(1 << dst_bpp_shift)
70077:     beq       0f
70077:     vshr.u16  q15, q12, #8
70077:     vadd.u16  q12, q12, q13
70077:     bilinear_interpolate_last_pixel src_fmt, dst_fmt
70077:     sub       WIDTH, WIDTH, #1
70077: 0:
64161:     vadd.u16  q13, q13, q13
70077:     vshr.u16  q15, q12, #8
70077:     vadd.u16  q12, q12, q13
70077: 
70077:     cmp       WIDTH, #2
70077:     blt       0f
70077:     tst       OUT, #(1 << (dst_bpp_shift + 1))
70077:     beq       0f
70077:     bilinear_interpolate_two_pixels src_fmt, dst_fmt
70077:     sub       WIDTH, WIDTH, #2
70077: 0:
70077: .if ((flags) & BILINEAR_FLAG_UNROLL_8) != 0
70077: /*********** 8 pixels per iteration *****************/
70077:     cmp       WIDTH, #4
70077:     blt       0f
70077:     tst       OUT, #(1 << (dst_bpp_shift + 2))
70077:     beq       0f
70077:     bilinear_interpolate_four_pixels src_fmt, dst_fmt
70077:     sub       WIDTH, WIDTH, #4
70077: 0:
70077:     subs      WIDTH, WIDTH, #8
70077:     blt       1f
70077:     mov       PF_OFFS, PF_OFFS, asr #(16 - src_bpp_shift)
70077:     bilinear_interpolate_eight_pixels_head src_fmt, dst_fmt
70077:     subs      WIDTH, WIDTH, #8
70077:     blt       5f
70077: 0:
70077:     bilinear_interpolate_eight_pixels_tail_head src_fmt, dst_fmt
70077:     subs      WIDTH, WIDTH, #8
70077:     bge       0b
70077: 5:
70077:     bilinear_interpolate_eight_pixels_tail src_fmt, dst_fmt
70077: 1:
70077:     tst       WIDTH, #4
70077:     beq       2f
70077:     bilinear_interpolate_four_pixels src_fmt, dst_fmt
70077: 2:
70077: .else
70077: /*********** 4 pixels per iteration *****************/
64161:     subs      WIDTH, WIDTH, #4
64161:     blt       1f
70077:     mov       PF_OFFS, PF_OFFS, asr #(16 - src_bpp_shift)
70077:     bilinear_interpolate_four_pixels_head src_fmt, dst_fmt
70077:     subs      WIDTH, WIDTH, #4
70077:     blt       5f
64161: 0:
70077:     bilinear_interpolate_four_pixels_tail_head src_fmt, dst_fmt
64161:     subs      WIDTH, WIDTH, #4
64161:     bge       0b
70077: 5:
70077:     bilinear_interpolate_four_pixels_tail src_fmt, dst_fmt
64161: 1:
70077: /****************************************************/
70077: .endif
70077:     /* handle the remaining trailing pixels */
64161:     tst       WIDTH, #2
64161:     beq       2f
64161:     bilinear_interpolate_two_pixels src_fmt, dst_fmt
64161: 2:
64161:     tst       WIDTH, #1
64161:     beq       3f
64161:     bilinear_interpolate_last_pixel src_fmt, dst_fmt
64161: 3:
70077: .if ((flags) & BILINEAR_FLAG_USE_ALL_NEON_REGS) != 0
70077:     vpop      {d8-d15}
70077: .endif
64161:     pop       {r4, r5, r6, r7, r8, r9}
64161:     bx        lr
64161: 
64161:     .unreq    OUT
64161:     .unreq    TOP
64161:     .unreq    WT
64161:     .unreq    WB
64161:     .unreq    X
64161:     .unreq    UX
64161:     .unreq    WIDTH
64161:     .unreq    TMP1
64161:     .unreq    TMP2
64161:     .unreq    PF_OFFS
64161:     .unreq    TMP3
64161:     .unreq    TMP4
70077:     .unreq    STRIDE
64161: .endfunc
64161: 
64161: .endm
64161: 
70077: /*****************************************************************************/
70077: 
70077: .set have_bilinear_interpolate_four_pixels_8888_8888, 1
70077: 
70077: .macro bilinear_interpolate_four_pixels_8888_8888_head
70077:     mov       TMP1, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP1, TOP, TMP1, asl #2
70077:     mov       TMP2, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP2, TOP, TMP2, asl #2
70077: 
70077:     vld1.32   {d22}, [TMP1], STRIDE
70077:     vld1.32   {d23}, [TMP1]
70077:     mov       TMP3, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP3, TOP, TMP3, asl #2
70077:     vmull.u8  q8, d22, d28
70077:     vmlal.u8  q8, d23, d29
70077: 
70077:     vld1.32   {d22}, [TMP2], STRIDE
70077:     vld1.32   {d23}, [TMP2]
70077:     mov       TMP4, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP4, TOP, TMP4, asl #2
70077:     vmull.u8  q9, d22, d28
70077:     vmlal.u8  q9, d23, d29
70077: 
70077:     vld1.32   {d22}, [TMP3], STRIDE
70077:     vld1.32   {d23}, [TMP3]
70077:     vmull.u8  q10, d22, d28
70077:     vmlal.u8  q10, d23, d29
70077: 
70077:     vshll.u16 q0, d16, #8
70077:     vmlsl.u16 q0, d16, d30
70077:     vmlal.u16 q0, d17, d30
70077: 
70077:     pld       [TMP4, PF_OFFS]
70077:     vld1.32   {d16}, [TMP4], STRIDE
70077:     vld1.32   {d17}, [TMP4]
70077:     pld       [TMP4, PF_OFFS]
70077:     vmull.u8  q11, d16, d28
70077:     vmlal.u8  q11, d17, d29
70077: 
70077:     vshll.u16 q1, d18, #8
70077:     vmlsl.u16 q1, d18, d31
70077: .endm
70077: 
70077: .macro bilinear_interpolate_four_pixels_8888_8888_tail
70077:     vmlal.u16 q1, d19, d31
70077:     vshr.u16  q15, q12, #8
70077:     vshll.u16 q2, d20, #8
70077:     vmlsl.u16 q2, d20, d30
70077:     vmlal.u16 q2, d21, d30
70077:     vshll.u16 q3, d22, #8
70077:     vmlsl.u16 q3, d22, d31
70077:     vmlal.u16 q3, d23, d31
70077:     vadd.u16  q12, q12, q13
70077:     vshrn.u32 d0, q0, #16
70077:     vshrn.u32 d1, q1, #16
70077:     vshrn.u32 d4, q2, #16
70077:     vshr.u16  q15, q12, #8
70077:     vshrn.u32 d5, q3, #16
70077:     vmovn.u16 d6, q0
70077:     vmovn.u16 d7, q2
70077:     vadd.u16  q12, q12, q13
70077:     vst1.32   {d6, d7}, [OUT, :128]!
70077: .endm
70077: 
70077: .macro bilinear_interpolate_four_pixels_8888_8888_tail_head
70077:     mov       TMP1, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP1, TOP, TMP1, asl #2
70077:     mov       TMP2, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP2, TOP, TMP2, asl #2
70077:         vmlal.u16 q1, d19, d31
70077:         vshr.u16  q15, q12, #8
70077:         vshll.u16 q2, d20, #8
70077:         vmlsl.u16 q2, d20, d30
70077:         vmlal.u16 q2, d21, d30
70077:         vshll.u16 q3, d22, #8
70077:     vld1.32   {d20}, [TMP1], STRIDE
70077:         vmlsl.u16 q3, d22, d31
70077:         vmlal.u16 q3, d23, d31
70077:     vld1.32   {d21}, [TMP1]
70077:     vmull.u8  q8, d20, d28
70077:     vmlal.u8  q8, d21, d29
70077:         vshrn.u32 d0, q0, #16
70077:         vshrn.u32 d1, q1, #16
70077:         vshrn.u32 d4, q2, #16
70077:     vld1.32   {d22}, [TMP2], STRIDE
70077:         vshrn.u32 d5, q3, #16
70077:         vadd.u16  q12, q12, q13
70077:     vld1.32   {d23}, [TMP2]
70077:     vmull.u8  q9, d22, d28
70077:     mov       TMP3, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP3, TOP, TMP3, asl #2
70077:     mov       TMP4, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP4, TOP, TMP4, asl #2
70077:     vmlal.u8  q9, d23, d29
70077:     vld1.32   {d22}, [TMP3], STRIDE
70077:         vshr.u16  q15, q12, #8
70077:     vld1.32   {d23}, [TMP3]
70077:     vmull.u8  q10, d22, d28
70077:     vmlal.u8  q10, d23, d29
70077:         vmovn.u16 d6, q0
70077:     vshll.u16 q0, d16, #8
70077:         vmovn.u16 d7, q2
70077:     vmlsl.u16 q0, d16, d30
70077:     vmlal.u16 q0, d17, d30
70077:     pld       [TMP4, PF_OFFS]
70077:     vld1.32   {d16}, [TMP4], STRIDE
70077:         vadd.u16  q12, q12, q13
70077:     vld1.32   {d17}, [TMP4]
70077:     pld       [TMP4, PF_OFFS]
70077:     vmull.u8  q11, d16, d28
70077:     vmlal.u8  q11, d17, d29
70077:         vst1.32   {d6, d7}, [OUT, :128]!
70077:     vshll.u16 q1, d18, #8
70077:     vmlsl.u16 q1, d18, d31
70077: .endm
70077: 
70077: /*****************************************************************************/
70077: 
70077: .set have_bilinear_interpolate_eight_pixels_8888_0565, 1
70077: 
70077: .macro bilinear_interpolate_eight_pixels_8888_0565_head
70077:     mov       TMP1, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP1, TOP, TMP1, asl #2
70077:     mov       TMP2, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP2, TOP, TMP2, asl #2
70077:     vld1.32   {d20}, [TMP1], STRIDE
70077:     vld1.32   {d21}, [TMP1]
70077:     vmull.u8  q8, d20, d28
70077:     vmlal.u8  q8, d21, d29
70077:     vld1.32   {d22}, [TMP2], STRIDE
70077:     vld1.32   {d23}, [TMP2]
70077:     vmull.u8  q9, d22, d28
70077:     mov       TMP3, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP3, TOP, TMP3, asl #2
70077:     mov       TMP4, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP4, TOP, TMP4, asl #2
70077:     vmlal.u8  q9, d23, d29
70077:     vld1.32   {d22}, [TMP3], STRIDE
70077:     vld1.32   {d23}, [TMP3]
70077:     vmull.u8  q10, d22, d28
70077:     vmlal.u8  q10, d23, d29
70077:     vshll.u16 q0, d16, #8
70077:     vmlsl.u16 q0, d16, d30
70077:     vmlal.u16 q0, d17, d30
70077:     pld       [TMP4, PF_OFFS]
70077:     vld1.32   {d16}, [TMP4], STRIDE
70077:     vld1.32   {d17}, [TMP4]
70077:     pld       [TMP4, PF_OFFS]
70077:     vmull.u8  q11, d16, d28
70077:     vmlal.u8  q11, d17, d29
70077:     vshll.u16 q1, d18, #8
70077:     vmlsl.u16 q1, d18, d31
70077: 
70077:     mov       TMP1, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP1, TOP, TMP1, asl #2
70077:     mov       TMP2, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP2, TOP, TMP2, asl #2
70077:         vmlal.u16 q1, d19, d31
70077:         vshr.u16  q15, q12, #8
70077:         vshll.u16 q2, d20, #8
70077:         vmlsl.u16 q2, d20, d30
70077:         vmlal.u16 q2, d21, d30
70077:         vshll.u16 q3, d22, #8
70077:     vld1.32   {d20}, [TMP1], STRIDE
70077:         vmlsl.u16 q3, d22, d31
70077:         vmlal.u16 q3, d23, d31
70077:     vld1.32   {d21}, [TMP1]
70077:     vmull.u8  q8, d20, d28
70077:     vmlal.u8  q8, d21, d29
70077:         vshrn.u32 d0, q0, #16
70077:         vshrn.u32 d1, q1, #16
70077:         vshrn.u32 d4, q2, #16
70077:     vld1.32   {d22}, [TMP2], STRIDE
70077:         vshrn.u32 d5, q3, #16
70077:         vadd.u16  q12, q12, q13
70077:     vld1.32   {d23}, [TMP2]
70077:     vmull.u8  q9, d22, d28
70077:     mov       TMP3, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP3, TOP, TMP3, asl #2
70077:     mov       TMP4, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP4, TOP, TMP4, asl #2
70077:     vmlal.u8  q9, d23, d29
70077:     vld1.32   {d22}, [TMP3], STRIDE
70077:         vshr.u16  q15, q12, #8
70077:     vld1.32   {d23}, [TMP3]
70077:     vmull.u8  q10, d22, d28
70077:     vmlal.u8  q10, d23, d29
70077:         vmovn.u16 d8, q0
70077:     vshll.u16 q0, d16, #8
70077:         vmovn.u16 d9, q2
70077:     vmlsl.u16 q0, d16, d30
70077:     vmlal.u16 q0, d17, d30
70077:     pld       [TMP4, PF_OFFS]
70077:     vld1.32   {d16}, [TMP4], STRIDE
70077:         vadd.u16  q12, q12, q13
70077:     vld1.32   {d17}, [TMP4]
70077:     pld       [TMP4, PF_OFFS]
70077:     vmull.u8  q11, d16, d28
70077:     vmlal.u8  q11, d17, d29
70077:     vshll.u16 q1, d18, #8
70077:     vmlsl.u16 q1, d18, d31
70077: .endm
70077: 
70077: .macro bilinear_interpolate_eight_pixels_8888_0565_tail
70077:     vmlal.u16 q1, d19, d31
70077:     vshr.u16  q15, q12, #8
70077:     vshll.u16 q2, d20, #8
70077:     vmlsl.u16 q2, d20, d30
70077:     vmlal.u16 q2, d21, d30
70077:     vshll.u16 q3, d22, #8
70077:     vmlsl.u16 q3, d22, d31
70077:     vmlal.u16 q3, d23, d31
70077:     vadd.u16  q12, q12, q13
70077:     vshrn.u32 d0, q0, #16
70077:     vshrn.u32 d1, q1, #16
70077:     vshrn.u32 d4, q2, #16
70077:     vshr.u16  q15, q12, #8
70077:     vshrn.u32 d5, q3, #16
70077:     vmovn.u16 d10, q0
70077:     vmovn.u16 d11, q2
70077:     vadd.u16  q12, q12, q13
70077: 
70077:     vuzp.u8   d8, d9
70077:     vuzp.u8   d10, d11
70077:     vuzp.u8   d9, d11
70077:     vuzp.u8   d8, d10
70077:     vshll.u8  q6, d9, #8
70077:     vshll.u8  q5, d10, #8
70077:     vshll.u8  q7, d8, #8
70077:     vsri.u16  q5, q6, #5
70077:     vsri.u16  q5, q7, #11
70077:     vst1.32   {d10, d11}, [OUT, :128]!
70077: .endm
70077: 
70077: .macro bilinear_interpolate_eight_pixels_8888_0565_tail_head
70077:     mov       TMP1, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP1, TOP, TMP1, asl #2
70077:     mov       TMP2, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP2, TOP, TMP2, asl #2
70077:         vmlal.u16 q1, d19, d31
70077:         vshr.u16  q15, q12, #8
70077:             vuzp.u8 d8, d9
70077:         vshll.u16 q2, d20, #8
70077:         vmlsl.u16 q2, d20, d30
70077:         vmlal.u16 q2, d21, d30
70077:         vshll.u16 q3, d22, #8
70077:     vld1.32   {d20}, [TMP1], STRIDE
70077:         vmlsl.u16 q3, d22, d31
70077:         vmlal.u16 q3, d23, d31
70077:     vld1.32   {d21}, [TMP1]
70077:     vmull.u8  q8, d20, d28
70077:     vmlal.u8  q8, d21, d29
70077:         vshrn.u32 d0, q0, #16
70077:         vshrn.u32 d1, q1, #16
70077:         vshrn.u32 d4, q2, #16
70077:     vld1.32   {d22}, [TMP2], STRIDE
70077:         vshrn.u32 d5, q3, #16
70077:         vadd.u16  q12, q12, q13
70077:     vld1.32   {d23}, [TMP2]
70077:     vmull.u8  q9, d22, d28
70077:     mov       TMP3, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP3, TOP, TMP3, asl #2
70077:     mov       TMP4, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP4, TOP, TMP4, asl #2
70077:     vmlal.u8  q9, d23, d29
70077:     vld1.32   {d22}, [TMP3], STRIDE
70077:         vshr.u16  q15, q12, #8
70077:     vld1.32   {d23}, [TMP3]
70077:     vmull.u8  q10, d22, d28
70077:     vmlal.u8  q10, d23, d29
70077:         vmovn.u16 d10, q0
70077:     vshll.u16 q0, d16, #8
70077:         vmovn.u16 d11, q2
70077:     vmlsl.u16 q0, d16, d30
70077:     vmlal.u16 q0, d17, d30
70077:     pld       [TMP4, PF_OFFS]
70077:     vld1.32   {d16}, [TMP4], STRIDE
70077:         vadd.u16  q12, q12, q13
70077:     vld1.32   {d17}, [TMP4]
70077:     pld       [TMP4, PF_OFFS]
70077:     vmull.u8  q11, d16, d28
70077:     vmlal.u8  q11, d17, d29
70077:             vuzp.u8 d10, d11
70077:     vshll.u16 q1, d18, #8
70077:     vmlsl.u16 q1, d18, d31
70077: 
70077:     mov       TMP1, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP1, TOP, TMP1, asl #2
70077:     mov       TMP2, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP2, TOP, TMP2, asl #2
70077:         vmlal.u16 q1, d19, d31
70077:             vuzp.u8 d9, d11
70077:         vshr.u16  q15, q12, #8
70077:         vshll.u16 q2, d20, #8
70077:             vuzp.u8 d8, d10
70077:         vmlsl.u16 q2, d20, d30
70077:         vmlal.u16 q2, d21, d30
70077:         vshll.u16 q3, d22, #8
70077:     vld1.32   {d20}, [TMP1], STRIDE
70077:         vmlsl.u16 q3, d22, d31
70077:         vmlal.u16 q3, d23, d31
70077:     vld1.32   {d21}, [TMP1]
70077:     vmull.u8  q8, d20, d28
70077:     vmlal.u8  q8, d21, d29
70077:             vshll.u8  q6, d9, #8
70077:             vshll.u8  q5, d10, #8
70077:             vshll.u8  q7, d8, #8
70077:         vshrn.u32 d0, q0, #16
70077:             vsri.u16  q5, q6, #5
70077:         vshrn.u32 d1, q1, #16
70077:             vsri.u16  q5, q7, #11
70077:         vshrn.u32 d4, q2, #16
70077:     vld1.32   {d22}, [TMP2], STRIDE
70077:         vshrn.u32 d5, q3, #16
70077:         vadd.u16  q12, q12, q13
70077:     vld1.32   {d23}, [TMP2]
70077:     vmull.u8  q9, d22, d28
70077:     mov       TMP3, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP3, TOP, TMP3, asl #2
70077:     mov       TMP4, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP4, TOP, TMP4, asl #2
70077:     vmlal.u8  q9, d23, d29
70077:     vld1.32   {d22}, [TMP3], STRIDE
70077:         vshr.u16  q15, q12, #8
70077:     vld1.32   {d23}, [TMP3]
70077:     vmull.u8  q10, d22, d28
70077:     vmlal.u8  q10, d23, d29
70077:         vmovn.u16 d8, q0
70077:     vshll.u16 q0, d16, #8
70077:         vmovn.u16 d9, q2
70077:     vmlsl.u16 q0, d16, d30
70077:     vmlal.u16 q0, d17, d30
70077:     pld       [TMP4, PF_OFFS]
70077:     vld1.32   {d16}, [TMP4], STRIDE
70077:         vadd.u16  q12, q12, q13
70077:     vld1.32   {d17}, [TMP4]
70077:     pld       [TMP4, PF_OFFS]
70077:     vmull.u8  q11, d16, d28
70077:     vmlal.u8  q11, d17, d29
70077:     vshll.u16 q1, d18, #8
70077:             vst1.32   {d10, d11}, [OUT, :128]!
70077:     vmlsl.u16 q1, d18, d31
70077: .endm
70077: /*****************************************************************************/
70077: 
64161: generate_bilinear_scanline_func \
70077:     pixman_scaled_bilinear_scanline_8888_8888_SRC_asm_neon, 8888, 8888, \
70077:     2, 2, 28, BILINEAR_FLAG_UNROLL_4
64161: 
64161: generate_bilinear_scanline_func \
70077:     pixman_scaled_bilinear_scanline_8888_0565_SRC_asm_neon, 8888, 0565, \
70077:     2, 1, 28, BILINEAR_FLAG_UNROLL_8 | BILINEAR_FLAG_USE_ALL_NEON_REGS
64161: 
64161: generate_bilinear_scanline_func \
70077:     pixman_scaled_bilinear_scanline_0565_x888_SRC_asm_neon, 0565, 8888, \
70077:     1, 2, 28, BILINEAR_FLAG_UNROLL_4
64161: 
64161: generate_bilinear_scanline_func \
70077:     pixman_scaled_bilinear_scanline_0565_0565_SRC_asm_neon, 0565, 0565, \
70077:     1, 1, 28, BILINEAR_FLAG_UNROLL_4
