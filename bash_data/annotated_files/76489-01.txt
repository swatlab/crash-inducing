70077: /*
70077:  * Copyright Â© 2011 SCore Corporation
70077:  *
70077:  * Permission is hereby granted, free of charge, to any person obtaining a
70077:  * copy of this software and associated documentation files (the "Software"),
70077:  * to deal in the Software without restriction, including without limitation
70077:  * the rights to use, copy, modify, merge, publish, distribute, sublicense,
70077:  * and/or sell copies of the Software, and to permit persons to whom the
70077:  * Software is furnished to do so, subject to the following conditions:
70077:  *
70077:  * The above copyright notice and this permission notice (including the next
70077:  * paragraph) shall be included in all copies or substantial portions of the
70077:  * Software.
70077:  *
70077:  * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
70077:  * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
70077:  * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
70077:  * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
70077:  * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
70077:  * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
70077:  * DEALINGS IN THE SOFTWARE.
70077:  *
70077:  * Author:  Siarhei Siamashka (siarhei.siamashka@nokia.com)
70077:  * Author:  Taekyun Kim (tkq.kim@samsung.com)
70077:  */
70077: 
70077: /*
70077:  * This file contains scaled bilinear scanline functions implemented
70077:  * using older siarhei's bilinear macro template.
70077:  *
70077:  * << General scanline function procedures >>
70077:  *  1. bilinear interpolate source pixels
70077:  *  2. load mask pixels
70077:  *  3. load destination pixels
70077:  *  4. duplicate mask to fill whole register
70077:  *  5. interleave source & destination pixels
70077:  *  6. apply mask to source pixels
70077:  *  7. combine source & destination pixels
70077:  *  8, Deinterleave final result
70077:  *  9. store destination pixels
70077:  *
70077:  * All registers with single number (i.e. src0, tmp0) are 64-bits registers.
70077:  * Registers with double numbers(src01, dst01) are 128-bits registers.
70077:  * All temp registers can be used freely outside the code block.
70077:  * Assume that symbol(register .req) OUT and MASK are defined at caller of these macro blocks.
70077:  *
70077:  * TODOs
70077:  *  Support 0565 pixel format
70077:  *  Optimization for two and last pixel cases
70077:  *
70077:  * Remarks
70077:  *  There can be lots of pipeline stalls inside code block and between code blocks.
70077:  *  Further optimizations will be done by new macro templates using head/tail_head/tail scheme.
70077:  */
70077: 
70077: /* Prevent the stack from becoming executable for no reason... */
70077: #if defined(__linux__) && defined (__ELF__)
70077: .section .note.GNU-stack,"",%progbits
70077: #endif
70077: 
70077: .text
70077: .fpu neon
70077: .arch armv7a
70077: .object_arch armv4
70077: .eabi_attribute 10, 0
70077: .eabi_attribute 12, 0
70077: .arm
70077: .altmacro
76489: .p2align 2
70077: 
70077: #include "pixman-arm-neon-asm.h"
70077: 
70077: /*
70077:  * Bilinear macros from pixman-arm-neon-asm.S
70077:  */
70077: 
70077: /* Supplementary macro for setting function attributes */
70077: .macro pixman_asm_function fname
70077:     .func fname
70077:     .global fname
70077: #ifdef __ELF__
70077:     .hidden fname
70077:     .type fname, %function
70077: #endif
70077: fname:
70077: .endm
70077: 
70077: /*
70077:  * Bilinear scaling support code which tries to provide pixel fetching, color
70077:  * format conversion, and interpolation as separate macros which can be used
70077:  * as the basic building blocks for constructing bilinear scanline functions.
70077:  */
70077: 
70077: .macro bilinear_load_8888 reg1, reg2, tmp
70077:     mov       TMP2, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP1, TOP, TMP2, asl #2
70077:     add       TMP2, BOTTOM, TMP2, asl #2
70077:     vld1.32   {reg1}, [TMP1]
70077:     vld1.32   {reg2}, [TMP2]
70077: .endm
70077: 
70077: .macro bilinear_load_0565 reg1, reg2, tmp
70077:     mov       TMP2, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP1, TOP, TMP2, asl #1
70077:     add       TMP2, BOTTOM, TMP2, asl #1
70077:     vld1.32   {reg2[0]}, [TMP1]
70077:     vld1.32   {reg2[1]}, [TMP2]
70077:     convert_four_0565_to_x888_packed reg2, reg1, reg2, tmp
70077: .endm
70077: 
70077: .macro bilinear_load_and_vertical_interpolate_two_8888 \
70077:                     acc1, acc2, reg1, reg2, reg3, reg4, tmp1, tmp2
70077: 
70077:     bilinear_load_8888 reg1, reg2, tmp1
70077:     vmull.u8  acc1, reg1, d28
70077:     vmlal.u8  acc1, reg2, d29
70077:     bilinear_load_8888 reg3, reg4, tmp2
70077:     vmull.u8  acc2, reg3, d28
70077:     vmlal.u8  acc2, reg4, d29
70077: .endm
70077: 
70077: .macro bilinear_load_and_vertical_interpolate_four_8888 \
70077:                 xacc1, xacc2, xreg1, xreg2, xreg3, xreg4, xacc2lo, xacc2hi \
70077:                 yacc1, yacc2, yreg1, yreg2, yreg3, yreg4, yacc2lo, yacc2hi
70077: 
70077:     bilinear_load_and_vertical_interpolate_two_8888 \
70077:                 xacc1, xacc2, xreg1, xreg2, xreg3, xreg4, xacc2lo, xacc2hi
70077:     bilinear_load_and_vertical_interpolate_two_8888 \
70077:                 yacc1, yacc2, yreg1, yreg2, yreg3, yreg4, yacc2lo, yacc2hi
70077: .endm
70077: 
70077: .macro bilinear_load_and_vertical_interpolate_two_0565 \
70077:                 acc1, acc2, reg1, reg2, reg3, reg4, acc2lo, acc2hi
70077: 
70077:     mov       TMP2, X, asr #16
70077:     add       X, X, UX
70077:     mov       TMP4, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP1, TOP, TMP2, asl #1
70077:     add       TMP2, BOTTOM, TMP2, asl #1
70077:     add       TMP3, TOP, TMP4, asl #1
70077:     add       TMP4, BOTTOM, TMP4, asl #1
70077:     vld1.32   {acc2lo[0]}, [TMP1]
70077:     vld1.32   {acc2hi[0]}, [TMP3]
70077:     vld1.32   {acc2lo[1]}, [TMP2]
70077:     vld1.32   {acc2hi[1]}, [TMP4]
70077:     convert_0565_to_x888 acc2, reg3, reg2, reg1
70077:     vzip.u8   reg1, reg3
70077:     vzip.u8   reg2, reg4
70077:     vzip.u8   reg3, reg4
70077:     vzip.u8   reg1, reg2
70077:     vmull.u8  acc1, reg1, d28
70077:     vmlal.u8  acc1, reg2, d29
70077:     vmull.u8  acc2, reg3, d28
70077:     vmlal.u8  acc2, reg4, d29
70077: .endm
70077: 
70077: .macro bilinear_load_and_vertical_interpolate_four_0565 \
70077:                 xacc1, xacc2, xreg1, xreg2, xreg3, xreg4, xacc2lo, xacc2hi \
70077:                 yacc1, yacc2, yreg1, yreg2, yreg3, yreg4, yacc2lo, yacc2hi
70077: 
70077:     mov       TMP2, X, asr #16
70077:     add       X, X, UX
70077:     mov       TMP4, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP1, TOP, TMP2, asl #1
70077:     add       TMP2, BOTTOM, TMP2, asl #1
70077:     add       TMP3, TOP, TMP4, asl #1
70077:     add       TMP4, BOTTOM, TMP4, asl #1
70077:     vld1.32   {xacc2lo[0]}, [TMP1]
70077:     vld1.32   {xacc2hi[0]}, [TMP3]
70077:     vld1.32   {xacc2lo[1]}, [TMP2]
70077:     vld1.32   {xacc2hi[1]}, [TMP4]
70077:     convert_0565_to_x888 xacc2, xreg3, xreg2, xreg1
70077:     mov       TMP2, X, asr #16
70077:     add       X, X, UX
70077:     mov       TMP4, X, asr #16
70077:     add       X, X, UX
70077:     add       TMP1, TOP, TMP2, asl #1
70077:     add       TMP2, BOTTOM, TMP2, asl #1
70077:     add       TMP3, TOP, TMP4, asl #1
70077:     add       TMP4, BOTTOM, TMP4, asl #1
70077:     vld1.32   {yacc2lo[0]}, [TMP1]
70077:     vzip.u8   xreg1, xreg3
70077:     vld1.32   {yacc2hi[0]}, [TMP3]
70077:     vzip.u8   xreg2, xreg4
70077:     vld1.32   {yacc2lo[1]}, [TMP2]
70077:     vzip.u8   xreg3, xreg4
70077:     vld1.32   {yacc2hi[1]}, [TMP4]
70077:     vzip.u8   xreg1, xreg2
70077:     convert_0565_to_x888 yacc2, yreg3, yreg2, yreg1
70077:     vmull.u8  xacc1, xreg1, d28
70077:     vzip.u8   yreg1, yreg3
70077:     vmlal.u8  xacc1, xreg2, d29
70077:     vzip.u8   yreg2, yreg4
70077:     vmull.u8  xacc2, xreg3, d28
70077:     vzip.u8   yreg3, yreg4
70077:     vmlal.u8  xacc2, xreg4, d29
70077:     vzip.u8   yreg1, yreg2
70077:     vmull.u8  yacc1, yreg1, d28
70077:     vmlal.u8  yacc1, yreg2, d29
70077:     vmull.u8  yacc2, yreg3, d28
70077:     vmlal.u8  yacc2, yreg4, d29
70077: .endm
70077: 
70077: .macro bilinear_store_8888 numpix, tmp1, tmp2
70077: .if numpix == 4
70077:     vst1.32   {d0, d1}, [OUT]!
70077: .elseif numpix == 2
70077:     vst1.32   {d0}, [OUT]!
70077: .elseif numpix == 1
70077:     vst1.32   {d0[0]}, [OUT, :32]!
70077: .else
70077:     .error bilinear_store_8888 numpix is unsupported
70077: .endif
70077: .endm
70077: 
70077: .macro bilinear_store_0565 numpix, tmp1, tmp2
70077:     vuzp.u8 d0, d1
70077:     vuzp.u8 d2, d3
70077:     vuzp.u8 d1, d3
70077:     vuzp.u8 d0, d2
70077:     convert_8888_to_0565 d2, d1, d0, q1, tmp1, tmp2
70077: .if numpix == 4
70077:     vst1.16   {d2}, [OUT]!
70077: .elseif numpix == 2
70077:     vst1.32   {d2[0]}, [OUT]!
70077: .elseif numpix == 1
70077:     vst1.16   {d2[0]}, [OUT]!
70077: .else
70077:     .error bilinear_store_0565 numpix is unsupported
70077: .endif
70077: .endm
70077: 
70077: 
70077: /*
70077:  * Macros for loading mask pixels into register 'mask'.
70077:  * vdup must be done in somewhere else.
70077:  */
70077: .macro bilinear_load_mask_x numpix, mask
70077: .endm
70077: 
70077: .macro bilinear_load_mask_8 numpix, mask
70077: .if numpix == 4
70077:     vld1.32     {mask[0]}, [MASK]!
70077: .elseif numpix == 2
70077:     vld1.16     {mask[0]}, [MASK]!
70077: .elseif numpix == 1
70077:     vld1.8      {mask[0]}, [MASK]!
70077: .else
70077:     .error bilinear_load_mask_8 numpix is unsupported
70077: .endif
70077: .endm
70077: 
70077: .macro bilinear_load_mask mask_fmt, numpix, mask
70077:     bilinear_load_mask_&mask_fmt numpix, mask
70077: .endm
70077: 
70077: 
70077: /*
70077:  * Macros for loading destination pixels into register 'dst0' and 'dst1'.
70077:  * Interleave should be done somewhere else.
70077:  */
70077: .macro bilinear_load_dst_0565_src numpix, dst0, dst1, dst01
70077: .endm
70077: 
70077: .macro bilinear_load_dst_8888_src numpix, dst0, dst1, dst01
70077: .endm
70077: 
70077: .macro bilinear_load_dst_8888 numpix, dst0, dst1, dst01
70077: .if numpix == 4
70077:     vld1.32     {dst0, dst1}, [OUT]
70077: .elseif numpix == 2
70077:     vld1.32     {dst0}, [OUT]
70077: .elseif numpix == 1
70077:     vld1.32     {dst0[0]}, [OUT]
70077: .else
70077:     .error bilinear_load_dst_8888 numpix is unsupported
70077: .endif
70077: .endm
70077: 
70077: .macro bilinear_load_dst_8888_over numpix, dst0, dst1, dst01
70077:     bilinear_load_dst_8888 numpix, dst0, dst1, dst01
70077: .endm
70077: 
70077: .macro bilinear_load_dst_8888_add numpix, dst0, dst1, dst01
70077:     bilinear_load_dst_8888 numpix, dst0, dst1, dst01
70077: .endm
70077: 
70077: .macro bilinear_load_dst dst_fmt, op, numpix, dst0, dst1, dst01
70077:     bilinear_load_dst_&dst_fmt&_&op numpix, dst0, dst1, dst01
70077: .endm
70077: 
70077: /*
70077:  * Macros for duplicating partially loaded mask to fill entire register.
70077:  * We will apply mask to interleaved source pixels, that is
70077:  *  (r0, r1, r2, r3, g0, g1, g2, g3) x (m0, m1, m2, m3, m0, m1, m2, m3)
70077:  *  (b0, b1, b2, b3, a0, a1, a2, a3) x (m0, m1, m2, m3, m0, m1, m2, m3)
70077:  * So, we need to duplicate loaded mask into whole register.
70077:  *
70077:  * For two pixel case
70077:  *  (r0, r1, x, x, g0, g1, x, x) x (m0, m1, m0, m1, m0, m1, m0, m1)
70077:  *  (b0, b1, x, x, a0, a1, x, x) x (m0, m1, m0, m1, m0, m1, m0, m1)
70077:  * We can do some optimizations for this including one pixel cases.
70077:  */
70077: .macro bilinear_duplicate_mask_x numpix, mask
70077: .endm
70077: 
70077: .macro bilinear_duplicate_mask_8 numpix, mask
70077: .if numpix == 4
70077:     vdup.32     mask, mask[0]
70077: .elseif numpix == 2
70077:     vdup.16     mask, mask[0]
70077: .elseif numpix == 1
70077:     vdup.8      mask, mask[0]
70077: .else
70077:     .error bilinear_duplicate_mask_8 is unsupported
70077: .endif
70077: .endm
70077: 
70077: .macro bilinear_duplicate_mask mask_fmt, numpix, mask
70077:     bilinear_duplicate_mask_&mask_fmt numpix, mask
70077: .endm
70077: 
70077: /*
70077:  * Macros for interleaving src and dst pixels to rrrr gggg bbbb aaaa form.
70077:  * Interleave should be done when maks is enabled or operator is 'over'.
70077:  */
70077: .macro bilinear_interleave src0, src1, dst0, dst1
70077:     vuzp.8      src0, src1
70077:     vuzp.8      dst0, dst1
70077:     vuzp.8      src0, src1
70077:     vuzp.8      dst0, dst1
70077: .endm
70077: 
70077: .macro bilinear_interleave_src_dst_x_src \
70077:                 numpix, src0, src1, src01, dst0, dst1, dst01
70077: .endm
70077: 
70077: .macro bilinear_interleave_src_dst_x_over \
70077:                 numpix, src0, src1, src01, dst0, dst1, dst01
70077: 
70077:     bilinear_interleave src0, src1, dst0, dst1
70077: .endm
70077: 
70077: .macro bilinear_interleave_src_dst_x_add \
70077:                 numpix, src0, src1, src01, dst0, dst1, dst01
70077: .endm
70077: 
70077: .macro bilinear_interleave_src_dst_8_src \
70077:                 numpix, src0, src1, src01, dst0, dst1, dst01
70077: 
70077:     bilinear_interleave src0, src1, dst0, dst1
70077: .endm
70077: 
70077: .macro bilinear_interleave_src_dst_8_over \
70077:                 numpix, src0, src1, src01, dst0, dst1, dst01
70077: 
70077:     bilinear_interleave src0, src1, dst0, dst1
70077: .endm
70077: 
70077: .macro bilinear_interleave_src_dst_8_add \
70077:                 numpix, src0, src1, src01, dst0, dst1, dst01
70077: 
70077:     bilinear_interleave src0, src1, dst0, dst1
70077: .endm
70077: 
70077: .macro bilinear_interleave_src_dst \
70077:                 mask_fmt, op, numpix, src0, src1, src01, dst0, dst1, dst01
70077: 
70077:     bilinear_interleave_src_dst_&mask_fmt&_&op \
70077:                 numpix, src0, src1, src01, dst0, dst1, dst01
70077: .endm
70077: 
70077: 
70077: /*
70077:  * Macros for applying masks to src pixels. (see combine_mask_u() function)
70077:  * src, dst should be in interleaved form.
70077:  * mask register should be in form (m0, m1, m2, m3).
70077:  */
70077: .macro bilinear_apply_mask_to_src_x \
70077:                 numpix, src0, src1, src01, mask, \
70077:                 tmp01, tmp23, tmp45, tmp67
70077: .endm
70077: 
70077: .macro bilinear_apply_mask_to_src_8 \
70077:                 numpix, src0, src1, src01, mask, \
70077:                 tmp01, tmp23, tmp45, tmp67
70077: 
70077:     vmull.u8        tmp01, src0, mask
70077:     vmull.u8        tmp23, src1, mask
70077:     /* bubbles */
70077:     vrshr.u16       tmp45, tmp01, #8
70077:     vrshr.u16       tmp67, tmp23, #8
70077:     /* bubbles */
70077:     vraddhn.u16     src0, tmp45, tmp01
70077:     vraddhn.u16     src1, tmp67, tmp23
70077: .endm
70077: 
70077: .macro bilinear_apply_mask_to_src \
70077:                 mask_fmt, numpix, src0, src1, src01, mask, \
70077:                 tmp01, tmp23, tmp45, tmp67
70077: 
70077:     bilinear_apply_mask_to_src_&mask_fmt \
70077:                 numpix, src0, src1, src01, mask, \
70077:                 tmp01, tmp23, tmp45, tmp67
70077: .endm
70077: 
70077: 
70077: /*
70077:  * Macros for combining src and destination pixels.
70077:  * Interleave or not is depending on operator 'op'.
70077:  */
70077: .macro bilinear_combine_src \
70077:                 numpix, src0, src1, src01, dst0, dst1, dst01, \
70077:                 tmp01, tmp23, tmp45, tmp67, tmp8
70077: .endm
70077: 
70077: .macro bilinear_combine_over \
70077:                 numpix, src0, src1, src01, dst0, dst1, dst01, \
70077:                 tmp01, tmp23, tmp45, tmp67, tmp8
70077: 
70077:     vdup.32     tmp8, src1[1]
70077:     /* bubbles */
70077:     vmvn.8      tmp8, tmp8
70077:     /* bubbles */
70077:     vmull.u8    tmp01, dst0, tmp8
70077:     /* bubbles */
70077:     vmull.u8    tmp23, dst1, tmp8
70077:     /* bubbles */
70077:     vrshr.u16   tmp45, tmp01, #8
70077:     vrshr.u16   tmp67, tmp23, #8
70077:     /* bubbles */
70077:     vraddhn.u16 dst0, tmp45, tmp01
70077:     vraddhn.u16 dst1, tmp67, tmp23
70077:     /* bubbles */
70077:     vqadd.u8    src01, dst01, src01
70077: .endm
70077: 
70077: .macro bilinear_combine_add \
70077:                 numpix, src0, src1, src01, dst0, dst1, dst01, \
70077:                 tmp01, tmp23, tmp45, tmp67, tmp8
70077: 
70077:     vqadd.u8    src01, dst01, src01
70077: .endm
70077: 
70077: .macro bilinear_combine \
70077:                 op, numpix, src0, src1, src01, dst0, dst1, dst01, \
70077:                 tmp01, tmp23, tmp45, tmp67, tmp8
70077: 
70077:     bilinear_combine_&op \
70077:                 numpix, src0, src1, src01, dst0, dst1, dst01, \
70077:                 tmp01, tmp23, tmp45, tmp67, tmp8
70077: .endm
70077: 
70077: /*
70077:  * Macros for final deinterleaving of destination pixels if needed.
70077:  */
70077: .macro bilinear_deinterleave numpix, dst0, dst1, dst01
70077:     vuzp.8      dst0, dst1
70077:     /* bubbles */
70077:     vuzp.8      dst0, dst1
70077: .endm
70077: 
70077: .macro bilinear_deinterleave_dst_x_src numpix, dst0, dst1, dst01
70077: .endm
70077: 
70077: .macro bilinear_deinterleave_dst_x_over numpix, dst0, dst1, dst01
70077:     bilinear_deinterleave numpix, dst0, dst1, dst01
70077: .endm
70077: 
70077: .macro bilinear_deinterleave_dst_x_add numpix, dst0, dst1, dst01
70077: .endm
70077: 
70077: .macro bilinear_deinterleave_dst_8_src numpix, dst0, dst1, dst01
70077:     bilinear_deinterleave numpix, dst0, dst1, dst01
70077: .endm
70077: 
70077: .macro bilinear_deinterleave_dst_8_over numpix, dst0, dst1, dst01
70077:     bilinear_deinterleave numpix, dst0, dst1, dst01
70077: .endm
70077: 
70077: .macro bilinear_deinterleave_dst_8_add numpix, dst0, dst1, dst01
70077:     bilinear_deinterleave numpix, dst0, dst1, dst01
70077: .endm
70077: 
70077: .macro bilinear_deinterleave_dst mask_fmt, op, numpix, dst0, dst1, dst01
70077:     bilinear_deinterleave_dst_&mask_fmt&_&op numpix, dst0, dst1, dst01
70077: .endm
70077: 
70077: 
70077: .macro bilinear_interpolate_last_pixel src_fmt, mask_fmt, dst_fmt, op
70077:     bilinear_load_&src_fmt d0, d1, d2
70077:     bilinear_load_mask mask_fmt, 1, d4
70077:     bilinear_load_dst dst_fmt, op, 1, d18, d19, q9
70077:     vmull.u8  q1, d0, d28
70077:     vmlal.u8  q1, d1, d29
70077:     vshr.u16  d30, d24, #8
70077:     /* 4 cycles bubble */
70077:     vshll.u16 q0, d2, #8
70077:     vmlsl.u16 q0, d2, d30
70077:     vmlal.u16 q0, d3, d30
70077:     /* 5 cycles bubble */
70077:     bilinear_duplicate_mask mask_fmt, 1, d4
70077:     vshrn.u32 d0, q0, #16
70077:     /* 3 cycles bubble */
70077:     vmovn.u16 d0, q0
70077:     /* 1 cycle bubble */
70077:     bilinear_interleave_src_dst \
70077:                 mask_fmt, op, 1, d0, d1, q0, d18, d19, q9
70077:     bilinear_apply_mask_to_src \
70077:                 mask_fmt, 1, d0, d1, q0, d4, \
70077:                 q3, q8, q10, q11
70077:     bilinear_combine \
70077:                 op, 1, d0, d1, q0, d18, d19, q9, \
70077:                 q3, q8, q10, q11, d5
70077:     bilinear_deinterleave_dst mask_fmt, op, 1, d0, d1, q0
70077:     bilinear_store_&dst_fmt 1, q2, q3
70077: .endm
70077: 
70077: .macro bilinear_interpolate_two_pixels src_fmt, mask_fmt, dst_fmt, op
70077:     bilinear_load_and_vertical_interpolate_two_&src_fmt \
70077:                 q1, q11, d0, d1, d20, d21, d22, d23
70077:     bilinear_load_mask mask_fmt, 2, d4
70077:     bilinear_load_dst dst_fmt, op, 2, d18, d19, q9
70077:     vshr.u16  q15, q12, #8
70077:     vadd.u16  q12, q12, q13
70077:     vshll.u16 q0, d2, #8
70077:     vmlsl.u16 q0, d2, d30
70077:     vmlal.u16 q0, d3, d30
70077:     vshll.u16 q10, d22, #8
70077:     vmlsl.u16 q10, d22, d31
70077:     vmlal.u16 q10, d23, d31
70077:     vshrn.u32 d30, q0, #16
70077:     vshrn.u32 d31, q10, #16
70077:     bilinear_duplicate_mask mask_fmt, 2, d4
70077:     vmovn.u16 d0, q15
70077:     bilinear_interleave_src_dst \
70077:                 mask_fmt, op, 2, d0, d1, q0, d18, d19, q9
70077:     bilinear_apply_mask_to_src \
70077:                 mask_fmt, 2, d0, d1, q0, d4, \
70077:                 q3, q8, q10, q11
70077:     bilinear_combine \
70077:                 op, 2, d0, d1, q0, d18, d19, q9, \
70077:                 q3, q8, q10, q11, d5
70077:     bilinear_deinterleave_dst mask_fmt, op, 2, d0, d1, q0
70077:     bilinear_store_&dst_fmt 2, q2, q3
70077: .endm
70077: 
70077: .macro bilinear_interpolate_four_pixels src_fmt, mask_fmt, dst_fmt, op
70077:     bilinear_load_and_vertical_interpolate_four_&src_fmt \
70077:                 q1, q11, d0, d1, d20, d21, d22, d23 \
70077:                 q3, q9,  d4, d5, d16, d17, d18, d19
70077:     pld       [TMP1, PF_OFFS]
70077:     vshr.u16  q15, q12, #8
70077:     vadd.u16  q12, q12, q13
70077:     vshll.u16 q0, d2, #8
70077:     vmlsl.u16 q0, d2, d30
70077:     vmlal.u16 q0, d3, d30
70077:     vshll.u16 q10, d22, #8
70077:     vmlsl.u16 q10, d22, d31
70077:     vmlal.u16 q10, d23, d31
70077:     vshr.u16  q15, q12, #8
70077:     vshll.u16 q2, d6, #8
70077:     vmlsl.u16 q2, d6, d30
70077:     vmlal.u16 q2, d7, d30
70077:     vshll.u16 q8, d18, #8
70077:     bilinear_load_mask mask_fmt, 4, d30
70077:     bilinear_load_dst dst_fmt, op, 4, d2, d3, q1
70077:     pld       [TMP2, PF_OFFS]
70077:     vmlsl.u16 q8, d18, d31
70077:     vmlal.u16 q8, d19, d31
70077:     vadd.u16  q12, q12, q13
70077:     vshrn.u32 d0, q0, #16
70077:     vshrn.u32 d1, q10, #16
70077:     vshrn.u32 d4, q2, #16
70077:     vshrn.u32 d5, q8, #16
70077:     bilinear_duplicate_mask mask_fmt, 4, d30
70077:     vmovn.u16 d0, q0
70077:     vmovn.u16 d1, q2
70077:     bilinear_interleave_src_dst \
70077:                 mask_fmt, op, 4, d0, d1, q0, d2, d3, q1
70077:     bilinear_apply_mask_to_src \
70077:                 mask_fmt, 4, d0, d1, q0, d30, \
70077:                 q3, q8, q9, q10
70077:     bilinear_combine \
70077:                 op, 4, d0, d1, q0, d2, d3, q1, \
70077:                 q3, q8, q9, q10, d22
70077:     bilinear_deinterleave_dst mask_fmt, op, 4, d0, d1, q0
70077:     bilinear_store_&dst_fmt 4, q2, q3
70077: .endm
70077: 
70077: .macro generate_bilinear_scanline_func_src_dst \
70077:                 fname, src_fmt, dst_fmt, op, \
70077:                 bpp_shift, prefetch_distance
70077: 
70077: pixman_asm_function fname
70077:     OUT       .req      r0
70077:     TOP       .req      r1
70077:     BOTTOM    .req      r2
70077:     WT        .req      r3
70077:     WB        .req      r4
70077:     X         .req      r5
70077:     UX        .req      r6
70077:     WIDTH     .req      ip
70077:     TMP1      .req      r3
70077:     TMP2      .req      r4
70077:     PF_OFFS   .req      r7
70077:     TMP3      .req      r8
70077:     TMP4      .req      r9
70077: 
70077:     mov       ip, sp
70077:     push      {r4, r5, r6, r7, r8, r9}
70077:     mov       PF_OFFS, #prefetch_distance
70077:     ldmia     ip, {WB, X, UX, WIDTH}
70077:     mul       PF_OFFS, PF_OFFS, UX
70077: 
70077:     cmp       WIDTH, #0
70077:     ble       3f
70077: 
70077:     vdup.u16  q12, X
70077:     vdup.u16  q13, UX
70077:     vdup.u8   d28, WT
70077:     vdup.u8   d29, WB
70077:     vadd.u16  d25, d25, d26
70077:     vadd.u16  q13, q13, q13
70077: 
70077:     subs      WIDTH, WIDTH, #4
70077:     blt       1f
70077:     mov       PF_OFFS, PF_OFFS, asr #(16 - bpp_shift)
70077: 0:
70077:     bilinear_interpolate_four_pixels src_fmt, x, dst_fmt, op
70077:     subs      WIDTH, WIDTH, #4
70077:     bge       0b
70077: 1:
70077:     tst       WIDTH, #2
70077:     beq       2f
70077:     bilinear_interpolate_two_pixels src_fmt, x, dst_fmt, op
70077: 2:
70077:     tst       WIDTH, #1
70077:     beq       3f
70077:     bilinear_interpolate_last_pixel src_fmt, x, dst_fmt, op
70077: 3:
70077:     pop       {r4, r5, r6, r7, r8, r9}
70077:     bx        lr
70077: 
70077:     .unreq    OUT
70077:     .unreq    TOP
70077:     .unreq    BOTTOM
70077:     .unreq    WT
70077:     .unreq    WB
70077:     .unreq    X
70077:     .unreq    UX
70077:     .unreq    WIDTH
70077:     .unreq    TMP1
70077:     .unreq    TMP2
70077:     .unreq    PF_OFFS
70077:     .unreq    TMP3
70077:     .unreq    TMP4
70077: .endfunc
70077: 
70077: .endm
70077: 
70077: .macro generate_bilinear_scanline_func_src_a8_dst \
70077:                 fname, src_fmt, dst_fmt, op, \
70077:                 bpp_shift, prefetch_distance
70077: 
70077: pixman_asm_function fname
70077:     OUT       .req      r0
70077:     MASK      .req      r1
70077:     TOP       .req      r2
70077:     BOTTOM    .req      r3
70077:     WT        .req      r4
70077:     WB        .req      r5
70077:     X         .req      r6
70077:     UX        .req      r7
70077:     WIDTH     .req      ip
70077:     TMP1      .req      r4
70077:     TMP2      .req      r5
70077:     PF_OFFS   .req      r8
70077:     TMP3      .req      r9
70077:     TMP4      .req      r10
70077: 
70077:     mov       ip, sp
70077:     push      {r4, r5, r6, r7, r8, r9, r10, ip}
70077:     mov       PF_OFFS, #prefetch_distance
70077:     ldmia     ip, {WT, WB, X, UX, WIDTH}
70077:     mul       PF_OFFS, PF_OFFS, UX
70077: 
70077:     cmp       WIDTH, #0
70077:     ble       3f
70077: 
70077:     vdup.u16  q12, X
70077:     vdup.u16  q13, UX
70077:     vdup.u8   d28, WT
70077:     vdup.u8   d29, WB
70077:     vadd.u16  d25, d25, d26
70077:     vadd.u16  q13, q13, q13
70077: 
70077:     subs      WIDTH, WIDTH, #4
70077:     blt       1f
70077:     mov       PF_OFFS, PF_OFFS, asr #(16 - bpp_shift)
70077: 0:
70077:     bilinear_interpolate_four_pixels src_fmt, 8, dst_fmt, op
70077:     subs      WIDTH, WIDTH, #4
70077:     bge       0b
70077: 1:
70077:     tst       WIDTH, #2
70077:     beq       2f
70077:     bilinear_interpolate_two_pixels src_fmt, 8, dst_fmt, op
70077: 2:
70077:     tst       WIDTH, #1
70077:     beq       3f
70077:     bilinear_interpolate_last_pixel src_fmt, 8, dst_fmt, op
70077: 3:
70077:     pop       {r4, r5, r6, r7, r8, r9, r10, ip}
70077:     bx        lr
70077: 
70077:     .unreq    OUT
70077:     .unreq    TOP
70077:     .unreq    BOTTOM
70077:     .unreq    WT
70077:     .unreq    WB
70077:     .unreq    X
70077:     .unreq    UX
70077:     .unreq    WIDTH
70077:     .unreq    MASK
70077:     .unreq    TMP1
70077:     .unreq    TMP2
70077:     .unreq    PF_OFFS
70077:     .unreq    TMP3
70077:     .unreq    TMP4
70077: .endfunc
70077: 
70077: .endm
70077: 
70077: generate_bilinear_scanline_func_src_dst \
70077:     pixman_scaled_bilinear_scanline_8888_8888_OVER_asm_neon, \
70077:     8888, 8888, over, 2, 28
70077: 
70077: generate_bilinear_scanline_func_src_dst \
70077:     pixman_scaled_bilinear_scanline_8888_8888_ADD_asm_neon, \
70077:     8888, 8888, add, 2, 28
70077: 
70077: generate_bilinear_scanline_func_src_a8_dst \
70077:     pixman_scaled_bilinear_scanline_8888_8_8888_SRC_asm_neon, \
70077:     8888, 8888, src, 2, 28
70077: 
70077: generate_bilinear_scanline_func_src_a8_dst \
70077:     pixman_scaled_bilinear_scanline_8888_8_0565_SRC_asm_neon, \
70077:     8888, 0565, src, 2, 28
70077: 
70077: generate_bilinear_scanline_func_src_a8_dst \
70077:     pixman_scaled_bilinear_scanline_0565_8_x888_SRC_asm_neon, \
70077:     0565, 8888, src, 1, 28
70077: 
70077: generate_bilinear_scanline_func_src_a8_dst \
70077:     pixman_scaled_bilinear_scanline_0565_8_0565_SRC_asm_neon, \
70077:     0565, 0565, src, 1, 28
70077: 
70077: generate_bilinear_scanline_func_src_a8_dst \
70077:     pixman_scaled_bilinear_scanline_8888_8_8888_OVER_asm_neon, \
70077:     8888, 8888, over, 2, 28
70077: 
70077: generate_bilinear_scanline_func_src_a8_dst \
70077:     pixman_scaled_bilinear_scanline_8888_8_8888_ADD_asm_neon, \
70077:     8888, 8888, add, 2, 28
