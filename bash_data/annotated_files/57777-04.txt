52826: /* -*- Mode: C++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*-
52826:  * vim: set ts=4 sw=4 et tw=99:
52826:  *
52826:  * ***** BEGIN LICENSE BLOCK *****
52826:  * Version: MPL 1.1/GPL 2.0/LGPL 2.1
52826:  *
52826:  * The contents of this file are subject to the Mozilla Public License Version
52826:  * 1.1 (the "License"); you may not use this file except in compliance with
52826:  * the License. You may obtain a copy of the License at
52826:  * http://www.mozilla.org/MPL/
52826:  *
52826:  * Software distributed under the License is distributed on an "AS IS" basis,
52826:  * WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
52826:  * for the specific language governing rights and limitations under the
52826:  * License.
52826:  *
52826:  * The Original Code is Mozilla SpiderMonkey JavaScript 1.9 code, released
52826:  * May 28, 2008.
52826:  *
52826:  * The Initial Developer of the Original Code is
52826:  *   Brendan Eich <brendan@mozilla.org>
52826:  *
52826:  * Contributor(s):
52826:  *   David Anderson <danderson@mozilla.com>
52826:  *   David Mandelin <dmandelin@mozilla.com>
52826:  *
52826:  * Alternatively, the contents of this file may be used under the terms of
52826:  * either of the GNU General Public License Version 2 or later (the "GPL"),
52826:  * or the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
52826:  * in which case the provisions of the GPL or the LGPL are applicable instead
52826:  * of those above. If you wish to allow use of your version of this file only
52826:  * under the terms of either the GPL or the LGPL, and not to allow others to
52826:  * use your version of this file under the terms of the MPL, indicate your
52826:  * decision by deleting the provisions above and replace them with the notice
52826:  * and other provisions required by the GPL or the LGPL. If you do not delete
52826:  * the provisions above, a recipient may use your version of this file under
52826:  * the terms of any one of the MPL, the GPL or the LGPL.
52826:  *
52826:  * ***** END LICENSE BLOCK ***** */
52826: #include "jsscope.h"
52826: #include "jsnum.h"
52826: #include "MonoIC.h"
52826: #include "StubCalls.h"
53590: #include "StubCalls-inl.h"
53301: #include "assembler/assembler/LinkBuffer.h"
52826: #include "assembler/assembler/RepatchBuffer.h"
53301: #include "assembler/assembler/MacroAssembler.h"
53590: #include "assembler/assembler/CodeLocation.h"
53301: #include "CodeGenIncludes.h"
53590: #include "methodjit/Compiler.h"
53590: #include "InlineFrameAssembler.h"
53263: #include "jsobj.h"
53840: 
53840: #include "jsinterpinlines.h"
53263: #include "jsobjinlines.h"
52826: #include "jsscopeinlines.h"
53590: #include "jsscriptinlines.h"
52826: 
52826: using namespace js;
52826: using namespace js::mjit;
52826: using namespace js::mjit::ic;
52826: 
53590: typedef JSC::MacroAssembler::RegisterID RegisterID;
53590: typedef JSC::MacroAssembler::Address Address;
53590: typedef JSC::MacroAssembler::Jump Jump;
53590: typedef JSC::MacroAssembler::Imm32 Imm32;
53590: typedef JSC::MacroAssembler::ImmPtr ImmPtr;
53590: typedef JSC::MacroAssembler::Call Call;
53590: 
53119: #if defined JS_MONOIC
53119: 
52826: static void
55503: PatchGetFallback(VMFrame &f, ic::MICInfo *ic)
52826: {
55503:     JSC::RepatchBuffer repatch(ic->stubEntry.executableAddress(), 64);
52826:     JSC::FunctionPtr fptr(JS_FUNC_TO_DATA_PTR(void *, stubs::GetGlobalName));
55503:     repatch.relink(ic->stubCall, fptr);
52826: }
52826: 
52826: void JS_FASTCALL
55503: ic::GetGlobalName(VMFrame &f, ic::MICInfo *ic)
52826: {
53840:     JSObject *obj = f.fp()->scopeChain().getGlobal();
53840:     JSAtom *atom = f.fp()->script()->getAtom(GET_INDEX(f.regs.pc));
52826:     jsid id = ATOM_TO_JSID(atom);
52826: 
55503:     JS_ASSERT(ic->kind == ic::MICInfo::GET);
53116: 
53531:     const Shape *shape = obj->nativeLookup(id);
53531:     if (!shape ||
53531:         !shape->hasDefaultGetterOrIsMethod() ||
53531:         !shape->hasSlot())
52826:     {
53531:         if (shape)
55503:             PatchGetFallback(f, ic);
52826:         stubs::GetGlobalName(f);
52826:         return;
52826:     }
53531:     uint32 slot = shape->slot;
52826: 
55503:     ic->u.name.touched = true;
52826: 
52826:     /* Patch shape guard. */
55503:     JSC::RepatchBuffer repatch(ic->entry.executableAddress(), 50);
55503:     repatch.repatch(ic->shape, obj->shape());
52826: 
52826:     /* Patch loads. */
52826:     slot *= sizeof(Value);
55503:     JSC::RepatchBuffer loads(ic->load.executableAddress(), 32, false);
53451: #if defined JS_CPU_X86
55503:     loads.repatch(ic->load.dataLabel32AtOffset(MICInfo::GET_DATA_OFFSET), slot);
55503:     loads.repatch(ic->load.dataLabel32AtOffset(MICInfo::GET_TYPE_OFFSET), slot + 4);
53451: #elif defined JS_CPU_ARM
55503:     // ic->load actually points to the LDR instruction which fetches the offset, but 'repatch'
53451:     // knows how to dereference it to find the integer value.
55503:     loads.repatch(ic->load.dataLabel32AtOffset(0), slot);
53269: #elif defined JS_PUNBOX64
55503:     loads.repatch(ic->load.dataLabel32AtOffset(ic->patchValueOffset), slot);
53269: #endif
52826: 
52826:     /* Do load anyway... this time. */
52826:     stubs::GetGlobalName(f);
52826: }
52826: 
52831: static void JS_FASTCALL
52831: SetGlobalNameSlow(VMFrame &f, uint32 index)
52831: {
54169:     JSScript *script = f.fp()->script();
54169:     JSAtom *atom = script->getAtom(GET_INDEX(f.regs.pc));
54169:     if (script->strictModeCode)
54169:         stubs::SetGlobalName<true>(f, atom);
54169:     else
54169:         stubs::SetGlobalName<false>(f, atom);
52831: }
52831: 
52831: static void
55503: PatchSetFallback(VMFrame &f, ic::MICInfo *ic)
52831: {
55503:     JSC::RepatchBuffer repatch(ic->stubEntry.executableAddress(), 64);
52831:     JSC::FunctionPtr fptr(JS_FUNC_TO_DATA_PTR(void *, SetGlobalNameSlow));
55503:     repatch.relink(ic->stubCall, fptr);
52831: }
52831: 
53512: static VoidStubAtom
53512: GetStubForSetGlobalName(VMFrame &f)
53512: {
54169:     JSScript *script = f.fp()->script();
53512:     // The property cache doesn't like inc ops, so we use a simpler
53512:     // stub for that case.
53512:     return js_CodeSpec[*f.regs.pc].format & (JOF_INC | JOF_DEC)
54169:          ? STRICT_VARIANT(stubs::SetGlobalNameDumb)
54169:          : STRICT_VARIANT(stubs::SetGlobalName);
53512: }
53512: 
52831: void JS_FASTCALL
55503: ic::SetGlobalName(VMFrame &f, ic::MICInfo *ic)
52831: {
53840:     JSObject *obj = f.fp()->scopeChain().getGlobal();
53840:     JSAtom *atom = f.fp()->script()->getAtom(GET_INDEX(f.regs.pc));
52831:     jsid id = ATOM_TO_JSID(atom);
52831: 
55503:     JS_ASSERT(ic->kind == ic::MICInfo::SET);
53116: 
53531:     const Shape *shape = obj->nativeLookup(id);
53531:     if (!shape ||
53531:         !shape->hasDefaultGetterOrIsMethod() ||
53531:         !shape->writable() ||
53531:         !shape->hasSlot())
52831:     {
53531:         if (shape)
55503:             PatchSetFallback(f, ic);
53512:         GetStubForSetGlobalName(f)(f, atom);
52831:         return;
52831:     }
53531:     uint32 slot = shape->slot;
52831: 
55503:     ic->u.name.touched = true;
52831: 
52831:     /* Patch shape guard. */
55503:     JSC::RepatchBuffer repatch(ic->entry.executableAddress(), 50);
55503:     repatch.repatch(ic->shape, obj->shape());
52831: 
52831:     /* Patch loads. */
52831:     slot *= sizeof(Value);
52831: 
55503:     JSC::RepatchBuffer stores(ic->load.executableAddress(), 32, false);
53451: #if defined JS_CPU_X86
55503:     stores.repatch(ic->load.dataLabel32AtOffset(MICInfo::SET_TYPE_OFFSET), slot + 4);
52831: 
52831:     uint32 dataOffset;
55503:     if (ic->u.name.typeConst)
52831:         dataOffset = MICInfo::SET_DATA_CONST_TYPE_OFFSET;
52831:     else
52831:         dataOffset = MICInfo::SET_DATA_TYPE_OFFSET;
55503:     stores.repatch(ic->load.dataLabel32AtOffset(dataOffset), slot);
53451: #elif defined JS_CPU_ARM
55503:     // ic->load actually points to the LDR instruction which fetches the offset, but 'repatch'
53451:     // knows how to dereference it to find the integer value.
55503:     stores.repatch(ic->load.dataLabel32AtOffset(0), slot);
53269: #elif defined JS_PUNBOX64
55503:     stores.repatch(ic->load.dataLabel32AtOffset(ic->patchValueOffset), slot);
53269: #endif
52831: 
53511:     // Actually implement the op the slow way.
53512:     GetStubForSetGlobalName(f)(f, atom);
52831: }
52831: 
56192: class EqualityICLinker : public LinkerHelper
56192: {
56192:     VMFrame &f;
56192: 
56192:   public:
56192:     EqualityICLinker(JSContext *cx, VMFrame &f)
56192:         : LinkerHelper(cx), f(f)
56192:     { }
56192: 
56192:     bool init(Assembler &masm) {
56192:         JSC::ExecutablePool *pool = LinkerHelper::init(masm);
56192:         if (!pool)
56192:             return false;
56192:         JSScript *script = f.fp()->script();
56192:         JITScript *jit = script->getJIT(f.fp()->isConstructing());
56192:         if (!jit->execPools.append(pool)) {
56192:             pool->release();
56192:             js_ReportOutOfMemory(cx);
56192:             return false;
56192:         }
56192:         return true;
56192:     }
56192: };
56192: 
56192: /* Rough over-estimate of how much memory we need to unprotect. */
56192: static const uint32 INLINE_PATH_LENGTH = 64;
56192: 
56192: class EqualityCompiler : public BaseCompiler
56192: {
56192:     VMFrame &f;
56192:     EqualityICInfo &ic;
56192: 
56192:     Vector<Jump, 4, SystemAllocPolicy> jumpList;
56192:     Jump trueJump;
56192:     Jump falseJump;
56192:     
56192:   public:
56192:     EqualityCompiler(VMFrame &f, EqualityICInfo &ic)
56192:         : BaseCompiler(f.cx), f(f), ic(ic), jumpList(SystemAllocPolicy())
56192:     {
56192:     }
56192: 
56192:     void linkToStub(Jump j)
56192:     {
56192:         jumpList.append(j);
56192:     }
56192: 
56192:     void linkTrue(Jump j)
56192:     {
56192:         trueJump = j;
56192:     }
56192: 
56192:     void linkFalse(Jump j)
56192:     {
56192:         falseJump = j;
56192:     }
56192:     
56192:     void generateStringPath(Assembler &masm)
56192:     {
56575:         const ValueRemat &lvr = ic.lvr;
56575:         const ValueRemat &rvr = ic.rvr;
56192: 
56575:         if (!lvr.isConstant() && !lvr.isType(JSVAL_TYPE_STRING)) {
56192:             Jump lhsFail = masm.testString(Assembler::NotEqual, lvr.typeReg());
56192:             linkToStub(lhsFail);
56192:         }
56192:         
56575:         if (!rvr.isConstant() && !rvr.isType(JSVAL_TYPE_STRING)) {
56192:             Jump rhsFail = masm.testString(Assembler::NotEqual, rvr.typeReg());
56192:             linkToStub(rhsFail);
56192:         }
56192: 
56192:         RegisterID tmp = ic.tempReg;
56192:         
56192:         /* Test if lhs/rhs are atomized. */
56192:         Imm32 atomizedFlags(JSString::FLAT | JSString::ATOMIZED);
56192:         
56192:         masm.load32(Address(lvr.dataReg(), offsetof(JSString, mLengthAndFlags)), tmp);
56192:         masm.and32(Imm32(JSString::TYPE_FLAGS_MASK), tmp);
56192:         Jump lhsNotAtomized = masm.branch32(Assembler::NotEqual, tmp, atomizedFlags);
56192:         linkToStub(lhsNotAtomized);
56192: 
56575:         if (!rvr.isConstant()) {
56192:             masm.load32(Address(rvr.dataReg(), offsetof(JSString, mLengthAndFlags)), tmp);
56192:             masm.and32(Imm32(JSString::TYPE_FLAGS_MASK), tmp);
56192:             Jump rhsNotAtomized = masm.branch32(Assembler::NotEqual, tmp, atomizedFlags);
56192:             linkToStub(rhsNotAtomized);
56192:         }
56192: 
56575:         if (rvr.isConstant()) {
56575:             JSString *str = rvr.value().toString();
56192:             JS_ASSERT(str->isAtomized());
56192:             Jump test = masm.branchPtr(ic.cond, lvr.dataReg(), ImmPtr(str));
56192:             linkTrue(test);
56192:         } else {
56192:             Jump test = masm.branchPtr(ic.cond, lvr.dataReg(), rvr.dataReg());
56192:             linkTrue(test);
56192:         }
56192: 
56192:         Jump fallthrough = masm.jump();
56192:         linkFalse(fallthrough);
56192:     }
56192: 
56192:     void generateObjectPath(Assembler &masm)
56192:     {
56192:         ValueRemat &lvr = ic.lvr;
56192:         ValueRemat &rvr = ic.rvr;
56192:         
56575:         if (!lvr.isConstant() && !lvr.isType(JSVAL_TYPE_OBJECT)) {
56192:             Jump lhsFail = masm.testObject(Assembler::NotEqual, lvr.typeReg());
56192:             linkToStub(lhsFail);
56192:         }
56192:         
56575:         if (!rvr.isConstant() && !rvr.isType(JSVAL_TYPE_OBJECT)) {
56192:             Jump rhsFail = masm.testObject(Assembler::NotEqual, rvr.typeReg());
56192:             linkToStub(rhsFail);
56192:         }
56192: 
56192:         Jump lhsHasEq = masm.branchTest32(Assembler::NonZero,
56192:                                           Address(lvr.dataReg(),
56192:                                                   offsetof(JSObject, flags)),
56192:                                           Imm32(JSObject::HAS_EQUALITY));
56192:         linkToStub(lhsHasEq);
56192: 
56575:         if (rvr.isConstant()) {
56575:             JSObject *obj = &rvr.value().toObject();
56192:             Jump test = masm.branchPtr(ic.cond, lvr.dataReg(), ImmPtr(obj));
56192:             linkTrue(test);
56192:         } else {
56192:             Jump test = masm.branchPtr(ic.cond, lvr.dataReg(), rvr.dataReg());
56192:             linkTrue(test);
56192:         }
56192: 
56192:         Jump fallthrough = masm.jump();
56192:         linkFalse(fallthrough);
56192:     }
56192: 
56192:     bool linkForIC(Assembler &masm)
56192:     {
56192:         EqualityICLinker buffer(cx, f);
56192:         if (!buffer.init(masm))
56192:             return false;
56192: 
56192:         /* Set the targets of all type test failures to go to the stub. */
56192:         for (size_t i = 0; i < jumpList.length(); i++)
56192:             buffer.link(jumpList[i], ic.stubEntry);
56192:         jumpList.clear();
56192: 
56192:         /* Set the targets for the the success and failure of the actual equality test. */
56192:         buffer.link(trueJump, ic.target);
56192:         buffer.link(falseJump, ic.fallThrough);
56192: 
56192:         CodeLocationLabel cs = buffer.finalizeCodeAddendum();
56192: 
56192:         /* Jump to the newly generated code instead of to the IC. */
56192:         JSC::RepatchBuffer jumpRepatcher(ic.jumpToStub.executableAddress(), INLINE_PATH_LENGTH);
56192:         jumpRepatcher.relink(ic.jumpToStub, cs);
56192: 
56192:         /* Overwrite the call to the IC with a call to the stub. */
56192:         JSC::RepatchBuffer stubRepatcher(ic.stubCall.executableAddress(), INLINE_PATH_LENGTH);
56192:         JSC::FunctionPtr fptr(JS_FUNC_TO_DATA_PTR(void *, ic.stub));
56192:         stubRepatcher.relink(ic.stubCall, fptr);
56192:         
56192:         return true;
56192:     }
56192: 
56192:     bool update()
56192:     {
56192:         if (!ic.generated) {
56192:             Assembler masm;
56192:             Value rval = f.regs.sp[-1];
56192:             Value lval = f.regs.sp[-2];
56192:             
56192:             if (rval.isObject() && lval.isObject()) {
56192:                 generateObjectPath(masm);
56192:                 ic.generated = true;
56192:             } else if (rval.isString() && lval.isString()) {
56192:                 generateStringPath(masm);
56192:                 ic.generated = true;
56192:             } else {
56192:                 return true;
56192:             }
56192: 
56192:             return linkForIC(masm);
56192:         }
56192: 
56192:         return true;
56192:     }
56192: };
56192: 
56192: JSBool JS_FASTCALL
56192: ic::Equality(VMFrame &f, ic::EqualityICInfo *ic)
56192: {
56192:     EqualityCompiler cc(f, *ic);
56192:     if (!cc.update())
56192:         THROWV(JS_FALSE);
56192: 
56192:     return ic->stub(f);
56192: }
56192: 
53590: static void * JS_FASTCALL
55503: SlowCallFromIC(VMFrame &f, ic::CallICInfo *ic)
53590: {
57717:     stubs::SlowCall(f, ic->frameSize.getArgc(f));
53590:     return NULL;
53301: }
53301: 
53590: static void * JS_FASTCALL
55503: SlowNewFromIC(VMFrame &f, ic::CallICInfo *ic)
53301: {
57717:     stubs::SlowNew(f, ic->frameSize.staticArgc());
53590:     return NULL;
53523: }
53522: 
53590: /*
53590:  * Calls have an inline path and an out-of-line path. The inline path is used
53590:  * in the fastest case: the method has JIT'd code, and |argc == nargs|.
53590:  * 
53590:  * The inline path and OOL path are separated by a guard on the identity of
53590:  * the callee object. This guard starts as NULL and always fails on the first
53590:  * hit. On the OOL path, the callee is verified to be both a function and a
53590:  * scripted function. If these conditions hold, |ic::Call| is invoked.
53590:  *
53590:  * |ic::Call| first ensures that the callee has JIT code. If it doesn't, the
53590:  * call to |ic::Call| is patched to a slow path. If it does have JIT'd code,
53590:  * the following cases can occur:
53590:  *
53590:  *   1) args != nargs: The call to |ic::Call| is patched with a dynamically
53590:  *      generated stub. This stub inlines a path that looks like:
53590:  *      ----
53590:  *      push frame
53590:  *      if (callee is not compiled) {
53590:  *          Compile(callee);
53590:  *      }
53590:  *      call callee->arityLabel
53590:  *
53590:  *      The arity label is a special entry point for correcting frames for
53590:  *      arity mismatches.
53590:  *
53590:  *   2) args == nargs, and the inline call site was not patched yet.
53590:  *      The guard dividing the two paths is patched to guard on the given
53590:  *      function object identity, and the proceeding call is patched to
53590:  *      directly call the JIT code.
53590:  *
53590:  *   3) args == nargs, and the inline call site was patched already.
53590:  *      A small stub is created which extends the original guard to also
53590:  *      guard on the JSFunction lying underneath the function object.
53590:  *
53590:  * If the OOL path does not have a scripted function, but does have a
53590:  * scripted native, then a small stub is generated which inlines the native
53590:  * invocation.
53590:  */
55463: class CallCompiler : public BaseCompiler
53590: {
53590:     VMFrame &f;
53590:     CallICInfo &ic;
53590:     bool callingNew;
53306: 
53590:   public:
53590:     CallCompiler(VMFrame &f, CallICInfo &ic, bool callingNew)
57717:       : BaseCompiler(f.cx), f(f), ic(ic), callingNew(callingNew)
53590:     {
53590:     }
53301: 
53590:     JSC::ExecutablePool *poolForSize(size_t size, CallICInfo::PoolIndex index)
53590:     {
55463:         JSC::ExecutablePool *ep = getExecPool(size);
55463:         if (!ep)
53590:             return NULL;
53590:         JS_ASSERT(!ic.pools[index]);
53590:         ic.pools[index] = ep;
53590:         return ep;
53590:     }
53301: 
53590:     bool generateFullCallStub(JSScript *script, uint32 flags)
53590:     {
53590:         /*
53590:          * Create a stub that works with arity mismatches. Like the fast-path,
53590:          * this allocates a frame on the caller side, but also performs extra
53590:          * checks for compilability. Perhaps this should be a separate, shared
53590:          * trampoline, but for now we generate it dynamically.
53590:          */
53590:         Assembler masm;
54163:         InlineFrameAssembler inlFrame(masm, ic, flags);
53590:         RegisterID t0 = inlFrame.tempRegs.takeAnyReg();
53301: 
53590:         /* Generate the inline frame creation. */
54832:         inlFrame.assemble(ic.funGuard.labelAtOffset(ic.joinPointOffset).executableAddress());
53301: 
53590:         /* funPtrReg is still valid. Check if a compilation is needed. */
53590:         Address scriptAddr(ic.funPtrReg, offsetof(JSFunction, u) +
53590:                            offsetof(JSFunction::U::Scripted, script));
53590:         masm.loadPtr(scriptAddr, t0);
53301: 
53590:         /*
53590:          * Test if script->nmap is NULL - same as checking ncode, but faster
53590:          * here since ncode has two failure modes and we need to load out of
53590:          * nmap anyway.
53590:          */
55503:         size_t offset = callingNew
55503:                         ? offsetof(JSScript, jitArityCheckCtor)
55503:                         : offsetof(JSScript, jitArityCheckNormal);
55503:         masm.loadPtr(Address(t0, offset), t0);
55503:         Jump hasCode = masm.branchPtr(Assembler::Above, t0, ImmPtr(JS_UNJITTABLE_SCRIPT));
53523: 
53590:         /* Try and compile. On success we get back the nmap pointer. */
53590:         masm.storePtr(JSFrameReg, FrameAddress(offsetof(VMFrame, regs.fp)));
57717:         void *compilePtr = JS_FUNC_TO_DATA_PTR(void *, stubs::CompileFunction);
57717:         if (ic.frameSize.isStatic()) {
57717:             masm.move(Imm32(ic.frameSize.staticArgc()), Registers::ArgReg1);
57766:             masm.fallibleVMCall(compilePtr, script->code, ic.frameSize.staticFrameDepth());
57717:         } else {
57717:             masm.load32(FrameAddress(offsetof(VMFrame, u.call.dynamicArgc)), Registers::ArgReg1);
57766:             masm.fallibleVMCall(compilePtr, script->code, -1);
57717:         }
54832:         masm.loadPtr(FrameAddress(offsetof(VMFrame, regs.fp)), JSFrameReg);
53590: 
53590:         Jump notCompiled = masm.branchTestPtr(Assembler::Zero, Registers::ReturnReg,
53590:                                               Registers::ReturnReg);
53590: 
54832:         masm.jump(Registers::ReturnReg);
53590: 
53590:         hasCode.linkTo(masm.label(), &masm);
53590: 
53590:         /* Get nmap[ARITY], set argc, call. */
57717:         if (ic.frameSize.isStatic())
57717:             masm.move(Imm32(ic.frameSize.staticArgc()), JSParamReg_Argc);
57717:         else
57717:             masm.load32(FrameAddress(offsetof(VMFrame, u.call.dynamicArgc)), JSParamReg_Argc);
54832:         masm.jump(t0);
53590: 
53590:         JSC::ExecutablePool *ep = poolForSize(masm.size(), CallICInfo::Pool_ScriptStub);
53590:         if (!ep)
53590:             return false;
53590: 
53590:         JSC::LinkBuffer buffer(&masm, ep);
53590:         buffer.link(notCompiled, ic.slowPathStart.labelAtOffset(ic.slowJoinOffset));
57717:         masm.finalize(buffer);
53590:         JSC::CodeLocationLabel cs = buffer.finalizeCodeAddendum();
53590: 
53590:         JaegerSpew(JSpew_PICs, "generated CALL stub %p (%d bytes)\n", cs.executableAddress(),
53590:                    masm.size());
53590: 
53590:         JSC::CodeLocationJump oolJump = ic.slowPathStart.jumpAtOffset(ic.oolJumpOffset);
53590:         uint8 *start = (uint8 *)oolJump.executableAddress();
53590:         JSC::RepatchBuffer repatch(start - 32, 64);
53590:         repatch.relink(oolJump, cs);
53590: 
53590:         return true;
53590:     }
53590: 
53590:     void patchInlinePath(JSScript *script, JSObject *obj)
53590:     {
57717:         JS_ASSERT(ic.frameSize.isStatic());
57717: 
53590:         /* Very fast path. */
53590:         uint8 *start = (uint8 *)ic.funGuard.executableAddress();
53590:         JSC::RepatchBuffer repatch(start - 32, 64);
53590: 
53590:         ic.fastGuardedObject = obj;
53590: 
55503:         JITScript *jit = script->getJIT(callingNew);
55503: 
53590:         repatch.repatch(ic.funGuard, obj);
54832:         repatch.relink(ic.funGuard.jumpAtOffset(ic.hotJumpOffset),
55503:                        JSC::CodeLocationLabel(jit->fastEntry));
53590: 
53590:         JaegerSpew(JSpew_PICs, "patched CALL path %p (obj: %p)\n", start, ic.fastGuardedObject);
53590:     }
53590: 
53590:     bool generateStubForClosures(JSObject *obj)
53590:     {
57717:         JS_ASSERT(ic.frameSize.isStatic());
57717: 
53590:         /* Slightly less fast path - guard on fun->getFunctionPrivate() instead. */
53590:         Assembler masm;
53590: 
53590:         Registers tempRegs;
53590:         tempRegs.takeReg(ic.funObjReg);
53590: 
53590:         RegisterID t0 = tempRegs.takeAnyReg();
53590: 
53590:         /* Guard that it's actually a function object. */
56575:         Jump claspGuard = masm.testObjClass(Assembler::NotEqual, ic.funObjReg, &js_FunctionClass);
53590: 
53590:         /* Guard that it's the same function. */
53590:         JSFunction *fun = obj->getFunctionPrivate();
53590:         masm.loadFunctionPrivate(ic.funObjReg, t0);
53590:         Jump funGuard = masm.branchPtr(Assembler::NotEqual, t0, ImmPtr(fun));
53590:         Jump done = masm.jump();
53590: 
53590:         JSC::ExecutablePool *ep = poolForSize(masm.size(), CallICInfo::Pool_ClosureStub);
53590:         if (!ep)
53590:             return false;
53590: 
53590:         JSC::LinkBuffer buffer(&masm, ep);
53590:         buffer.link(claspGuard, ic.slowPathStart);
53590:         buffer.link(funGuard, ic.slowPathStart);
53590:         buffer.link(done, ic.funGuard.labelAtOffset(ic.hotPathOffset));
53590:         JSC::CodeLocationLabel cs = buffer.finalizeCodeAddendum();
53590: 
53590:         JaegerSpew(JSpew_PICs, "generated CALL closure stub %p (%d bytes)\n",
53590:                    cs.executableAddress(), masm.size());
53590: 
53590:         uint8 *start = (uint8 *)ic.funJump.executableAddress();
53590:         JSC::RepatchBuffer repatch(start - 32, 64);
53590:         repatch.relink(ic.funJump, cs);
53590: 
53590:         ic.hasJsFunCheck = true;
53590: 
53590:         return true;
53590:     }
53590: 
53590:     bool generateNativeStub()
53590:     {
57718:         /* Snapshot the frameDepth before SplatApplyArgs modifies it. */
57718:         uintN initialFrameDepth = f.regs.sp - f.regs.fp->slots();
57718: 
57717:         /*
57717:          * SplatApplyArgs has not been called, so we call it here before
57717:          * potentially touching f.u.call.dynamicArgc.
57717:          */
57717:         Value *vp;
57717:         if (ic.frameSize.isStatic()) {
57718:             JS_ASSERT(f.regs.sp - f.regs.fp->slots() == (int)ic.frameSize.staticFrameDepth());
57717:             vp = f.regs.sp - (2 + ic.frameSize.staticArgc());
57717:         } else {
57717:             JS_ASSERT(*f.regs.pc == JSOP_FUNAPPLY && GET_ARGC(f.regs.pc) == 2);
57717:             if (!ic::SplatApplyArgs(f))       /* updates regs.sp */
57717:                 THROWV(true);
57718:             vp = f.regs.sp - (2 + f.u.call.dynamicArgc);
57717:         }
53590: 
53590:         JSObject *obj;
53590:         if (!IsFunctionObject(*vp, &obj))
53590:             return false;
53590: 
53590:         JSFunction *fun = obj->getFunctionPrivate();
53590:         if ((!callingNew && !fun->isNative()) || (callingNew && !fun->isConstructor()))
53590:             return false;
53590: 
53590:         if (callingNew)
53590:             vp[1].setMagicWithObjectOrNullPayload(NULL);
53590: 
57717:         if (!CallJSNative(cx, fun->u.n.native, ic.frameSize.getArgc(f), vp))
53590:             THROWV(true);
53590: 
53590:         /* Right now, take slow-path for IC misses or multiple stubs. */
53590:         if (ic.fastGuardedNative || ic.hasJsFunCheck)
53590:             return true;
53590: 
53590:         /* Native MIC needs to warm up first. */
53590:         if (!ic.hit) {
53590:             ic.hit = true;
53590:             return true;
53590:         }
53590: 
53590:         /* Generate fast-path for calling this native. */
53590:         Assembler masm;
53590: 
53590:         /* Guard on the function object identity, for now. */
53590:         Jump funGuard = masm.branchPtr(Assembler::NotEqual, ic.funObjReg, ImmPtr(obj));
53590: 
57717:         /* N.B. After this call, the frame will have a dynamic frame size. */
57717:         if (ic.frameSize.isDynamic()) {
57766:             masm.fallibleVMCall(JS_FUNC_TO_DATA_PTR(void *, ic::SplatApplyArgs),
57718:                                 f.regs.pc, initialFrameDepth);
57717:         }
57717: 
53590:         Registers tempRegs;
53590: #ifndef JS_CPU_X86
53590:         tempRegs.takeReg(Registers::ArgReg0);
53590:         tempRegs.takeReg(Registers::ArgReg1);
53590:         tempRegs.takeReg(Registers::ArgReg2);
53590: #endif
53590:         RegisterID t0 = tempRegs.takeAnyReg();
53590: 
53590:         /* Store pc. */
53590:         masm.storePtr(ImmPtr(cx->regs->pc),
54832:                        FrameAddress(offsetof(VMFrame, regs.pc)));
53301: 
57717:         /* Store sp (if not already set by ic::SplatApplyArgs). */
57717:         if (ic.frameSize.isStatic()) {
57718:             uint32 spOffset = sizeof(JSStackFrame) + initialFrameDepth * sizeof(Value);
53590:             masm.addPtr(Imm32(spOffset), JSFrameReg, t0);
54832:             masm.storePtr(t0, FrameAddress(offsetof(VMFrame, regs.sp)));
57717:         }
54832: 
54832:         /* Store fp. */
54832:         masm.storePtr(JSFrameReg, FrameAddress(offsetof(VMFrame, regs.fp)));
53301: 
57766:         /* Grab cx. */
53590: #ifdef JS_CPU_X86
53590:         RegisterID cxReg = tempRegs.takeAnyReg();
53590: #else
53590:         RegisterID cxReg = Registers::ArgReg0;
53590: #endif
53590:         masm.loadPtr(FrameAddress(offsetof(VMFrame, cx)), cxReg);
53301: 
53590:         /* Compute vp. */
53590: #ifdef JS_CPU_X86
53590:         RegisterID vpReg = t0;
53590: #else
53590:         RegisterID vpReg = Registers::ArgReg2;
53590: #endif
57717:         MaybeRegisterID argcReg;
57717:         if (ic.frameSize.isStatic()) {
57717:             uint32 vpOffset = sizeof(JSStackFrame) + (vp - f.regs.fp->slots()) * sizeof(Value);
57717:             masm.addPtr(Imm32(vpOffset), JSFrameReg, vpReg);
57717:         } else {
57717:             argcReg = tempRegs.takeAnyReg();
57717:             masm.load32(FrameAddress(offsetof(VMFrame, u.call.dynamicArgc)), argcReg.reg());
57717:             masm.loadPtr(FrameAddress(offsetof(VMFrame, regs.sp)), vpReg);
53590: 
57717:             /* vpOff = (argc + 2) * sizeof(Value) */
57717:             RegisterID vpOff = tempRegs.takeAnyReg();
57717:             masm.move(argcReg.reg(), vpOff);
57717:             masm.add32(Imm32(2), vpOff);  /* callee, this */
57717:             JS_STATIC_ASSERT(sizeof(Value) == 8);
57717:             masm.lshift32(Imm32(3), vpOff);
57717:             masm.subPtr(vpOff, vpReg);
57717: 
57717:             tempRegs.putReg(vpOff);
57717:         }
53590: 
53590:         /* Mark vp[1] as magic for |new|. */
53590:         if (callingNew) {
53590:             Value v;
53590:             v.setMagicWithObjectOrNullPayload(NULL);
53590:             masm.storeValue(v, Address(vpReg, sizeof(Value)));
53523:         }
53306: 
57766:         masm.setupABICall(Registers::NormalCall, 3);
57766:         masm.storeArg(2, vpReg);
57717:         if (ic.frameSize.isStatic())
57766:             masm.storeArg(1, Imm32(ic.frameSize.staticArgc()));
57717:         else
57766:             masm.storeArg(1, argcReg.reg());
57766:         masm.storeArg(0, cxReg);
57766:         masm.callWithABI(JS_FUNC_TO_DATA_PTR(void *, fun->u.n.native));
53590: 
53590:         Jump hasException = masm.branchTest32(Assembler::Zero, Registers::ReturnReg,
53590:                                               Registers::ReturnReg);
53590:         
53301: 
53590:         Jump done = masm.jump();
53301: 
53590:         /* Move JaegerThrowpoline into register for very far jump on x64. */
53590:         hasException.linkTo(masm.label(), &masm);
57766:         masm.throwInJIT();
53326: 
53590:         JSC::ExecutablePool *ep = poolForSize(masm.size(), CallICInfo::Pool_NativeStub);
53590:         if (!ep)
53590:             THROWV(true);
53301: 
53590:         JSC::LinkBuffer buffer(&masm, ep);
53590:         buffer.link(done, ic.slowPathStart.labelAtOffset(ic.slowJoinOffset));
53590:         buffer.link(funGuard, ic.slowPathStart);
57717:         masm.finalize(buffer);
53590:         
53590:         JSC::CodeLocationLabel cs = buffer.finalizeCodeAddendum();
53590: 
53590:         JaegerSpew(JSpew_PICs, "generated native CALL stub %p (%d bytes)\n",
53590:                    cs.executableAddress(), masm.size());
53590: 
53590:         uint8 *start = (uint8 *)ic.funJump.executableAddress();
53590:         JSC::RepatchBuffer repatch(start - 32, 64);
53590:         repatch.relink(ic.funJump, cs);
53590: 
53590:         ic.fastGuardedNative = obj;
53590: 
53590:         return true;
53301:     }
53301: 
53590:     void *update()
53590:     {
54163:         stubs::UncachedCallResult ucr;
53590:         if (callingNew)
57717:             stubs::UncachedNewHelper(f, ic.frameSize.staticArgc(), &ucr);
53590:         else
57717:             stubs::UncachedCallHelper(f, ic.frameSize.getArgc(f), &ucr);
53590: 
54163:         // If the function cannot be jitted (generally unjittable or empty script),
54163:         // patch this site to go to a slow path always.
54163:         if (!ucr.codeAddr) {
53590:             JSC::CodeLocationCall oolCall = ic.slowPathStart.callAtOffset(ic.oolCallOffset);
53590:             uint8 *start = (uint8 *)oolCall.executableAddress();
53590:             JSC::RepatchBuffer repatch(start - 32, 64);
53590:             JSC::FunctionPtr fptr = callingNew
53590:                                     ? JSC::FunctionPtr(JS_FUNC_TO_DATA_PTR(void *, SlowNewFromIC))
53590:                                     : JSC::FunctionPtr(JS_FUNC_TO_DATA_PTR(void *, SlowCallFromIC));
53590:             repatch.relink(oolCall, fptr);
53590:             return NULL;
53590:         }
53590:             
54163:         JSFunction *fun = ucr.fun;
54163:         JS_ASSERT(fun);
54163:         JSScript *script = fun->script();
54163:         JS_ASSERT(script);
54163:         JSObject *callee = ucr.callee;
54163:         JS_ASSERT(callee);
54163: 
53590:         uint32 flags = callingNew ? JSFRAME_CONSTRUCTING : 0;
53590: 
53590:         if (!ic.hit) {
54163:             ic.hit = true;
54163:             return ucr.codeAddr;
54163:         }
54163: 
57717:         if (!ic.frameSize.isStatic() || ic.frameSize.staticArgc() != fun->nargs) {
53590:             if (!generateFullCallStub(script, flags))
53590:                 THROWV(NULL);
53590:         } else {
53590:             if (!ic.fastGuardedObject) {
54163:                 patchInlinePath(script, callee);
53590:             } else if (!ic.hasJsFunCheck &&
53590:                        !ic.fastGuardedNative &&
53590:                        ic.fastGuardedObject->getFunctionPrivate() == fun) {
53590:                 /*
53590:                  * Note: Multiple "function guard" stubs are not yet
53590:                  * supported, thus the fastGuardedNative check.
53590:                  */
54163:                 if (!generateStubForClosures(callee))
53590:                     THROWV(NULL);
53590:             } else {
53590:                 if (!generateFullCallStub(script, flags))
53590:                     THROWV(NULL);
53590:             }
53590:         }
53590: 
54163:         return ucr.codeAddr;
53590:     }
53590: };
53590: 
53590: void * JS_FASTCALL
55503: ic::Call(VMFrame &f, CallICInfo *ic)
53590: {
55503:     CallCompiler cc(f, *ic, false);
53590:     return cc.update();
53590: }
53590: 
53590: void * JS_FASTCALL
55503: ic::New(VMFrame &f, CallICInfo *ic)
53590: {
55503:     CallCompiler cc(f, *ic, true);
53590:     return cc.update();
53590: }
53590: 
53590: void JS_FASTCALL
55503: ic::NativeCall(VMFrame &f, CallICInfo *ic)
53590: {
55503:     CallCompiler cc(f, *ic, false);
53590:     if (!cc.generateNativeStub())
57717:         stubs::SlowCall(f, ic->frameSize.getArgc(f));
53590: }
53590: 
53590: void JS_FASTCALL
55503: ic::NativeNew(VMFrame &f, CallICInfo *ic)
53590: {
55503:     CallCompiler cc(f, *ic, true);
53590:     if (!cc.generateNativeStub())
57717:         stubs::SlowNew(f, ic->frameSize.staticArgc());
57717: }
57717: 
57717: static inline bool
57717: BumpStack(VMFrame &f, uintN inc)
57717: {
57717:     static const unsigned MANY_ARGS = 1024;
57717:     static const unsigned MIN_SPACE = 500;
57717: 
57717:     /* If we are not passing many args, treat this as a normal call. */
57717:     if (inc < MANY_ARGS) {
57717:         if (f.regs.sp + inc < f.stackLimit)
57717:             return true;
57717:         StackSpace &stack = f.cx->stack();
57777:         if (!stack.bumpCommitAndLimit(f.entryfp, f.regs.sp, inc, &f.stackLimit)) {
57717:             js_ReportOverRecursed(f.cx);
57717:             return false;
57717:         }
57717:         return true;
57717:     }
57717: 
57717:     /*
57717:      * The purpose of f.stackLimit is to catch over-recursion based on
57717:      * assumptions about the average frame size. 'apply' with a large number of
57717:      * arguments breaks these assumptions and can result in premature "out of
57717:      * script quota" errors. Normally, apply will go through js::Invoke, which
57717:      * effectively starts a fresh stackLimit. Here, we bump f.stackLimit,
57717:      * if necessary, to allow for this 'apply' call, and a reasonable number of
57717:      * subsequent calls, to succeed without hitting the stackLimit. In theory,
57717:      * this a recursive chain containing apply to circumvent the stackLimit.
57717:      * However, since each apply call must consume at least MANY_ARGS slots,
57717:      * this sequence will quickly reach the end of the stack and OOM.
57717:      */
57717: 
57717:     uintN incWithSpace = inc + MIN_SPACE;
57717:     Value *bumpedWithSpace = f.regs.sp + incWithSpace;
57717:     if (bumpedWithSpace < f.stackLimit)
57717:         return true;
57717: 
57717:     StackSpace &stack = f.cx->stack();
57777:     if (stack.bumpCommitAndLimit(f.entryfp, f.regs.sp, incWithSpace, &f.stackLimit))
57717:         return true;
57717: 
57717:     if (!stack.ensureSpace(f.cx, f.regs.sp, incWithSpace))
57717:         return false;
57717:     f.stackLimit = bumpedWithSpace;
57717:     return true;
57717: }
57717: 
57717: /*
57717:  * SplatApplyArgs is only called for expressions of the form |f.apply(x, y)|.
57717:  * Additionally, the callee has already been checked to be the native apply.
57717:  * All successful paths through SplatApplyArgs must set f.u.call.dynamicArgc
57717:  * and f.regs.sp.
57717:  */
57717: JSBool JS_FASTCALL
57717: ic::SplatApplyArgs(VMFrame &f)
57717: {
57717:     JSContext *cx = f.cx;
57718:     JS_ASSERT(GET_ARGC(f.regs.pc) == 2);
57718: 
57718:     /*
57718:      * The lazyArgsObj flag indicates an optimized call |f.apply(x, arguments)|
57718:      * where the args obj has not been created or pushed on the stack. Thus,
57718:      * if lazyArgsObj is set, the stack for |f.apply(x, arguments)| is:
57718:      *
57718:      *  | Function.prototype.apply | f | x |
57718:      *
57718:      * Otherwise, if !lazyArgsObj, the stack is a normal 2-argument apply:
57718:      *
57718:      *  | Function.prototype.apply | f | x | arguments |
57718:      */
57718:     if (f.u.call.lazyArgsObj) {
57718:         Value *vp = f.regs.sp - 3;
57718:         JS_ASSERT(JS_CALLEE(cx, vp).toObject().getFunctionPrivate()->u.n.native == js_fun_apply);
57718: 
57718:         JSStackFrame *fp = f.regs.fp;
57718:         if (!fp->hasOverriddenArgs() &&
57718:             (!fp->hasArgsObj() ||
57744:              (fp->hasArgsObj() && !fp->argsObj().isArgsLengthOverridden() &&
57744:               !js_PrototypeHasIndexedProperties(cx, &fp->argsObj())))) {
57718: 
57718:             uintN n = fp->numActualArgs();
57718:             if (!BumpStack(f, n))
57718:                 THROWV(false);
57718:             f.regs.sp += n;
57718: 
57718:             Value *argv = JS_ARGV(cx, vp + 1 /* vp[1]'s argv */);
57718:             if (fp->hasArgsObj())
57718:                 fp->forEachCanonicalActualArg(CopyNonHoleArgsTo(&fp->argsObj(), argv));
57718:             else
57718:                 fp->forEachCanonicalActualArg(CopyTo(argv));
57718: 
57718:             f.u.call.dynamicArgc = n;
57718:             return true;
57718:         }
57718: 
57718:         /*
57718:          * Can't optimize; push the arguments object so that the stack matches
57718:          * the !lazyArgsObj stack state described above.
57718:          */
57718:         f.regs.sp++;
57718:         if (!js_GetArgsValue(cx, fp, &vp[3]))
57718:             THROWV(false);
57718:     }
57718: 
57717:     Value *vp = f.regs.sp - 4;
57717:     JS_ASSERT(JS_CALLEE(cx, vp).toObject().getFunctionPrivate()->u.n.native == js_fun_apply);
57717: 
57717:     /*
57717:      * This stub should mimic the steps taken by js_fun_apply. Step 1 and part
57717:      * of Step 2 have already been taken care of by calling jit code.
57717:      */
57717: 
57717:     /* Step 2 (part 2). */
57717:     if (vp[3].isNullOrUndefined()) {
57717:         f.regs.sp--;
57717:         f.u.call.dynamicArgc = 0;
57717:         return true;
57717:     }
57717: 
57717:     /* Step 3. */
57717:     if (!vp[3].isObject()) {
57717:         JS_ReportErrorNumber(cx, js_GetErrorMessage, NULL, JSMSG_BAD_APPLY_ARGS, js_apply_str);
57717:         THROWV(false);
57717:     }
57717: 
57717:     /* Steps 4-5. */
57717:     JSObject *aobj = &vp[3].toObject();
57717:     jsuint length;
57717:     if (!js_GetLengthProperty(cx, aobj, &length))
57717:         THROWV(false);
57717: 
57717:     JS_ASSERT(!JS_ON_TRACE(cx));
57717: 
57717:     /* Step 6. */
57717:     uintN n = uintN(JS_MIN(length, JS_ARGS_LENGTH_MAX));
57717: 
57717:     intN delta = n - 1;
57717:     if (delta > 0 && !BumpStack(f, delta))
57717:         THROWV(false);
57717:     f.regs.sp += delta;
57717: 
57717:     /* Steps 7-8. */
57717:     if (!GetElements(cx, aobj, n, f.regs.sp - n))
57717:         THROWV(false);
57717: 
57717:     f.u.call.dynamicArgc = n;
57717:     return true;
53590: }
53301: 
53405: void
55503: JITScript::purgeMICs()
53405: {
55503:     for (uint32 i = 0; i < nMICs; i++) {
55503:         ic::MICInfo &mic = mics[i];
53405:         switch (mic.kind) {
53405:           case ic::MICInfo::SET:
53405:           case ic::MICInfo::GET:
53405:           {
53405:             /* Patch shape guard. */
53405:             JSC::RepatchBuffer repatch(mic.entry.executableAddress(), 50);
53409:             repatch.repatch(mic.shape, int(JSObjectMap::INVALID_SHAPE));
53405: 
53405:             /* 
53405:              * If the stub call was patched, leave it alone -- it probably will
53405:              * just be invalidated again.
53405:              */
53405:             break;
53405:           }
53405:           default:
53405:             JS_NOT_REACHED("Unknown MIC type during purge");
53405:             break;
53405:         }
53405:     }
53405: }
53405: 
53590: void
55503: ic::PurgeMICs(JSContext *cx, JSScript *script)
53590: {
55503:     /* MICs are purged during GC to handle changing shapes. */
55503:     JS_ASSERT(cx->runtime->gcRegenShapes);
55503: 
55503:     if (script->jitNormal)
55503:         script->jitNormal->purgeMICs();
55503:     if (script->jitCtor)
55503:         script->jitCtor->purgeMICs();
55503: }
55503: 
55503: void
55503: JITScript::sweepCallICs()
55503: {
55503:     for (uint32 i = 0; i < nCallICs; i++) {
55503:         ic::CallICInfo &ic = callICs[i];
53590: 
53590:         /*
53590:          * If the object is unreachable, we're guaranteed not to be currently
53590:          * executing a stub generated by a guard on that object. This lets us
53590:          * precisely GC call ICs while keeping the identity guard safe.
53590:          */
54707:         bool fastFunDead = ic.fastGuardedObject && IsAboutToBeFinalized(ic.fastGuardedObject);
54707:         bool nativeDead = ic.fastGuardedNative && IsAboutToBeFinalized(ic.fastGuardedNative);
53590: 
53590:         if (!fastFunDead && !nativeDead)
53590:             continue;
53590: 
53590:         uint8 *start = (uint8 *)ic.funGuard.executableAddress();
53590:         JSC::RepatchBuffer repatch(start - 32, 64);
53590: 
53590:         if (fastFunDead) {
53590:             repatch.repatch(ic.funGuard, NULL);
53590:             ic.releasePool(CallICInfo::Pool_ClosureStub);
53590:             ic.hasJsFunCheck = false;
53590:             ic.fastGuardedObject = NULL;
53590:         }
53590: 
53590:         if (nativeDead) {
53590:             ic.releasePool(CallICInfo::Pool_NativeStub);
53590:             ic.fastGuardedNative = NULL;
53590:         }
53590: 
53590:         repatch.relink(ic.funJump, ic.slowPathStart);
53590: 
53590:         ic.hit = false;
53590:     }
53590: }
53590: 
55503: void
55503: ic::SweepCallICs(JSScript *script)
55503: {
55503:     if (script->jitNormal)
55503:         script->jitNormal->sweepCallICs();
55503:     if (script->jitCtor)
55503:         script->jitCtor->sweepCallICs();
55503: }
55503: 
53119: #endif /* JS_MONOIC */
53405: 
