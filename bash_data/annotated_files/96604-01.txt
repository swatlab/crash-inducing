69223: /* -*- Mode: C; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*-
69223:  * vim: set ts=4 sw=4 et tw=79 ft=cpp:
69223:  *
69223:  * ***** BEGIN LICENSE BLOCK *****
69223:  * Version: MPL 1.1/GPL 2.0/LGPL 2.1
69223:  *
69223:  * The contents of this file are subject to the Mozilla Public License Version
69223:  * 1.1 (the "License"); you may not use this file except in compliance with
69223:  * the License. You may obtain a copy of the License at
69223:  * http://www.mozilla.org/MPL/
69223:  *
69223:  * Software distributed under the License is distributed on an "AS IS" basis,
69223:  * WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
69223:  * for the specific language governing rights and limitations under the
69223:  * License.
69223:  *
69223:  * The Original Code is SpiderMonkey JavaScript engine.
69223:  *
69223:  * The Initial Developer of the Original Code is
69223:  * Mozilla Corporation.
69223:  * Portions created by the Initial Developer are Copyright (C) 2009
69223:  * the Initial Developer. All Rights Reserved.
69223:  *
69223:  * Contributor(s):
69223:  *   Luke Wagner <luke@mozilla.com>
69223:  *
69223:  * Alternatively, the contents of this file may be used under the terms of
69223:  * either the GNU General Public License Version 2 or later (the "GPL"), or
69223:  * the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
69223:  * in which case the provisions of the GPL or the LGPL are applicable instead
69223:  * of those above. If you wish to allow use of your version of this file only
69223:  * under the terms of either the GPL or the LGPL, and not to allow others to
69223:  * use your version of this file under the terms of the MPL, indicate your
69223:  * decision by deleting the provisions above and replace them with the notice
69223:  * and other provisions required by the GPL or the LGPL. If you do not delete
69223:  * the provisions above, a recipient may use your version of this file under
69223:  * the terms of any one of the MPL, the GPL or the LGPL.
69223:  *
69223:  * ***** END LICENSE BLOCK ***** */
69223: 
90005: #include "jscntxt.h"
69223: #include "jsgcmark.h"
69223: #include "methodjit/MethodJIT.h"
69223: #include "Stack.h"
69223: 
69223: #include "jsgcinlines.h"
69223: #include "jsobjinlines.h"
69223: 
69223: #include "Stack-inl.h"
69223: 
69223: /* Includes to get to low-level memory-mapping functionality. */
69223: #ifdef XP_WIN
69223: # include "jswin.h"
69223: #elif defined(XP_OS2)
69223: # define INCL_DOSMEMMGR
69223: # include <os2.h>
69223: #else
69223: # include <unistd.h>
69223: # include <sys/mman.h>
69223: # if !defined(MAP_ANONYMOUS)
69223: #  if defined(MAP_ANON)
69223: #   define MAP_ANONYMOUS MAP_ANON
69223: #  else
69223: #   define MAP_ANONYMOUS 0
69223: #  endif
69223: # endif
69223: #endif
69223: 
69223: using namespace js;
69223: 
69223: /*****************************************************************************/
69223: 
71697: void
71697: StackFrame::initExecuteFrame(JSScript *script, StackFrame *prev, FrameRegs *regs,
71697:                              const Value &thisv, JSObject &scopeChain, ExecuteType type)
71697: {
71697:     /*
71697:      * See encoding of ExecuteType. When GLOBAL isn't set, we are executing a
71697:      * script in the context of another frame and the frame type is determined
71697:      * by the context.
71697:      */
86077:     flags_ = type | HAS_SCOPECHAIN | HAS_BLOCKCHAIN | HAS_PREVPC;
71697:     if (!(flags_ & GLOBAL))
71697:         flags_ |= (prev->flags_ & (FUNCTION | GLOBAL));
71697: 
71697:     Value *dstvp = (Value *)this - 2;
71697:     dstvp[1] = thisv;
71697: 
71697:     if (isFunctionFrame()) {
71697:         dstvp[0] = prev->calleev();
71697:         exec = prev->exec;
87583:         u.evalScript = script;
71697:     } else {
71697:         JS_ASSERT(isGlobalFrame());
71697:         dstvp[0] = NullValue();
71697:         exec.script = script;
71697: #ifdef DEBUG
87583:         u.evalScript = (JSScript *)0xbad;
71697: #endif
71697:     }
71697: 
71697:     scopeChain_ = &scopeChain;
71697:     prev_ = prev;
71697:     prevpc_ = regs ? regs->pc : (jsbytecode *)0xbad;
76193:     prevInline_ = regs ? regs->inlined() : NULL;
86077:     blockChain_ = NULL;
71697: 
71697: #ifdef DEBUG
71697:     ncode_ = (void *)0xbad;
71697:     Debug_SetValueRangeToCrashOnTouch(&rval_, 1);
71697:     hookData_ = (void *)0xbad;
71697:     annotation_ = (void *)0xbad;
71697: #endif
71697: 
71697:     if (prev && prev->annotation())
71697:         setAnnotation(prev->annotation());
71697: }
71697: 
71697: void
71697: StackFrame::initDummyFrame(JSContext *cx, JSObject &chain)
71697: {
71697:     PodZero(this);
71697:     flags_ = DUMMY | HAS_PREVPC | HAS_SCOPECHAIN;
71697:     initPrev(cx);
71697:     JS_ASSERT(chain.isGlobal());
71697:     setScopeChainNoCallObj(chain);
71697: }
71697: 
90005: template <class T, class U, StackFrame::TriggerPostBarriers doPostBarrier>
71697: void
90005: StackFrame::stealFrameAndSlots(StackFrame *fp, T *vp, StackFrame *otherfp, U *othervp,
90005:                                Value *othersp)
71697: {
90005:     JS_ASSERT((U *)vp == (U *)this - ((U *)otherfp - othervp));
90005:     JS_ASSERT((Value *)othervp == otherfp->actualArgs() - 2);
71697:     JS_ASSERT(othersp >= otherfp->slots());
71697:     JS_ASSERT(othersp <= otherfp->base() + otherfp->numSlots());
90005:     JS_ASSERT((T *)fp - vp == (U *)otherfp - othervp);
71697: 
90005:     /* Copy args, StackFrame, and slots. */
90005:     U *srcend = (U *)otherfp->formalArgsEnd();
90005:     T *dst = vp;
90005:     for (U *src = othervp; src < srcend; src++, dst++)
90005:         *dst = *src;
71697: 
90005:     *fp = *otherfp;
90005:     if (doPostBarrier)
90005:         fp->writeBarrierPost();
90005: 
90005:     srcend = (U *)othersp;
90005:     dst = (T *)fp->slots();
90005:     for (U *src = (U *)otherfp->slots(); src < srcend; src++, dst++)
90005:         *dst = *src;
71697: 
71697:     /*
71697:      * Repoint Call, Arguments, Block and With objects to the new live frame.
71697:      * Call and Arguments are done directly because we have pointers to them.
71697:      * Block and With objects are done indirectly through 'liveFrame'. See
71697:      * js_LiveFrameToFloating comment in jsiter.h.
71697:      */
71697:     if (hasCallObj()) {
86483:         CallObject &obj = callObj();
86483:         obj.setStackFrame(this);
71697:         otherfp->flags_ &= ~HAS_CALL_OBJ;
71697:         if (js_IsNamedLambda(fun())) {
86483:             DeclEnvObject &env = obj.enclosingScope().asDeclEnv();
86483:             env.setStackFrame(this);
71697:         }
71697:     }
71697:     if (hasArgsObj()) {
71697:         ArgumentsObject &argsobj = argsObj();
71697:         if (argsobj.isNormalArguments())
78065:             argsobj.setStackFrame(this);
71697:         else
78065:             JS_ASSERT(!argsobj.maybeStackFrame());
71697:         otherfp->flags_ &= ~HAS_ARGS_OBJ;
71697:     }
71697: }
71697: 
90005: /* Note: explicit instantiation for js_NewGenerator located in jsiter.cpp. */
90005: template void StackFrame::stealFrameAndSlots<Value, HeapValue, StackFrame::NoPostBarrier>(
90005:                                              StackFrame *, Value *,
90005:                                              StackFrame *, HeapValue *, Value *);
90005: template void StackFrame::stealFrameAndSlots<HeapValue, Value, StackFrame::DoPostBarrier>(
90005:                                              StackFrame *, HeapValue *,
90005:                                              StackFrame *, Value *, Value *);
90005: 
90005: void
90005: StackFrame::writeBarrierPost()
90005: {
90005:     /* This needs to follow the same rules as in js_TraceStackFrame. */
90005:     if (scopeChain_)
90005:         JSObject::writeBarrierPost(scopeChain_, (void *)&scopeChain_);
90005:     if (isDummyFrame())
90005:         return;
90005:     if (hasArgsObj())
90005:         JSObject::writeBarrierPost(argsObj_, (void *)&argsObj_);
90005:     if (isScriptFrame()) {
90005:         if (isFunctionFrame()) {
90005:             JSFunction::writeBarrierPost((JSObject *)exec.fun, (void *)&exec.fun);
90005:             if (isEvalFrame())
90005:                 JSScript::writeBarrierPost(u.evalScript, (void *)&u.evalScript);
90005:         } else {
90005:             JSScript::writeBarrierPost(exec.script, (void *)&exec.script);
90005:         }
90005:     }
90005:     if (hasReturnValue())
90005:         HeapValue::writeBarrierPost(rval_, &rval_);
90005: }
90005: 
69223: #ifdef DEBUG
69223: JSObject *const StackFrame::sInvalidScopeChain = (JSObject *)0xbeef;
69223: #endif
69223: 
69223: jsbytecode *
76193: StackFrame::prevpcSlow(JSInlinedSite **pinlined)
69223: {
69223:     JS_ASSERT(!(flags_ & HAS_PREVPC));
69223: #if defined(JS_METHODJIT) && defined(JS_MONOIC)
69223:     StackFrame *p = prev();
71696:     mjit::JITScript *jit = p->script()->getJIT(p->isConstructing());
76193:     prevpc_ = jit->nativeToPC(ncode_, &prevInline_);
69223:     flags_ |= HAS_PREVPC;
76193:     if (pinlined)
76193:         *pinlined = prevInline_;
69223:     return prevpc_;
69223: #else
69223:     JS_NOT_REACHED("Unknown PC for frame");
69223:     return NULL;
69223: #endif
69223: }
69223: 
76193: jsbytecode *
76193: StackFrame::pcQuadratic(const ContextStack &stack, StackFrame *next, JSInlinedSite **pinlined)
76193: {
76193:     JS_ASSERT_IF(next, next->prev() == this);
76193: 
76193:     StackSegment &seg = stack.space().containingSegment(this);
76193:     FrameRegs &regs = seg.regs();
76193: 
76193:     /*
76193:      * This isn't just an optimization; seg->computeNextFrame(fp) is only
76193:      * defined if fp != seg->currentFrame.
76193:      */
76193:     if (regs.fp() == this) {
76193:         if (pinlined)
76193:             *pinlined = regs.inlined();
76193:         return regs.pc;
76193:     }
76193: 
76193:     if (!next)
76193:         next = seg.computeNextFrame(this);
76193:     return next->prevpc(pinlined);
76193: }
76193: 
89965: void
89965: StackFrame::mark(JSTracer *trc)
89965: {
89965:     /*
89965:      * Normally we would use MarkRoot here, except that generators also take
89965:      * this path. However, generators use a special write barrier when the stack
89965:      * frame is copied to the floating frame. Therefore, no barrier is needed.
89965:      */
91557:     if (flags_ & HAS_SCOPECHAIN)
91557:         gc::MarkObjectUnbarriered(trc, &scopeChain_, "scope chain");
89965:     if (isDummyFrame())
89965:         return;
89965:     if (hasArgsObj())
91557:         gc::MarkObjectUnbarriered(trc, &argsObj_, "arguments");
89965:     if (isFunctionFrame()) {
91557:         gc::MarkObjectUnbarriered(trc, &exec.fun, "fun");
89965:         if (isEvalFrame())
91557:             gc::MarkScriptUnbarriered(trc, &u.evalScript, "eval script");
89965:     } else {
91557:         gc::MarkScriptUnbarriered(trc, &exec.script, "script");
89965:     }
89965:     if (IS_GC_MARKING_TRACER(trc))
89965:         script()->compartment()->active = true;
90302:     gc::MarkValueUnbarriered(trc, &returnValue(), "rval");
89965: }
89965: 
71697: /*****************************************************************************/
71697: 
71697: bool
71697: StackSegment::contains(const StackFrame *fp) const
69223: {
71697:     /* NB: this depends on the continuity of segments in memory. */
71697:     return (Value *)fp >= slotsBegin() && (Value *)fp <= (Value *)maybefp();
69223: }
69223: 
71697: bool
71697: StackSegment::contains(const FrameRegs *regs) const
71697: {
71697:     return regs && contains(regs->fp());
71697: }
69223: 
71697: bool
71697: StackSegment::contains(const CallArgsList *call) const
69223: {
71697:     if (!call || !calls_)
69223:         return false;
69223: 
71697:     /* NB: this depends on the continuity of segments in memory. */
79387:     Value *vp = call->array();
87583:     return vp > slotsBegin() && vp <= calls_->array();
69223: }
69223: 
69223: StackFrame *
71697: StackSegment::computeNextFrame(const StackFrame *f) const
69223: {
71697:     JS_ASSERT(contains(f) && f != fp());
69223: 
71697:     StackFrame *next = fp();
69223:     StackFrame *prev;
71697:     while ((prev = next->prev()) != f)
69223:         next = prev;
69223:     return next;
69223: }
69223: 
71697: Value *
71697: StackSegment::end() const
71697: {
71697:     /* NB: this depends on the continuity of segments in memory. */
71697:     JS_ASSERT_IF(calls_ || regs_, contains(calls_) || contains(regs_));
71697:     Value *p = calls_
71697:                ? regs_
71697:                  ? Max(regs_->sp, calls_->end())
71697:                  : calls_->end()
71697:                : regs_
71697:                  ? regs_->sp
71697:                  : slotsBegin();
71697:     JS_ASSERT(p >= slotsBegin());
71697:     return p;
71697: }
71697: 
71697: FrameRegs *
71697: StackSegment::pushRegs(FrameRegs &regs)
71697: {
71697:     JS_ASSERT_IF(contains(regs_), regs.fp()->prev() == regs_->fp());
71697:     FrameRegs *prev = regs_;
71697:     regs_ = &regs;
71697:     return prev;
71697: }
71697: 
71697: void
71697: StackSegment::popRegs(FrameRegs *regs)
71697: {
71697:     JS_ASSERT_IF(regs && contains(regs->fp()), regs->fp() == regs_->fp()->prev());
71697:     regs_ = regs;
71697: }
71697: 
71697: void
71697: StackSegment::pushCall(CallArgsList &callList)
71697: {
71697:     callList.prev_ = calls_;
71697:     calls_ = &callList;
71697: }
71697: 
71697: void
71767: StackSegment::pointAtCall(CallArgsList &callList)
71767: {
71767:     calls_ = &callList;
71767: }
71767: 
71767: void
71697: StackSegment::popCall()
71697: {
71697:     calls_ = calls_->prev_;
71697: }
71697: 
69223: /*****************************************************************************/
69223: 
69223: StackSpace::StackSpace()
73495:   : seg_(NULL),
73495:     base_(NULL),
73495:     conservativeEnd_(NULL),
73495: #ifdef XP_WIN
69223:     commitEnd_(NULL),
73495: #endif
73495:     defaultEnd_(NULL),
73495:     trustedEnd_(NULL)
73495: {
73495:     assertInvariants();
73495: }
69223: 
69223: bool
69223: StackSpace::init()
69223: {
69223:     void *p;
69223: #ifdef XP_WIN
69223:     p = VirtualAlloc(NULL, CAPACITY_BYTES, MEM_RESERVE, PAGE_READWRITE);
69223:     if (!p)
69223:         return false;
69223:     void *check = VirtualAlloc(p, COMMIT_BYTES, MEM_COMMIT, PAGE_READWRITE);
69223:     if (p != check)
69223:         return false;
69223:     base_ = reinterpret_cast<Value *>(p);
73495:     conservativeEnd_ = commitEnd_ = base_ + COMMIT_VALS;
73495:     trustedEnd_ = base_ + CAPACITY_VALS;
73495:     defaultEnd_ = trustedEnd_ - BUFFER_VALS;
69223: #elif defined(XP_OS2)
69223:     if (DosAllocMem(&p, CAPACITY_BYTES, PAG_COMMIT | PAG_READ | PAG_WRITE | OBJ_ANY) &&
69223:         DosAllocMem(&p, CAPACITY_BYTES, PAG_COMMIT | PAG_READ | PAG_WRITE))
69223:         return false;
69223:     base_ = reinterpret_cast<Value *>(p);
73495:     trustedEnd_ = base_ + CAPACITY_VALS;
73495:     conservativeEnd_ = defaultEnd_ = trustedEnd_ - BUFFER_VALS;
69223: #else
69223:     JS_ASSERT(CAPACITY_BYTES % getpagesize() == 0);
69223:     p = mmap(NULL, CAPACITY_BYTES, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
69223:     if (p == MAP_FAILED)
69223:         return false;
69223:     base_ = reinterpret_cast<Value *>(p);
73495:     trustedEnd_ = base_ + CAPACITY_VALS;
73495:     conservativeEnd_ = defaultEnd_ = trustedEnd_ - BUFFER_VALS;
69223: #endif
73495:     assertInvariants();
69223:     return true;
69223: }
69223: 
69223: StackSpace::~StackSpace()
69223: {
73495:     assertInvariants();
69223:     JS_ASSERT(!seg_);
69223:     if (!base_)
69223:         return;
69223: #ifdef XP_WIN
69223:     VirtualFree(base_, (commitEnd_ - base_) * sizeof(Value), MEM_DECOMMIT);
69223:     VirtualFree(base_, 0, MEM_RELEASE);
69223: #elif defined(XP_OS2)
69223:     DosFreeMem(base_);
69223: #else
69223: #ifdef SOLARIS
69223:     munmap((caddr_t)base_, CAPACITY_BYTES);
69223: #else
69223:     munmap(base_, CAPACITY_BYTES);
69223: #endif
69223: #endif
69223: }
69223: 
71697: StackSegment &
76193: StackSpace::containingSegment(const StackFrame *target) const
69223: {
71697:     for (StackSegment *s = seg_; s; s = s->prevInMemory()) {
69223:         if (s->contains(target))
69223:             return *s;
69223:     }
69223:     JS_NOT_REACHED("frame not in stack space");
69223:     return *(StackSegment *)NULL;
69223: }
69223: 
69223: void
89965: StackSpace::markFrameSlots(JSTracer *trc, StackFrame *fp, Value *slotsEnd, jsbytecode *pc)
89965: {
89965:     Value *slotsBegin = fp->slots();
89965: 
89965:     if (!fp->isScriptFrame()) {
89965:         JS_ASSERT(fp->isDummyFrame());
90129:         gc::MarkValueRootRange(trc, slotsBegin, slotsEnd, "vm_stack");
89965:         return;
89965:     }
89965: 
89965:     /* If it's a scripted frame, we should have a pc. */
89965:     JS_ASSERT(pc);
89965: 
89965:     JSScript *script = fp->script();
89965:     if (!script->hasAnalysis() || !script->analysis()->ranLifetimes()) {
90129:         gc::MarkValueRootRange(trc, slotsBegin, slotsEnd, "vm_stack");
89965:         return;
89965:     }
89965: 
89965:     /*
89965:      * If the JIT ran a lifetime analysis, then it may have left garbage in the
89965:      * slots considered not live. We need to avoid marking them. Additionally,
89965:      * in case the analysis information is thrown out later, we overwrite these
89965:      * dead slots with valid values so that future GCs won't crash. Analysis
89965:      * results are thrown away during the sweeping phase, so we always have at
89965:      * least one GC to do this.
89965:      */
89965:     analyze::AutoEnterAnalysis aea(script->compartment());
89965:     analyze::ScriptAnalysis *analysis = script->analysis();
89965:     uint32_t offset = pc - script->code;
89965:     Value *fixedEnd = slotsBegin + script->nfixed;
89965:     for (Value *vp = slotsBegin; vp < fixedEnd; vp++) {
89965:         uint32_t slot = analyze::LocalSlot(script, vp - slotsBegin);
89965: 
96130:         /*
96130:          * Will this slot be synced by the JIT? If not, replace with a dummy
96130:          * value with the same type tag.
96130:          */
96604:         if (!analysis->trackSlot(slot) || analysis->liveness(slot).live(offset)) {
90302:             gc::MarkValueRoot(trc, vp, "vm_stack");
96604:         } else if (vp->isDouble()) {
96604:             *vp = DoubleValue(0.0);
96604:         } else {
96604:             /*
96604:              * It's possible that *vp may not be a valid Value. For example, it
96604:              * may be tagged as a NullValue but the low bits may be nonzero so
96604:              * that isNull() returns false. This can cause problems later on
96604:              * when marking the value. Extracting the type in this way and then
96604:              * overwriting the value circumvents the problem.
96604:              */
96604:             JSValueType type = vp->extractNonDoubleType();
96604:             if (type == JSVAL_TYPE_INT32)
96604:                 *vp = Int32Value(0);
96604:             else if (type == JSVAL_TYPE_UNDEFINED)
96604:                 *vp = UndefinedValue();
96604:             else if (type == JSVAL_TYPE_BOOLEAN)
96604:                 *vp = BooleanValue(false);
96604:             else if (type == JSVAL_TYPE_STRING)
96604:                 *vp = StringValue(trc->runtime->atomState.nullAtom);
96604:             else if (type == JSVAL_TYPE_NULL)
96604:                 *vp = NullValue();
96604:             else if (type == JSVAL_TYPE_OBJECT)
96131:                 *vp = ObjectValue(fp->scopeChain().global());
96604:         }
89965:     }
89965: 
90129:     gc::MarkValueRootRange(trc, fixedEnd, slotsEnd, "vm_stack");
89965: }
89965: 
89965: void
69223: StackSpace::mark(JSTracer *trc)
69223: {
69223:     /*
69223:      * JIT code can leave values in an incoherent (i.e., unsafe for precise
69223:      * marking) state, hence MarkStackRangeConservatively.
69223:      */
69223: 
71697:     /* NB: this depends on the continuity of segments in memory. */
71697:     Value *nextSegEnd = firstUnused();
71697:     for (StackSegment *seg = seg_; seg; seg = seg->prevInMemory()) {
71697:         /*
71697:          * A segment describes a linear region of memory that contains a stack
71697:          * of native and interpreted calls. For marking purposes, though, we
71697:          * only need to distinguish between frames and values and mark
71697:          * accordingly. Since native calls only push values on the stack, we
71697:          * can effectively lump them together and just iterate over interpreted
71697:          * calls. Thus, marking can view the stack as the regex:
71697:          *   (segment slots (frame slots)*)*
71697:          * which gets marked in reverse order.
71697:          */
71697:         Value *slotsEnd = nextSegEnd;
89965:         jsbytecode *pc = seg->maybepc();
71697:         for (StackFrame *fp = seg->maybefp(); (Value *)fp > (Value *)seg; fp = fp->prev()) {
89965:             /* Mark from fp->slots() to slotsEnd. */
89965:             markFrameSlots(trc, fp, slotsEnd, pc);
89965: 
89965:             fp->mark(trc);
71697:             slotsEnd = (Value *)fp;
89965: 
89965:             JSInlinedSite *site;
89965:             pc = fp->prevpc(&site);
89965:             JS_ASSERT_IF(fp->prev(), !site);
69223:         }
90129:         gc::MarkValueRootRange(trc, seg->slotsBegin(), slotsEnd, "vm_stack");
71697:         nextSegEnd = (Value *)seg;
69223:     }
69223: }
69223: 
90410: void
90410: StackSpace::markActiveCompartments()
90410: {
90410:     for (StackSegment *seg = seg_; seg; seg = seg->prevInMemory()) {
90410:         for (StackFrame *fp = seg->maybefp(); (Value *)fp > (Value *)seg; fp = fp->prev())
90410:             MarkCompartmentActive(fp);
90410:     }
90410: }
90410: 
69223: JS_FRIEND_API(bool)
76885: StackSpace::ensureSpaceSlow(JSContext *cx, MaybeReportError report, Value *from, ptrdiff_t nvals,
76885:                             JSCompartment *dest) const
69223: {
73495:     assertInvariants();
73495: 
76885:     /* See CX_COMPARTMENT comment. */
76885:     if (dest == (JSCompartment *)CX_COMPARTMENT)
76885:         dest = cx->compartment;
76885: 
76885:     bool trusted = !dest || dest->principals == cx->runtime->trustedPrincipals();
73495:     Value *end = trusted ? trustedEnd_ : defaultEnd_;
73495: 
73495:     /*
73495:      * conservativeEnd_ must stay below defaultEnd_: if conservativeEnd_ were
73495:      * to be bumped past defaultEnd_, untrusted JS would be able to consume the
73495:      * buffer space at the end of the stack reserved for trusted JS.
73495:      */
73495: 
73495:     if (end - from < nvals) {
73495:         if (report)
73495:             js_ReportOverRecursed(cx);
69223:         return false;
69223:     }
69223: 
73495: #ifdef XP_WIN
73495:     if (commitEnd_ - from < nvals) {
69223:         Value *newCommit = commitEnd_;
69223:         Value *request = from + nvals;
69223: 
69223:         /* Use a dumb loop; will probably execute once. */
73495:         JS_ASSERT((trustedEnd_ - newCommit) % COMMIT_VALS == 0);
69223:         do {
69223:             newCommit += COMMIT_VALS;
73495:             JS_ASSERT((trustedEnd_ - newCommit) >= 0);
69223:         } while (newCommit < request);
69223: 
69223:         /* The cast is safe because CAPACITY_BYTES is small. */
84755:         int32_t size = static_cast<int32_t>(newCommit - commitEnd_) * sizeof(Value);
69223: 
69223:         if (!VirtualAlloc(commitEnd_, size, MEM_COMMIT, PAGE_READWRITE)) {
73495:             if (report)
73495:                 js_ReportOverRecursed(cx);
69223:             return false;
69223:         }
69223: 
69223:         commitEnd_ = newCommit;
73495:         conservativeEnd_ = Min(commitEnd_, defaultEnd_);
73495:         assertInvariants();
69223:     }
69223: #endif
69223: 
73495:     return true;
73495: }
73495: 
69223: bool
91237: StackSpace::tryBumpLimit(JSContext *cx, Value *from, unsigned nvals, Value **limit)
69223: {
73495:     if (!ensureSpace(cx, REPORT_ERROR, from, nvals))
71363:         return false;
73495:     *limit = conservativeEnd_;
69223:     return true;
69223: }
69223: 
71366: size_t
83122: StackSpace::sizeOfCommitted()
71366: {
73495: #ifdef XP_WIN
71366:     return (commitEnd_ - base_) * sizeof(Value);
73495: #else
73495:     return (trustedEnd_ - base_) * sizeof(Value);
73495: #endif
71366: }
71366: 
69223: /*****************************************************************************/
69223: 
69223: ContextStack::ContextStack(JSContext *cx)
71697:   : seg_(NULL),
88135:     space_(&cx->runtime->stackSpace),
69223:     cx_(cx)
88135: {}
69223: 
69223: ContextStack::~ContextStack()
69223: {
69223:     JS_ASSERT(!seg_);
69223: }
69223: 
69223: bool
71697: ContextStack::onTop() const
69223: {
71697:     return seg_ && seg_ == space().seg_;
69223: }
69223: 
71697: bool
71697: ContextStack::containsSlow(const StackFrame *target) const
69223: {
71697:     for (StackSegment *s = seg_; s; s = s->prevInContext()) {
71697:         if (s->contains(target))
71697:             return true;
71697:     }
71697:     return false;
71697: }
71697: 
71697: /*
71697:  * This helper function brings the ContextStack to the top of the thread stack
71697:  * (so that it can be extended to push a frame and/or arguments) by potentially
71697:  * pushing a StackSegment. The 'pushedSeg' outparam indicates whether such a
71697:  * segment was pushed (and hence whether the caller needs to call popSegment).
71697:  *
71697:  * Additionally, to minimize calls to ensureSpace, ensureOnTop ensures that
71697:  * there is space for nvars slots on top of the stack.
71697:  */
71697: Value *
91237: ContextStack::ensureOnTop(JSContext *cx, MaybeReportError report, unsigned nvars,
76885:                           MaybeExtend extend, bool *pushedSeg, JSCompartment *dest)
71697: {
71697:     Value *firstUnused = space().firstUnused();
71697: 
77377: #ifdef JS_METHODJIT
77377:     /*
77377:      * The only calls made by inlined methodjit frames can be to other JIT
77398:      * frames associated with the same VMFrame. If we try to Invoke(),
77398:      * Execute() or so forth, any topmost inline frame will need to be
77398:      * expanded (along with other inline frames in the compartment).
77398:      * To avoid pathological behavior here, make sure to mark any topmost
77398:      * function as uninlineable, which will expand inline frames if there are
77398:      * any and prevent the function from being inlined in the future.
77377:      */
77426:     if (FrameRegs *regs = cx->maybeRegs()) {
77410:         JSFunction *fun = NULL;
77426:         if (JSInlinedSite *site = regs->inlined()) {
87654:             mjit::JITChunk *chunk = regs->fp()->jit()->chunk(regs->pc);
87654:             fun = chunk->inlineFrames()[site->inlineIndex].fun;
77426:         } else {
77426:             StackFrame *fp = regs->fp();
77426:             if (fp->isFunctionFrame()) {
77426:                 JSFunction *f = fp->fun();
77426:                 if (f->isInterpreted())
77426:                     fun = f;
77426:             }
77426:         }
77426: 
77410:         if (fun) {
77410:             fun->script()->uninlineable = true;
77410:             types::MarkTypeObjectFlags(cx, fun, types::OBJECT_FLAG_UNINLINEABLE);
77398:         }
77398:     }
77410:     JS_ASSERT_IF(cx->hasfp(), !cx->regs().inlined());
77377: #endif
77377: 
71697:     if (onTop() && extend) {
76885:         if (!space().ensureSpace(cx, report, firstUnused, nvars, dest))
71697:             return NULL;
71697:         return firstUnused;
71697:     }
71697: 
76885:     if (!space().ensureSpace(cx, report, firstUnused, VALUES_PER_STACK_SEGMENT + nvars, dest))
71697:         return NULL;
71697: 
71697:     FrameRegs *regs;
71697:     CallArgsList *calls;
71697:     if (seg_ && extend) {
71697:         regs = seg_->maybeRegs();
71697:         calls = seg_->maybeCalls();
71697:     } else {
71697:         regs = NULL;
71697:         calls = NULL;
71697:     }
71697: 
71697:     seg_ = new(firstUnused) StackSegment(seg_, space().seg_, regs, calls);
71697:     space().seg_ = seg_;
71697:     *pushedSeg = true;
71697:     return seg_->slotsBegin();
69223: }
69223: 
69223: void
71697: ContextStack::popSegment()
69223: {
71697:     space().seg_ = seg_->prevInMemory();
71697:     seg_ = seg_->prevInContext();
71697: 
71697:     if (!seg_)
71697:         cx_->maybeMigrateVersionOverride();
69223: }
69223: 
71697: bool
91237: ContextStack::pushInvokeArgs(JSContext *cx, unsigned argc, InvokeArgsGuard *iag)
69223: {
74602:     JS_ASSERT(argc <= StackSpace::ARGS_LENGTH_MAX);
74602: 
91237:     unsigned nvars = 2 + argc;
73495:     Value *firstUnused = ensureOnTop(cx, REPORT_ERROR, nvars, CAN_EXTEND, &iag->pushedSeg_);
71697:     if (!firstUnused)
71697:         return false;
69223: 
91376:     MakeRangeGCSafe(firstUnused, nvars);
90857: 
71697:     ImplicitCast<CallArgs>(*iag) = CallArgsFromVp(argc, firstUnused);
71697: 
71697:     seg_->pushCall(*iag);
71697:     JS_ASSERT(space().firstUnused() == iag->end());
71697:     iag->setPushed(*this);
71697:     return true;
69223: }
69223: 
69223: void
71697: ContextStack::popInvokeArgs(const InvokeArgsGuard &iag)
69223: {
71697:     JS_ASSERT(iag.pushed());
71697:     JS_ASSERT(onTop());
71697:     JS_ASSERT(space().firstUnused() == seg_->calls().end());
69223: 
71697:     seg_->popCall();
90750:     if (iag.pushedSeg_)
71697:         popSegment();
69223: }
69223: 
71697: bool
71697: ContextStack::pushInvokeFrame(JSContext *cx, const CallArgs &args,
77341:                               InitialFrameFlags initial, InvokeFrameGuard *ifg)
69223: {
71697:     JS_ASSERT(onTop());
71697:     JS_ASSERT(space().firstUnused() == args.end());
69223: 
71697:     JSObject &callee = args.callee();
83234:     JSFunction *fun = callee.toFunction();
71697:     JSScript *script = fun->script();
69223: 
89722:     StackFrame::Flags flags = ToFrameFlags(initial);
73495:     StackFrame *fp = getCallFrame(cx, REPORT_ERROR, args, fun, script, &flags);
71697:     if (!fp)
71697:         return false;
69223: 
89722:     fp->initCallFrame(cx, *fun, script, args.length(), flags);
71697:     ifg->regs_.prepareToRun(*fp, script);
69223: 
71697:     ifg->prevRegs_ = seg_->pushRegs(ifg->regs_);
71697:     JS_ASSERT(space().firstUnused() == ifg->regs_.sp);
71697:     ifg->setPushed(*this);
71697:     return true;
69223: }
69223: 
69223: bool
71695: ContextStack::pushExecuteFrame(JSContext *cx, JSScript *script, const Value &thisv,
71695:                                JSObject &scopeChain, ExecuteType type,
71695:                                StackFrame *evalInFrame, ExecuteFrameGuard *efg)
69223: {
71697:     /*
71697:      * Even though global code and indirect eval do not execute in the context
71697:      * of the current frame, prev-link these to the current frame so that the
71697:      * callstack looks right to the debugger (via CAN_EXTEND). This is safe
71697:      * since the scope chain is what determines name lookup and access, not
71697:      * prev-links.
71697:      *
71697:      * Eval-in-frame is the exception since it prev-links to an arbitrary frame
71697:      * (possibly in the middle of some previous segment). Thus pass CANT_EXTEND
71697:      * (to start a new segment) and link the frame and call chain manually
71697:      * below.
71697:      */
71697:     CallArgsList *evalInFrameCalls = NULL;  /* quell overwarning */
71697:     StackFrame *prev;
71697:     MaybeExtend extend;
71697:     if (evalInFrame) {
71697:         /* Though the prev-frame is given, need to search for prev-call. */
71697:         StackIter iter(cx, StackIter::GO_THROUGH_SAVED);
71697:         while (!iter.isScript() || iter.fp() != evalInFrame)
71697:             ++iter;
71697:         evalInFrameCalls = iter.calls_;
71697:         prev = evalInFrame;
71697:         extend = CANT_EXTEND;
71697:     } else {
71697:         prev = maybefp();
71697:         extend = CAN_EXTEND;
71697:     }
71695: 
91237:     unsigned nvars = 2 /* callee, this */ + VALUES_PER_STACK_FRAME + script->nslots;
73495:     Value *firstUnused = ensureOnTop(cx, REPORT_ERROR, nvars, extend, &efg->pushedSeg_);
71697:     if (!firstUnused)
71697:         return NULL;
71695: 
71697:     StackFrame *fp = reinterpret_cast<StackFrame *>(firstUnused + 2);
71697:     fp->initExecuteFrame(script, prev, seg_->maybeRegs(), thisv, scopeChain, type);
71697:     SetValueRangeToUndefined(fp->slots(), script->nfixed);
71697:     efg->regs_.prepareToRun(*fp, script);
71695: 
71697:     /* pushRegs() below links the prev-frame; manually link the prev-call. */
71697:     if (evalInFrame && evalInFrameCalls)
71767:         seg_->pointAtCall(*evalInFrameCalls);
71697: 
71697:     efg->prevRegs_ = seg_->pushRegs(efg->regs_);
71697:     JS_ASSERT(space().firstUnused() == efg->regs_.sp);
71697:     efg->setPushed(*this);
71695:     return true;
71695: }
71695: 
71695: bool
76885: ContextStack::pushDummyFrame(JSContext *cx, JSCompartment *dest, JSObject &scopeChain, DummyFrameGuard *dfg)
71697: {
76885:     JS_ASSERT(dest == scopeChain.compartment());
76885: 
91237:     unsigned nvars = VALUES_PER_STACK_FRAME;
76885:     Value *firstUnused = ensureOnTop(cx, REPORT_ERROR, nvars, CAN_EXTEND, &dfg->pushedSeg_, dest);
71697:     if (!firstUnused)
86462:         return false;
71697: 
71697:     StackFrame *fp = reinterpret_cast<StackFrame *>(firstUnused);
71697:     fp->initDummyFrame(cx, scopeChain);
71697:     dfg->regs_.initDummyFrame(*fp);
71697: 
77448:     cx->setCompartment(dest);
71697:     dfg->prevRegs_ = seg_->pushRegs(dfg->regs_);
71697:     JS_ASSERT(space().firstUnused() == dfg->regs_.sp);
71697:     dfg->setPushed(*this);
71697:     return true;
71697: }
71697: 
71697: void
71697: ContextStack::popFrame(const FrameGuard &fg)
71697: {
71697:     JS_ASSERT(fg.pushed());
71697:     JS_ASSERT(onTop());
71697:     JS_ASSERT(space().firstUnused() == fg.regs_.sp);
71697:     JS_ASSERT(&fg.regs_ == &seg_->regs());
71697: 
77884:     if (fg.regs_.fp()->isNonEvalFunctionFrame())
77884:         fg.regs_.fp()->functionEpilogue();
71697: 
71697:     seg_->popRegs(fg.prevRegs_);
90750:     if (fg.pushedSeg_)
71697:         popSegment();
71697: 
71697:     /*
71697:      * NB: this code can call out and observe the stack (e.g., through GC), so
71697:      * it should only be called from a consistent stack state.
71697:      */
71697:     if (!hasfp())
71697:         cx_->resetCompartment();
71697: }
71697: 
71697: bool
71695: ContextStack::pushGeneratorFrame(JSContext *cx, JSGenerator *gen, GeneratorFrameGuard *gfg)
71695: {
71695:     StackFrame *genfp = gen->floatingFrame();
90005:     HeapValue *genvp = gen->floatingStack;
91237:     unsigned vplen = (HeapValue *)genfp - genvp;
71695: 
91237:     unsigned nvars = vplen + VALUES_PER_STACK_FRAME + genfp->numSlots();
73495:     Value *firstUnused = ensureOnTop(cx, REPORT_ERROR, nvars, CAN_EXTEND, &gfg->pushedSeg_);
71697:     if (!firstUnused)
71695:         return false;
71695: 
71697:     StackFrame *stackfp = reinterpret_cast<StackFrame *>(firstUnused + vplen);
71695:     Value *stackvp = (Value *)stackfp - vplen;
71695: 
71697:     /* Save this for popGeneratorFrame. */
71695:     gfg->gen_ = gen;
71695:     gfg->stackvp_ = stackvp;
71695: 
87962:     /*
87962:      * Trigger incremental barrier on the floating frame's generator object.
87962:      * This is normally traced through only by associated arguments/call
87962:      * objects, but only when the generator is not actually on the stack.
87962:      * We don't need to worry about generational barriers as the generator
87962:      * object has a trace hook and cannot be nursery allocated.
87962:      */
87962:     JSObject *genobj = js_FloatingFrameToGenerator(genfp)->obj;
87962:     JS_ASSERT(genobj->getClass()->trace);
87962:     JSObject::writeBarrierPre(genobj);
87962: 
71695:     /* Copy from the generator's floating frame to the stack. */
90005:     stackfp->stealFrameAndSlots<Value, HeapValue, StackFrame::NoPostBarrier>(
90005:                                 stackfp, stackvp, genfp, genvp, gen->regs.sp);
71695:     stackfp->resetGeneratorPrev(cx);
71695:     stackfp->unsetFloatingGenerator();
71697:     gfg->regs_.rebaseFromTo(gen->regs, *stackfp);
71695: 
71697:     gfg->prevRegs_ = seg_->pushRegs(gfg->regs_);
71697:     JS_ASSERT(space().firstUnused() == gfg->regs_.sp);
71697:     gfg->setPushed(*this);
69223:     return true;
69223: }
69223: 
69223: void
71697: ContextStack::popGeneratorFrame(const GeneratorFrameGuard &gfg)
69223: {
71697:     JSGenerator *gen = gfg.gen_;
71695:     StackFrame *genfp = gen->floatingFrame();
90005:     HeapValue *genvp = gen->floatingStack;
71695: 
71697:     const FrameRegs &stackRegs = gfg.regs_;
71695:     StackFrame *stackfp = stackRegs.fp();
71697:     Value *stackvp = gfg.stackvp_;
71695: 
71695:     /* Copy from the stack to the generator's floating frame. */
71697:     gen->regs.rebaseFromTo(stackRegs, *genfp);
90005:     genfp->stealFrameAndSlots<HeapValue, Value, StackFrame::DoPostBarrier>(
90005:                               genfp, genvp, stackfp, stackvp, stackRegs.sp);
71695:     genfp->setFloatingGenerator();
71695: 
71697:     /* ~FrameGuard/popFrame will finish the popping. */
71697:     JS_ASSERT(ImplicitCast<const FrameGuard>(gfg).pushed());
69223: }
69223: 
69223: bool
71697: ContextStack::saveFrameChain()
69223: {
76885:     JSCompartment *dest = NULL;
73495: 
71697:     bool pushedSeg;
76885:     if (!ensureOnTop(cx_, REPORT_ERROR, 0, CANT_EXTEND, &pushedSeg, dest))
69223:         return false;
73495: 
71697:     JS_ASSERT(pushedSeg);
71697:     JS_ASSERT(!hasfp());
73495:     JS_ASSERT(onTop() && seg_->isEmpty());
73495: 
71697:     cx_->resetCompartment();
69223:     return true;
69223: }
69223: 
69223: void
71697: ContextStack::restoreFrameChain()
69223: {
71697:     JS_ASSERT(onTop() && seg_->isEmpty());
69223: 
71697:     popSegment();
69223:     cx_->resetCompartment();
69223: }
69223: 
69223: /*****************************************************************************/
69223: 
71697: void
71697: StackIter::poisonRegs()
69223: {
71697:     sp_ = (Value *)0xbad;
71697:     pc_ = (jsbytecode *)0xbad;
92133:     script_ = (JSScript *)0xbad;
69223: }
71697: 
71697: void
71697: StackIter::popFrame()
71697: {
71697:     StackFrame *oldfp = fp_;
71697:     JS_ASSERT(seg_->contains(oldfp));
71697:     fp_ = fp_->prev();
71697:     if (seg_->contains(fp_)) {
76193:         JSInlinedSite *inline_;
76193:         pc_ = oldfp->prevpc(&inline_);
76193:         JS_ASSERT(!inline_);
71697: 
71697:         /*
71697:          * If there is a CallArgsList element between oldfp and fp_, then sp_
71697:          * is ignored, so we only consider the case where there is no
71697:          * intervening CallArgsList. The stack representation is not optimized
71697:          * for this operation so we need to do a full case analysis of how
71697:          * frames are pushed by considering each ContextStack::push*Frame.
71697:          */
71697:         if (oldfp->isGeneratorFrame()) {
71697:             /* Generator's args do not overlap with the caller's expr stack. */
71697:             sp_ = (Value *)oldfp->actualArgs() - 2;
71697:         } else if (oldfp->isNonEvalFunctionFrame()) {
71697:             /*
71697:              * When Invoke is called from a native, there will be an enclosing
71697:              * pushInvokeArgs which pushes a CallArgsList element so we can
71697:              * ignore that case. The other two cases of function call frames are
71697:              * Invoke called directly from script and pushInlineFrmae. In both
71697:              * cases, the actual arguments of the callee should be included in
71697:              * the caller's expr stack.
71697:              */
71697:             sp_ = oldfp->actualArgsEnd();
71697:         } else if (oldfp->isFramePushedByExecute()) {
71697:             /* pushExecuteFrame pushes exactly (callee, this) before frame. */
71697:             sp_ = (Value *)oldfp - 2;
71697:         } else {
71697:             /* pushDummyFrame pushes exactly 0 slots before frame. */
71697:             JS_ASSERT(oldfp->isDummyFrame());
71697:             sp_ = (Value *)oldfp;
71363:         }
92133: 
92133:         script_ = fp_->maybeScript();
71697:     } else {
71697:         poisonRegs();
71697:     }
71697: }
71697: 
71697: void
71697: StackIter::popCall()
71697: {
71697:     CallArgsList *oldCall = calls_;
71697:     JS_ASSERT(seg_->contains(oldCall));
71697:     calls_ = calls_->prev();
71697:     if (seg_->contains(fp_)) {
71697:         /* pc_ keeps its same value. */
71697:         sp_ = oldCall->base();
71697:     } else {
71697:         poisonRegs();
71697:     }
71697: }
71697: 
71697: void
71697: StackIter::settleOnNewSegment()
71697: {
71697:     if (FrameRegs *regs = seg_->maybeRegs()) {
71697:         sp_ = regs->sp;
71697:         pc_ = regs->pc;
92133:         if (fp_)
92133:             script_ = fp_->maybeScript();
71697:     } else {
71697:         poisonRegs();
71697:     }
71697: }
71697: 
71697: void
71697: StackIter::startOnSegment(StackSegment *seg)
71697: {
71697:     seg_ = seg;
71697:     fp_ = seg_->maybefp();
71697:     calls_ = seg_->maybeCalls();
71697:     settleOnNewSegment();
71697: }
71697: 
74739: static void JS_NEVER_INLINE
74739: CrashIfInvalidSlot(StackFrame *fp, Value *vp)
74739: {
74739:     if (vp < fp->slots() || vp >= fp->slots() + fp->script()->nslots) {
74739:         JS_ASSERT(false && "About to dereference invalid slot");
74739:         *(int *)0xbad = 0;  // show up nicely in crash-stats
90070:         MOZ_Assert("About to dereference invalid slot", __FILE__, __LINE__);
74739:     }
74739: }
74739: 
71697: void
71697: StackIter::settleOnNewState()
71697: {
71697:     /*
71697:      * There are elements of the calls_ and fp_ chains that we want to skip
71697:      * over so iterate until we settle on one or until there are no more.
71697:      */
71697:     while (true) {
71697:         if (!fp_ && !calls_) {
71697:             if (savedOption_ == GO_THROUGH_SAVED && seg_->prevInContext()) {
71697:                 startOnSegment(seg_->prevInContext());
71697:                 continue;
71697:             }
71697:             state_ = DONE;
71363:             return;
69223:         }
69223: 
71697:         /* Check if popFrame/popCall changed segment. */
71697:         bool containsFrame = seg_->contains(fp_);
71697:         bool containsCall = seg_->contains(calls_);
71697:         while (!containsFrame && !containsCall) {
71697:             seg_ = seg_->prevInContext();
71697:             containsFrame = seg_->contains(fp_);
71697:             containsCall = seg_->contains(calls_);
71363: 
71697:             /* Eval-in-frame allows jumping into the middle of a segment. */
71697:             if (containsFrame && seg_->fp() != fp_) {
71697:                 /* Avoid duplicating logic; seg_ contains fp_, so no iloop. */
71697:                 StackIter tmp = *this;
71697:                 tmp.startOnSegment(seg_);
71697:                 while (!tmp.isScript() || tmp.fp() != fp_)
71697:                     ++tmp;
71697:                 JS_ASSERT(tmp.state_ == SCRIPTED && tmp.seg_ == seg_ && tmp.fp_ == fp_);
71697:                 *this = tmp;
71697:                 return;
71697:             }
71697:             /* There is no eval-in-frame equivalent for native calls. */
71697:             JS_ASSERT_IF(containsCall, &seg_->calls() == calls_);
71697:             settleOnNewSegment();
71363:         }
71363: 
71697:         /*
71697:          * In case of both a scripted frame and call record, use linear memory
71697:          * ordering to decide which was the most recent.
71697:          */
79387:         if (containsFrame && (!containsCall || (Value *)fp_ >= calls_->array())) {
71697:             /* Nobody wants to see dummy frames. */
71697:             if (fp_->isDummyFrame()) {
71697:                 popFrame();
71697:                 continue;
71697:             }
69223: 
69223:             /*
71697:              * As an optimization, there is no CallArgsList element pushed for
71697:              * natives called directly by a script (compiled or interpreted).
71697:              * We catch these by inspecting the bytecode and stack. This check
71697:              * relies on the property that, at a call opcode,
71697:              *
71697:              *   regs.sp == vp + 2 + argc
71697:              *
80310:              * The Function.prototype.call optimization leaves no record when
80310:              * 'this' is a native function. Thus, if the following expression
80310:              * runs and breaks in the debugger, the call to 'replace' will not
80310:              * appear on the callstack.
71697:              *
71697:              *   (String.prototype.replace).call('a',/a/,function(){debugger});
71697:              *
71697:              * Function.prototype.call will however appear, hence the debugger
71697:              * can, by inspecting 'args.thisv', give some useful information.
80310:              *
80310:              * For Function.prototype.apply, the situation is even worse: since
80310:              * a dynamic number of arguments have been pushed onto the stack
80310:              * (see SplatApplyArgs), there is no efficient way to know how to
80310:              * find the callee. Thus, calls to apply are lost completely.
69223:              */
84195:             JSOp op = JSOp(*pc_);
71697:             if (op == JSOP_CALL || op == JSOP_FUNCALL) {
91237:                 unsigned argc = GET_ARGC(pc_);
91237:                 DebugOnly<unsigned> spoff = sp_ - fp_->base();
82642:                 JS_ASSERT_IF(cx_->stackIterAssertionEnabled,
72072:                              spoff == js_ReconstructStackDepth(cx_, fp_->script(), pc_));
71697:                 Value *vp = sp_ - (2 + argc);
71697: 
74739:                 CrashIfInvalidSlot(fp_, vp);
71697:                 if (IsNativeFunction(*vp)) {
71697:                     state_ = IMPLICIT_NATIVE;
71697:                     args_ = CallArgsFromVp(argc, vp);
71697:                     return;
71697:                 }
71697:             }
71697: 
71697:             state_ = SCRIPTED;
95385:             script_ = fp_->script();
80310:             JS_ASSERT_IF(op != JSOP_FUNAPPLY,
95385:                          sp_ >= fp_->base() && sp_ <= fp_->slots() + script_->nslots);
95385:             JS_ASSERT(pc_ >= script_->code && pc_ < script_->code + script_->length);
71697:             return;
71697:         }
71697: 
71697:         /*
71697:          * A CallArgsList element is pushed for any call to Invoke, regardless
71697:          * of whether the callee is a scripted function or even a callable
71697:          * object. Thus, it is necessary to filter calleev for natives.
71697:          *
71697:          * Second, stuff can happen after the args are pushed but before/after
71697:          * the actual call, so only consider "active" calls. (Since Invoke
71697:          * necessarily clobbers the callee, "active" is also necessary to
71697:          * ensure that the callee slot is valid.)
71697:          */
71697:         if (calls_->active() && IsNativeFunction(calls_->calleev())) {
71697:             state_ = NATIVE;
71697:             args_ = *calls_;
71697:             return;
71697:         }
71697: 
71697:         /* Pop the call and keep looking. */
71697:         popCall();
71697:     }
71697: }
71697: 
71697: StackIter::StackIter(JSContext *cx, SavedOption savedOption)
71697:   : cx_(cx),
71697:     savedOption_(savedOption)
71697: {
76193: #ifdef JS_METHODJIT
95016:     CompartmentVector &v = cx->runtime->compartments;
95016:     for (size_t i = 0; i < v.length(); i++)
95016:         mjit::ExpandInlineFrames(v[i]);
76193: #endif
76193: 
71697:     if (StackSegment *seg = cx->stack.seg_) {
71697:         startOnSegment(seg);
71697:         settleOnNewState();
69223:     } else {
71697:         state_ = DONE;
69223:     }
69223: }
71697: 
71697: StackIter &
71697: StackIter::operator++()
71697: {
71697:     switch (state_) {
71697:       case DONE:
95385:         JS_NOT_REACHED("Unexpected state");
71697:       case SCRIPTED:
71697:         popFrame();
71697:         settleOnNewState();
71697:         break;
71697:       case NATIVE:
71697:         popCall();
71697:         settleOnNewState();
71697:         break;
71697:       case IMPLICIT_NATIVE:
71697:         state_ = SCRIPTED;
71697:         break;
71697:     }
71363:     return *this;
71363: }
71363: 
71363: bool
71697: StackIter::operator==(const StackIter &rhs) const
71363: {
71697:     return done() == rhs.done() &&
71697:            (done() ||
71697:             (isScript() == rhs.isScript() &&
71697:              ((isScript() && fp() == rhs.fp()) ||
71697:               (!isScript() && nativeArgs().base() == rhs.nativeArgs().base()))));
69223: }
69223: 
95385: bool
95385: StackIter::isFunctionFrame() const
95385: {
95385:     switch (state_) {
95385:       case DONE:
95392:         break;
95385:       case SCRIPTED:
95385:         return fp()->isFunctionFrame();
95385:       case NATIVE:
95385:       case IMPLICIT_NATIVE:
95394:         return false;
95385:     }
95392:     JS_NOT_REACHED("Unexpected state");
95392:     return false;
95385: }
95385: 
95385: bool
95385: StackIter::isEvalFrame() const
95385: {
95385:     switch (state_) {
95385:       case DONE:
95392:         break;
95385:       case SCRIPTED:
95385:         return fp()->isEvalFrame();
95385:       case NATIVE:
95385:       case IMPLICIT_NATIVE:
95385:         return false;
95385:     }
95392:     JS_NOT_REACHED("Unexpected state");
95392:     return false;
95385: }
95385: 
95385: bool
95385: StackIter::isNonEvalFunctionFrame() const
95385: {
95385:     JS_ASSERT(!done());
95385:     switch (state_) {
95385:       case DONE:
95392:         break;
95385:       case SCRIPTED:
95385:         return fp()->isNonEvalFunctionFrame();
95385:       case NATIVE:
95385:       case IMPLICIT_NATIVE:
95385:         return !isEvalFrame() && isFunctionFrame();
95385:     }
95392:     JS_NOT_REACHED("Unexpected state");
95392:     return false;
95385: }
95385: 
95385: JSObject &
95385: StackIter::callee() const
95385: {
95385:     switch (state_) {
95385:       case DONE:
95392:         break;
95385:       case SCRIPTED:
95394:         JS_ASSERT(isFunctionFrame());
95385:         return fp()->callee();
95385:       case NATIVE:
95385:       case IMPLICIT_NATIVE:
95385:         return nativeArgs().callee();
95385:     }
95392:     JS_NOT_REACHED("Unexpected state");
95392:     return *(JSObject *) NULL;
95385: }
95385: 
95385: Value
95385: StackIter::calleev() const
95385: {
95385:     switch (state_) {
95385:       case DONE:
95392:         break;
95385:       case SCRIPTED:
95394:         JS_ASSERT(isFunctionFrame());
95385:         return fp()->calleev();
95385:       case NATIVE:
95385:       case IMPLICIT_NATIVE:
95385:         return nativeArgs().calleev();
95385:     }
95392:     JS_NOT_REACHED("Unexpected state");
95392:     return Value();
95385: }
95385: 
69223: /*****************************************************************************/
69223: 
71697: AllFramesIter::AllFramesIter(StackSpace &space)
71697:   : seg_(space.seg_),
71697:     fp_(seg_ ? seg_->maybefp() : NULL)
87854: {
87854:     settle();
87854: }
69223: 
69223: AllFramesIter&
69223: AllFramesIter::operator++()
69223: {
69223:     JS_ASSERT(!done());
69223:     fp_ = fp_->prev();
87854:     settle();
69223:     return *this;
69223: }
87854: 
87854: void
87854: AllFramesIter::settle()
87854: {
87854:     while (seg_ && (!fp_ || !seg_->contains(fp_))) {
87854:         seg_ = seg_->prevInMemory();
87854:         fp_ = seg_ ? seg_->maybefp() : NULL;
87854:     }
87854: 
87854:     JS_ASSERT(!!seg_ == !!fp_);
87854:     JS_ASSERT_IF(fp_, seg_->contains(fp_));
87854: }
