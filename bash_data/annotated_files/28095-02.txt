17368: /* -*- Mode: C++; c-basic-offset: 4; indent-tabs-mode: t; tab-width: 4 -*- */
17275: /* ***** BEGIN LICENSE BLOCK *****
17275:  * Version: MPL 1.1/GPL 2.0/LGPL 2.1
17275:  *
17275:  * The contents of this file are subject to the Mozilla Public License Version
17275:  * 1.1 (the "License"); you may not use this file except in compliance with
17275:  * the License. You may obtain a copy of the License at
17275:  * http://www.mozilla.org/MPL/
17275:  *
17275:  * Software distributed under the License is distributed on an "AS IS" basis,
17275:  * WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
17275:  * for the specific language governing rights and limitations under the
17275:  * License.
17275:  *
17275:  * The Original Code is [Open Source Virtual Machine].
17275:  *
17275:  * The Initial Developer of the Original Code is
17275:  * Adobe System Incorporated.
17275:  * Portions created by the Initial Developer are Copyright (C) 2004-2007
17275:  * the Initial Developer. All Rights Reserved.
17275:  *
17275:  * Contributor(s):
17275:  *   Adobe AS3 Team
17275:  *
17275:  * Alternatively, the contents of this file may be used under the terms of
17275:  * either the GNU General Public License Version 2 or later (the "GPL"), or
17275:  * the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
17275:  * in which case the provisions of the GPL or the LGPL are applicable instead
17275:  * of those above. If you wish to allow use of your version of this file only
17275:  * under the terms of either the GPL or the LGPL, and not to allow others to
17275:  * use your version of this file under the terms of the MPL, indicate your
17275:  * decision by deleting the provisions above and replace them with the notice
17275:  * and other provisions required by the GPL or the LGPL. If you do not delete
17275:  * the provisions above, a recipient may use your version of this file under
17275:  * the terms of any one of the MPL, the GPL or the LGPL.
17275:  *
17275:  * ***** END LICENSE BLOCK ***** */
17275: 
17275: #include "nanojit.h"
17275: #include <stdio.h>
20893: #include <ctype.h>
20893: 
20893: #ifdef PERFM
20893: #include "../vprof/vprof.h"
20893: #endif /* PERFM */
17275: 
17275: namespace nanojit
17275: {
17275:     using namespace avmplus;
17275: 	#ifdef FEATURE_NANOJIT
17275: 
17308: 	const uint8_t operandCount[] = {
21672: #define OPDEF(op, number, operands) \
21672:         operands,
21672: #define OPDEF64(op, number, operands) \
21672:         operands,
21672: #include "LIRopcode.tbl"
21672: #undef OPDEF
21672: #undef OPDEF64
21672:         0
17308: 	};
17275: 
17275: 	// LIR verbose specific
17275: 	#ifdef NJ_VERBOSE
17275: 
17308: 	const char* lirNames[] = {
21672: #define OPDEF(op, number, operands) \
21672:         #op,
21672: #define OPDEF64(op, number, operands) \
21672:         #op,
21672: #include "LIRopcode.tbl"
21672: #undef OPDEF
21672: #undef OPDEF64
21672:         NULL
17308: 	};
17275: 
17275: 	#endif /* NANOJIT_VEBROSE */
17275: 	
17275: 	// implementation
17275: 
17275: #ifdef NJ_PROFILE
17275: 	// @todo fixup move to nanojit.h
17275: 	#undef counter_value
17275: 	#define counter_value(x)		x
17275: #endif /* NJ_PROFILE */
17275: 
17275: 	//static int32_t buffer_count = 0;
17275: 	
17275: 	// LCompressedBuffer
17275: 	LirBuffer::LirBuffer(Fragmento* frago, const CallInfo* functions)
25097: 		: _frago(frago),
25097: #ifdef NJ_VERBOSE
25097: 		  names(NULL),
25097: #endif
25097: 		  _functions(functions), abi(ABI_FASTCALL),
25097: 		  state(NULL), param1(NULL), sp(NULL), rp(NULL),
25097: 		  _pages(frago->core()->GetGC())
17275: 	{
23264: 		rewind();
17275: 	}
17275: 
17275: 	LirBuffer::~LirBuffer()
17275: 	{
17275: 		clear();
22647: 		verbose_only(if (names) NJ_DELETE(names);)
17275: 		_frago = 0;
17275: 	}
17275: 	
17275: 	void LirBuffer::clear()
17275: 	{
17275: 		// free all the memory and clear the stats
22662: 		_frago->pagesRelease(_pages);
22662: 		NanoAssert(!_pages.size());
17275: 		_unused = 0;
17275: 		_stats.lir = 0;
17275: 		_noMem = 0;
23437: 		_nextPage = 0;
20946: 		for (int i = 0; i < NumSavedRegs; ++i)
21477: 			savedRegs[i] = NULL;
21477: 		explicitSavedRegs = false;
17275: 	}
17275: 
23264:     void LirBuffer::rewind()
23264: 	{
23264: 		clear();
23437: 		// pre-allocate the current and the next page we will be using
23264: 		Page* start = pageAlloc();
23264: 		_unused = start ? &start->lir[0] : NULL;
23437: 		_nextPage = pageAlloc();
23437: 		NanoAssert((_unused && _nextPage) || _noMem);
23264:     }
23264: 
20893: 	int32_t LirBuffer::insCount() 
20893: 	{
20893: 		// doesn't include embedded constants nor LIR_skip payload
17668: 		return _stats.lir;
17668: 	}
22668: 
20893: 	int32_t LirBuffer::byteCount() 
20893: 	{
22662: 		return ((_pages.size() ? _pages.size()-1 : 0) * sizeof(Page)) +
20893: 			((int32_t)_unused - (int32_t)pageTop(_unused));
17668: 	}
17668: 
17275: 	Page* LirBuffer::pageAlloc()
17275: 	{
17275: 		Page* page = _frago->pageAlloc();
17275: 		if (page)
22662: 			_pages.add(page);
17275: 		else
17275: 			_noMem = 1;
17275: 		return page;
17275: 	}
17275: 	
17275: 	LInsp LirBuffer::next()
17275: 	{
17275: 		return _unused;
17275: 	}
17275: 
22662: 	void LirBufWriter::ensureRoom(uint32_t count)
17275: 	{
28095: 		NanoAssert(count <= NJ_PAGE_SIZE - (LIR_FAR_SLOTS + 1) * sizeof(LIns));
22662: 		LInsp before = _buf->next();
22662: 		LInsp after = before+count+LIR_FAR_SLOTS;
22662: 		// transition to the next page?
22662: 		if (!samepage(before,after))
22662: 		{
23231: 			// we don't want this to fail, so we always have a page in reserve
23231: 			NanoAssert(_buf->_nextPage);
23231: 			_buf->_unused = &_buf->_nextPage->lir[0];	
22662: 			// link LIR stream back to prior instruction (careful insLink relies on _unused...)
22662: 			insLinkTo(LIR_skip, before-1);
23231: 			_buf->_nextPage = _buf->pageAlloc();
23231: 			NanoAssert(_buf->_nextPage || _buf->_noMem);
22662: 		}
17275: 	}
17275: 
22662: 	LInsp LirBufWriter::insLinkTo(LOpcode op, LInsp to)
17275: 	{
22662: 		LInsp l = _buf->next();
22662: 		NanoAssert(samepage(l,l+LIR_FAR_SLOTS)); // must have called ensureRoom()
22662:         if (can24bReach(l,to))
17275: 		{
27540: 		    NanoStaticAssert(LIR_nearskip == LIR_skip - 1);
27540: 		    NanoStaticAssert(LIR_neartramp == LIR_tramp - 1);
22662:             l->initOpcode(LOpcode(op-1)); // nearskip or neartramp
22662:             l->setimm24(to-l);
22662:             _buf->commit(1);
22662: 			_buf->_stats.lir++;
17275:         }
22662:         else
22662: 		{
22662: 			l = insLinkToFar(op,to);
22662: 		}
22662: 		return l;
22662: 	}
22662: 
22662: 	LInsp LirBufWriter::insLinkToFar(LOpcode op, LInsp to)
22662: 	{
22662: 		LirFarIns* ov = (LirFarIns*) _buf->next();
22662: 		ov->v = to;
22662: 		ov->i.initOpcode(op);
22662: 		_buf->commit(LIR_FAR_SLOTS);
22662: 		_buf->_stats.lir++;
22662: 
22662: 		NanoAssert( (LInsp)(ov+1) == _buf->next() );
22662: 		return &(ov->i);
22662: 	}
22662: 	
22662: 	void LirBufWriter::makeReachable(LInsp& o, LInsp from)
22662: 	{
22662: 		if (o && !can8bReach(from,o))
22662: 		{
22662: 			if (o == _buf->sp && spref && can8bReach(from, spref)) {
22662: 				o = spref;
22662: 				return;
22662: 			}
22662: 			if (o == _buf->rp && rpref && can8bReach(from, rpref)) {
22662: 				o = rpref;
22662: 				return;
22662: 			}
22662: 
22662: 			// need a trampoline to get to from
22662: 			LInsp tramp = insLinkTo(LIR_tramp, o);  // will produce neartramp if possible
22662: 			NanoAssert( tramp->ref() == o && samepage(from,tramp) );
22662: 			if (o == _buf->sp)
22662: 				spref = tramp;
22662: 			else if (o == _buf->rp)
22662: 				rpref = tramp;
22662: 			o = tramp;
22662: 		}
22662: 	}
22662: 
22662: 	void LirBufWriter::prepFor(LInsp& i1, LInsp& i2, LInsp& i3)
22662: 	{
22662: 		uint32_t i = 0;  // count of operands
22662: 		i += (i1) ? 1 : 0;
22662: 		i += (i2) ? 1 : 0;
22662: 		i += (i3) ? 1 : 0;
22662: 		
22662: 		uint32_t count = (LIR_FAR_SLOTS*i)+1;  // count of LIns if all operands require tramp
22662: 		ensureRoom(count);
22662: 		NanoAssert( samepage(_buf->next()+count,_buf->next()) );
22662: 		
22662: 		// guaranteed space for far tramps if necc.
22662: 		LInsp from = _buf->next()+count;
22662: 		makeReachable(i1, from);
22662: 		makeReachable(i2, from);
22662: 		makeReachable(i3, from);
22662: 		NanoAssert(from>i1 && from>i2 && from>i3);
17275: 	}
17275: 
17275: 	LInsp LirBuffer::commit(uint32_t count)
17275: 	{
17275: 		NanoAssertMsg( samepage(_unused, _unused+count), "You need to call ensureRoom first!" );
17275: 		return _unused += count;
17275: 	}
17275: 	
17687: 	uint32_t LIns::reference(LIns *r) const
17275: 	{
17275: 		int delta = this-r-1;
17275: 		NanoAssert(isU8(delta));
17275: 		return delta;
17275: 	}
17275: 
17687:     LIns* LIns::deref(int32_t off) const
17687:     {
17687: 		LInsp i = (LInsp) this-1 - off;
20893:         while (i && i->isTramp()) {
17687:             i = i->ref();
20893:         }
17687: 		return i;
17687:     }
17687: 
17308: 	LInsp LirBufWriter::insStore(LInsp val, LInsp base, LInsp off)
17275: 	{
17308: 		LOpcode op = val->isQuad() ? LIR_stq : LIR_st;
17308: 		NanoAssert(val && base && off);
22662: 		prepFor(val, base, off);
17275: 		LInsp l = _buf->next();
17275: 		l->initOpcode(op);
22662: 		l->setOprnd1(val);
22662: 		l->setOprnd2(base);
22662: 		l->setOprnd3(off);
17275: 		_buf->commit(1);
17275: 		_buf->_stats.lir++;
17275: 		return l;
17275: 	}
17275: 	
17308: 	LInsp LirBufWriter::insStorei(LInsp val, LInsp base, int32_t d)
17275: 	{
17308: 		LOpcode op = val->isQuad() ? LIR_stqi : LIR_sti;
17308: 		NanoAssert(val && base && isS8(d));
22662: 		LInsp u3=0;
22662: 		prepFor(val, base, u3);
17275: 		LInsp l = _buf->next();
17275: 		l->initOpcode(op);
22662: 		l->setOprnd1(val);
22662: 		l->setOprnd2(base);
17275: 		l->setDisp(int8_t(d));
17275: 		_buf->commit(1);
17275: 		_buf->_stats.lir++;
17275: 		return l;
17275: 	}
17275: 
17275: 	LInsp LirBufWriter::ins0(LOpcode op)
17275: 	{
17308: 		ensureRoom(1);
20893:         LirBuffer *b = this->_buf;
20893: 		LInsp l = b->next();
17275: 		l->initOpcode(op);
20893: 		b->commit(1);
20893: 		b->_stats.lir++;
17275: 		return l;
17275: 	}
17275: 	
17275: 	LInsp LirBufWriter::ins1(LOpcode op, LInsp o1)
17275: 	{
22662: 		LInsp u2=0,u3=0;
22662: 		prepFor(o1,u2,u3);
17275: 		LInsp l = _buf->next();
17275: 		l->initOpcode(op);
22662: 		l->setOprnd1(o1);
17275: 		_buf->commit(1);
17275: 		_buf->_stats.lir++;
17275: 		return l;
17275: 	}
17275: 	
17275: 	LInsp LirBufWriter::ins2(LOpcode op, LInsp o1, LInsp o2)
17275: 	{
22662: 		LInsp u3=0;
22662: 		prepFor(o1,o2,u3);
17275: 		LInsp l = _buf->next();
17275: 		l->initOpcode(op);
22662: 		l->setOprnd1(o1);
22662: 		l->setOprnd2(o2);		
17275: 		_buf->commit(1);
17275: 		_buf->_stats.lir++;
17275: 		return l;
17275: 	}
17275: 
17275: 	LInsp LirBufWriter::insLoad(LOpcode op, LInsp base, LInsp d)
17275: 	{
17275: 		return ins2(op,base,d);
17275: 	}
17275: 
20931: 	LInsp LirBufWriter::insGuard(LOpcode op, LInsp c, LInsp data)
17275: 	{
17275: 		return ins2(op, c, data);
17275: 	}
17275: 
20893: 	LInsp LirBufWriter::insBranch(LOpcode op, LInsp condition, LInsp toLabel)
17275: 	{
20893: 		if (!toLabel)
20893: 			toLabel = insFar(LIR_tramp,0); //empty tramp
20893:         if (!condition) {
20893:             // unconditional, just point to something
20893:             condition = toLabel;
20893:         }
20893: 	    return ins2(op,condition,toLabel);
20893: 	}
20893: 
20893:     LInsp LirBufWriter::insAlloc(int32_t size)
20893:     {
20893:         size = (size+3)>>2; // # of required 32bit words
20893:         NanoAssert(isU16(size));
17275: 		ensureRoom(1);
17275: 		LInsp l = _buf->next();
20893: 		l->initOpcode(LIR_alloc);
20893: 		l->i.imm16 = uint16_t(size);
17275: 		_buf->commit(1);
17275: 		_buf->_stats.lir++;
17275: 		return l;
17275:     }
17275: 
20893:     LInsp LirBufWriter::insParam(int32_t arg, int32_t kind)
20893:     {
20893: 		ensureRoom(1);
20893:         LirBuffer *b = this->_buf;
20893: 		LInsp l = b->next();
20893: 		l->initOpcode(LIR_param);
20893:         NanoAssert(isU8(arg) && isU8(kind));
20893: 		l->c.imm8a = arg;
20893:         l->c.imm8b = kind;
20893:         if (kind) {
20893:             NanoAssert(arg < NumSavedRegs);
21477:             b->savedRegs[arg] = l;
21477:             b->explicitSavedRegs = true;
20893:         }
20893: 		b->commit(1);
20893: 		b->_stats.lir++;
20893: 		return l;
20893:     }
20893: 	
17687: 	LInsp LirBufWriter::insFar(LOpcode op, LInsp target)
17275: 	{
22662: 		ensureRoom(LIR_FAR_SLOTS);  // make room for it
22662: 		LInsp l = insLinkToFar(op, target);
20893: 		_buf->_stats.lir++;
17275: 		return l;
17275: 	}
18633: 	
17275: 	LInsp LirBufWriter::insImm(int32_t imm)
17275: 	{
17275: 		if (isS16(imm)) {
17275: 			ensureRoom(1);
17275: 			LInsp l = _buf->next();
17275: 			l->initOpcode(LIR_short);
17275: 			l->setimm16(imm);
17275: 			_buf->commit(1);
17275: 			_buf->_stats.lir++;
17275: 			return l;
17275: 		} else {
22662: 			ensureRoom(LIR_IMM32_SLOTS);
22662: 			LirImm32Ins* l = (LirImm32Ins*)_buf->next();
22662: 			l->v = imm;
22662: 			l->i.initOpcode(LIR_int);
22662: 			_buf->commit(LIR_IMM32_SLOTS);	
22662: 			_buf->_stats.lir++;
22662: 			NanoAssert((LInsp)(l+1)==_buf->next());
22662: 			return &(l->i);
17275: 		}
17275: 	}
17275: 	
17275: 	LInsp LirBufWriter::insImmq(uint64_t imm)
17275: 	{
22662: 		ensureRoom(LIR_IMM64_SLOTS);
22662: 		LirImm64Ins* l = (LirImm64Ins*)_buf->next();
22662: 		l->v[0] = int32_t(imm);
22662: 		l->v[1] = int32_t(imm>>32);
22662: 		l->i.initOpcode(LIR_quad);
22662: 		_buf->commit(LIR_IMM64_SLOTS);	
22662: 		_buf->_stats.lir++;
22662: 		NanoAssert((LInsp)(l+1)==_buf->next());
22662: 		return &(l->i);
17275: 	}
17275: 
17275: 	LInsp LirBufWriter::skip(size_t size)
17275: 	{
17275:         const uint32_t n = (size+sizeof(LIns)-1)/sizeof(LIns);
22662: 		ensureRoom(n); // make room for it
22662:  		LInsp last = _buf->next()-1;  // safe, next()-1+n guaranteed to be on same page
17275: 		_buf->commit(n);
22662: 		NanoAssert(samepage(last,_buf->next()));
22662: 		ensureRoom(LIR_FAR_SLOTS);
22662: 		return insLinkTo(LIR_skip, last);
17275: 	}
17275: 
17275: 	LInsp LirReader::read()	
17275: 	{
17275: 		LInsp cur = _i;
17275: 		if (!cur)
17275: 			return 0;
17275: 		LIns* i = cur;
17275: 		LOpcode iop = i->opcode();
17275: 		do
17275: 		{
17275: 			switch (iop)
17275: 			{					
17275: 				default:
17275: 					i--;
17275: 					break;
17275: 
18254: #if defined NANOJIT_64BIT
18254:             	case LIR_callh:
18254: #endif
17687: 				case LIR_call:
17687: 				case LIR_fcall:
20893:                 case LIR_calli:
20893:                 case LIR_fcalli:
22662: 					NanoAssert( samepage(i,i+1-i->callInsWords()) );
20408: 					i -= i->callInsWords();
17275: 					break;
17275: 
17687: 				case LIR_skip:
17687: 				case LIR_nearskip:
17687: 					NanoAssert(i->ref() != i);
17687: 					i = i->ref();
17687: 					break;
17687: 
17687:                 case LIR_tramp:
22662:                     NanoAssert(samepage(i,i+1-LIR_FAR_SLOTS));
22662: 					i -= LIR_FAR_SLOTS;
18633:                     break;
18633: 
17275: 				case LIR_int:
22662:                     NanoAssert(samepage(i,i+1-LIR_IMM32_SLOTS));
22662: 					i -= LIR_IMM32_SLOTS;					
17275: 					break;
17275: 
17275: 				case LIR_quad:
22662:                     NanoAssert(samepage(i,i+1-LIR_IMM64_SLOTS));
22662: 					i -= LIR_IMM64_SLOTS;					
17275: 					break;
17275: 
20893: 				case LIR_start:
17275: 					_i = 0;  // start of trace
17275: 					return cur;
17275: 			}
17275: 			iop = i->opcode();
17275: 		}
17275: 		while (is_trace_skip_tramp(iop)||iop==LIR_2);
17275: 		_i = i;
17275: 		return cur;
17275: 	}
17275: 
17275: 	bool FASTCALL isCmp(LOpcode c) {
25469: 		return (c >= LIR_eq && c <= LIR_uge) || (c >= LIR_feq && c <= LIR_fge);
17275: 	}
17275:     
17368: 	bool FASTCALL isCond(LOpcode c) {
17368: 		return (c == LIR_ov) || (c == LIR_cs) || isCmp(c);
17368: 	}
17368: 
20893:     bool FASTCALL isFloat(LOpcode c) {
20893:         switch (c) {
20893:             default:
20893:                 return false;
20893:             case LIR_fadd:
20893:             case LIR_fsub:
20893:             case LIR_fmul:
20893:             case LIR_fdiv:
20893:             case LIR_fneg:
20893:             case LIR_fcall:
20893:             case LIR_fcalli:
20893:             case LIR_i2f:
20893:             case LIR_u2f:
20893:                 return true;
20893:         }
20893:     }
20893:     
17275: 	bool LIns::isCmp() const {
17275: 		return nanojit::isCmp(u.code);
17275: 	}
17275: 
17368:     bool LIns::isCond() const {
17368:         return nanojit::isCond(u.code);
17368:     }
17368: 	
18254: 	bool LIns::isQuad() const {
22664: 		#ifdef AVMPLUS_64BIT
22664: 			// callh in 64bit cpu's means a call that returns an int64 in a single register
22664: 			return (u.code & LIR64) != 0 || u.code == LIR_callh;
22664: 		#else
22664: 			// callh in 32bit cpu's means the 32bit MSW of an int64 result in 2 registers
22664: 			return (u.code & LIR64) != 0;
22664: 		#endif
18254: 	}
18254:     
17275: 	bool LIns::isconstval(int32_t val) const
17275: 	{
17275: 		return isconst() && constval()==val;
17275: 	}
17275: 
17275: 	bool LIns::isconstq() const
17275: 	{	
17275: 		return isop(LIR_quad);
17275: 	}
17275: 
18113: 	bool LIns::isconstp() const
18113: 	{
18113:     #ifdef AVMPLUS_64BIT
18113: 	    return isconstq();
18113: 	#else
18113: 	    return isconst();
18113:     #endif
18113: 	}
18113: 
17275: 	bool FASTCALL isCse(LOpcode op) {
17275: 		op = LOpcode(op & ~LIR64);
21469: 		return op >= LIR_ldcs && op <= LIR_uge;
17275: 	}
17275: 
17275:     bool LIns::isCse(const CallInfo *functions) const
17275:     { 
25469: 		return nanojit::isCse(u.code) || (isCall() && callInfo()->_cse);
17275:     }
17275: 
17275: 	void LIns::setimm16(int32_t x)
17275: 	{
17275: 		NanoAssert(isS16(x));
17275: 		i.imm16 = int16_t(x);
17275: 	}
17275: 
20893: 	void LIns::setimm24(int32_t x)
20893: 	{
20893: 		NanoAssert(isS24(x));
20893: 		t.imm24 = x;
20893: 	}
20893: 
17275: 	void LIns::setresv(uint32_t resv)
17275: 	{
17275: 		NanoAssert(isU8(resv));
17275: 		g.resv = resv;
17275: 	}
17275: 
17275: 	void LIns::initOpcode(LOpcode op)
17275: 	{
17687: 		i.code = op;
17687: 		i.imm16 = 0;
17687:         i.resv = 0;
17275: 	}
17275: 
17275: 	void LIns::setOprnd1(LInsp r)
17275: 	{
17275: 		u.oprnd_1 = reference(r);
17275: 	}
17275: 
17275: 	void LIns::setOprnd2(LInsp r)
17275: 	{
17275: 		u.oprnd_2 = reference(r);
17275: 	}
17275: 
17275: 	void LIns::setOprnd3(LInsp r)
17275: 	{
17275: 		u.oprnd_3 = reference(r);
17275: 	}
17275: 
17275:     void LIns::setDisp(int8_t d)
17275:     {
17275:         sti.disp = d;
17275:     }
17275: 
20893:     LIns **LIns::targetAddr() {
20893: 		NanoAssert(isBranch());
20893: 		LInsp i = (LInsp) this-1 - u.oprnd_2;
20893:         NanoAssert(i->isTramp());
20893:         LInsp ref;
20893:         while ((ref=i->ref()) != 0 && ref->isTramp())
20893:             i = ref;
20893: 		NanoAssert(i->isop(LIR_tramp));
22662: 		LirFarIns* ov = (LirFarIns*)(i-LIR_FAR_SLOTS+1);
22662: 		return &(ov->v);
20893:     }
20893: 
20893:     void LIns::target(LInsp label) {
20893:         NanoAssert(label && label->isop(LIR_label));
20893:         *(targetAddr()) = label;
20893: 	}
20893: 
20893: 	LInsp LIns::getTarget()
20893: 	{
20893:         NanoAssert(isBranch());
20893:         return oprnd2();
20893: 	}
20893: 
17275: 	LInsp	LIns::oprnd1() const	
17275: 	{
17687:         return deref(u.oprnd_1);
17275: 	}
17275: 	
17275: 	LInsp	LIns::oprnd2() const
17275: 	{ 
17687:         return deref(u.oprnd_2);
17275: 	}
17275: 
17275: 	LInsp	LIns::oprnd3() const
17275: 	{ 
17687:         return deref(u.oprnd_3);
17275: 	}
17275: 
17275:     void *LIns::payload() const
17275:     {
17687:         NanoAssert(opcode()==LIR_skip || opcode()==LIR_nearskip);
17687:         return (void*) (ref()+1);
17275:     }
17275: 
22662: 	LIns* LIns::ref() const	
22662: 	{ 
22662: 		LIns const *r = 0;
22662: 		if (t.code&1)
22662: 			r = this + t.imm24;
22662: 		else
22662: 		{
22662: 			LirFarIns* l = (LirFarIns*)(this-LIR_FAR_SLOTS+1);
22662: 			r = l->v;
22662: 		}
22662: 		return (const LInsp)r;
22662: 	}
22662: 
22662: 	int32_t LIns::imm32() const	
22662: 	{ 
22662: 		LirImm32Ins* l = (LirImm32Ins*)(this-LIR_IMM32_SLOTS+1);
22662: 		return l->v; 
22662: 	}
22662: 
22662: 	uint64_t LIns::constvalq() const
22662: 	{
22662: 		LirImm64Ins* l = (LirImm64Ins*)(this-LIR_IMM64_SLOTS+1);
22662:     #ifdef AVMPLUS_UNALIGNED_ACCESS
22662:         int* ptr = (int*)l->v;
22662:         return *(const uint64_t*)ptr;
22662:     #else
22662:         union { uint64_t tmp; int32_t dst[2]; } u;
25109: 		#ifdef AVMPLUS_BIG_ENDIAN
25109:         u.dst[0] = l->v[1];
25109:         u.dst[1] = l->v[0];
25109: 		#else
22662:         u.dst[0] = l->v[0];
22662:         u.dst[1] = l->v[1];
25109: 		#endif
22662:         return u.tmp;
22662:     #endif
22662: 	}
22662: 
22662: 	double LIns::constvalf() const
22662: 	{
22662: 		LirImm64Ins* l = (LirImm64Ins*)(this-LIR_IMM64_SLOTS+1);
22662: 		NanoAssert(isconstq());
22662: 	#ifdef AVMPLUS_UNALIGNED_ACCESS
22662:         int* ptr = (int*)l->v;
22662: 		return *(const double*)ptr;
22662: 	#else
22662: 		union { uint32_t dst[2]; double tmpf; } u;
25109: 		#ifdef AVMPLUS_BIG_ENDIAN
25109: 		u.dst[0] = l->v[1];
25109: 		u.dst[1] = l->v[0];
25109: 		#else
22662: 		u.dst[0] = l->v[0];
22662: 		u.dst[1] = l->v[1];
25109: 		#endif
22662: 		return u.tmpf;
22662: 	#endif
22662: 	}
22662: 
22662: 	size_t LIns::callInsWords() const
22662: 	{
22662: 		return LIR_CALL_SLOTS + argwords(argc());
22662: 	}
22662: 
22662: 	const CallInfo* LIns::callInfo() const
22662: 	{
22662: 		LirCallIns* l = (LirCallIns*)(this-LIR_CALL_SLOTS+1);
22662: 		return l->ci; 
22662: 	}
22662: 
22662: 	// index args in r-l order.  arg(0) is rightmost arg
22662: 	LIns* LIns::arg(uint32_t i) 
22662: 	{
22662: 		NanoAssert(i < argc());
22662: 		LirCallIns* l = (LirCallIns*)(this-LIR_CALL_SLOTS+1);
22662: 		uint8_t* offs = (uint8_t*)l - (i+1);
22662: 		return deref(*offs);
22662: 	}
22662: 
17275:     LIns* LirWriter::ins2i(LOpcode v, LIns* oprnd1, int32_t imm)
17275:     {
17275:         return ins2(v, oprnd1, insImm(imm));
17275:     }
17275: 
17275:     bool insIsS16(LInsp i)
17275:     {
17275:         if (i->isconst()) {
17275:             int c = i->constval();
17275:             return isS16(c);
17275:         }
18254:         if (i->isop(LIR_cmov) || i->isop(LIR_qcmov)) {
17275:             LInsp vals = i->oprnd2();
17275:             return insIsS16(vals->oprnd1()) && insIsS16(vals->oprnd2());
17275:         }
17275:         if (i->isCmp())
17275:             return true;
17275:         // many other possibilities too.
17275:         return false;
17275:     }
17275: 
17275: 	LIns* ExprFilter::ins1(LOpcode v, LIns* i)
17275: 	{
17275: 		if (v == LIR_qlo) {
17275: 			if (i->isconstq())
17275: 				return insImm(int32_t(i->constvalq()));
17275: 			if (i->isop(LIR_qjoin))
17275: 				return i->oprnd1();
17275: 		}
17275: 		else if (v == LIR_qhi) {
17275: 			if (i->isconstq())
17275: 				return insImm(int32_t(i->constvalq()>>32));
17275: 			if (i->isop(LIR_qjoin))
17275: 				return i->oprnd2();
17275: 		}
20893: 		else if (i->isconst()) {
20893: 			int32_t c = i->constval();
20893: 			if (v == LIR_neg)
20893: 				return insImm(-c);
20893: 			if (v == LIR_not)
20893: 				return insImm(~c);
20893: 		}
17275: 		else if (v == i->opcode() && (v == LIR_not || v == LIR_neg || v == LIR_fneg)) {
20893:             // not(not(x)) = x;  neg(neg(x)) = x;  fneg(fneg(x)) = x;
17275: 			return i->oprnd1();
17275: 		}
20893:         /* [ed 8.27.08] this causes a big slowdown in gameoflife.as.  why?
20893:         else if (i->isconst()) {
20893:             if (v == LIR_i2f) {
20893:                 return insImmf(i->constval());
20893:             }
20893:             else if (v == LIR_u2f) {
20893:                 return insImmf((uint32_t)i->constval());
20893:             }
20893:         }*/
17275: 
17275: 		// todo
17275: 		// -(a-b) = b-a
17275: 
17275: 		return out->ins1(v, i);
17275: 	}
17275: 
17275:     LIns* ExprFilter::ins2(LOpcode v, LIns* oprnd1, LIns* oprnd2)
17275: 	{
17275: 		NanoAssert(oprnd1 && oprnd2);
18254: 		if (v == LIR_cmov || v == LIR_qcmov) {
17275: 			if (oprnd2->oprnd1() == oprnd2->oprnd2()) {
17275: 				// c ? a : a => a
17275: 				return oprnd2->oprnd1();
17275: 			}
19998: 			if (oprnd1->isconst()) {
19998: 			    // const ? x : y => return x or y depending on const
19998: 			    return oprnd1->constval() ? oprnd2->oprnd1() : oprnd2->oprnd2();
19998: 			}
17275: 		}
17275: 		if (oprnd1 == oprnd2)
17275: 		{
17275: 			if (v == LIR_xor || v == LIR_sub ||
17308: 				v == LIR_ult || v == LIR_ugt || v == LIR_gt || v == LIR_lt)
17275: 				return insImm(0);
17275: 			if (v == LIR_or || v == LIR_and)
17275: 				return oprnd1;
17308: 			if (v == LIR_le || v == LIR_ule || v == LIR_ge || v == LIR_uge) {
17275: 				// x <= x == 1; x >= x == 1
17275: 				return insImm(1);
17275: 			}
17275: 		}
17275: 		if (oprnd1->isconst() && oprnd2->isconst())
17275: 		{
28094: 			int32_t c1 = oprnd1->constval();
28094: 			int32_t c2 = oprnd2->constval();
28094: 			double d;
28094: 			int32_t r;
17275: 			if (v == LIR_qjoin) {
17275: 				uint64_t q = c1 | uint64_t(c2)<<32;
17275: 				return insImmq(q);
17275: 			}
28094: 			switch (v) {
28094: 			case LIR_eq:
17275: 				return insImm(c1 == c2);
28094: 			case LIR_ov:
17368:                 return insImm((c2 != 0) && ((c1 + c2) <= c1)); 
28094: 			case LIR_cs:
17368:                 return insImm((c2 != 0) && ((uint32_t(c1) + uint32_t(c2)) <= uint32_t(c1)));
28094: 			case LIR_lt:
17275: 				return insImm(c1 < c2);
28094: 			case LIR_gt:
17275: 				return insImm(c1 > c2);
28094: 			case LIR_le:
17275: 				return insImm(c1 <= c2);
28094: 			case LIR_ge:
17275: 				return insImm(c1 >= c2);
28094: 			case LIR_ult:
17275: 				return insImm(uint32_t(c1) < uint32_t(c2));
28094: 			case LIR_ugt:
17275: 				return insImm(uint32_t(c1) > uint32_t(c2));
28094: 			case LIR_ule:
17275: 				return insImm(uint32_t(c1) <= uint32_t(c2));
28094: 			case LIR_uge:
17275: 				return insImm(uint32_t(c1) >= uint32_t(c2));
28094: 			case LIR_rsh:
17275: 				return insImm(int32_t(c1) >> int32_t(c2));
28094: 			case LIR_lsh:
17275: 				return insImm(int32_t(c1) << int32_t(c2));
28094: 			case LIR_ush:
17275: 				return insImm(uint32_t(c1) >> int32_t(c2));
28094: 			case LIR_or:
19998:                 return insImm(uint32_t(c1) | int32_t(c2));
28094: 			case LIR_and:
19998:                 return insImm(uint32_t(c1) & int32_t(c2));
28094: 			case LIR_xor:
19998:                 return insImm(uint32_t(c1) ^ int32_t(c2));
28094: 			case LIR_add:
28094: 			    d = double(c1) + double(c2);
28094: 			fold:
28094: 			    r = int32_t(d);
28094: 			    if (r == d)
28094:                     return insImm(r);
28094: 			    break;
28094: 			case LIR_sub:
28094: 			    d = double(c1) - double(c2);
28094: 			    goto fold;
28094: 			case LIR_mul:
28094: 			    d = double(c1) * double(c2);
28094: 			    goto fold;
28094: 			default:
28094: 			    ;
28094: 			}
17275: 		}
17275: 		else if (oprnd1->isconstq() && oprnd2->isconstq())
17275: 		{
17275: 			double c1 = oprnd1->constvalf();
20893: 			double c2 = oprnd2->constvalf();
28094: 			switch (v) {
28094: 			case LIR_feq:
17275: 				return insImm(c1 == c2);
28094: 			case LIR_flt:
17275: 				return insImm(c1 < c2);
28094: 			case LIR_fgt:
17275: 				return insImm(c1 > c2);
28094: 			case LIR_fle:
17275: 				return insImm(c1 <= c2);
28094: 			case LIR_fge:
17275: 				return insImm(c1 >= c2);
28094:             case LIR_fadd:
28094:                 return insImmf(c1 + c2);
28094:             case LIR_fsub:
28094:                 return insImmf(c1 - c2);
28094:             case LIR_fmul:
28094:                 return insImmf(c1 * c2);
28094:             case LIR_fdiv:
28094:                 return insImmf(c1 / c2);
28094:             default:
28094:                 ;
28094: 			}
17275: 		}
17275: 		else if (oprnd1->isconst() && !oprnd2->isconst())
17275: 		{
20893: 			if (v == LIR_add || v == LIR_addp || v == LIR_mul ||
17275: 				v == LIR_fadd || v == LIR_fmul ||
17275: 				v == LIR_xor || v == LIR_or || v == LIR_and ||
17275: 				v == LIR_eq) {
17275: 				// move const to rhs
17275: 				LIns* t = oprnd2;
17275: 				oprnd2 = oprnd1;
17275: 				oprnd1 = t;
17275: 			}
17308: 			else if (v >= LIR_lt && v <= LIR_uge) {
21672: 				NanoStaticAssert((LIR_lt ^ 1) == LIR_gt);
21672: 				NanoStaticAssert((LIR_le ^ 1) == LIR_ge);
21672: 				NanoStaticAssert((LIR_ult ^ 1) == LIR_ugt);
21672: 				NanoStaticAssert((LIR_ule ^ 1) == LIR_uge);
21672: 
17275: 				// move const to rhs, swap the operator
17275: 				LIns *t = oprnd2;
17275: 				oprnd2 = oprnd1;
17275: 				oprnd1 = t;
17275: 				v = LOpcode(v^1);
17275: 			}
17275: 		}
17275: 
17275: 		if (oprnd2->isconst())
17275: 		{
17275: 			int c = oprnd2->constval();
17275: 			if (v == LIR_add && oprnd1->isop(LIR_add) && oprnd1->oprnd2()->isconst()) {
17275: 				// add(add(x,c1),c2) => add(x,c1+c2)
17275: 				c += oprnd1->oprnd2()->constval();
17275: 				oprnd2 = insImm(c);
17275: 				oprnd1 = oprnd1->oprnd1();
17275: 			}
17275: 			else if (v == LIR_sub && oprnd1->isop(LIR_add) && oprnd1->oprnd2()->isconst()) {
17275: 				// sub(add(x,c1),c2) => add(x,c1-c2)
17275: 				c = oprnd1->oprnd2()->constval() - c;
17275: 				oprnd2 = insImm(c);
17275: 				oprnd1 = oprnd1->oprnd1();
17275: 				v = LIR_add;
17275: 			}
17275: 			else if (v == LIR_rsh && c == 16 && oprnd1->isop(LIR_lsh) &&
17275: 					 oprnd1->oprnd2()->isconstval(16)) {
17275: 				if (insIsS16(oprnd1->oprnd1())) {
17275: 					// rsh(lhs(x,16),16) == x, if x is S16
17275: 					return oprnd1->oprnd1();
17275: 				}
17275: 			}
17275: 			else if (v == LIR_ult) {
18254: 				if (oprnd1->isop(LIR_cmov) || oprnd1->isop(LIR_qcmov)) {
17275: 					LInsp a = oprnd1->oprnd2()->oprnd1();
17275: 					LInsp b = oprnd1->oprnd2()->oprnd2();
17275: 					if (a->isconst() && b->isconst()) {
17275: 						bool a_lt = uint32_t(a->constval()) < uint32_t(oprnd2->constval());
17275: 						bool b_lt = uint32_t(b->constval()) < uint32_t(oprnd2->constval());
17275: 						if (a_lt == b_lt)
17275: 							return insImm(a_lt);
17275: 					}
17275: 				}
17275: 			}
17275: 
17275: 			if (c == 0)
17275: 			{
20893: 				if (v == LIR_add || v == LIR_addp || v == LIR_or || v == LIR_xor ||
17275: 					v == LIR_sub || v == LIR_lsh || v == LIR_rsh || v == LIR_ush)
17275: 					return oprnd1;
17275: 				else if (v == LIR_and || v == LIR_mul)
17275: 					return oprnd2;
17275: 				else if (v == LIR_eq && oprnd1->isop(LIR_or) && 
17275: 					oprnd1->oprnd2()->isconst() &&
17275: 					oprnd1->oprnd2()->constval() != 0) {
17275: 					// (x or c) != 0 if c != 0
17275: 					return insImm(0);
17275: 				}
17275: 			}
25469: 			else if (c == -1 || (c == 1 && oprnd1->isCmp())) {
17275: 				if (v == LIR_or) {
17275: 					// x | -1 = -1, cmp | 1 = 1
17275: 					return oprnd2;
17275: 				}
17275: 				else if (v == LIR_and) {
17275: 					// x & -1 = x, cmp & 1 = cmp
17275: 					return oprnd1;
17275: 				}
17275: 			}
17275: 		}
17275: 
17275: 		LInsp i;
17275: 		if (v == LIR_qjoin && oprnd1->isop(LIR_qlo) && oprnd2->isop(LIR_qhi) 
18263: 			&& (i = oprnd1->oprnd1()) == oprnd2->oprnd1()) {
17275: 			// qjoin(qlo(x),qhi(x)) == x
17275: 			return i;
17275: 		}
17275: 
17275: 		return out->ins2(v, oprnd1, oprnd2);
17275: 	}
17275: 
20931: 	LIns* ExprFilter::insGuard(LOpcode v, LInsp c, LInsp x)
17275: 	{
17516: 		if (v == LIR_xt || v == LIR_xf) {
17275: 			if (c->isconst()) {
25469: 				if ((v == LIR_xt && !c->constval()) || (v == LIR_xf && c->constval())) {
17275: 					return 0; // no guard needed
17275: 				}
17275: 				else {
18131: #ifdef JS_TRACER
25108: 					// We're emitting a guard that will always fail. Any code
25108: 					// emitted after this guard is dead code. We could
25108: 					// silently optimize out the rest of the emitted code, but
25108: 					// this could indicate a performance problem or other bug,
25108: 					// so assert in debug builds.
25108: 					NanoAssertMsg(0, "Constantly false guard detected");
18131: #endif
18130: 					return out->insGuard(LIR_x, out->insImm(1), x);
17275: 				}
17275: 			}
17275: 			else {
26665: 				NanoStaticAssert((LIR_xt ^ 1) == LIR_xf);
17275: 				while (c->isop(LIR_eq) && c->oprnd1()->isCmp() && 
17275: 					c->oprnd2()->isconstval(0)) {
17275: 				    // xt(eq(cmp,0)) => xf(cmp)   or   xf(eq(cmp,0)) => xt(cmp)
17275: 				    v = LOpcode(v^1);
17275: 				    c = c->oprnd1();
17275: 				}
17275: 			}
17275: 		}
17275: 		return out->insGuard(v, c, x);
17275: 	}
17275: 
20893:     LIns* ExprFilter::insBranch(LOpcode v, LIns *c, LIns *t)
20893:     {
20893:         if (v == LIR_jt || v == LIR_jf) {
20893:             while (c->isop(LIR_eq) && c->oprnd1()->isCmp() && c->oprnd2()->isconstval(0)) {
20893:                 // jt(eq(cmp,0)) => jf(cmp)   or   jf(eq(cmp,0)) => jt(cmp)
20893:                 v = LOpcode(v ^ 1);
20893:                 c = c->oprnd1();
20893:             }
20893:         }
20893:         return out->insBranch(v, c, t);
20893:     }
20893: 
17275:     LIns* LirWriter::insLoadi(LIns *base, int disp) 
17275:     { 
17275:         return insLoad(LIR_ld,base,disp);
17275:     }
17275: 
17275: 	LIns* LirWriter::insLoad(LOpcode op, LIns *base, int disp)
17275: 	{
17275: 		return insLoad(op, base, insImm(disp));
17275: 	}
17275: 
20893:     LIns* LirWriter::store(LInsp value, LInsp base, int32_t d)
20893:     {
20893: 		return isS8(d) ? insStorei(value, base, d)
20893: 			: insStore(value, base, insImm(d));
20893:     }
20893: 
17275: 	LIns* LirWriter::ins_eq0(LIns* oprnd1)
17275: 	{
17275: 		return ins2i(LIR_eq, oprnd1, 0);
17275: 	}
17275: 
20893:     LIns* LirWriter::insImmf(double f)
20893:     {
20893:         union {
20893:             double f;
20893:             uint64_t q;
20893:         } u;
20893:         u.f = f;
20893:         return insImmq(u.q);
20893:     }
20893: 
17275: 	LIns* LirWriter::qjoin(LInsp lo, LInsp hi)
17275: 	{
17275: 		return ins2(LIR_qjoin, lo, hi);
17275: 	}
17275: 
17368: 	LIns* LirWriter::insImmPtr(const void *ptr)
17368: 	{
17368: 		return sizeof(ptr) == 8 ? insImmq((uintptr_t)ptr) : insImm((intptr_t)ptr);
17368: 	}
17368: 
19997: 	LIns* LirWriter::ins_choose(LIns* cond, LIns* iftrue, LIns* iffalse)
17275: 	{
17275: 		// if not a conditional, make it implicitly an ==0 test (then flop results)
17275: 		if (!cond->isCmp())
17275: 		{
17275: 			cond = ins_eq0(cond);
17275: 			LInsp tmp = iftrue;
17275: 			iftrue = iffalse;
17275: 			iffalse = tmp;
17275: 		}
17275: 
22706: 		if (true/*avmplus::AvmCore::use_cmov()*/)
17275: 		{
18656: 			return ins2((iftrue->isQuad() || iffalse->isQuad()) ? LIR_qcmov : LIR_cmov, cond, ins2(LIR_2, iftrue, iffalse));
17275: 		}
17275: 
17275: 		// @todo -- it might be better to use a short conditional branch rather than
17275: 		// the bit-twiddling on systems that don't provide a conditional move instruction.
17275: 		LInsp ncond = ins1(LIR_neg, cond); // cond ? -1 : 0
17275: 		return ins2(LIR_or, 
17275: 					ins2(LIR_and, iftrue, ncond), 
17275: 					ins2(LIR_and, iffalse, ins1(LIR_not, ncond)));
17275: 	}
17275: 
20408:     LIns* LirBufWriter::insCall(const CallInfo *ci, LInsp args[])
17275: 	{
17275: 		static const LOpcode k_callmap[] = { LIR_call, LIR_fcall, LIR_call, LIR_callh };
20893: 		static const LOpcode k_callimap[] = { LIR_calli, LIR_fcalli, LIR_calli, LIR_skip };
17275: 
20408: 		uint32_t argt = ci->_argtypes;
20893:         LOpcode op = (ci->isIndirect() ? k_callimap : k_callmap)[argt & 3];
20893:         NanoAssert(op != LIR_skip); // LIR_skip here is just an error condition
17687: 
26678:         ArgSize sizes[MAXARGS];
20919:         int32_t argc = ci->get_sizes(sizes);
17275: 
26545: 		if (AvmCore::config.soft_float) {
17275: 			if (op == LIR_fcall)
17275: 				op = LIR_callh;
26545: 		}
26545: 
23103: 		//
23103: 		// An example of the what we're trying to serialize:
23103: 		//
23103: 		// byte                                             word
23103: 		// ----                                             ----
23103: 		//    N  [ arg tramp #0 ------------------------ ]  K
23103: 		//  N+4  [ arg tramp #1 ------------------------ ]  K+1
23103: 		//  N+8  [ arg tramp #2 ------------------------ ]  K+2
23103: 		// N+12  [ arg tramp #3 ------------------------ ]  K+3
23103: 		// N+16  [ argoff3 | argoff2 | argoff1 | argoff0 ]  K+4
23103: 		// N+20  [ CallInfo* --------------------------- ]  K+5
23103: 		// N+24  [ LIR_call ---------| imm8a=0 | imm8b=4 ]  K+6
23103: 		//
23103: 		// In this example:
23103: 		//    32 bit words
23103: 		//    'argc' = 4
23103: 		//    'words' = argwords(argc) = 1  (word K+4       )
23103: 		//    'LIR_CALL_SLOTS' = 2          (words K+5 - K+6)
23103: 		//    'insSz' = 1+2 = 3             (words K+4 - K+6)
23103: 		//    'from' = next + (insSz - 1)   (word K+6       )
23103: 		//
17275: 
20920: 		NanoAssert(argc <= (int)MAXARGS);
17687: 		uint32_t words = argwords(argc);
22662: 		int32_t insSz = words + LIR_CALL_SLOTS; // words need for offsets + size of instruction
23720: 		ensureRoom(argc * LIR_FAR_SLOTS + insSz);  // argc=# possible tramps for args
23103: 
23103: 		// Argument deltas are calculated relative to the final LIns,
23103: 		// which is the last word in the cluster.
23720: 		LInsp from = _buf->next() + argc * LIR_FAR_SLOTS + insSz - 1; 
20919: 		for (int32_t i=0; i < argc; i++)
22662: 			makeReachable(args[i], from);
22662: 
22662: 		// skip 'words' needed for call parameters
22662: 		LirCallIns *l = (LirCallIns*) (_buf->next()+words);
22662: 		l->ci = ci;
22662: 
22662: 		// call parameters laid in reverse order
22662: 		uint8_t* offs = (uint8_t*)l;
20919: 		for (int32_t i=0; i < argc; i++)
22662: 			*--offs = (uint8_t) l->i.reference(args[i]);
22662: 		NanoAssert((LInsp)offs>=_buf->next());
22662: 		
22663: #ifndef NANOJIT_64BIT
22662: 		l->i.initOpcode(op==LIR_callh ? LIR_call : op);
18254: #else
22662: 		l->i.initOpcode(op);
18254: #endif
22662: 		l->i.c.imm8a = 0;
22662: 		l->i.c.imm8b = argc;
22662: 		_buf->commit(insSz);	
17687: 		_buf->_stats.lir++;
22662: 		NanoAssert((LInsp)(l+1)==_buf->next());
22662: 		return &(l->i);
17275: 	}
17275: 
17275:     using namespace avmplus;
17275: 
20893: 	StackFilter::StackFilter(LirFilter *in, GC *gc, LirBuffer *lirbuf, LInsp sp) 
20893: 		: LirFilter(in), gc(gc), lirbuf(lirbuf), sp(sp), top(0)
17275: 	{}
17275: 
17516: 	LInsp StackFilter::read() 
17275: 	{
17275: 		for (;;) 
17275: 		{
17275: 			LInsp i = in->read();
17275: 			if (!i)
17275: 				return i;
17275: 			if (i->isStore())
17275: 			{
17275: 				LInsp base = i->oprnd2();
17516: 				if (base == sp) 
17275: 				{
17275: 					LInsp v = i->oprnd1();
17275: 					int d = i->immdisp() >> 2;
17275: 					if (d >= top) {
17668: 						continue;
17275: 					} else {
17275: 						d = top - d;
17275: 						if (v->isQuad()) {
17275: 							// storing 8 bytes
17275: 							if (stk.get(d) && stk.get(d-1)) {
17668: 								continue;
17275: 							} else {
17275: 								stk.set(gc, d);
17275: 								stk.set(gc, d-1);
17275: 							}
17275: 						}
17275: 						else {
17275: 							// storing 4 bytes
17275: 							if (stk.get(d))
17668: 								continue;
17275: 							else
17275: 								stk.set(gc, d);
17275: 						}
17275: 					}
17275: 				}
17275: 			}
20893: 			/* 
20893: 			 * NB: If there is a backward branch other than the loop-restart branch, this is
20893: 			 * going to be wrong. Unfortunately there doesn't seem to be an easy way to detect
20893: 			 * such branches. Just do not create any.
20893: 			 */
17275: 			else if (i->isGuard())
17275: 			{
17275: 				stk.reset();
17516: 				top = getTop(i) >> 2;
17275: 			}
17275: 			return i;
17275: 		}
17275: 	}
17275: 
17275: 	//
17275: 	// inlined/separated version of SuperFastHash
17275: 	// This content is copyrighted by Paul Hsieh, For reference see : http://www.azillionmonkeys.com/qed/hash.html
17275: 	//
17275: 	inline uint32_t _hash8(uint32_t hash, const uint8_t data)
17275: 	{
17275: 		hash += data;
17275: 		hash ^= hash << 10;
17275: 		hash += hash >> 1;
17275: 		return hash;
17275: 	}
17275: 
17275: 	inline uint32_t _hash32(uint32_t hash, const uint32_t data)
17275: 	{
17275: 		const uint32_t dlo = data & 0xffff;
17275: 		const uint32_t dhi = data >> 16;
17275: 		hash += dlo;
17275: 		const uint32_t tmp = (dhi << 11) ^ hash;
17275: 		hash = (hash << 16) ^ tmp;
17275: 		hash += hash >> 11;
17275: 		return hash;
17275: 	}
17275: 	
17275: 	inline uint32_t _hashptr(uint32_t hash, const void* data)
17275: 	{
17916: #ifdef NANOJIT_64BIT
17275: 		hash = _hash32(hash, uint32_t(uintptr_t(data) >> 32));
17275: 		hash = _hash32(hash, uint32_t(uintptr_t(data)));
17275: 		return hash;
17275: #else
17275: 		return _hash32(hash, uint32_t(data));
17275: #endif
17275: 	}
17275: 
17275: 	inline uint32_t _hashfinish(uint32_t hash)
17275: 	{
17275: 		/* Force "avalanching" of final 127 bits */
17275: 		hash ^= hash << 3;
17275: 		hash += hash >> 5;
17275: 		hash ^= hash << 4;
17275: 		hash += hash >> 17;
17275: 		hash ^= hash << 25;
17275: 		hash += hash >> 6;
17275: 		return hash;
17275: 	}
17275: 
17275: 	LInsHashSet::LInsHashSet(GC* gc) : 
20893: 			m_used(0), m_cap(kInitialCap), m_gc(gc)
17275: 	{
17378: #ifdef MEMORY_INFO
20893: //		m_list.set_meminfo_name("LInsHashSet.list");
17378: #endif
22668:         LInsp *list = (LInsp*) gc->Alloc(sizeof(LInsp)*m_cap, GC::kZero);
20893:         WB(gc, this, &m_list, list);
20893: 	}
20893: 
21486:     LInsHashSet::~LInsHashSet()
21486:     {
21486:         m_gc->Free(m_list);
21486:     }
21486: 
20893:     void LInsHashSet::clear() {
20893:         memset(m_list, 0, sizeof(LInsp)*m_cap);
20893:         m_used = 0;
17275:     }
17275: 	
17275: 	/*static*/ uint32_t FASTCALL LInsHashSet::hashcode(LInsp i)
17275: 	{
17275: 		const LOpcode op = i->opcode();
17275: 		switch (op)
17275: 		{
17275: 			case LIR_short:
17275: 				return hashimm(i->imm16());
17275: 			case LIR_int:
17275: 				return hashimm(i->imm32());
17275: 			case LIR_quad:
17275: 				return hashimmq(i->constvalq());
17275: 			case LIR_call:
17275: 			case LIR_fcall:
18254: #if defined NANOJIT_64BIT
18254: 			case LIR_callh:
18254: #endif
17275: 			{
17275: 				LInsp args[10];
17687: 				int32_t argc = i->argc();
17275: 				NanoAssert(argc < 10);
17687: 				for (int32_t j=0; j < argc; j++)
17687: 					args[j] = i->arg(j);
20408: 				return hashcall(i->callInfo(), argc, args);
17275: 			} 
17275: 			default:
17308: 				if (operandCount[op] == 2)
17275: 					return hash2(op, i->oprnd1(), i->oprnd2());
17275: 				else
17275: 					return hash1(op, i->oprnd1());
17275: 		}
17275: 	}
17275: 
17275: 	/*static*/ bool FASTCALL LInsHashSet::equals(LInsp a, LInsp b) 
17275: 	{
17275: 		if (a==b)
17275: 			return true;
17275: 		AvmAssert(a->opcode() == b->opcode());
17275: 		const LOpcode op = a->opcode();
17275: 		switch (op)
17275: 		{
17275: 			case LIR_short:
17275: 			{
17275: 				return a->imm16() == b->imm16();
17275: 			} 
17275: 			case LIR_int:
17275: 			{
17275: 				return a->imm32() == b->imm32();
17275: 			} 
17275: 			case LIR_quad:
17275: 			{
17275: 				return a->constvalq() == b->constvalq();
17275: 			}
17275: 			case LIR_call:
17275: 			case LIR_fcall:
18254: #if defined NANOJIT_64BIT
18254: 			case LIR_callh:
18254: #endif
17275: 			{
20408: 				if (a->callInfo() != b->callInfo()) return false;
17687: 				uint32_t argc=a->argc();
17687:                 NanoAssert(argc == b->argc());
17687: 				for (uint32_t i=0; i < argc; i++)
17687: 					if (a->arg(i) != b->arg(i))
17275: 						return false;
17275: 				return true;
17275: 			} 
17275: 			default:
17275: 			{
17308: 				const uint32_t count = operandCount[op];
17275: 				if ((count >= 1 && a->oprnd1() != b->oprnd1()) ||
17275: 					(count >= 2 && a->oprnd2() != b->oprnd2()))
17275: 					return false;
17275: 				return true;
17275: 			}
17275: 		}
17275: 	}
17275: 
17275: 	void FASTCALL LInsHashSet::grow()
17275: 	{
20893: 		const uint32_t newcap = m_cap << 1;
22668:         LInsp *newlist = (LInsp*) m_gc->Alloc(newcap * sizeof(LInsp), GC::kZero);
20893:         LInsp *list = m_list;
17378: #ifdef MEMORY_INFO
20893: //		newlist.set_meminfo_name("LInsHashSet.list");
17378: #endif
20893: 		for (uint32_t i=0, n=m_cap; i < n; i++) {
20893: 			LInsp name = list[i];
17275: 			if (!name) continue;
17275: 			uint32_t j = find(name, hashcode(name), newlist, newcap);
20893:             newlist[j] = name;
17275: 		}
20893:         m_cap = newcap;
20956:         m_gc->Free(list);
20893:         WB(m_gc, this, &m_list, newlist);
17275: 	}
17275: 
20893: 	uint32_t FASTCALL LInsHashSet::find(LInsp name, uint32_t hash, const LInsp *list, uint32_t cap)
17275: 	{
17275: 		const uint32_t bitmask = (cap - 1) & ~0x1;
17275: 
17275: 		uint32_t n = 7 << 1;
17275: 		hash &= bitmask;  
17275: 		LInsp k;
20893: 		while ((k = list[hash]) != NULL &&
17275: 			(!LIns::sameop(k,name) || !equals(k, name)))
17275: 		{
17275: 			hash = (hash + (n += 2)) & bitmask;		// quadratic probe
17275: 		}
17275: 		return hash;
17275: 	}
17275: 
17275: 	LInsp LInsHashSet::add(LInsp name, uint32_t k)
17275: 	{
17275: 		// this is relatively short-lived so let's try a more aggressive load factor
17275: 		// in the interest of improving performance
20893: 		if (((m_used+1)<<1) >= m_cap) // 0.50
17275: 		{
17275: 			grow();
20893: 			k = find(name, hashcode(name), m_list, m_cap);
17275: 		}
20893: 		NanoAssert(!m_list[k]);
17275: 		m_used++;
20893:         return m_list[k] = name;
17275: 	}
17275: 
17308: 	void LInsHashSet::replace(LInsp i)
17308: 	{
20893:         LInsp *list = m_list;
20893: 		uint32_t k = find(i, hashcode(i), list, m_cap);
20893: 		if (list[k]) {
17308: 			// already there, so replace it
20893: 			list[k] = i;
17308: 		} else {
17308: 			add(i, k);
17308: 		}
17308: 	}
17308: 
17275: 	uint32_t LInsHashSet::hashimm(int32_t a) {
17275: 		return _hashfinish(_hash32(0,a));
17275: 	}
17275: 
17275: 	uint32_t LInsHashSet::hashimmq(uint64_t a) {
17275: 		uint32_t hash = _hash32(0, uint32_t(a >> 32));
17275: 		return _hashfinish(_hash32(hash, uint32_t(a)));
17275: 	}
17275: 
17275: 	uint32_t LInsHashSet::hash1(LOpcode op, LInsp a) {
17275: 		uint32_t hash = _hash8(0,uint8_t(op));
17275: 		return _hashfinish(_hashptr(hash, a));
17275: 	}
17275: 
17275: 	uint32_t LInsHashSet::hash2(LOpcode op, LInsp a, LInsp b) {
17275: 		uint32_t hash = _hash8(0,uint8_t(op));
17275: 		hash = _hashptr(hash, a);
17275: 		return _hashfinish(_hashptr(hash, b));
17275: 	}
17275: 
20408: 	uint32_t LInsHashSet::hashcall(const CallInfo *ci, uint32_t argc, LInsp args[]) {
20408: 		uint32_t hash = _hashptr(0, ci);
17275: 		for (int32_t j=argc-1; j >= 0; j--)
17275: 			hash = _hashptr(hash,args[j]);
17275: 		return _hashfinish(hash);
17275: 	}
17275: 
17275: 	LInsp LInsHashSet::find32(int32_t a, uint32_t &i)
17275: 	{
20893: 		uint32_t cap = m_cap;
20893: 		const LInsp *list = m_list;
17275: 		const uint32_t bitmask = (cap - 1) & ~0x1;
17275: 		uint32_t hash = hashimm(a) & bitmask;
17275: 		uint32_t n = 7 << 1;
17275: 		LInsp k;
20893: 		while ((k = list[hash]) != NULL && 
17275: 			(!k->isconst() || k->constval() != a))
17275: 		{
17275: 			hash = (hash + (n += 2)) & bitmask;		// quadratic probe
17275: 		}
17275: 		i = hash;
17275: 		return k;
17275: 	}
17275: 
17275: 	LInsp LInsHashSet::find64(uint64_t a, uint32_t &i)
17275: 	{
20893: 		uint32_t cap = m_cap;
20893: 		const LInsp *list = m_list;
17275: 		const uint32_t bitmask = (cap - 1) & ~0x1;
17275: 		uint32_t hash = hashimmq(a) & bitmask;  
17275: 		uint32_t n = 7 << 1;
17275: 		LInsp k;
20893: 		while ((k = list[hash]) != NULL && 
17275: 			(!k->isconstq() || k->constvalq() != a))
17275: 		{
17275: 			hash = (hash + (n += 2)) & bitmask;		// quadratic probe
17275: 		}
17275: 		i = hash;
17275: 		return k;
17275: 	}
17275: 
17275: 	LInsp LInsHashSet::find1(LOpcode op, LInsp a, uint32_t &i)
17275: 	{
20893: 		uint32_t cap = m_cap;
20893: 		const LInsp *list = m_list;
17275: 		const uint32_t bitmask = (cap - 1) & ~0x1;
17275: 		uint32_t hash = hash1(op,a) & bitmask;  
17275: 		uint32_t n = 7 << 1;
17275: 		LInsp k;
20893: 		while ((k = list[hash]) != NULL && 
17275: 			(k->opcode() != op || k->oprnd1() != a))
17275: 		{
17275: 			hash = (hash + (n += 2)) & bitmask;		// quadratic probe
17275: 		}
17275: 		i = hash;
17275: 		return k;
17275: 	}
17275: 
17275: 	LInsp LInsHashSet::find2(LOpcode op, LInsp a, LInsp b, uint32_t &i)
17275: 	{
20893: 		uint32_t cap = m_cap;
20893: 		const LInsp *list = m_list;
17275: 		const uint32_t bitmask = (cap - 1) & ~0x1;
17275: 		uint32_t hash = hash2(op,a,b) & bitmask;  
17275: 		uint32_t n = 7 << 1;
17275: 		LInsp k;
20893: 		while ((k = list[hash]) != NULL && 
17275: 			(k->opcode() != op || k->oprnd1() != a || k->oprnd2() != b))
17275: 		{
17275: 			hash = (hash + (n += 2)) & bitmask;		// quadratic probe
17275: 		}
17275: 		i = hash;
17275: 		return k;
17275: 	}
17275: 
17275: 	bool argsmatch(LInsp i, uint32_t argc, LInsp args[])
17275: 	{
17687: 		for (uint32_t j=0; j < argc; j++)
17687: 			if (i->arg(j) != args[j])
17275: 				return false;
17275: 		return true;
17275: 	}
17275: 
20408: 	LInsp LInsHashSet::findcall(const CallInfo *ci, uint32_t argc, LInsp args[], uint32_t &i)
17275: 	{
20893: 		uint32_t cap = m_cap;
20893: 		const LInsp *list = m_list;
17275: 		const uint32_t bitmask = (cap - 1) & ~0x1;
20408: 		uint32_t hash = hashcall(ci, argc, args) & bitmask;  
17275: 		uint32_t n = 7 << 1;
17275: 		LInsp k;
20893: 		while ((k = list[hash]) != NULL &&
20408: 			(!k->isCall() || k->callInfo() != ci || !argsmatch(k, argc, args)))
17275: 		{
17275: 			hash = (hash + (n += 2)) & bitmask;		// quadratic probe
17275: 		}
17275: 		i = hash;
17275: 		return k;
17275: 	}
17275: 
20931:     GuardRecord *LIns::record()
17275:     {
17275:         NanoAssert(isGuard());
20931:         return (GuardRecord*)oprnd2()->payload();
17275:     }
17275: 
17275: #ifdef NJ_VERBOSE
17275:     class RetiredEntry: public GCObject
17275:     {
17275:     public:
17275:         List<LInsp, LIST_NonGCObjects> live;
17275:         LInsp i;
17275:         RetiredEntry(GC *gc): live(gc) {}
17275:     };
17275: 	class LiveTable 
17275: 	{
17275: 	public:
17275: 		SortedMap<LInsp,LInsp,LIST_NonGCObjects> live;
17275:         List<RetiredEntry*, LIST_GCObjects> retired;
17275: 		int maxlive;
17275: 		LiveTable(GC *gc) : live(gc), retired(gc), maxlive(0) {}
18056:         ~LiveTable()
18056:         {
18056:             for (size_t i = 0; i < retired.size(); i++) {
22647:                 NJ_DELETE(retired.get(i));
18056:             }
18056: 
18056:         }
17275: 		void add(LInsp i, LInsp use) {
17275:             if (!i->isconst() && !i->isconstq() && !live.containsKey(i)) {
22668:                 NanoAssert(size_t(i->opcode()) < sizeof(lirNames) / sizeof(lirNames[0]));
17275:                 live.put(i,use);
17275:             }
17275: 		}
17275:         void retire(LInsp i, GC *gc) {
22647:             RetiredEntry *e = NJ_NEW(gc, RetiredEntry)(gc);
17275:             e->i = i;
17275:             for (int j=0, n=live.size(); j < n; j++) {
17275:                 LInsp l = live.keyAt(j);
17687:                 if (!l->isStore() && !l->isGuard())
17275:                     e->live.add(l);
17275:             }
17275:             int size=0;
17275: 		    if ((size = e->live.size()) > maxlive)
17275: 			    maxlive = size;
17275: 
17275:             live.remove(i);
17275:             retired.add(e);
17275: 		}
17275: 		bool contains(LInsp i) {
17275: 			return live.containsKey(i);
17275: 		}
17275: 	};
17275: 
20893:     void live(GC *gc, LirBuffer *lirbuf)
17275: 	{
17275: 		// traverse backwards to find live exprs and a few other stats.
17275: 
17275: 		LiveTable live(gc);
17275: 		uint32_t exits = 0;
17275:         LirReader br(lirbuf);
20893: 		StackFilter sf(&br, gc, lirbuf, lirbuf->sp);
20893: 		StackFilter r(&sf, gc, lirbuf, lirbuf->rp);
17275:         int total = 0;
20893:         if (lirbuf->state)
20893:             live.add(lirbuf->state, r.pos());
17275: 		for (LInsp i = r.read(); i != 0; i = r.read())
17275: 		{
17275:             total++;
17275: 
17275:             // first handle side-effect instructions
20893: 			if (!i->isCse(lirbuf->_functions))
17275: 			{
17275: 				live.add(i,0);
17275:                 if (i->isGuard())
17275:                     exits++;
17275: 			}
17275: 
17275: 			// now propagate liveness
17275: 			if (live.contains(i))
17275: 			{
17275: 				live.retire(i,gc);
22668:                 NanoAssert(size_t(i->opcode()) < sizeof(operandCount) / sizeof(operandCount[0]));
17275: 				if (i->isStore()) {
17275: 					live.add(i->oprnd2(),i); // base
17275: 					live.add(i->oprnd1(),i); // val
17275: 				}
18254:                 else if (i->isop(LIR_cmov) || i->isop(LIR_qcmov)) {
17275:                     live.add(i->oprnd1(),i);
17275:                     live.add(i->oprnd2()->oprnd1(),i);
17275:                     live.add(i->oprnd2()->oprnd2(),i);
17275:                 }
17308: 				else if (operandCount[i->opcode()] == 1) {
17275: 				    live.add(i->oprnd1(),i);
17275: 				}
17308: 				else if (operandCount[i->opcode()] == 2) {
17275: 					live.add(i->oprnd1(),i);
17275: 					live.add(i->oprnd2(),i);
17275: 				}
17687: 				else if (i->isCall()) {
17687: 					for (int j=0, c=i->argc(); j < c; j++)
17687: 						live.add(i->arg(j),i);
17275: 				}
17275: 			}
17275: 		}
17275:  
20893: 		printf("live instruction count %d, total %u, max pressure %d\n",
17275: 			live.retired.size(), total, live.maxlive);
20893:         printf("side exits %u\n", exits);
17275: 
17275: 		// print live exprs, going forwards
20893: 		LirNameMap *names = lirbuf->names;
20893:         bool newblock = true;
17275: 		for (int j=live.retired.size()-1; j >= 0; j--) 
17275:         {
17275:             RetiredEntry *e = live.retired[j];
20893:             char livebuf[4000], *s=livebuf;
17275:             *s = 0;
20893:             if (!newblock && e->i->isop(LIR_label)) {
20893:                 printf("\n");
20893:             }
20893:             newblock = false;
17275:             for (int k=0,n=e->live.size(); k < n; k++) {
17275: 				strcpy(s, names->formatRef(e->live[k]));
17275: 				s += strlen(s);
17275: 				*s++ = ' '; *s = 0;
17275: 				NanoAssert(s < livebuf+sizeof(livebuf));
17275:             }
17275: 			printf("%-60s %s\n", livebuf, names->formatIns(e->i));
20893:             if (e->i->isGuard() || e->i->isBranch() || isRet(e->i->opcode())) {
17275: 				printf("\n");
20893:                 newblock = true;
20893:             }
17275: 		}
17275: 	}
17275: 
18056:     LabelMap::Entry::~Entry()
18056:     {
18056:     }
18056: 
18056:     LirNameMap::Entry::~Entry()
18056:     {
18056:     }
18056: 
18056:     LirNameMap::~LirNameMap()
18056:     {
18056:         Entry *e;
18056: 
18056:         while ((e = names.removeLast()) != NULL) {
19057:             labels->core->freeString(e->name);
22647:             NJ_DELETE(e);
18056:         }
18056:     }
18056: 
19057: 	bool LirNameMap::addName(LInsp i, Stringp name) {
17308: 		if (!names.containsKey(i)) { 
22647: 			Entry *e = NJ_NEW(labels->core->gc, Entry)(name);
17275: 			names.put(i, e);
19057:             return true;
17275: 		}
19057:         return false;
17308: 	}
17275: 	void LirNameMap::addName(LInsp i, const char *name) {
19057:         Stringp new_name = labels->core->newString(name);
19057:         if (!addName(i, new_name)) {
19057:             labels->core->freeString(new_name);
19057:         }
17275: 	}
17275: 
17275: 	void LirNameMap::copyName(LInsp i, const char *s, int suffix) {
17275: 		char s2[200];
20893: 		if (isdigit(s[strlen(s)-1])) {
20893: 			// if s ends with a digit, add '_' to clarify the suffix
20893: 			sprintf(s2,"%s_%d", s, suffix);
20893: 		} else {
17275: 			sprintf(s2,"%s%d", s, suffix);
20893: 		}
17275: 		addName(i, labels->core->newString(s2));
17275: 	}
17275: 
17275: 	void LirNameMap::formatImm(int32_t c, char *buf) {
17275: 		if (c >= 10000 || c <= -10000)
17275: 			sprintf(buf,"#%s",labels->format((void*)c));
17275:         else
17275:             sprintf(buf,"%d", c);
17275: 	}
17275: 
17275: 	const char* LirNameMap::formatRef(LIns *ref)
17275: 	{
17275: 		char buffer[200], *buf=buffer;
17275: 		buf[0]=0;
17275: 		GC *gc = labels->core->gc;
17275: 		if (names.containsKey(ref)) {
17275: 			StringNullTerminatedUTF8 cname(gc, names.get(ref)->name);
17275: 			strcat(buf, cname.c_str());
17275: 		}
17275: 		else if (ref->isconstq()) {
18220: #if defined NANOJIT_64BIT
18220:             sprintf(buf, "#0x%lx", (nj_printf_ld)ref->constvalq());
18220: #else
17275: 			formatImm(uint32_t(ref->constvalq()>>32), buf);
17275: 			buf += strlen(buf);
17275: 			*buf++ = ':';
17275: 			formatImm(uint32_t(ref->constvalq()), buf);
18220: #endif
17275: 		}
17275: 		else if (ref->isconst()) {
17275: 			formatImm(ref->constval(), buf);
17275: 		}
17275: 		else {
17275: 			if (ref->isCall()) {
19041: #if !defined NANOJIT_64BIT
18773: 				if (ref->isop(LIR_callh)) {
18773: 					// we've presumably seen the other half already
18773: 					ref = ref->oprnd1();
18773: 				} else {
19041: #endif
20408: 					copyName(ref, ref->callInfo()->_name, funccounts.add(ref->callInfo()));
19041: #if !defined NANOJIT_64BIT
18773: 				}
19041: #endif
17275: 			} else {
22668:                 NanoAssert(size_t(ref->opcode()) < sizeof(lirNames) / sizeof(lirNames[0]));
17308: 				copyName(ref, lirNames[ref->opcode()], lircounts.add(ref->opcode()));
17275: 			}
17275: 			StringNullTerminatedUTF8 cname(gc, names.get(ref)->name);
17275: 			strcat(buf, cname.c_str());
17275: 		}
17275: 		return labels->dup(buffer);
17275: 	}
17275: 
17275: 	const char* LirNameMap::formatIns(LIns* i)
17275: 	{
17275: 		char sbuf[200];
17275: 		char *s = sbuf;
17308: 		LOpcode op = i->opcode();
17308: 		switch(op)
17275: 		{
17275: 			case LIR_short:
17275: 			case LIR_int:
17275: 			{
17275:                 sprintf(s, "%s", formatRef(i));
17275: 				break;
17275: 			}
17275: 
20893:             case LIR_alloc: {
20893:                 sprintf(s, "%s = %s %d", formatRef(i), lirNames[op], i->size());
20893:                 break;
20893:             }
20893: 
17275: 			case LIR_quad:
17275: 			{
17275: 				int32_t *p = (int32_t*) (i-2);
20893: 				sprintf(s, "#%X:%X /* %g */", p[1], p[0], i->constvalf());
17275: 				break;
17275: 			}
17275: 
17275: 			case LIR_loop:
20893: 			case LIR_start:
17308: 				sprintf(s, "%s", lirNames[op]);
17275: 				break;
17275: 
18254: #if defined NANOJIT_64BIT
18254: 			case LIR_callh:
18254: #endif
17275: 			case LIR_fcall:
17275: 			case LIR_call: {
22706: 				sprintf(s, "%s = %s ( ", formatRef(i), i->callInfo()->_name);
17687: 				for (int32_t j=i->argc()-1; j >= 0; j--) {
17275: 					s += strlen(s);
17687: 					sprintf(s, "%s ",formatRef(i->arg(j)));
17275: 				}
17275: 				s += strlen(s);
17275: 				sprintf(s, ")");
17275: 				break;
17275: 			}
20893: 			case LIR_fcalli:
20893: 			case LIR_calli: {
20893:                 int32_t argc = i->argc();
22706: 				sprintf(s, "%s = [%s] ( ", formatRef(i), formatRef(i->arg(argc-1)));
20893:                 s += strlen(s);
20893:                 argc--;
20893: 				for (int32_t j=argc-1; j >= 0; j--) {
20893: 					s += strlen(s);
20893: 					sprintf(s, "%s ",formatRef(i->arg(j)));
20893: 				}
20893: 				s += strlen(s);
20893: 				sprintf(s, ")");
20893: 				break;
20893: 			}
17275: 
20893: 			case LIR_param: { 
20893: 				uint32_t arg = i->imm8();
20893: 				if (!i->imm8b()) {
20893: 					if (arg < sizeof(Assembler::argRegs)/sizeof(Assembler::argRegs[0])) {
20893: 						sprintf(s, "%s = %s %d %s", formatRef(i), lirNames[op],
20893: 							arg, gpn(Assembler::argRegs[arg]));
20893: 					} else {
20893: 						sprintf(s, "%s = %s %d", formatRef(i), lirNames[op], arg);
20893: 					}
20893: 				} else {
20893: 					sprintf(s, "%s = %s %d %s", formatRef(i), lirNames[op],
20893: 						arg, gpn(Assembler::savedRegs[arg]));
20893: 				}
20893: 				break;
20893: 			}
20893: 
20893: 			case LIR_label:
20893:                 sprintf(s, "%s:", formatRef(i));
17275: 				break;
17275: 
20893: 			case LIR_jt:
20893: 			case LIR_jf:
20893:                 sprintf(s, "%s %s -> %s", lirNames[op], formatRef(i->oprnd1()), 
20893:                     i->oprnd2() ? formatRef(i->oprnd2()) : "unpatched");
20893: 				break;
20893: 
20893: 			case LIR_j:
20893:                 sprintf(s, "%s -> %s", lirNames[op], 
20893:                     i->oprnd2() ? formatRef(i->oprnd2()) : "unpatched");
20893: 				break;
20893: 
20893:             case LIR_live:
20893: 			case LIR_ret:
20893:             case LIR_fret:
20893:                 sprintf(s, "%s %s", lirNames[op], formatRef(i->oprnd1()));
20893: 				break;
20893: 				
20893:             case LIR_callh:
17275: 			case LIR_neg:
17275: 			case LIR_fneg:
17275: 			case LIR_i2f:
17275: 			case LIR_u2f:
17275: 			case LIR_qlo:
17275: 			case LIR_qhi:
17368:             case LIR_ov:
17368:             case LIR_cs:
17687: 			case LIR_not: 
20893: 				sprintf(s, "%s = %s %s", formatRef(i), lirNames[op], formatRef(i->oprnd1()));
17275: 				break;
17275: 
17516: 			case LIR_x:
17275: 			case LIR_xt:
17516: 			case LIR_xf:
24289: 			case LIR_xbarrier:
25099: 			case LIR_xtbl:
17516: 				formatGuard(i, s);
17275: 				break;
17516: 
17275: 			case LIR_add:
20893: 			case LIR_addp:
17275: 			case LIR_sub: 
17275: 		 	case LIR_mul: 
17275: 			case LIR_fadd:
17275: 			case LIR_fsub: 
17275: 		 	case LIR_fmul: 
17275: 			case LIR_fdiv: 
17275: 			case LIR_and: 
17275: 			case LIR_or: 
17275: 			case LIR_xor: 
17275: 			case LIR_lsh: 
17275: 			case LIR_rsh:
17275: 			case LIR_ush:
17275: 			case LIR_eq:
17275: 			case LIR_lt:
17275: 			case LIR_le:
17275: 			case LIR_gt:
17275: 			case LIR_ge:
17275: 			case LIR_ult:
17275: 			case LIR_ule:
17275: 			case LIR_ugt:
17275: 			case LIR_uge:
17308: 			case LIR_feq:
17308: 			case LIR_flt:
17308: 			case LIR_fle:
17308: 			case LIR_fgt:
17308: 			case LIR_fge:
18220:             case LIR_qiadd:
18220:             case LIR_qiand:
18220:             case LIR_qilsh:
18645:             case LIR_qior:
20893: 				sprintf(s, "%s = %s %s, %s", formatRef(i), lirNames[op],
17275: 					formatRef(i->oprnd1()), 
17275: 					formatRef(i->oprnd2()));
17275: 				break;
17275: 
18773: 			case LIR_qjoin:
22706: 				sprintf(s, "%s (%s), %s", lirNames[op],
18773: 					formatIns(i->oprnd1()), 
18773:  					formatRef(i->oprnd2()));
18773:  				break;
18773: 
18254: 			case LIR_qcmov:
17275: 			case LIR_cmov:
20893:                 sprintf(s, "%s = %s %s ? %s : %s", formatRef(i), lirNames[op],
17275: 					formatRef(i->oprnd1()), 
17275: 					formatRef(i->oprnd2()->oprnd1()), 
17275: 					formatRef(i->oprnd2()->oprnd2()));
17275: 				break;
17275: 
17275: 			case LIR_ld: 
17275: 			case LIR_ldc: 
17275: 			case LIR_ldq: 
20893: 			case LIR_ldqc: 
17275: 			case LIR_ldcb:
21469: 			case LIR_ldcs:
20893: 				sprintf(s, "%s = %s %s[%s]", formatRef(i), lirNames[op],
17275: 					formatRef(i->oprnd1()), 
17275: 					formatRef(i->oprnd2()));
17275: 				break;
17275: 
17275: 			case LIR_st: 
17275:             case LIR_sti:
17308: 			case LIR_stq: 
17308:             case LIR_stqi:
20893: 				sprintf(s, "%s %s[%d] = %s", lirNames[op],
17275: 					formatRef(i->oprnd2()), 
17275: 					i->immdisp(), 
17275: 					formatRef(i->oprnd1()));
17275: 				break;
17275: 
17275: 			default:
17275: 				sprintf(s, "?");
17275: 				break;
17275: 		}
17275: 		return labels->dup(sbuf);
17275: 	}
17275: 
17275: 
17275: #endif
17275: 	CseFilter::CseFilter(LirWriter *out, GC *gc)
17275: 		: LirWriter(out), exprs(gc) {}
17275: 
17275: 	LIns* CseFilter::insImm(int32_t imm)
17275: 	{
17275: 		uint32_t k;
17275: 		LInsp found = exprs.find32(imm, k);
17275: 		if (found)
17275: 			return found;
17275: 		return exprs.add(out->insImm(imm), k);
17275: 	}
17275: 
17275: 	LIns* CseFilter::insImmq(uint64_t q)
17275: 	{
17275: 		uint32_t k;
17275: 		LInsp found = exprs.find64(q, k);
17275: 		if (found)
17275: 			return found;
17275: 		return exprs.add(out->insImmq(q), k);
17275: 	}
17275: 
21789: 	LIns* CseFilter::ins0(LOpcode v)
21789: 	{
21789: 	    if (v == LIR_label)
21789: 	        exprs.clear();
21789: 	    return out->ins0(v);
21789: 	}
21789: 	
17275: 	LIns* CseFilter::ins1(LOpcode v, LInsp a)
17275: 	{
17275: 		if (isCse(v)) {
17308: 			NanoAssert(operandCount[v]==1);
17275: 			uint32_t k;
17275: 			LInsp found = exprs.find1(v, a, k);
17275: 			if (found)
17275: 				return found;
17275: 			return exprs.add(out->ins1(v,a), k);
17275: 		}
17275: 		return out->ins1(v,a);
17275: 	}
17275: 
17275: 	LIns* CseFilter::ins2(LOpcode v, LInsp a, LInsp b)
17275: 	{
17275: 		if (isCse(v)) {
17308: 			NanoAssert(operandCount[v]==2);
17275: 			uint32_t k;
17275: 			LInsp found = exprs.find2(v, a, b, k);
17275: 			if (found)
17275: 				return found;
17275: 			return exprs.add(out->ins2(v,a,b), k);
17275: 		}
17275: 		return out->ins2(v,a,b);
17275: 	}
17275: 
17275: 	LIns* CseFilter::insLoad(LOpcode v, LInsp base, LInsp disp)
17275: 	{
17275: 		if (isCse(v)) {
17308: 			NanoAssert(operandCount[v]==2);
17275: 			uint32_t k;
17275: 			LInsp found = exprs.find2(v, base, disp, k);
17275: 			if (found)
17275: 				return found;
17275: 			return exprs.add(out->insLoad(v,base,disp), k);
17275: 		}
17275: 		return out->insLoad(v,base,disp);
17275: 	}
17275: 
20931: 	LInsp CseFilter::insGuard(LOpcode v, LInsp c, LInsp x)
17275: 	{
17275: 		if (isCse(v)) {
17275: 			// conditional guard
17308: 			NanoAssert(operandCount[v]==1);
17275: 			uint32_t k;
17275: 			LInsp found = exprs.find1(v, c, k);
17275: 			if (found)
17275: 				return 0;
17275: 			return exprs.add(out->insGuard(v,c,x), k);
17275: 		}
17275: 		return out->insGuard(v, c, x);
17275: 	}
17275: 
20408: 	LInsp CseFilter::insCall(const CallInfo *ci, LInsp args[])
17275: 	{
20408: 		if (ci->_cse) {
17275: 			uint32_t k;
20408:             uint32_t argc = ci->count_args();
20408: 			LInsp found = exprs.findcall(ci, argc, args, k);
17275: 			if (found)
17275: 				return found;
20408: 			return exprs.add(out->insCall(ci, args), k);
17275: 		}
20408: 		return out->insCall(ci, args);
17275: 	}
17275: 
17308: 	CseReader::CseReader(LirFilter *in, LInsHashSet *exprs, const CallInfo *functions)
17308: 		: LirFilter(in), exprs(exprs), functions(functions)
17308: 	{}
17308: 
17308: 	LInsp CseReader::read()
17308: 	{
17308: 		LInsp i = in->read();
17308: 		if (i) {
17308: 			if (i->isCse(functions))
17308: 				exprs->replace(i);
17308: 		}
17308: 		return i;
17308: 	}
17308: 
17275:     LIns* FASTCALL callArgN(LIns* i, uint32_t n)
17275: 	{
17687: 		return i->arg(i->argc()-n-1);
17275: 	}
17275: 
17275:     void compile(Assembler* assm, Fragment* triggerFrag)
17275:     {
17308:         Fragmento *frago = triggerFrag->lirbuf->_frago;
17308:         AvmCore *core = frago->core();
17275:         GC *gc = core->gc;
17275: 
17275: 		verbose_only( StringList asmOutput(gc); )
17275: 		verbose_only( assm->_outputCache = &asmOutput; )
17275: 
17275: 		verbose_only(if (assm->_verbose && core->config.verbose_live)
20893: 			live(gc, triggerFrag->lirbuf);)
17275: 
17275: 		bool treeCompile = core->config.tree_opt && (triggerFrag->kind == BranchTrace);
17275: 		RegAllocMap regMap(gc);
17275: 		NInsList loopJumps(gc);
17378: #ifdef MEMORY_INFO
20893: //		loopJumps.set_meminfo_name("LIR loopjumps");
17378: #endif
18099: 		assm->beginAssembly(triggerFrag, &regMap);
21483: 		if (assm->error())
21483: 			return;
17275: 
17275: 		//fprintf(stderr, "recompile trigger %X kind %d\n", (int)triggerFrag, triggerFrag->kind);
17275: 		Fragment* root = triggerFrag;
17275: 		if (treeCompile)
17275: 		{
17275: 			// recompile the entire tree
17308: 			root = triggerFrag->root;
17275: 			root->fragEntry = 0;
21490: 			root->loopEntry = 0;
17308: 			root->releaseCode(frago);
17275: 			
17275: 			// do the tree branches
17275: 			Fragment* frag = root->treeBranches;
17275: 			while(frag)
17275: 			{
17275: 				// compile til no more frags
17275: 				if (frag->lastIns)
17275: 				{
17308: 					assm->assemble(frag, loopJumps);
17308: 					verbose_only(if (assm->_verbose) 
17308: 						assm->outputf("compiling branch %s ip %s",
17308: 							frago->labels->format(frag),
17485: 							frago->labels->format(frag->ip)); )
17275: 					
17275: 					NanoAssert(frag->kind == BranchTrace);
22647: 					RegAlloc* regs = NJ_NEW(gc, RegAlloc)();
17275: 					assm->copyRegisters(regs);
17275: 					assm->releaseRegisters();
20931: 					SideExit* exit = frag->spawnedFrom;
17275: 					regMap.put(exit, regs);
17275: 				}
17275: 				frag = frag->treeBranches;
17275: 			}
17275: 		}
17275: 		
17275: 		// now the the main trunk
17308: 		assm->assemble(root, loopJumps);
17308: 		verbose_only(if (assm->_verbose) 
17308: 			assm->outputf("compiling trunk %s",
17308: 				frago->labels->format(root));)
20893: 		NanoAssert(!frago->core()->config.tree_opt || root == root->anchor || root->kind == MergeTrace);			
17275: 		assm->endAssembly(root, loopJumps);
17275: 			
17275: 		// reverse output so that assembly is displayed low-to-high
17275: 		verbose_only( assm->_outputCache = 0; )
17275: 		verbose_only(for(int i=asmOutput.size()-1; i>=0; --i) { assm->outputf("%s",asmOutput.get(i)); } );
17275: 
20931: 		if (assm->error()) {
17275: 			root->fragEntry = 0;
21490: 			root->loopEntry = 0;
17275: 		}
20893:     }
18056: 
20893:     LInsp LoadFilter::insLoad(LOpcode v, LInsp base, LInsp disp)
20893:     {
20893:         if (base != sp && base != rp && (v == LIR_ld || v == LIR_ldq)) {
20893:             uint32_t k;
20893:             LInsp found = exprs.find2(v, base, disp, k);
20893:             if (found)
20893:                 return found;
20893:             return exprs.add(out->insLoad(v,base,disp), k);
18056:         }
20893:         return out->insLoad(v, base, disp);
20893:     }
20893: 
20893:     void LoadFilter::clear(LInsp p)
20893:     {
20893:         if (p != sp && p != rp)
20893:             exprs.clear();
20893:     }
20893: 
20893:     LInsp LoadFilter::insStore(LInsp v, LInsp b, LInsp d)
20893:     {
20893:         clear(b);
20893:         return out->insStore(v, b, d);
20893:     }
20893: 
20893:     LInsp LoadFilter::insStorei(LInsp v, LInsp b, int32_t d)
20893:     {
20893:         clear(b);
20893:         return out->insStorei(v, b, d);
20893:     }
20893: 
22668:     LInsp LoadFilter::insCall(const CallInfo *ci, LInsp args[])
20893:     {
22668:         if (!ci->_cse)
20893:             exprs.clear();
22668:         return out->insCall(ci, args);
20893:     }
20893: 
20893:     LInsp LoadFilter::ins0(LOpcode op)
20893:     {
20893:         if (op == LIR_label)
20893:             exprs.clear();
20893:         return out->ins0(op);
17275:     }
17275: 
17275: 	#endif /* FEATURE_NANOJIT */
17275: 
17275: #if defined(NJ_VERBOSE)
17275:     LabelMap::LabelMap(AvmCore *core, LabelMap* parent)
17275:         : parent(parent), names(core->gc), addrs(core->config.verbose_addrs), end(buf), core(core)
17275: 	{}
17275: 
18056:     LabelMap::~LabelMap()
18056:     {
24618:         clear();
24618:     }
24618: 
24618:     void LabelMap::clear()
24618:     {
18056:         Entry *e;
18056:         while ((e = names.removeLast()) != NULL) {
19057:             core->freeString(e->name);
22647:             NJ_DELETE(e);
18056:         } 
18056:     }
18056: 
17275:     void LabelMap::add(const void *p, size_t size, size_t align, const char *name)
17275: 	{
17516: 		if (!this || names.containsKey(p))
17516: 			return;
17275: 		add(p, size, align, core->newString(name));
17275: 	}
17275: 
17275:     void LabelMap::add(const void *p, size_t size, size_t align, Stringp name)
17275:     {
17516: 		if (!this || names.containsKey(p))
17516: 			return;
22647: 		Entry *e = NJ_NEW(core->gc, Entry)(name, size<<align, align);
17275: 		names.put(p, e);
17275:     }
17275: 
17275:     const char *LabelMap::format(const void *p)
17275:     {
17275: 		char b[200];
17275: 		int i = names.findNear(p);
17275: 		if (i >= 0) {
17275: 			const void *start = names.keyAt(i);
17275: 			Entry *e = names.at(i);
17275: 			const void *end = (const char*)start + e->size;
17275: 			avmplus::StringNullTerminatedUTF8 cname(core->gc, e->name);
17275: 			const char *name = cname.c_str();
17275: 			if (p == start) {
17275: 				if (addrs)
17275: 					sprintf(b,"%p %s",p,name);
17275: 				else
17275: 					strcpy(b, name);
17275: 				return dup(b);
17275: 			}
17275: 			else if (p > start && p < end) {
20893: 				int32_t d = int32_t(intptr_t(p)-intptr_t(start)) >> e->align;
17275: 				if (addrs)
17275: 					sprintf(b, "%p %s+%d", p, name, d);
17275: 				else
17275: 					sprintf(b,"%s+%d", name, d);
17275: 				return dup(b);
17275: 			}
17516: 			else {
17516: 				if (parent)
17516: 					return parent->format(p);
17516: 
17516: 				sprintf(b, "%p", p);
17516: 				return dup(b);
17516: 			}
17275: 		}
17275: 		if (parent)
17275: 			return parent->format(p);
17275: 
17275: 		sprintf(b, "%p", p);
17275: 		return dup(b);
17275:     }
17275: 
17275: 	const char *LabelMap::dup(const char *b)
17275: 	{
20893: 		size_t need = strlen(b)+1;
17275: 		char *s = end;
17275: 		end += need;
17275: 		if (end > buf+sizeof(buf)) {
17275: 			s = buf;
17275: 			end = s+need;
17275: 		}
17275: 		strcpy(s, b);
17275: 		return s;
17275: 	}
17516: 
17516: 	// copy all labels to parent, adding newbase to label addresses
17516: 	void LabelMap::promoteAll(const void *newbase)
17516: 	{
17516: 		for (int i=0, n=names.size(); i < n; i++) {
17916: 			void *base = (char*)newbase + (intptr_t)names.keyAt(i);
17516: 			parent->names.put(base, names.at(i));
17516: 		}
17516: 	}
17275: #endif // NJ_VERBOSE
17275: }
17275: 	
