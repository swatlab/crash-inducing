30266: /* -*- Mode: C++; c-basic-offset: 4; indent-tabs-mode: nil; tab-width: 4 -*- */
30266: /* vi: set ts=4 sw=4 expandtab: (add to ~/.vimrc: set modeline modelines=5) */
17275: /* ***** BEGIN LICENSE BLOCK *****
17275:  * Version: MPL 1.1/GPL 2.0/LGPL 2.1
17275:  *
17275:  * The contents of this file are subject to the Mozilla Public License Version
17275:  * 1.1 (the "License"); you may not use this file except in compliance with
17275:  * the License. You may obtain a copy of the License at
17275:  * http://www.mozilla.org/MPL/
17275:  *
17275:  * Software distributed under the License is distributed on an "AS IS" basis,
17275:  * WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
17275:  * for the specific language governing rights and limitations under the
17275:  * License.
17275:  *
17275:  * The Original Code is [Open Source Virtual Machine].
17275:  *
17275:  * The Initial Developer of the Original Code is
17275:  * Adobe System Incorporated.
17275:  * Portions created by the Initial Developer are Copyright (C) 2004-2007
17275:  * the Initial Developer. All Rights Reserved.
17275:  *
17275:  * Contributor(s):
17275:  *   Adobe AS3 Team
17275:  *
17275:  * Alternatively, the contents of this file may be used under the terms of
17275:  * either the GNU General Public License Version 2 or later (the "GPL"), or
17275:  * the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
17275:  * in which case the provisions of the GPL or the LGPL are applicable instead
17275:  * of those above. If you wish to allow use of your version of this file only
17275:  * under the terms of either the GPL or the LGPL, and not to allow others to
17275:  * use your version of this file under the terms of the MPL, indicate your
17275:  * decision by deleting the provisions above and replace them with the notice
17275:  * and other provisions required by the GPL or the LGPL. If you do not delete
17275:  * the provisions above, a recipient may use your version of this file under
17275:  * the terms of any one of the MPL, the GPL or the LGPL.
17275:  *
17275:  * ***** END LICENSE BLOCK ***** */
17275: 
17275: #include "nanojit.h"
17275: 
20893: #ifdef FEATURE_NANOJIT
20893: 
33561: #ifdef VTUNE
33561: #include "../core/CodegenLIR.h"
33561: #endif
33561: 
33939: #ifdef _MSC_VER
33939:     // disable some specific warnings which are normally useful, but pervasive in the code-gen macros
33939:     #pragma warning(disable:4310) // cast truncates constant value
33939: #endif
33939: 
17275: namespace nanojit
17275: {
17275:     /**
17275:      * Need the following:
17275:      *
17275:      *    - merging paths ( build a graph? ), possibly use external rep to drive codegen
17275:      */
35087:     Assembler::Assembler(CodeAlloc& codeAlloc, Allocator& dataAlloc, Allocator& alloc, AvmCore* core, LogControl* logc)
33174:         : codeList(NULL)
31507:         , alloc(alloc)
31475:         , _codeAlloc(codeAlloc)
35087:         , _dataAlloc(dataAlloc)
33174:         , _thisfrag(NULL)
32642:         , _branchStateMap(alloc)
32642:         , _patches(alloc)
32642:         , _labels(alloc)
37665:     #if NJ_USES_QUAD_CONSTANTS
37665:         , _quadConstants(alloc)
37665:     #endif
33174:         , _epilogue(NULL)
33174:         , _err(None)
33551:     #if PEDANTIC
33551:         , pedanticTop(NULL)
33551:     #endif
33561:     #ifdef VTUNE
33561:         , cgen(NULL)
33561:     #endif
31475:         , config(core->config)
17275:     {
33125:         VMPI_memset(&_stats, 0, sizeof(_stats));
17275:         nInit(core);
33125:         (void)logc;
29883:         verbose_only( _logc = logc; )
29883:         verbose_only( _outputCache = 0; )
35364:         verbose_only( outline[0] = '\0'; )
29883:         verbose_only( outlineEOL[0] = '\0'; )
17275: 
31475:         reset();
17275:     }
17275: 
36667: #ifdef _DEBUG
36667: 
36667:     /*static*/ LIns* const AR::BAD_ENTRY = (LIns*)0xdeadbeef;
36667: 
36677:     void AR::validateQuick()
36677:     {
36677:         NanoAssert(_highWaterMark < NJ_MAX_STACK_ENTRY);
36677:         NanoAssert(_entries[0] == NULL);
36677:         // Only check a few entries around _highWaterMark.
36677:         uint32_t const RADIUS = 4;
36677:         uint32_t const lo = (_highWaterMark > 1 + RADIUS ? _highWaterMark - RADIUS : 1);
36677:         uint32_t const hi = (_highWaterMark + 1 + RADIUS < NJ_MAX_STACK_ENTRY ? _highWaterMark + 1 + RADIUS : NJ_MAX_STACK_ENTRY);
36677:         for (uint32_t i = lo; i <= _highWaterMark; ++i)
36677:             NanoAssert(_entries[i] != BAD_ENTRY);
36677:         for (uint32_t i = _highWaterMark+1; i < hi; ++i)
36677:             NanoAssert(_entries[i] == BAD_ENTRY);
36677:     }
36677: 
36677:     void AR::validateFull()
36667:     {
36667:         NanoAssert(_highWaterMark < NJ_MAX_STACK_ENTRY);
36676:         NanoAssert(_entries[0] == NULL);
36667:         for (uint32_t i = 1; i <= _highWaterMark; ++i)
36667:             NanoAssert(_entries[i] != BAD_ENTRY);
36667:         for (uint32_t i = _highWaterMark+1; i < NJ_MAX_STACK_ENTRY; ++i)
36667:             NanoAssert(_entries[i] == BAD_ENTRY);
36667:     }
36667: 
36677:      void AR::validate()
36677:      {
36677:         static uint32_t validateCounter = 0;
36677:          if (++validateCounter >= 100)
36677:          {
36677:              validateFull();
36677:              validateCounter = 0;
36677:          }
36677:          else
36677:          {
36677:              validateQuick();
36677:          }
36677:      }
36677: 
36667: #endif
36667: 
36664:      inline void AR::clear()
36664:      {
36664:          _highWaterMark = 0;
36676:         NanoAssert(_entries[0] == NULL);
36667:     #ifdef _DEBUG
36676:         for (uint32_t i = 1; i < NJ_MAX_STACK_ENTRY; ++i)
36667:             _entries[i] = BAD_ENTRY;
36667:     #endif
36664:      }
36664: 
36664:      bool AR::Iter::next(LIns*& ins, uint32_t& nStackSlots, int32_t& arIndex)
36664:      {
36664:          while (++_i <= _ar._highWaterMark)
36664:          {
36664:              if ((ins = _ar._entries[_i]) != NULL)
36664:              {
36664:                  nStackSlots = nStackSlotsFor(ins);
36664:                  _i += nStackSlots - 1;
36664:                  arIndex = _i;
36664:                  return true;
36664:              }
36664:          }
36664:          ins = NULL;
36664:          nStackSlots = 0;
36664:          arIndex = 0;
36664:          return false;
36664:      }
36664: 
17275:     void Assembler::arReset()
17275:     {
36664:         _activation.clear();
32642:         _branchStateMap.clear();
32642:         _patches.clear();
32642:         _labels.clear();
37665:     #if NJ_USES_QUAD_CONSTANTS
37665:         _quadConstants.clear();
37665:     #endif
17275:     }
17275: 
17275:     void Assembler::registerResetAll()
17275:     {
17275:         nRegisterResetAll(_allocator);
17275: 
32666:         // At start, should have some registers free and none active.
32666:         NanoAssert(0 != _allocator.free);
32666:         NanoAssert(0 == _allocator.countActive());
32793: #ifdef NANOJIT_IA32
17275:         debug_only(_fpuStkDepth = 0; )
32793: #endif
17275:     }
17275: 
35316:     // Finds a register in 'allow' to store the result of 'ins', evicting one
35316:     // if necessary.  Doesn't consider the prior state of 'ins' (except that
35316:     // ins->isUsed() must be true).
35316:     Register Assembler::registerAlloc(LIns* ins, RegisterMask allow)
17275:     {
35316:         RegisterMask allowedAndFree = allow & _allocator.free;
35316:         Register r;
35316:         NanoAssert(ins->isUsed());
17275: 
35316:         if (allowedAndFree) {
31941:             // At least one usable register is free -- no need to steal.
31941:             // Pick a preferred one if possible.
31941:             RegisterMask preferredAndFree = allowedAndFree & SavedRegs;
31941:             RegisterMask set = ( preferredAndFree ? preferredAndFree : allowedAndFree );
35316:             r = nRegisterAllocFromSet(set);
35316:             _allocator.addActive(r, ins);
35316:             ins->setReg(r);
35316:         } else {
35316:             counter_increment(steals);
35316: 
35316:             // Nothing free, steal one.
35316:             // LSRA says pick the one with the furthest use.
36553:             LIns* vic = findVictim(allow);
36553:             NanoAssert(vic->isUsed());
36553:             r = vic->getReg();
35316: 
36553:             evict(vic);
35316: 
35316:             // r ends up staying active, but the LIns defining it changes.
36553:             _allocator.removeFree(r);
35316:             _allocator.addActive(r, ins);
35316:             ins->setReg(r);
35316:         }
36553: 
17275:         return r;
17275:     }
17275: 
35316:     // Finds a register in 'allow' to store a temporary value (one not
35316:     // associated with a particular LIns), evicting one if necessary.  The
35316:     // returned register is marked as being free and so can only be safely
36553:     // used for code generation purposes until the regstate is next inspected
36553:     // or updated.
35316:     Register Assembler::registerAllocTmp(RegisterMask allow)
35316:     {
35316:         LIns dummyIns;
35316:         dummyIns.markAsUsed();
35316:         Register r = registerAlloc(&dummyIns, allow);
18776: 
35316:         // Mark r as free, ready for use as a temporary value.
35316:         _allocator.removeActive(r);
35316:         _allocator.addFree(r);
17275:         return r;
17275:      }
17275: 
20893:     /**
20893:      * these instructions don't have to be saved & reloaded to spill,
20893:      * they can just be recalculated w/out any inputs.
20893:      */
20893:     bool Assembler::canRemat(LIns *i) {
32582:         return i->isconst() || i->isconstq() || i->isop(LIR_alloc);
20893:     }
20893: 
32784:     void Assembler::codeAlloc(NIns *&start, NIns *&end, NIns *&eip
32784:                               verbose_only(, size_t &nBytes))
31475:     {
31475:         // save the block we just filled
31475:         if (start)
31475:             CodeAlloc::add(codeList, start, end);
31475: 
31475:         // CodeAlloc contract: allocations never fail
31492:         _codeAlloc.alloc(start, end);
32784:         verbose_only( nBytes += (end - start) * sizeof(NIns); )
31475:         NanoAssert(uintptr_t(end) - uintptr_t(start) >= (size_t)LARGEST_UNDERRUN_PROT);
31475:         eip = end;
33561: 
33561:         #ifdef VTUNE
33561:         if (_nIns && _nExitIns) {
33561:             //cgen->jitAddRecord((uintptr_t)list->code, 0, 0, true); // add placeholder record for top of page
33561:             cgen->jitCodePosUpdate((uintptr_t)list->code);
33561:             cgen->jitPushInfo(); // new page requires new entry
33561:         }
33561:         #endif
31475:     }
31475: 
31475:     void Assembler::reset()
17275:     {
31475:         _nIns = 0;
31475:         _nExitIns = 0;
31475:         codeStart = codeEnd = 0;
31475:         exitStart = exitEnd = 0;
31475:         _stats.pages = 0;
31475:         codeList = 0;
31475: 
31475:         nativePageReset();
17275:         registerResetAll();
17275:         arReset();
17275:     }
17275: 
17275:     #ifdef _DEBUG
17275:     void Assembler::pageValidate()
17275:     {
31475:         if (error()) return;
35356:         // This may be a normal code chunk or an exit code chunk.
35356:         NanoAssertMsg(containsPtr(codeStart, codeEnd, _nIns),
31475:                      "Native instruction pointer overstep paging bounds; check overrideProtect for last instruction");
17275:     }
17275:     #endif
17275: 
17275:     #ifdef _DEBUG
17275: 
36664:     bool AR::isValidEntry(uint32_t idx, LIns* ins) const
36664:     {
36664:         return idx > 0 && idx <= _highWaterMark && _entries[idx] == ins;
36664:     }
36664: 
36677:     void AR::checkForResourceConsistency(const RegAlloc& regs)
36664:     {
36677:         validate();
36664:         for (uint32_t i = 1; i <= _highWaterMark; ++i)
36664:         {
36664:             LIns* ins = _entries[i];
36664:             if (!ins)
36664:                 continue;
36664:             Register r = ins->getReg();
36664:             uint32_t arIndex = ins->getArIndex();
36664:             NanoAssert(arIndex != 0);
36664:             if (ins->isop(LIR_alloc)) {
36664:                 int const n = i + (ins->size()>>2);
36664:                 for (int j=i+1; j < n; j++) {
36664:                     NanoAssert(_entries[j]==ins);
36664:                 }
36664:                 NanoAssert(arIndex == (uint32_t)n-1);
36664:                 i = n-1;
36664:             }
36664:             else if (ins->isQuad()) {
36667:                 NanoAssert(_entries[i + 1]==ins);
36664:                 i += 1; // skip high word
36664:             }
36664:             else {
36664:                 NanoAssertMsg(arIndex == i, "Stack record index mismatch");
36664:             }
36664:             NanoAssertMsg(r == UnknownReg || regs.isConsistent(r, ins), "Register record mismatch");
36664:         }
36664:     }
36664: 
17275:     void Assembler::resourceConsistencyCheck()
17275:     {
33174:         NanoAssert(!error());
17275: 
17275: #ifdef NANOJIT_IA32
25469:         NanoAssert((_allocator.active[FST0] && _fpuStkDepth == -1) ||
25469:             (!_allocator.active[FST0] && _fpuStkDepth == 0));
17275: #endif
17275: 
36664:         _activation.checkForResourceConsistency(_allocator);
17275: 
20893:         registerConsistencyCheck();
17275:     }
17275: 
20893:     void Assembler::registerConsistencyCheck()
17275:     {
35042:         RegisterMask managed = _allocator.managed;
35042:         for (Register r = FirstReg; r <= LastReg; r = nextreg(r)) {
35042:             if (rmask(r) & managed) {
35042:                 // A register managed by register allocation must be either
35042:                 // free or active, but not both.
35042:                 if (_allocator.isFree(r)) {
35042:                     NanoAssertMsgf(_allocator.getActive(r)==0,
35042:                         "register %s is free but assigned to ins", gpn(r));
35042:                 } else {
35042:                     // An LIns defining a register must have that register in
35042:                     // its reservation.
35042:                     LIns* ins = _allocator.getActive(r);
35042:                     NanoAssert(ins);
35042:                     NanoAssertMsg(r == ins->getReg(), "Register record mismatch");
17275:                 }
35042:             } else {
35042:                 // A register not managed by register allocation must be
35042:                 // neither free nor active.
35042:                 NanoAssert(!_allocator.isFree(r));
35042:                 NanoAssert(!_allocator.getActive(r));
17275:             }
17275:         }
17275:     }
17275:     #endif /* _DEBUG */
17275: 
37705:     void Assembler::findRegFor2(RegisterMask allowa, LIns* ia, Register& ra,
37705:                                 RegisterMask allowb, LIns* ib, Register& rb)
17275:     {
37705:         // There should be some overlap between 'allowa' and 'allowb', else
37705:         // there's no point calling this function.
37705:         NanoAssert(allowa & allowb);
37705: 
35365:         if (ia == ib) {
37705:             ra = rb = findRegFor(ia, allowa & allowb);  // use intersection(allowa, allowb)
35365:         } else {
35365:             // You might think we could just do this:
35365:             //
37705:             //   ra = findRegFor(ia, allowa);
37705:             //   rb = findRegFor(ib, allowb & ~rmask(ra));
35365:             //
35365:             // But if 'ib' was already in an allowed register, the first
35365:             // findRegFor() call could evict it, whereupon the second
35365:             // findRegFor() call would immediately restore it, which is
35365:             // sub-optimal.  What we effectively do instead is this:
35365:             //
37705:             //   ra = findRegFor(ia, allowa & ~rmask(rb));
37705:             //   rb = findRegFor(ib, allowb & ~rmask(ra));
35365:             //
35365:             // but we have to determine what 'rb' initially is to avoid the
35365:             // mutual dependency between the assignments.
37705:             bool rbDone = !ib->isUnusedOrHasUnknownReg() && (rb = ib->getReg(), allowb & rmask(rb));
35365:             if (rbDone) {
37705:                 allowa &= ~rmask(rb);   // ib already in an allowable reg, keep that one
17275:             }
37705:             ra = findRegFor(ia, allowa);
31941:             if (!rbDone) {
37705:                 allowb &= ~rmask(ra);
37705:                 rb = findRegFor(ib, allowb);
17275:             }
17275:         }
17275:     }
17275: 
17275:     Register Assembler::findSpecificRegFor(LIns* i, Register w)
17275:     {
17275:         return findRegFor(i, rmask(w));
17275:     }
17275: 
36553:     // Like findRegFor(), but called when the LIns is used as a pointer.  It
36553:     // doesn't have to be called, findRegFor() can still be used, but it can
36553:     // optimize the LIR_alloc case by indexing off FP, thus saving the use of
36553:     // a GpReg.
36553:     //
36552:     Register Assembler::getBaseReg(LIns *i, int &d, RegisterMask allow)
20919:     {
33551:     #if !PEDANTIC
32582:         if (i->isop(LIR_alloc)) {
36553:             // The value of a LIR_alloc is a pointer to its stack memory,
36553:             // which is always relative to FP.  So we can just return FP if we
36553:             // also adjust 'd' (and can do so in a valid manner).  Or, in the
36553:             // PEDANTIC case, we can just assign a register as normal;
36553:             // findRegFor() will allocate the stack memory for LIR_alloc if
36553:             // necessary.
36552:             d += findMemFor(i);
20919:             return FP;
32598:         }
33551:     #else
33551:         (void) d;
33551:     #endif
20919:         return findRegFor(i, allow);
20919:     }
20919: 
37705:     // Like findRegFor2(), but used for stores where the base value has the
37705:     // same type as the stored value, eg. in asm_store32() on 32-bit platforms
37705:     // and asm_store64() on 64-bit platforms.  Similar to getBaseReg(),
37705:     // findRegFor2() can be called instead, but this function can optimize the
37705:     // case where the base value is a LIR_alloc.
37705:     void Assembler::getBaseReg2(RegisterMask allowValue, LIns* value, Register& rv,
37705:                                 RegisterMask allowBase, LIns* base, Register& rb, int &d)
37705:     {
37705:     #if !PEDANTIC
37705:         if (base->isop(LIR_alloc)) {
37705:             rb = FP;
37705:             d += findMemFor(base);
37705:             rv = findRegFor(value, allowValue);
37705:             return;
37705:         }
37705:     #else
37705:         (void) d;
37705:     #endif
37705:         findRegFor2(allowValue, value, rv, allowBase, base, rb);
37705:     }
37705: 
32662:     // Finds a register in 'allow' to hold the result of 'ins'.  Used when we
36553:     // encounter a use of 'ins'.  The actions depend on the prior regstate of
32662:     // 'ins':
32662:     // - If the result of 'ins' is not in any register, we find an allowed
32662:     //   one, evicting one if necessary.
32662:     // - If the result of 'ins' is already in an allowed register, we use that.
32662:     // - If the result of 'ins' is already in a not-allowed register, we find an
32662:     //   allowed one and move it.
32662:     //
32662:     Register Assembler::findRegFor(LIns* ins, RegisterMask allow)
17275:     {
32662:         if (ins->isop(LIR_alloc)) {
36553:             // Never allocate a reg for this without stack space too.
32662:             findMemFor(ins);
20893:         }
20893: 
32727:         Register r;
17275: 
32727:         if (!ins->isUsed()) {
36553:             // 'ins' is unused, ie. dead after this point.  Mark it as used
36553:             // and allocate it a register.
32727:             ins->markAsUsed();
32662:             RegisterMask prefer = hint(ins, allow);
35316:             r = registerAlloc(ins, prefer);
32601: 
32727:         } else if (!ins->hasKnownReg()) {
36553:             // 'ins' is in a spill slot.  Allocate it a register.
32662:             RegisterMask prefer = hint(ins, allow);
35316:             r = registerAlloc(ins, prefer);
32601: 
32727:         } else if (rmask(r = ins->getReg()) & allow) {
36553:             // 'ins' is in an allowed register.
20893:             _allocator.useActive(r);
17275: 
32601:         } else {
36553:             // 'ins' is in a register (r) that's not in 'allow'.
32662:             RegisterMask prefer = hint(ins, allow);
33125: #ifdef NANOJIT_IA32
32601:             if (((rmask(r)&XmmRegs) && !(allow&XmmRegs)) ||
32601:                 ((rmask(r)&x87Regs) && !(allow&x87Regs)))
17275:             {
20893:                 // x87 <-> xmm copy required
20893:                 //_nvprof("fpu-evict",1);
36553:                 evict(ins);
35316:                 r = registerAlloc(ins, prefer);
32601:             } else
33174: #elif defined(NANOJIT_PPC)
33174:             if (((rmask(r)&GpRegs) && !(allow&GpRegs)) ||
33174:                 ((rmask(r)&FpRegs) && !(allow&FpRegs)))
33174:             {
36553:                 evict(ins);
35318:                 r = registerAlloc(ins, prefer);
33174:             } else
32601: #endif
17275:             {
32662:                 // The post-state register holding 'ins' is 's', the pre-state
32662:                 // register holding 'ins' is 'r'.  For example, if s=eax and
32662:                 // r=ecx:
32662:                 //
32662:                 // pre-state:   ecx(ins)
32662:                 // instruction: mov eax, ecx
32662:                 // post-state:  eax(ins)
32662:                 //
36553:                 Register s = r;
32727:                 _allocator.retire(r);
35316:                 r = registerAlloc(ins, prefer);
36553: 
36553:                 // 'ins' is in 'allow', in register r (different to the old r);
36553:                 //  s is the old r.
32601:                 if ((rmask(s) & GpRegs) && (rmask(r) & GpRegs)) {
36553:                     MR(s, r);   // move 'ins' from its pre-state reg (r) to its post-state reg (s)
36553:                 } else {
32601:                     asm_nongp_copy(s, r);
17275:                 }
17275:             }
17275:         }
36553: 
32601:         return r;
32601:     }
32601: 
34577:     // Like findSpecificRegFor(), but only for when 'r' is known to be free
34577:     // and 'ins' is known to not already have a register allocated.  Updates
36553:     // the regstate (maintaining the invariants) but does not generate any
36553:     // code.  The return value is redundant, always being 'r', but it's
34577:     // sometimes useful to have it there for assignments.
34577:     Register Assembler::findSpecificRegForUnallocated(LIns* ins, Register r)
34577:     {
34577:         if (ins->isop(LIR_alloc)) {
34577:             // never allocate a reg for this w/out stack space too
34577:             findMemFor(ins);
34577:         }
34577: 
34577:         NanoAssert(ins->isUnusedOrHasUnknownReg());
34577:         NanoAssert(_allocator.free & rmask(r));
34577: 
34577:         if (!ins->isUsed())
34577:             ins->markAsUsed();
34577:         ins->setReg(r);
34577:         _allocator.removeFree(r);
34577:         _allocator.addActive(r, ins);
34577: 
34577:         return r;
34577:     }
34577: 
37665: #if NJ_USES_QUAD_CONSTANTS
37665:     const uint64_t* Assembler::findQuadConstant(uint64_t q)
37665:     {
37665:         uint64_t* p = _quadConstants.get(q);
37665:         if (!p)
37665:         {
37665:             p = new (_dataAlloc) uint64_t;
37665:             *p = q;
37665:             _quadConstants.put(q, p);
37665:         }
37665:         return p;
37665:     }
37665: #endif
37665: 
32727:     int Assembler::findMemFor(LIns *ins)
17275:     {
37665: #if NJ_USES_QUAD_CONSTANTS
37665:         NanoAssert(!ins->isconstq());
37665: #endif
32727:         if (!ins->isUsed())
32727:             ins->markAsUsed();
32727:         if (!ins->getArIndex()) {
36664:             uint32_t const arIndex = arReserve(ins);
36664:             ins->setArIndex(arIndex);
36664:             NanoAssert(_activation.isValidEntry(ins->getArIndex(), ins) == (arIndex != 0));
20893:         }
32727:         return disp(ins);
17275:     }
17275: 
36553:     // XXX: this function is dangerous and should be phased out;
36553:     // See bug 513615.  Calls to it should replaced it with a
36553:     // prepareResultReg() / generate code / freeResourcesOf() sequence.
32727:     Register Assembler::prepResultReg(LIns *ins, RegisterMask allow)
17275:     {
35354: #ifdef NANOJIT_IA32
35354:         const bool pop = (allow & rmask(FST0)) &&
35354:                          (ins->isUnusedOrHasUnknownReg() || ins->getReg() != FST0);
35354: #else
35354:         const bool pop = false;
35354: #endif
32727:         Register r = findRegFor(ins, allow);
32727:         freeRsrcOf(ins, pop);
32727:         return r;
17275:     }
17275: 
36553:     // Finds a register in 'allow' to hold the result of 'ins'.  Also
36553:     // generates code to spill the result if necessary.  Called just prior to
36553:     // generating the code for 'ins' (because we generate code backwards).
36553:     //
36553:     // An example where no spill is necessary.  Lines marked '*' are those
36553:     // done by this function.
36553:     //
36553:     //   regstate:  R
36553:     //   asm:       define res into r
36553:     // * regstate:  R + r(res)
36553:     //              ...
36553:     //   asm:       use res in r
36553:     //
36553:     // An example where a spill is necessary.
36553:     //
36553:     //   regstate:  R
36553:     //   asm:       define res into r
36553:     // * regstate:  R + r(res)
36553:     // * asm:       spill res from r
36553:     //   regstate:  R
36553:     //              ...
36553:     //   asm:       restore res into r2
36553:     //   regstate:  R + r2(res) + other changes from "..."
36553:     //   asm:       use res in r2
36553:     //
36553:     Register Assembler::prepareResultReg(LIns *ins, RegisterMask allow)
36553:     {
36553:         // At this point, we know the result of 'ins' result has a use later
36553:         // in the code.  (Exception: if 'ins' is a call to an impure function
36553:         // the return value may not be used, but 'ins' will still be present
36553:         // because it has side-effects.)  It may have had to be evicted, in
36553:         // which case the restore will have already been generated, so we now
36553:         // generate the spill (unless the restore was actually a
36553:         // rematerialize, in which case it's not necessary).
36553:         //
36553:         // As for 'pop':  it's only relevant on i386 and if 'allow' includes
36553:         // FST0, in which case we have to pop if 'ins' isn't in FST0 in the
36553:         // post-regstate.  This could be because 'ins' is unused, 'ins' is in
36553:         // a spill slot, or 'ins' is in an XMM register.
36553: #ifdef NANOJIT_IA32
36553:         const bool pop = (allow & rmask(FST0)) &&
36553:                          (ins->isUnusedOrHasUnknownReg() || ins->getReg() != FST0);
36553: #else
36553:         const bool pop = false;
36553: #endif
36553:         Register r = findRegFor(ins, allow);
36553:         asm_spilli(ins, pop);
36553:         return r;
36553:     }
36553: 
32727:     void Assembler::asm_spilli(LInsp ins, bool pop)
20919:     {
32727:         int d = disp(ins);
32727:         Register r = ins->getReg();
35364:         verbose_only( if (d && (_logc->lcbits & LC_Assembly)) {
35364:                          setOutputForEOL("  <= spill %s",
32727:                          _thisfrag->lirbuf->names->formatRef(ins)); } )
33994:         asm_spill(r, d, pop, ins->isQuad());
20919:     }
20919: 
36553:     // XXX: This function is error-prone and should be phased out; see bug 513615.
32727:     void Assembler::freeRsrcOf(LIns *ins, bool pop)
17275:     {
32727:         Register r = ins->getReg();
32727:         if (isKnownReg(r)) {
32727:             asm_spilli(ins, pop);
32727:             _allocator.retire(r);   // free any register associated with entry
17275:         }
36664:         arFreeIfInUse(ins);        // free any stack stack space associated with entry
32727:         ins->markAsClear();
17275:     }
17275: 
36553:     // Frees all record of registers and spill slots used by 'ins'.
36553:     void Assembler::freeResourcesOf(LIns *ins)
36553:     {
36553:         Register r = ins->getReg();
36553:         if (isKnownReg(r)) {
36553:             _allocator.retire(r);   // free any register associated with entry
36553:         }
36664:         arFreeIfInUse(ins);        // free any stack stack space associated with entry
36553:         ins->markAsClear();
36553:     }
36553: 
36553:     // Frees 'r' in the RegAlloc regstate, if it's not already free.
32621:     void Assembler::evictIfActive(Register r)
17275:     {
32626:         if (LIns* vic = _allocator.getActive(r)) {
36553:             NanoAssert(vic->getReg() == r);
36553:             evict(vic);
32621:         }
32621:     }
32621: 
36553:     // Frees 'r' (which currently holds the result of 'vic') in the regstate.
36553:     // An example:
32662:     //
36553:     //   pre-regstate:  eax(ld1)
32662:     //   instruction:   mov ebx,-4(ebp) <= restore add1   # %ebx is dest
36553:     //   post-regstate: eax(ld1) ebx(add1)
32662:     //
32662:     // At run-time we are *restoring* 'add1' into %ebx, hence the call to
32662:     // asm_restore().  But at regalloc-time we are moving backwards through
32662:     // the code, so in that sense we are *evicting* 'add1' from %ebx.
32662:     //
36553:     void Assembler::evict(LIns* vic)
32621:     {
32621:         // Not free, need to steal.
32621:         counter_increment(steals);
32621: 
36553:         Register r = vic->getReg();
36553: 
32621:         NanoAssert(!_allocator.isFree(r));
32621:         NanoAssert(vic == _allocator.getActive(r));
32621: 
35364:         verbose_only( if (_logc->lcbits & LC_Assembly) {
35364:                         setOutputForEOL("  <= restore %s",
35364:                         _thisfrag->lirbuf->names->formatRef(vic)); } )
35365:         asm_restore(vic, r);
36553: 
36553:         _allocator.retire(r);
36553:         if (vic->isUsed())
36553:             vic->setReg(UnknownReg);
36553: 
36553:         // At this point 'vic' is unused (if rematerializable), or in a spill
36553:         // slot (if not).
17275:     }
17275: 
17275:     void Assembler::patch(GuardRecord *lr)
17275:     {
30740:         if (!lr->jmp) // the guard might have been eliminated as redundant
30740:             return;
20931:         Fragment *frag = lr->exit->target;
20893:         NanoAssert(frag->fragEntry != 0);
32583:         nPatchBranch((NIns*)lr->jmp, frag->fragEntry);
34321:         CodeAlloc::flushICache(lr->jmp, LARGEST_BRANCH_PATCH);
32583:         verbose_only(verbose_outputf("patching jump at %p to target %p\n",
32583:             lr->jmp, frag->fragEntry);)
17275:     }
17275: 
20931:     void Assembler::patch(SideExit *exit)
17275:     {
20931:         GuardRecord *rec = exit->guards;
31511:         NanoAssert(rec);
20931:         while (rec) {
20931:             patch(rec);
20946:             rec = rec->next;
20931:         }
17275:     }
17275: 
25099: #ifdef NANOJIT_IA32
25099:     void Assembler::patch(SideExit* exit, SwitchInfo* si)
25099:     {
25099:         for (GuardRecord* lr = exit->guards; lr; lr = lr->next) {
25099:             Fragment *frag = lr->exit->target;
25099:             NanoAssert(frag->fragEntry != 0);
25099:             si->table[si->index] = frag->fragEntry;
25099:         }
25099:     }
25099: #endif
25099: 
17308:     NIns* Assembler::asm_exit(LInsp guard)
17275:     {
20931:         SideExit *exit = guard->record()->exit;
17275:         NIns* at = 0;
32642:         if (!_branchStateMap.get(exit))
17275:         {
17308:             at = asm_leave_trace(guard);
17275:         }
17275:         else
17275:         {
32642:             RegAlloc* captured = _branchStateMap.get(exit);
20893:             intersectRegisterState(*captured);
17275:             at = exit->target->fragEntry;
20893:             NanoAssert(at != 0);
32642:             _branchStateMap.remove(exit);
17275:         }
17275:         return at;
17275:     }
17275: 
17308:     NIns* Assembler::asm_leave_trace(LInsp guard)
17275:     {
17275:         verbose_only( int32_t nativeSave = _stats.native );
29883:         verbose_only( verbose_outputf("----------------------------------- ## END exit block %p", guard);)
17275: 
36672:         // This point is unreachable.  So free all the registers.  If an
36672:         // instruction has a stack entry we will leave it alone, otherwise we
36672:         // free it entirely.  intersectRegisterState() will restore.
17275:         RegAlloc capture = _allocator;
17275:         releaseRegisters();
17275: 
35356:         swapCodeChunks();
17275:         _inExit = true;
17275: 
32793: #ifdef NANOJIT_IA32
17275:         debug_only( _sv_fpuStkDepth = _fpuStkDepth; _fpuStkDepth = 0; )
32793: #endif
17275: 
17516:         nFragExit(guard);
17516: 
36672:         // Restore the callee-saved register and parameters.
21477:         assignSavedRegs();
21477:         assignParamRegs();
17275: 
20893:         intersectRegisterState(capture);
17275: 
17275:         // this can be useful for breaking whenever an exit is taken
17275:         //INT3();
17275:         //NOP();
17275: 
17275:         // we are done producing the exit logic for the guard so demark where our exit block code begins
22669:         NIns* jmpTarget = _nIns;     // target in exit path for our mainline conditional jump
17275: 
17275:         // swap back pointers, effectively storing the last location used in the exit path
35356:         swapCodeChunks();
17275:         _inExit = false;
17275: 
35356:         //verbose_only( verbose_outputf("         LIR_xt/xf swapCodeChunks, _nIns is now %08X(%08X), _nExitIns is now %08X(%08X)",_nIns, *_nIns,_nExitIns,*_nExitIns) );
29883:         verbose_only( verbose_outputf("%010lx:", (unsigned long)jmpTarget);)
29883:         verbose_only( verbose_outputf("----------------------------------- ## BEGIN exit block (LIR_xt|LIR_xf)") );
17275: 
17275: #ifdef NANOJIT_IA32
21683:         NanoAssertMsgf(_fpuStkDepth == _sv_fpuStkDepth, "LIR_xtf, _fpuStkDepth=%d, expect %d",_fpuStkDepth, _sv_fpuStkDepth);
17275:         debug_only( _fpuStkDepth = _sv_fpuStkDepth; _sv_fpuStkDepth = 9999; )
17275: #endif
17275: 
17275:         verbose_only(_stats.exitnative += (_stats.native-nativeSave));
17275: 
22669:         return jmpTarget;
17275:     }
17275: 
32642:     void Assembler::beginAssembly(Fragment *frag)
17275:     {
32784:         verbose_only( codeBytes = 0; )
32784:         verbose_only( exitBytes = 0; )
32784: 
31475:         reset();
31475: 
31475:         NanoAssert(codeList == 0);
31475:         NanoAssert(codeStart == 0);
31475:         NanoAssert(codeEnd == 0);
31475:         NanoAssert(exitStart == 0);
31475:         NanoAssert(exitEnd == 0);
31475:         NanoAssert(_nIns == 0);
31475:         NanoAssert(_nExitIns == 0);
24863: 
20893:         _thisfrag = frag;
31511:         _inExit = false;
17275: 
17275:         counter_reset(native);
17275:         counter_reset(exitnative);
17275:         counter_reset(steals);
17275:         counter_reset(spills);
17275:         counter_reset(remats);
17275: 
17275:         setError(None);
17275: 
17275:         // native code gen buffer setup
17275:         nativePageSetup();
17275: 
17275:         // make sure we got memory at least one page
17308:         if (error()) return;
17275: 
20893: #ifdef PERFM
20893:         _stats.pages = 0;
20893:         _stats.codeStart = _nIns-1;
20893:         _stats.codeExitStart = _nExitIns-1;
20893: #endif /* PERFM */
20893: 
32634:         _epilogue = NULL;
17275: 
32634:         nBeginAssembly();
17275:     }
17275: 
34341:     void Assembler::assemble(Fragment* frag, LirFilter* reader)
17275:     {
17308:         if (error()) return;
17275:         _thisfrag = frag;
17275: 
32784:         // check the fragment is starting out with a sane profiling state
32784:         verbose_only( NanoAssert(frag->nStaticExits == 0); )
32784:         verbose_only( NanoAssert(frag->nCodeBytes == 0); )
32784:         verbose_only( NanoAssert(frag->nExitBytes == 0); )
32784:         verbose_only( NanoAssert(frag->profCount == 0); )
32784:         verbose_only( if (_logc->lcbits & LC_FragProfile)
32784:                           NanoAssert(frag->profFragID > 0);
32784:                       else
32784:                           NanoAssert(frag->profFragID == 0); )
32784: 
17275:         _inExit = false;
30253: 
34341:         gen(reader);
20893: 
20893:         if (!error()) {
20893:             // patch all branches
32642:             NInsMap::Iter iter(_patches);
31512:             while (iter.next()) {
31512:                 NIns* where = iter.key();
35087:                 LIns* target = iter.value();
35087:                 if (target->isop(LIR_jtbl)) {
35087:                     // Need to patch up a whole jump table, 'where' is the table.
35087:                     LIns *jtbl = target;
35087:                     NIns** native_table = (NIns**) where;
35087:                     for (uint32_t i = 0, n = jtbl->getTableSize(); i < n; i++) {
35087:                         LabelState* lstate = _labels.get(jtbl->getTarget(i));
35087:                         NIns* ntarget = lstate->addr;
35087:                         if (ntarget) {
35087:                             native_table[i] = ntarget;
35087:                         } else {
27017:                             setError(UnknownBranch);
20893:                             break;
20893:                         }
20893:                     }
35087:                 } else {
35087:                     // target is a label for a single-target branch
35087:                     LabelState *lstate = _labels.get(target);
35087:                     NIns* ntarget = lstate->addr;
35087:                     if (ntarget) {
35087:                         nPatchBranch(where, ntarget);
35087:                     } else {
35087:                         setError(UnknownBranch);
35087:                         break;
35087:                     }
35087:                 }
35087:             }
20893:         }
17275:     }
17275: 
32617:     void Assembler::endAssembly(Fragment* frag)
17275:     {
24486:         // don't try to patch code if we are in an error state since we might have partially
24486:         // overwritten the code cache already
31475:         if (error()) {
31475:             // something went wrong, release all allocated code memory
31492:             _codeAlloc.freeAll(codeList);
37698:             if (_nExitIns)
31492:             	_codeAlloc.free(exitStart, exitEnd);
31492:             _codeAlloc.free(codeStart, codeEnd);
37698:             codeList = NULL;
24486:             return;
31475:         }
24486: 
31475:         NIns* fragEntry = genPrologue();
22648:         verbose_only( asm_output("[prologue]"); )
17275: 
36664:         debug_only(_activation.checkForResourceLeaks());
17275: 
35356:         NanoAssert(!_inExit);
31475:         // save used parts of current block on fragment's code list, free the rest
31475: #ifdef NANOJIT_ARM
32784:         // [codeStart, _nSlot) ... gap ... [_nIns, codeEnd)
37698:         if (_nExitIns) {
31492: 			_codeAlloc.addRemainder(codeList, exitStart, exitEnd, _nExitSlot, _nExitIns);
37698: 			verbose_only( exitBytes -= (_nExitIns - _nExitSlot) * sizeof(NIns); )
37698:         }
31492:         _codeAlloc.addRemainder(codeList, codeStart, codeEnd, _nSlot, _nIns);
32788:         verbose_only( codeBytes -= (_nIns - _nSlot) * sizeof(NIns); )
31475: #else
32784:         // [codeStart ... gap ... [_nIns, codeEnd))
37698:         if (_nExitIns) {
31492:         	_codeAlloc.addRemainder(codeList, exitStart, exitEnd, exitStart, _nExitIns);
37698:             verbose_only( exitBytes -= (_nExitIns - exitStart) * sizeof(NIns); )
37698:         }
31492:         _codeAlloc.addRemainder(codeList, codeStart, codeEnd, codeStart, _nIns);
32784:         verbose_only( codeBytes -= (_nIns - codeStart) * sizeof(NIns); )
31475: #endif
31475: 
31475:         // at this point all our new code is in the d-cache and not the i-cache,
31475:         // so flush the i-cache on cpu's that need it.
34321:         CodeAlloc::flushICache(codeList);
31475: 
31475:         // save entry point pointers
31071:         frag->fragEntry = fragEntry;
31475:         frag->setCode(_nIns);
33125:         PERFM_NVPROF("code", CodeAlloc::size(codeList));
17275: 
32793: #ifdef NANOJIT_IA32
31475:         NanoAssertMsgf(_fpuStkDepth == 0,"_fpuStkDepth %d\n",_fpuStkDepth);
32793: #endif
17275: 
31475:         debug_only( pageValidate(); )
32642:         NanoAssert(_branchStateMap.isEmpty());
17275:     }
17275: 
17275:     void Assembler::releaseRegisters()
17275:     {
17275:         for (Register r = FirstReg; r <= LastReg; r = nextreg(r))
17275:         {
32727:             LIns *ins = _allocator.getActive(r);
35042:             if (ins) {
35042:                 // Clear reg allocation, preserve stack allocation.
17275:                 _allocator.retire(r);
32727:                 NanoAssert(r == ins->getReg());
32727:                 ins->setReg(UnknownReg);
17275: 
35042:                 if (!ins->getArIndex()) {
32727:                     ins->markAsClear();
17275:                 }
17275:             }
17275:         }
17275:     }
17275: 
20893: #ifdef PERFM
20893: #define countlir_live() _nvprof("lir-live",1)
20893: #define countlir_ret() _nvprof("lir-ret",1)
20893: #define countlir_alloc() _nvprof("lir-alloc",1)
20893: #define countlir_var() _nvprof("lir-var",1)
20893: #define countlir_use() _nvprof("lir-use",1)
20893: #define countlir_def() _nvprof("lir-def",1)
20893: #define countlir_imm() _nvprof("lir-imm",1)
20893: #define countlir_param() _nvprof("lir-param",1)
20893: #define countlir_cmov() _nvprof("lir-cmov",1)
20893: #define countlir_ld() _nvprof("lir-ld",1)
20893: #define countlir_ldq() _nvprof("lir-ldq",1)
20893: #define countlir_alu() _nvprof("lir-alu",1)
20893: #define countlir_qjoin() _nvprof("lir-qjoin",1)
20893: #define countlir_qlo() _nvprof("lir-qlo",1)
20893: #define countlir_qhi() _nvprof("lir-qhi",1)
20893: #define countlir_fpu() _nvprof("lir-fpu",1)
20893: #define countlir_st() _nvprof("lir-st",1)
20893: #define countlir_stq() _nvprof("lir-stq",1)
20893: #define countlir_jmp() _nvprof("lir-jmp",1)
20893: #define countlir_jcc() _nvprof("lir-jcc",1)
20893: #define countlir_label() _nvprof("lir-label",1)
20893: #define countlir_xcc() _nvprof("lir-xcc",1)
20893: #define countlir_x() _nvprof("lir-x",1)
20893: #define countlir_call() _nvprof("lir-call",1)
35087: #define countlir_jtbl() _nvprof("lir-jtbl",1)
20893: #else
20893: #define countlir_live()
20893: #define countlir_ret()
20893: #define countlir_alloc()
20893: #define countlir_var()
20893: #define countlir_use()
20893: #define countlir_def()
20893: #define countlir_imm()
20893: #define countlir_param()
20893: #define countlir_cmov()
20893: #define countlir_ld()
20893: #define countlir_ldq()
20893: #define countlir_alu()
20893: #define countlir_qjoin()
20893: #define countlir_qlo()
20893: #define countlir_qhi()
20893: #define countlir_fpu()
20893: #define countlir_st()
20893: #define countlir_stq()
20893: #define countlir_jmp()
20893: #define countlir_jcc()
20893: #define countlir_label()
20893: #define countlir_xcc()
20893: #define countlir_x()
20893: #define countlir_call()
35087: #define countlir_jtbl()
20893: #endif
20893: 
32642:     void Assembler::gen(LirFilter* reader)
17275:     {
32784:         NanoAssert(_thisfrag->nStaticExits == 0);
32784: 
33174:         // trace must end with LIR_x, LIR_[f]ret, LIR_xtbl, or LIR_[f]live
25085:         NanoAssert(reader->pos()->isop(LIR_x) ||
25099:                    reader->pos()->isop(LIR_ret) ||
32598:                    reader->pos()->isop(LIR_fret) ||
32598:                    reader->pos()->isop(LIR_xtbl) ||
33125:                    reader->pos()->isop(LIR_flive) ||
33125:                    reader->pos()->isop(LIR_live));
17275: 
31513:         InsList pending_lives(alloc);
30253: 
30277:         for (LInsp ins = reader->read(); !ins->isop(LIR_start) && !error();
30277:                                          ins = reader->read())
17275:         {
32662:             /* What's going on here: we're visiting all the LIR instructions
32662:                in the buffer, working strictly backwards in buffer-order, and
32662:                generating machine instructions for them as we go.
30277: 
32662:                For each LIns, we first determine whether it's actually
32662:                necessary, and if not skip it.  Otherwise we generate code for
32662:                it.  There are two kinds of "necessary" instructions:
30277: 
32662:                - "Statement" instructions, which have side effects.  Anything
32662:                  that could change control flow or the state of memory.
30277: 
32662:                - "Value" or "expression" instructions, which compute a value
32662:                  based only on the operands to the instruction (and, in the
32662:                  case of loads, the state of memory).  Because we visit
32662:                  instructions in reverse order, if some previously visited
32662:                  instruction uses the value computed by this instruction, then
32662:                  this instruction will already have a register assigned to
35365:                  hold that value.  Hence we can consult the instruction to
35365:                  detect whether its value is in fact used (i.e. not dead).
30277: 
32662:               Note that the backwards code traversal can make register
32662:               allocation confusing.  (For example, we restore a value before
32662:               we spill it!)  In particular, words like "before" and "after"
32662:               must be used very carefully -- their meaning at regalloc-time is
32662:               opposite to their meaning at run-time.  We use the term
36553:               "pre-regstate" to refer to the register allocation state that
36553:               occurs prior to an instruction's execution, and "post-regstate"
36553:               to refer to the state that occurs after an instruction's
36553:               execution, e.g.:
30277: 
36553:                 pre-regstate:  ebx(ins)
32662:                 instruction:   mov eax, ebx     // mov dst, src
36553:                 post-regstate: eax(ins)
32662: 
36553:               At run-time, the instruction updates the pre-regstate into the
36553:               post-regstate (and these states are the real machine's
36553:               regstates).  But when allocating registers, because we go
36553:               backwards, the pre-regstate is constructed from the
36553:               post-regstate (and these regstates are those stored in
36553:               RegAlloc).
36553: 
36553:               One consequence of generating code backwards is that we tend to
36553:               both spill and restore registers as early (at run-time) as
36553:               possible;  this is good for tolerating memory latency.  If we
36553:               generated code forwards, we would expect to both spill and
36553:               restore registers as late (at run-time) as possible;  this might
36553:               be better for reducing register pressure.
30277:             */
32727:             bool required = ins->isStmt() || ins->isUsed();
30277:             if (!required)
30277:                 continue;
30277: 
35364: #ifdef NJ_VERBOSE
36553:             // Output the post-regstate (registers and/or activation).
35364:             // Because asm output comes in reverse order, doing it now means
35364:             // it is printed after the LIR and asm, exactly when the
36553:             // post-regstate should be shown.
35364:             if ((_logc->lcbits & LC_Assembly) && (_logc->lcbits & LC_Activation))
35364:                 printActivationState();
35364:             if ((_logc->lcbits & LC_Assembly) && (_logc->lcbits & LC_RegAlloc))
35364:                 printRegState();
35364: #endif
35364: 
17275:             LOpcode op = ins->opcode();
17275:             switch(op)
17275:             {
17275:                 default:
36390:                     NanoAssertMsgf(false, "unsupported LIR instruction: %d\n", op);
17275:                     break;
17275: 
32659:                 case LIR_regfence:
32755:                     evictAllActiveRegs();
32659:                     break;
32659: 
32646:                 case LIR_flive:
20893:                 case LIR_live: {
20893:                     countlir_live();
32598:                     LInsp op1 = ins->oprnd1();
32598:                     // alloca's are meant to live until the point of the LIR_live instruction, marking
32598:                     // other expressions as live ensures that they remain so at loop bottoms.
32598:                     // alloca areas require special treatment because they are accessed indirectly and
32598:                     // the indirect accesses are invisible to the assembler, other than via LIR_live.
32598:                     // other expression results are only accessed directly in ways that are visible to
32598:                     // the assembler, so extending those expression's lifetimes past the last loop edge
32598:                     // isn't necessary.
32598:                     if (op1->isop(LIR_alloc)) {
32598:                         findMemFor(op1);
32598:                     } else {
32646:                         pending_lives.add(ins);
32598:                     }
20893:                     break;
20893:                 }
20893: 
32631:                 case LIR_fret:
33125:                 case LIR_ret:  {
20893:                     countlir_ret();
32631:                     asm_ret(ins);
22706:                     break;
33125:                 }
20893: 
36553:                 // Allocate some stack space.  The value of this instruction
20893:                 // is the address of the stack space.
32582:                 case LIR_alloc: {
20893:                     countlir_alloc();
32727:                     NanoAssert(ins->getArIndex() != 0);
32727:                     Register r = ins->getReg();
32727:                     if (isKnownReg(r)) {
36553:                         asm_restore(ins, r);
20893:                         _allocator.retire(r);
32727:                         ins->setReg(UnknownReg);
20893:                     }
36553:                     freeResourcesOf(ins);
20893:                     break;
20893:                 }
17275:                 case LIR_int:
17275:                 {
20893:                     countlir_imm();
20921:                     asm_int(ins);
17275:                     break;
17275:                 }
32640:                 case LIR_float:
17275:                 case LIR_quad:
17275:                 {
20893:                     countlir_imm();
17378:                     asm_quad(ins);
17275:                     break;
17275:                 }
29869: #if !defined NANOJIT_64BIT
17275:                 case LIR_callh:
17275:                 {
17275:                     // return result of quad-call in register
17275:                     prepResultReg(ins, rmask(retRegs[1]));
17275:                     // if hi half was used, we must use the call to ensure it happens
22661:                     findSpecificRegFor(ins->oprnd1(), retRegs[0]);
17275:                     break;
17275:                 }
29869: #endif
32598:                 case LIR_param:
17275:                 {
20893:                     countlir_param();
20921:                     asm_param(ins);
17275:                     break;
17275:                 }
17275:                 case LIR_qlo:
17275:                 {
20893:                     countlir_qlo();
20921:                     asm_qlo(ins);
17275:                     break;
17275:                 }
17275:                 case LIR_qhi:
17275:                 {
20893:                     countlir_qhi();
20921:                     asm_qhi(ins);
17275:                     break;
17275:                 }
18254:                 case LIR_qcmov:
17275:                 case LIR_cmov:
17275:                 {
20893:                     countlir_cmov();
20921:                     asm_cmov(ins);
17275:                     break;
17275:                 }
36372:                 case LIR_ldzb:
36372:                 case LIR_ldzs:
36372:                 case LIR_ldsb:
36372:                 case LIR_ldss:
36372:                 case LIR_ldcsb:
36372:                 case LIR_ldcss:
17275:                 case LIR_ld:
17275:                 case LIR_ldc:
17275:                 case LIR_ldcb:
21469:                 case LIR_ldcs:
17275:                 {
20893:                     countlir_ld();
36372:                     asm_load32(ins);
17275:                     break;
17275:                 }
36372: 
36372:                 case LIR_ld32f:
36372:                 case LIR_ldc32f:
17275:                 case LIR_ldq:
20893:                 case LIR_ldqc:
37020:                 case LIR_ldf:
37020:                 case LIR_ldfc:
17275:                 {
20893:                     countlir_ldq();
17275:                     asm_load64(ins);
17275:                     break;
17275:                 }
17275:                 case LIR_neg:
17275:                 case LIR_not:
17275:                 {
20893:                     countlir_alu();
20921:                     asm_neg_not(ins);
17275:                     break;
17275:                 }
17275:                 case LIR_qjoin:
17275:                 {
20893:                     countlir_qjoin();
17275:                     asm_qjoin(ins);
17275:                     break;
17275:                 }
17275: 
18220: #if defined NANOJIT_64BIT
18220:                 case LIR_qiadd:
18220:                 case LIR_qiand:
18220:                 case LIR_qilsh:
32575:                 case LIR_qursh:
32575:                 case LIR_qirsh:
18645:                 case LIR_qior:
32575:                 case LIR_qaddp:
32575:                 case LIR_qxor:
18220:                 {
18220:                     asm_qbinop(ins);
18220:                     break;
18220:                 }
18220: #endif
18220: 
17275:                 case LIR_add:
30437:                 case LIR_iaddp:
17275:                 case LIR_sub:
17275:                 case LIR_mul:
17275:                 case LIR_and:
17275:                 case LIR_or:
17275:                 case LIR_xor:
17275:                 case LIR_lsh:
17275:                 case LIR_rsh:
17275:                 case LIR_ush:
29354:                 case LIR_div:
29354:                 case LIR_mod:
17275:                 {
20893:                     countlir_alu();
20921:                     asm_arith(ins);
17275:                     break;
17275:                 }
17275:                 case LIR_fneg:
17275:                 {
20893:                     countlir_fpu();
17378:                     asm_fneg(ins);
17275:                     break;
17275:                 }
17275:                 case LIR_fadd:
17275:                 case LIR_fsub:
17275:                 case LIR_fmul:
17275:                 case LIR_fdiv:
17275:                 {
20893:                     countlir_fpu();
17378:                     asm_fop(ins);
17275:                     break;
17275:                 }
17275:                 case LIR_i2f:
17275:                 {
20893:                     countlir_fpu();
17378:                     asm_i2f(ins);
17275:                     break;
17275:                 }
17275:                 case LIR_u2f:
17275:                 {
20893:                     countlir_fpu();
17378:                     asm_u2f(ins);
17275:                     break;
17275:                 }
37700:                 case LIR_f2i:
37700:                 {
37700:                     countlir_fpu();
37700:                     asm_f2i(ins);
37700:                     break;
37700:                 }
32575:                 case LIR_i2q:
32575:                 case LIR_u2q:
32575:                 {
32575:                     countlir_alu();
32575:                     asm_promote(ins);
32575:                     break;
32575:                 }
36372:                 case LIR_stb:
36372:                 case LIR_sts:
17275:                 case LIR_sti:
17275:                 {
20893:                     countlir_st();
36372:                     asm_store32(op, ins->oprnd1(), ins->disp(), ins->oprnd2());
17308:                     break;
17308:                 }
36372:                 case LIR_st32f:
17308:                 case LIR_stqi:
37020:                 case LIR_stfi:
17308:                 {
20893:                     countlir_stq();
17275:                     LIns* value = ins->oprnd1();
17275:                     LIns* base = ins->oprnd2();
30238:                     int dr = ins->disp();
37020:                     if (value->isop(LIR_qjoin) && op == LIR_stfi)
20893:                     {
37020:                         // This is correct for little-endian only.
36373:                         asm_store32(LIR_sti, value->oprnd1(), dr, base);
36373:                         asm_store32(LIR_sti, value->oprnd2(), dr+4, base);
17308:                     }
20893:                     else
20893:                     {
36372:                         asm_store64(op, value, dr, base);
17308:                     }
17275:                     break;
17275:                 }
20893: 
20893:                 case LIR_j:
20893:                 {
20893:                     countlir_jmp();
20893:                     LInsp to = ins->getTarget();
32642:                     LabelState *label = _labels.get(to);
20893:                     // the jump is always taken so whatever register state we
20893:                     // have from downstream code, is irrelevant to code before
20893:                     // this jump.  so clear it out.  we will pick up register
20893:                     // state from the jump target, if we have seen that label.
20893:                     releaseRegisters();
20893:                     if (label && label->addr) {
20893:                         // forward jump - pick up register state from target.
20893:                         unionRegisterState(label->regs);
20893:                         JMP(label->addr);
20893:                     }
20893:                     else {
20893:                         // backwards jump
30253:                         handleLoopCarriedExprs(pending_lives);
20893:                         if (!label) {
20893:                             // save empty register state at loop header
32642:                             _labels.add(to, 0, _allocator);
20893:                         }
20893:                         else {
20893:                             intersectRegisterState(label->regs);
20893:                         }
20893:                         JMP(0);
32642:                         _patches.put(_nIns, to);
20893:                     }
20893:                     break;
20893:                 }
20893: 
20893:                 case LIR_jt:
20893:                 case LIR_jf:
20893:                 {
20893:                     countlir_jcc();
20893:                     LInsp to = ins->getTarget();
20893:                     LIns* cond = ins->oprnd1();
32642:                     LabelState *label = _labels.get(to);
20893:                     if (label && label->addr) {
20893:                         // forward jump to known label.  need to merge with label's register state.
20893:                         unionRegisterState(label->regs);
30730:                         asm_branch(op == LIR_jf, cond, label->addr);
20893:                     }
20893:                     else {
20893:                         // back edge.
30253:                         handleLoopCarriedExprs(pending_lives);
20893:                         if (!label) {
20893:                             // evict all registers, most conservative approach.
32755:                             evictAllActiveRegs();
32642:                             _labels.add(to, 0, _allocator);
20893:                         }
20893:                         else {
20893:                             // evict all registers, most conservative approach.
20893:                             intersectRegisterState(label->regs);
20893:                         }
30730:                         NIns *branch = asm_branch(op == LIR_jf, cond, 0);
32642:                         _patches.put(branch,to);
20893:                     }
20893:                     break;
20893:                 }
35087: 
35087:                 #if NJ_JTBL_SUPPORTED
35087:                 case LIR_jtbl:
35087:                 {
35087:                     countlir_jtbl();
35087:                     // Multiway jump can contain both forward and backward jumps.
35087:                     // Out of range indices aren't allowed or checked.
35087:                     // Code after this jtbl instruction is unreachable.
35087:                     releaseRegisters();
35087:                     AvmAssert(_allocator.countActive() == 0);
35087: 
35087:                     uint32_t count = ins->getTableSize();
35087:                     bool has_back_edges = false;
35087: 
36553:                     // Merge the regstates of labels we have already seen.
35087:                     for (uint32_t i = count; i-- > 0;) {
35087:                         LIns* to = ins->getTarget(i);
35087:                         LabelState *lstate = _labels.get(to);
35087:                         if (lstate) {
35087:                             unionRegisterState(lstate->regs);
35087:                             asm_output("   %u: [&%s]", i, _thisfrag->lirbuf->names->formatRef(to));
35087:                         } else {
35087:                             has_back_edges = true;
35087:                         }
35087:                     }
35087:                     asm_output("forward edges");
35087: 
35087:                     // In a multi-way jump, the register allocator has no ability to deal
35087:                     // with two existing edges that have conflicting register assignments, unlike
35087:                     // a conditional branch where code can be inserted on the fall-through path
35087:                     // to reconcile registers.  So, frontends *must* insert LIR_regfence at labels of
35087:                     // forward jtbl jumps.  Check here to make sure no registers were picked up from
35087:                     // any forward edges.
35087:                     AvmAssert(_allocator.countActive() == 0);
35087: 
35087:                     if (has_back_edges) {
35087:                         handleLoopCarriedExprs(pending_lives);
35087:                         // save merged (empty) register state at target labels we haven't seen yet
35087:                         for (uint32_t i = count; i-- > 0;) {
35087:                             LIns* to = ins->getTarget(i);
35087:                             LabelState *lstate = _labels.get(to);
35087:                             if (!lstate) {
35087:                                 _labels.add(to, 0, _allocator);
35087:                                 asm_output("   %u: [&%s]", i, _thisfrag->lirbuf->names->formatRef(to));
35087:                             }
35087:                         }
35087:                         asm_output("backward edges");
35087:                     }
35087: 
35087:                     // Emit the jump instruction, which allocates 1 register for the jump index.
35087:                     NIns** native_table = new (_dataAlloc) NIns*[count];
35312:                     asm_output("[%p]:", (void*)native_table);
35087:                     _patches.put((NIns*)native_table, ins);
35087:                     asm_jtbl(ins, native_table);
35087:                     break;
35087:                 }
35087:                 #endif
35087: 
20893:                 case LIR_label:
20893:                 {
20893:                     countlir_label();
32642:                     LabelState *label = _labels.get(ins);
32784:                     // add profiling inc, if necessary.
32784:                     verbose_only( if (_logc->lcbits & LC_FragProfile) {
32784:                         if (ins == _thisfrag->loopLabel)
32784:                             asm_inc_m32(& _thisfrag->profCount);
32784:                     })
20893:                     if (!label) {
20893:                         // label seen first, normal target of forward jump, save addr & allocator
32642:                         _labels.add(ins, _nIns, _allocator);
20893:                     }
20893:                     else {
20893:                         // we're at the top of a loop
32626:                         NanoAssert(label->addr == 0);
32755:                         //evictAllActiveRegs();
20893:                         intersectRegisterState(label->regs);
20893:                         label->addr = _nIns;
20893:                     }
32784:                     verbose_only( if (_logc->lcbits & LC_Assembly) {
36388:                         asm_output("[%s]", _thisfrag->lirbuf->names->formatRef(ins));
32784:                     })
20893:                     break;
20893:                 }
24289:                 case LIR_xbarrier: {
24289:                     break;
24289:                 }
25099: #ifdef NANOJIT_IA32
25099:                 case LIR_xtbl: {
25099:                     NIns* exit = asm_exit(ins); // does intersectRegisterState()
25099:                     asm_switch(ins, exit);
25099:                     break;
25099:                 }
25099: #else
25099:                  case LIR_xtbl:
25099:                     NanoAssertMsg(0, "Not supported for this architecture");
25099:                     break;
25099: #endif
17275:                 case LIR_xt:
17275:                 case LIR_xf:
17275:                 {
32784:                     verbose_only( _thisfrag->nStaticExits++; )
20893:                     countlir_xcc();
17275:                     // we only support cmp with guard right now, also assume it is 'close' and only emit the branch
20893:                     NIns* exit = asm_exit(ins); // does intersectRegisterState()
17275:                     LIns* cond = ins->oprnd1();
30730:                     asm_branch(op == LIR_xf, cond, exit);
17275:                     break;
17275:                 }
17275:                 case LIR_x:
17275:                 {
32784:                     verbose_only( _thisfrag->nStaticExits++; )
20893:                     countlir_x();
17275:                     // generate the side exit branch on the main trace.
17308:                     NIns *exit = asm_exit(ins);
17275:                     JMP( exit );
17275:                     break;
17275:                 }
20921: 
17308:                 case LIR_feq:
17308:                 case LIR_fle:
17308:                 case LIR_flt:
17308:                 case LIR_fgt:
17308:                 case LIR_fge:
17308:                 {
20893:                     countlir_fpu();
20921:                     asm_fcond(ins);
17308:                     break;
17308:                 }
17275:                 case LIR_eq:
17368:                 case LIR_ov:
17275:                 case LIR_le:
17275:                 case LIR_lt:
17275:                 case LIR_gt:
17275:                 case LIR_ge:
17275:                 case LIR_ult:
17275:                 case LIR_ule:
17275:                 case LIR_ugt:
17275:                 case LIR_uge:
32574: #ifdef NANOJIT_64BIT
32574:                 case LIR_qeq:
32574:                 case LIR_qle:
32574:                 case LIR_qlt:
32574:                 case LIR_qgt:
32574:                 case LIR_qge:
32574:                 case LIR_qult:
32574:                 case LIR_qule:
32574:                 case LIR_qugt:
32574:                 case LIR_quge:
32574: #endif
17275:                 {
20893:                     countlir_alu();
20921:                     asm_cond(ins);
17275:                     break;
17275:                 }
17275: 
17275:                 case LIR_fcall:
32598:             #ifdef NANOJIT_64BIT
32598:                 case LIR_qcall:
32598:             #endif
32598:                 case LIR_icall:
17275:                 {
20893:                     countlir_call();
17687:                     asm_call(ins);
33561:                     break;
17275:                 }
33561: 
33561:                 #ifdef VTUNE
33561:                 case LIR_file:
33561:                 {
33561:                     // we traverse backwards so we are now hitting the file
33561:                     // that is associated with a bunch of LIR_lines we already have seen
33561:                     uintptr_t currentFile = ins->oprnd1()->imm32();
33561:                     cgen->jitFilenameUpdate(currentFile);
33561:                     break;
33561:                 }
33561:                 case LIR_line:
33561:                 {
33561:                     // add a new table entry, we don't yet knwo which file it belongs
33561:                     // to so we need to add it to the update table too
33561:                     // note the alloc, actual act is delayed; see above
33561:                     uint32_t currentLine = (uint32_t) ins->oprnd1()->imm32();
33561:                     cgen->jitLineNumUpdate(currentLine);
33561:                     cgen->jitAddRecord((uintptr_t)_nIns, 0, currentLine, true);
33561:                     break;
33561:                 }
33561:                 #endif // VTUNE
17275:             }
17275: 
31936: #ifdef NJ_VERBOSE
31935:             // We have to do final LIR printing inside this loop.  If we do it
31935:             // before this loop, we we end up printing a lot of dead LIR
31935:             // instructions.
31935:             //
31935:             // We print the LIns after generating the code.  This ensures that
31935:             // the LIns will appear in debug output *before* the generated
31935:             // code, because Assembler::outputf() prints everything in reverse.
31935:             //
31935:             // Note that some live LIR instructions won't be printed.  Eg. an
31935:             // immediate won't be printed unless it is explicitly loaded into
31935:             // a register (as opposed to being incorporated into an immediate
31935:             // field in another machine instruction).
31935:             //
31935:             if (_logc->lcbits & LC_Assembly) {
33092:                 LirNameMap* names = _thisfrag->lirbuf->names;
33092:                 outputf("    %s", names->formatIns(ins));
31935:                 if (ins->isGuard() && ins->oprnd1()) {
33091:                     // Special case: code is generated for guard conditions at
33091:                     // the same time that code is generated for the guard
33091:                     // itself.  If the condition is only used by the guard, we
33091:                     // must print it now otherwise it won't get printed.  So
33091:                     // we do print it now, with an explanatory comment.  If
33091:                     // the condition *is* used again we'll end up printing it
33091:                     // twice, but that's ok.
33091:                     outputf("    %s       # codegen'd with the %s",
33092:                             names->formatIns(ins->oprnd1()), lirNames[op]);
33091: 
33091:                 } else if (ins->isop(LIR_cmov) || ins->isop(LIR_qcmov)) {
33091:                     // Likewise for cmov conditions.
33091:                     outputf("    %s       # codegen'd with the %s",
33092:                             names->formatIns(ins->oprnd1()), lirNames[op]);
33092: 
33092:                 } else if (ins->isop(LIR_mod)) {
33092:                     // There's a similar case when a div feeds into a mod.
33092:                     outputf("    %s       # codegen'd with the mod",
33092:                             names->formatIns(ins->oprnd1()));
31935:                 }
31935:             }
31936: #endif
31935: 
21483:             if (error())
21483:                 return;
21483: 
33561:         #ifdef VTUNE
33561:             cgen->jitCodePosUpdate((uintptr_t)_nIns);
33561:         #endif
33561: 
17275:             // check that all is well (don't check in exit paths since its more complicated)
17275:             debug_only( pageValidate(); )
17275:             debug_only( resourceConsistencyCheck();  )
17275:         }
17275:     }
17275: 
25099:     /*
25099:      * Write a jump table for the given SwitchInfo and store the table
25099:      * address in the SwitchInfo. Every entry will initially point to
25099:      * target.
25099:      */
25099:     void Assembler::emitJumpTable(SwitchInfo* si, NIns* target)
25099:     {
34322:         si->table = (NIns **) alloc.alloc(si->count * sizeof(NIns*));
34322:         for (uint32_t i = 0; i < si->count; ++i)
34322:             si->table[i] = target;
25099:     }
25099: 
21477:     void Assembler::assignSavedRegs()
17275:     {
36672:         // Restore saved regsters.
20893:         LirBuffer *b = _thisfrag->lirbuf;
20893:         for (int i=0, n = NumSavedRegs; i < n; i++) {
21477:             LIns *p = b->savedRegs[i];
20893:             if (p)
34577:                 findSpecificRegForUnallocated(p, savedRegs[p->paramArg()]);
20893:         }
20893:     }
18776: 
21477:     void Assembler::reserveSavedRegs()
20893:     {
20893:         LirBuffer *b = _thisfrag->lirbuf;
20893:         for (int i=0, n = NumSavedRegs; i < n; i++) {
21477:             LIns *p = b->savedRegs[i];
20893:             if (p)
20893:                 findMemFor(p);
20893:         }
20893:     }
18776: 
21477:     void Assembler::assignParamRegs()
21477:     {
21477:         LInsp state = _thisfrag->lirbuf->state;
21477:         if (state)
34577:             findSpecificRegForUnallocated(state, argRegs[state->paramArg()]);
21477:         LInsp param1 = _thisfrag->lirbuf->param1;
21477:         if (param1)
34577:             findSpecificRegForUnallocated(param1, argRegs[param1->paramArg()]);
21477:     }
21477: 
30253:     void Assembler::handleLoopCarriedExprs(InsList& pending_lives)
20893:     {
20893:         // ensure that exprs spanning the loop are marked live at the end of the loop
21477:         reserveSavedRegs();
32646:         for (Seq<LIns*> *p = pending_lives.get(); p != NULL; p = p->tail) {
32646:             LIns *i = p->head;
32646:             NanoAssert(i->isop(LIR_live) || i->isop(LIR_flive));
32646:             LIns *op1 = i->oprnd1();
34384:             // must findMemFor even if we're going to findRegFor; loop-carried
34384:             // operands may spill on another edge, and we need them to always
34384:             // spill to the same place.
37665: #if NJ_USES_QUAD_CONSTANTS
37665:             // exception: if quad constants are true constants, we should
37665:             // never call findMemFor on those ops
37665:             if (!op1->isconstq())
37665: #endif
37665:             {
32646:                 findMemFor(op1);
37665:             }
34384:             if (! (op1->isconst() || op1->isconstf() || op1->isconstq()))
32646:                 findRegFor(op1, i->isop(LIR_flive) ? FpRegs : GpRegs);
32646:         }
33125: 
33125:         // clear this list since we have now dealt with those lifetimes.  extending
33125:         // their lifetimes again later (earlier in the code) serves no purpose.
30451:         pending_lives.clear();
20893:     }
20893: 
36664:     void AR::freeEntryAt(uint32_t idx)
20893:     {
36667:         NanoAssert(idx > 0 && idx <= _highWaterMark);
36667: 
36676:         // NB: this loop relies on using entry[0] being NULL,
36676:         // so that we are guaranteed to terminate
36676:         // without access negative entries.
36664:         LIns* i = _entries[idx];
36677:         NanoAssert(i != NULL);
20893:         do {
36677:             _entries[idx] = NULL;
20893:             idx--;
36664:         } while (_entries[idx] == i);
17275:     }
17275: 
17275: #ifdef NJ_VERBOSE
35364:     void Assembler::printRegState()
17275:     {
35364:         char* s = &outline[0];
35364:         VMPI_memset(s, ' ', 26);  s[26] = '\0';
35364:         s += VMPI_strlen(s);
35364:         VMPI_sprintf(s, "RR");
35364:         s += VMPI_strlen(s);
17275: 
35364:         for (Register r = FirstReg; r <= LastReg; r = nextreg(r)) {
35364:             LIns *ins = _allocator.getActive(r);
35364:             if (ins) {
35364:                 NanoAssertMsg(!_allocator.isFree(r),
35364:                               "Coding error; register is both free and active! " );
35364:                 const char* n = _thisfrag->lirbuf->names->formatRef(ins);
35364: 
35364:                 if (ins->isop(LIR_param) && ins->paramKind()==1 &&
35364:                     r == Assembler::savedRegs[ins->paramArg()])
35364:                 {
35364:                     // dont print callee-saved regs that arent used
35364:                     continue;
35364:                 }
35364: 
35364:                 const char* rname = ins->isQuad() ? fpn(r) : gpn(r);
35364:                 VMPI_sprintf(s, " %s(%s)", rname, n);
35364:                 s += VMPI_strlen(s);
35364:             }
35364:         }
35364:         output();
35364:     }
35364: 
35364:     void Assembler::printActivationState()
35364:     {
17275:         char* s = &outline[0];
35364:         VMPI_memset(s, ' ', 26);  s[26] = '\0';
33114:         s += VMPI_strlen(s);
35364:         VMPI_sprintf(s, "AR");
33114:         s += VMPI_strlen(s);
33174: 
36664:         LIns* ins = 0;
36664:         uint32_t nStackSlots = 0;
36664:         int32_t arIndex = 0;
36664:         for (AR::Iter iter(_activation); iter.next(ins, nStackSlots, arIndex); )
36664:         {
33174:             const char* n = _thisfrag->lirbuf->names->formatRef(ins);
36664:             if (nStackSlots > 1) {
36664:                 VMPI_sprintf(s," %d-%d(%s)", 4*arIndex, 4*(arIndex+nStackSlots-1), n);
33174:             }
33174:             else {
36664:                 VMPI_sprintf(s," %d(%s)", 4*arIndex, n);
33174:             }
33114:             s += VMPI_strlen(s);
17275:         }
35364:         output();
17275:     }
17275: #endif
17275: 
36664:     inline bool AR::isEmptyRange(uint32_t start, uint32_t nStackSlots) const
36664:     {
36664:         for (uint32_t i=0; i < nStackSlots; i++)
36664:         {
36665:             if (_entries[start-i] != NULL)
20893:                 return false;
20893:         }
20893:         return true;
20893:     }
20893: 
36664:     uint32_t AR::reserveEntry(LIns* ins)
17275:     {
36664:         uint32_t const nStackSlots = nStackSlotsFor(ins);
20893: 
36667:         if (nStackSlots == 1)
36667:         {
36667:             for (uint32_t i = 1; i <= _highWaterMark; i++)
36667:             {
36667:                 if (_entries[i] == NULL)
36667:                 {
36664:                     _entries[i] = ins;
36667:                     return i;
20893:                 }
20893:             }
36667:             uint32_t const spaceLeft = NJ_MAX_STACK_ENTRY - _highWaterMark - 1;
36667:             if (spaceLeft >= 1)
36667:             {
36667:                  NanoAssert(_entries[_highWaterMark+1] == BAD_ENTRY);
36667:                 _entries[++_highWaterMark] = ins;
36667:                 return _highWaterMark;
20893:              }
20893:         }
36667:         else
36667:         {
20893:             // alloc larger block on 8byte boundary.
36667:             uint32_t const start = nStackSlots + (nStackSlots & 1);
36667:             for (uint32_t i = start; i <= _highWaterMark; i += 2)
36667:             {
36667:                 if (isEmptyRange(i, nStackSlots))
36667:                 {
20893:                     // place the entry in the table and mark the instruction with it
36667:                     for (uint32_t j=0; j < nStackSlots; j++)
36667:                     {
36667:                         NanoAssert(i-j <= _highWaterMark);
36667:                         NanoAssert(_entries[i-j] == NULL);
36665:                         _entries[i-j] = ins;
20893:                     }
36667:                     return i;
20893:                 }
20893:             }
36667: 
36667:             // Be sure to account for any 8-byte-round-up when calculating spaceNeeded.
36667:             uint32_t const spaceLeft = NJ_MAX_STACK_ENTRY - _highWaterMark - 1;
36667:             uint32_t const spaceNeeded = nStackSlots + (_highWaterMark & 1);
36667:             if (spaceLeft >= spaceNeeded)
36667:             {
36667:                 if (_highWaterMark & 1)
36667:                 {
36667:                     NanoAssert(_entries[_highWaterMark+1] == BAD_ENTRY);
36667:                     _entries[_highWaterMark+1] = NULL;
20893:                 }
36667:                 _highWaterMark += spaceNeeded;
36667:                 for (uint32_t j = 0; j < nStackSlots; j++)
36667:                 {
36667:                     NanoAssert(_highWaterMark-j < NJ_MAX_STACK_ENTRY);
36667:                     NanoAssert(_entries[_highWaterMark-j] == BAD_ENTRY);
36667:                     _entries[_highWaterMark-j] = ins;
20893:                 }
36667:                 return _highWaterMark;
36667:             }
36667:         }
36667:         // no space. oh well.
36667:         return 0;
36664:     }
36664: 
36664:     #ifdef _DEBUG
36664:     void AR::checkForResourceLeaks() const
36664:     {
36664:         for (uint32_t i = 1; i <= _highWaterMark; i++) {
36667:             NanoAssertMsgf(_entries[i] == NULL, "frame entry %d wasn't freed\n",-4*i);
36664:         }
36664:     }
36664:     #endif
36664: 
36664:     uint32_t Assembler::arReserve(LIns* ins)
36664:     {
36664:         uint32_t i = _activation.reserveEntry(ins);
36664:         if (!i)
17275:             setError(StackFull);
36664:         return i;
17275:     }
36664: 
36664:     void Assembler::arFreeIfInUse(LIns* ins)
36664:     {
36664:         uint32_t arIndex = ins->getArIndex();
36664:         if (arIndex) {
36664:             NanoAssert(_activation.isValidEntry(arIndex, ins));
36664:             _activation.freeEntryAt(arIndex);        // free any stack stack space associated with entry
36664:         }
17275:     }
17275: 
20893:     /**
20893:      * move regs around so the SavedRegs contains the highest priority regs.
20893:      */
20893:     void Assembler::evictScratchRegs()
20893:     {
20893:         // find the top GpRegs that are candidates to put in SavedRegs
20893: 
20893:         // tosave is a binary heap stored in an array.  the root is tosave[0],
20893:         // left child is at i+1, right child is at i+2.
20893: 
20893:         Register tosave[LastReg-FirstReg+1];
20893:         int len=0;
20893:         RegAlloc *regs = &_allocator;
20893:         for (Register r = FirstReg; r <= LastReg; r = nextreg(r)) {
20893:             if (rmask(r) & GpRegs) {
36553:                 LIns *ins = regs->getActive(r);
36553:                 if (ins) {
36553:                     if (canRemat(ins)) {
36553:                         NanoAssert(ins->getReg() == r);
36553:                         evict(ins);
20893:                     }
20893:                     else {
20893:                         int32_t pri = regs->getPriority(r);
20893:                         // add to heap by adding to end and bubbling up
20893:                         int j = len++;
20893:                         while (j > 0 && pri > regs->getPriority(tosave[j/2])) {
20893:                             tosave[j] = tosave[j/2];
20893:                             j /= 2;
20893:                         }
20893:                         NanoAssert(size_t(j) < sizeof(tosave)/sizeof(tosave[0]));
20893:                         tosave[j] = r;
20893:                     }
20893:                 }
20893:             }
20893:         }
20893: 
20893:         // now primap has the live exprs in priority order.
20893:         // allocate each of the top priority exprs to a SavedReg
20893: 
20893:         RegisterMask allow = SavedRegs;
20893:         while (allow && len > 0) {
20893:             // get the highest priority var
20893:             Register hi = tosave[0];
22659:             if (!(rmask(hi) & SavedRegs)) {
36553:                 LIns *ins = regs->getActive(hi);
36553:                 Register r = findRegFor(ins, allow);
20893:                 allow &= ~rmask(r);
22659:             }
22659:             else {
22659:                 // hi is already in a saved reg, leave it alone.
22659:                 allow &= ~rmask(hi);
22659:             }
20893: 
20893:             // remove from heap by replacing root with end element and bubbling down.
20893:             if (allow && --len > 0) {
20893:                 Register last = tosave[len];
20893:                 int j = 0;
20893:                 while (j+1 < len) {
20893:                     int child = j+1;
20893:                     if (j+2 < len && regs->getPriority(tosave[j+2]) > regs->getPriority(tosave[j+1]))
20893:                         child++;
20893:                     if (regs->getPriority(last) > regs->getPriority(tosave[child]))
20893:                         break;
20893:                     tosave[j] = tosave[child];
20893:                     j = child;
20893:                 }
20893:                 tosave[j] = last;
20893:             }
20893:         }
20893: 
20893:         // now evict everything else.
32755:         evictSomeActiveRegs(~SavedRegs);
20893:     }
20893: 
32755:     void Assembler::evictAllActiveRegs()
32755:     {
32755:         // generate code to restore callee saved registers
32755:         // @todo speed this up
32755:         for (Register r = FirstReg; r <= LastReg; r = nextreg(r)) {
32755:             evictIfActive(r);
32755:         }
32755:     }
32755: 
32755:     void Assembler::evictSomeActiveRegs(RegisterMask regs)
17275:     {
17275:         // generate code to restore callee saved registers
17275:         // @todo speed this up
20893:         for (Register r = FirstReg; r <= LastReg; r = nextreg(r)) {
32662:             if ((rmask(r) & regs)) {
32662:                 evictIfActive(r);
17275:             }
17275:         }
17275:     }
17275: 
17275:     /**
36553:      * Merge the current regstate with a previously stored version.
37015:      *
37015:      * Situation                            Change to _allocator
37015:      * ---------                            --------------------
37015:      * !current & !saved
37015:      * !current &  saved                    add saved
37015:      *  current & !saved                    evict current (unionRegisterState does nothing)
37015:      *  current &  saved & current==saved
37015:      *  current &  saved & current!=saved   evict current, add saved
17275:      */
20893:     void Assembler::intersectRegisterState(RegAlloc& saved)
17275:     {
37015:         Register regsTodo[LastReg + 1];
37015:         LIns* insTodo[LastReg + 1];
37015:         int nTodo = 0;
37015: 
37015:         // Do evictions and pops first.
22648:         verbose_only(bool shouldMention=false; )
35352:         // The obvious thing to do here is to iterate from FirstReg to LastReg.
35352:         // viz: for (Register r = FirstReg; r <= LastReg; r = nextreg(r)) ...
35352:         // However, on ARM that causes lower-numbered integer registers
35352:         // to be be saved at higher addresses, which inhibits the formation
35352:         // of load/store multiple instructions.  Hence iterate the loop the
35352:         // other way.  The "r <= LastReg" guards against wraparound in
35352:         // the case where Register is treated as unsigned and FirstReg is zero.
36372:         //
36372:         // Note, the loop var is deliberately typed as int (*not* Register)
36372:         // to outsmart compilers that will otherwise report
36372:         // "error: comparison is always true due to limited range of data type".
36372:         for (int ri = LastReg; ri >= FirstReg && ri <= LastReg; ri = int(prevreg(Register(ri))))
17275:         {
36372:             Register const r = Register(ri);
17275:             LIns* curins = _allocator.getActive(r);
17275:             LIns* savedins = saved.getActive(r);
37015:             if (curins != savedins)
17275:             {
37015:                 if (savedins) {
37015:                     regsTodo[nTodo] = r;
37015:                     insTodo[nTodo] = savedins;
37015:                     nTodo++;
17275:                 }
20893:                 if (curins) {
20893:                     //_nvprof("intersect-evict",1);
22648:                     verbose_only( shouldMention=true; )
36553:                     NanoAssert(curins->getReg() == r);
36553:                     evict(curins);
20893:                 }
17275: 
17275:                 #ifdef NANOJIT_IA32
22648:                 if (savedins && (rmask(r) & x87Regs)) {
22648:                     verbose_only( shouldMention=true; )
17275:                     FSTP(r);
22648:                 }
17275:                 #endif
17275:             }
17275:         }
37015:         // Now reassign mainline registers.
37015:         for (int i = 0; i < nTodo; i++) {
37015:             findSpecificRegFor(insTodo[i], regsTodo[i]);
37015:         }
29883:         verbose_only(
29883:             if (shouldMention)
37015:                 verbose_outputf("## merging registers (intersect) with existing edge");
29883:         )
20893:     }
17275: 
20893:     /**
20893:      * Merge the current state of the registers with a previously stored version.
37015:      *
37015:      * Situation                            Change to _allocator
37015:      * ---------                            --------------------
37015:      * !current & !saved                    none
37015:      * !current &  saved                    add saved
37015:      *  current & !saved                    none (intersectRegisterState evicts current)
37015:      *  current &  saved & current==saved   none
37015:      *  current &  saved & current!=saved   evict current, add saved
20893:      */
20893:     void Assembler::unionRegisterState(RegAlloc& saved)
20893:     {
37015:         Register regsTodo[LastReg + 1];
37015:         LIns* insTodo[LastReg + 1];
37015:         int nTodo = 0;
37015: 
37015:         // Do evictions and pops first.
22648:         verbose_only(bool shouldMention=false; )
20893:         for (Register r = FirstReg; r <= LastReg; r = nextreg(r))
20893:         {
20893:             LIns* curins = _allocator.getActive(r);
20893:             LIns* savedins = saved.getActive(r);
37015:             if (curins != savedins)
20893:             {
37015:                 if (savedins) {
37015:                     regsTodo[nTodo] = r;
37015:                     insTodo[nTodo] = savedins;
37015:                     nTodo++;
20893:                 }
20893:                 if (curins && savedins) {
20893:                     //_nvprof("union-evict",1);
22648:                     verbose_only( shouldMention=true; )
36553:                     NanoAssert(curins->getReg() == r);
36553:                     evict(curins);
20893:                 }
20893: 
20893:                 #ifdef NANOJIT_IA32
20893:                 if (rmask(r) & x87Regs) {
20893:                     if (savedins) {
20893:                         FSTP(r);
20893:                     }
37015:                     else if (curins) {
20893:                         // saved state did not have fpu reg allocated,
20893:                         // so we must evict here to keep x87 stack balanced.
37015:                         evict(curins);
20893:                     }
22648:                     verbose_only( shouldMention=true; )
20893:                 }
20893:                 #endif
20893:             }
20893:         }
37015:         // Now reassign mainline registers.
37015:         for (int i = 0; i < nTodo; i++) {
37015:             findSpecificRegFor(insTodo[i], regsTodo[i]);
20893:         }
37015:         verbose_only(
37015:             if (shouldMention)
37015:                 verbose_outputf("## merging registers (union) with existing edge");
37015:         )
17275:     }
17275: 
32662:     // Scan table for instruction with the lowest priority, meaning it is used
28549:     // furthest in the future.
32662:     LIns* Assembler::findVictim(RegisterMask allow)
28549:     {
36553:         NanoAssert(allow);
36553:         LIns *ins, *vic = 0;
28549:         int allow_pri = 0x7fffffff;
28549:         for (Register r = FirstReg; r <= LastReg; r = nextreg(r))
28549:         {
36553:             if ((allow & rmask(r)) && (ins = _allocator.getActive(r)) != 0)
28549:             {
36553:                 int pri = canRemat(ins) ? 0 : _allocator.getPriority(r);
36553:                 if (!vic || pri < allow_pri) {
36553:                     vic = ins;
28549:                     allow_pri = pri;
28549:                 }
28549:             }
28549:         }
36553:         NanoAssert(vic != 0);
36553:         return vic;
28549:     }
28549: 
17275: #ifdef NJ_VERBOSE
17275:     char Assembler::outline[8192];
22648:     char Assembler::outlineEOL[512];
22648: 
35364:     void Assembler::output()
22648:     {
35364:         // The +1 is for the terminating NUL char.
35364:         VMPI_strncat(outline, outlineEOL, sizeof(outline)-(strlen(outline)+1));
35364: 
35364:         if (_outputCache) {
35364:             char* str = new (alloc) char[VMPI_strlen(outline)+1];
35364:             VMPI_strcpy(str, outline);
35364:             _outputCache->insert(str);
35364:         } else {
35364:             _logc->printf("%s\n", outline);
35364:         }
35364: 
35364:         outline[0] = '\0';
22648:         outlineEOL[0] = '\0';
22648:     }
17275: 
17275:     void Assembler::outputf(const char* format, ...)
17275:     {
17275:         va_list args;
17275:         va_start(args, format);
35364: 
17275:         outline[0] = '\0';
35364:         vsprintf(outline, format, args);
35364:         output();
17275:     }
17275: 
35364:     void Assembler::setOutputForEOL(const char* format, ...)
17275:     {
35364:         va_list args;
35364:         va_start(args, format);
17275: 
29861:         outlineEOL[0] = '\0';
35364:         vsprintf(outlineEOL, format, args);
17275:     }
33125: #endif // NJ_VERBOSE
17275: 
17275:     uint32_t CallInfo::_count_args(uint32_t mask) const
17275:     {
17275:         uint32_t argc = 0;
17275:         uint32_t argt = _argtypes;
20893:         for (uint32_t i = 0; i < MAXARGS; ++i) {
31051:             argt >>= ARGSIZE_SHIFT;
22661:             if (!argt)
22661:                 break;
17275:             argc += (argt & mask) != 0;
17275:         }
17275:         return argc;
17275:     }
17687: 
17687:     uint32_t CallInfo::get_sizes(ArgSize* sizes) const
17687:     {
17687:         uint32_t argt = _argtypes;
17687:         uint32_t argc = 0;
20893:         for (uint32_t i = 0; i < MAXARGS; i++) {
31051:             argt >>= ARGSIZE_SHIFT;
31051:             ArgSize a = ArgSize(argt & ARGSIZE_MASK_ANY);
17687:             if (a != ARGSIZE_NONE) {
17687:                 sizes[argc++] = a;
22661:             } else {
22661:                 break;
17275:             }
17687:         }
17687:         return argc;
17687:     }
20893: 
20893:     void LabelStateMap::add(LIns *label, NIns *addr, RegAlloc &regs) {
31509:         LabelState *st = new (alloc) LabelState(addr, regs);
20893:         labels.put(label, st);
17687:     }
20893: 
20893:     LabelState* LabelStateMap::get(LIns *label) {
20893:         return labels.get(label);
20893:     }
20893: }
33125: #endif /* FEATURE_NANOJIT */
