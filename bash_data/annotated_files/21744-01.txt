    1: /* -*- Mode: C; tab-width: 8; indent-tabs-mode: nil; c-basic-offset: 4 -*-
    1:  * vim: set ts=8 sw=4 et tw=78:
    1:  *
    1:  * ***** BEGIN LICENSE BLOCK *****
    1:  * Version: MPL 1.1/GPL 2.0/LGPL 2.1
    1:  *
    1:  * The contents of this file are subject to the Mozilla Public License Version
    1:  * 1.1 (the "License"); you may not use this file except in compliance with
    1:  * the License. You may obtain a copy of the License at
    1:  * http://www.mozilla.org/MPL/
    1:  *
    1:  * Software distributed under the License is distributed on an "AS IS" basis,
    1:  * WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
    1:  * for the specific language governing rights and limitations under the
    1:  * License.
    1:  *
    1:  * The Original Code is Mozilla Communicator client code, released
    1:  * March 31, 1998.
    1:  *
    1:  * The Initial Developer of the Original Code is
    1:  * Netscape Communications Corporation.
    1:  * Portions created by the Initial Developer are Copyright (C) 1998
    1:  * the Initial Developer. All Rights Reserved.
    1:  *
    1:  * Contributor(s):
    1:  *
    1:  * Alternatively, the contents of this file may be used under the terms of
    1:  * either of the GNU General Public License Version 2 or later (the "GPL"),
    1:  * or the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
    1:  * in which case the provisions of the GPL or the LGPL are applicable instead
    1:  * of those above. If you wish to allow use of your version of this file only
    1:  * under the terms of either the GPL or the LGPL, and not to allow others to
    1:  * use your version of this file under the terms of the MPL, indicate your
    1:  * decision by deleting the provisions above and replace them with the notice
    1:  * and other provisions required by the GPL or the LGPL. If you do not delete
    1:  * the provisions above, a recipient may use your version of this file under
    1:  * the terms of any one of the MPL, the GPL or the LGPL.
    1:  *
    1:  * ***** END LICENSE BLOCK ***** */
    1: 
    1: /*
    1:  * JS Mark-and-Sweep Garbage Collector.
    1:  *
    1:  * This GC allocates fixed-sized things with sizes up to GC_NBYTES_MAX (see
    1:  * jsgc.h). It allocates from a special GC arena pool with each arena allocated
    1:  * using malloc. It uses an ideally parallel array of flag bytes to hold the
    1:  * mark bit, finalizer type index, etc.
    1:  *
    1:  * XXX swizzle page to freelist for better locality of reference
    1:  */
    1: #include "jsstddef.h"
    1: #include <stdlib.h>     /* for free */
17182: #include <math.h>
    1: #include <string.h>     /* for memset used when DEBUG */
    1: #include "jstypes.h"
    1: #include "jsutil.h" /* Added by JSIFY */
    1: #include "jshash.h" /* Added by JSIFY */
17182: #include "jsbit.h"
17182: #include "jsclist.h"
17182: #include "jsprf.h"
    1: #include "jsapi.h"
    1: #include "jsatom.h"
    1: #include "jscntxt.h"
18863: #include "jsversion.h"
    1: #include "jsdbgapi.h"
    1: #include "jsexn.h"
    1: #include "jsfun.h"
    1: #include "jsgc.h"
    1: #include "jsinterp.h"
    1: #include "jsiter.h"
    1: #include "jslock.h"
    1: #include "jsnum.h"
    1: #include "jsobj.h"
 3235: #include "jsparse.h"
    1: #include "jsscope.h"
    1: #include "jsscript.h"
    1: #include "jsstr.h"
17976: #include "jstracer.h"
    1: 
    1: #if JS_HAS_XML_SUPPORT
    1: #include "jsxml.h"
    1: #endif
    1: 
    1: /*
12478:  * Check if posix_memalign is available.
    1:  */
12478: #if _POSIX_C_SOURCE >= 200112L || _XOPEN_SOURCE >= 600 || MOZ_MEMORY
12478: # define HAS_POSIX_MEMALIGN 1
12478: #else
12478: # define HAS_POSIX_MEMALIGN 0
12478: #endif
12478: 
12478: /*
15458:  * jemalloc provides posix_memalign.
12478:  */
15458: #ifdef MOZ_MEMORY
15458: extern "C" {
15458: #include "../../memory/jemalloc/jemalloc.h"
15458: }
12478: #endif
12478: 
12478: /*
12478:  * Include the headers for mmap unless we have posix_memalign and do not
12478:  * insist on mmap.
12478:  */
12478: #if JS_GC_USE_MMAP || (!defined JS_GC_USE_MMAP && !HAS_POSIX_MEMALIGN)
12478: # if defined(XP_WIN)
 5917: #  ifndef JS_GC_USE_MMAP
 5917: #   define JS_GC_USE_MMAP 1
    1: #  endif
12478: #  include <windows.h>
12478: # else
12478: #  if defined(XP_UNIX) || defined(XP_BEOS)
12478: #   include <unistd.h>
 5917: #  endif
12478: #  if _POSIX_MAPPED_FILES > 0
12478: #   ifndef JS_GC_USE_MMAP
12478: #    define JS_GC_USE_MMAP 1
 5917: #   endif
 5917: #   include <sys/mman.h>
 5917: 
 5917: /* On Mac OS X MAP_ANONYMOUS is not defined. */
 5917: #   if !defined(MAP_ANONYMOUS) && defined(MAP_ANON)
 5917: #    define MAP_ANONYMOUS MAP_ANON
 5917: #   endif
12478: #  else
12478: #   if JS_GC_USE_MMAP
12478: #    error "JS_GC_USE_MMAP is set when mmap is not available"
12478: #   endif
12478: #  endif
 5917: # endif
 5917: #endif
    1: 
    1: /*
12282:  * A GC arena contains a fixed number of flag bits for each thing in its heap,
12282:  * and supports O(1) lookup of a flag given its thing's address.
    1:  *
 5917:  * To implement this, we allocate things of the same size from a GC arena
 5917:  * containing GC_ARENA_SIZE bytes aligned on GC_ARENA_SIZE boundary. The
 5917:  * following picture shows arena's layout:
    1:  *
 5917:  *  +------------------------------+--------------------+---------------+
 5917:  *  | allocation area for GC thing | flags of GC things | JSGCArenaInfo |
 5917:  *  +------------------------------+--------------------+---------------+
    1:  *
12282:  * To find the flag bits for the thing we calculate the thing index counting
12282:  * from arena's start using:
    1:  *
12282:  *   thingIndex = (thingAddress & GC_ARENA_MASK) / thingSize
12282:  *
12282:  * The details of flag's lookup depend on thing's kind. For all GC things
12282:  * except doubles we use one byte of flags where the 4 bits determine thing's
12282:  * type and the rest is used to implement GC marking, finalization and
12282:  * locking. We calculate the address of flag's byte using:
12282:  *
 5917:  *   flagByteAddress =
12282:  *       (thingAddress | GC_ARENA_MASK) - sizeof(JSGCArenaInfo) - thingIndex
12282:  *
 5917:  * where
12282:  *
 5917:  *   (thingAddress | GC_ARENA_MASK) - sizeof(JSGCArenaInfo)
    1:  *
12282:  * is the last byte of flags' area.
12282:  *
12282:  * This implies that the things are allocated from the start of their area and
12282:  * flags are allocated from the end. This arrangement avoids a relatively
12282:  * expensive calculation of the location of the boundary separating things and
12282:  * flags. The boundary's offset from the start of the arena is given by:
12282:  *
12282:  *   thingsPerArena * thingSize
12282:  *
12282:  * where thingsPerArena is the number of things that the arena can hold:
12282:  *
12282:  *   (GC_ARENA_SIZE - sizeof(JSGCArenaInfo)) / (thingSize + 1).
12282:  *
12282:  * To allocate doubles we use a specialized arena. It can contain only numbers
12282:  * so we do not need the type bits. Moreover, since the doubles do not require
12282:  * a finalizer and very few of them are locked via js_LockGCThing API, we use
12282:  * just one bit of flags per double to denote if it was marked during the
12282:  * marking phase of the GC. The locking is implemented via a hash table. Thus
12282:  * for doubles the flag area becomes a bitmap.
    1:  *
12478:  * JS_GC_USE_MMAP macro governs the choice of the aligned arena allocator.
12483:  * When it is true, a platform-dependent function like mmap is used to get
12478:  * memory aligned on CPU page boundaries. If the macro is false or undefined,
12478:  * posix_memalign is used when available. Otherwise the code uses malloc to
12478:  * over-allocate a chunk with js_gcArenasPerChunk aligned arenas. The
12478:  * approximate space overhead of this is 1/js_gcArenasPerChunk. For details,
12478:  * see NewGCChunk/DestroyGCChunk below.
12282:  *
12478:  * The code also allocates arenas in chunks when JS_GC_USE_MMAP is 1 to
12478:  * minimize the overhead of mmap/munmap. In this case js_gcArenasPerChunk can
12478:  * not be a compile-time constant as the system page size is not known until
12478:  * runtime.
    1:  */
 5917: #if JS_GC_USE_MMAP
12478: static uint32 js_gcArenasPerChunk = 0;
12478: static JSBool js_gcUseMmap = JS_FALSE;
12478: #elif HAS_POSIX_MEMALIGN
12478: # define js_gcArenasPerChunk 1
    1: #else
12478: # define js_gcArenasPerChunk 7
    1: #endif
12478: 
12478: #if defined(js_gcArenasPerChunk) && js_gcArenasPerChunk == 1
12478: # define CHUNKED_ARENA_ALLOCATION 0
12478: #else
12478: # define CHUNKED_ARENA_ALLOCATION 1
12478: #endif
12478: 
 5917: #define GC_ARENA_SHIFT              12
 5917: #define GC_ARENA_MASK               ((jsuword) JS_BITMASK(GC_ARENA_SHIFT))
 5917: #define GC_ARENA_SIZE               JS_BIT(GC_ARENA_SHIFT)
    1: 
    1: /*
12478:  * JS_GC_ARENA_PAD defines the number of bytes to pad JSGCArenaInfo structure.
12478:  * It is used to improve allocation efficiency when using posix_memalign. If
12478:  * malloc's implementation uses internal headers, then calling
    1:  *
12478:  *   posix_memalign(&p, GC_ARENA_SIZE, GC_ARENA_SIZE * js_gcArenasPerChunk)
    1:  *
12478:  * in a sequence leaves holes between allocations of the size GC_ARENA_SIZE
12478:  * due to the need to fit headers. JS_GC_ARENA_PAD mitigates that so the code
12478:  * calls
    1:  *
12478:  *     posix_memalign(&p, GC_ARENA_SIZE,
12478:  *                    GC_ARENA_SIZE * js_gcArenasPerChunk - JS_GC_ARENA_PAD)
    1:  *
12478:  * When JS_GC_ARENA_PAD is equal or greater than the number of words in the
12478:  * system header, the system can pack all allocations together without holes.
    1:  *
12478:  * With JS_GC_USE_MEMALIGN we want at least 2 word pad unless posix_memalign
12478:  * comes from jemalloc that does not use any headers/trailers.
12478:  */
12478: #ifndef JS_GC_ARENA_PAD
12478: # if HAS_POSIX_MEMALIGN && !MOZ_MEMORY
12478: #  define JS_GC_ARENA_PAD (2 * JS_BYTES_PER_WORD)
12478: # else
12478: #  define JS_GC_ARENA_PAD 0
12478: # endif
12478: #endif
12478: 
 5917: struct JSGCArenaInfo {
 5917:     /*
12282:      * Allocation list for the arena or NULL if the arena holds double values.
 5917:      */
 5917:     JSGCArenaList   *list;
 5917: 
 5917:     /*
 5917:      * Pointer to the previous arena in a linked list. The arena can either
 5917:      * belong to one of JSContext.gcArenaList lists or, when it does not have
 5917:      * any allocated GC things, to the list of free arenas in the chunk with
 5917:      * head stored in JSGCChunkInfo.lastFreeArena.
 5917:      */
 5917:     JSGCArenaInfo   *prev;
 5917: 
12478: #if !CHUNKED_ARENA_ALLOCATION
12478:     jsuword         prevUntracedPage;
12478: #else
 5917:     /*
 5917:      * A link field for the list of arenas with marked but not yet traced
 5917:      * things. The field is encoded as arena's page to share the space with
 5917:      * firstArena and arenaIndex fields.
 5917:      */
 5917:     jsuword         prevUntracedPage :  JS_BITS_PER_WORD - GC_ARENA_SHIFT;
 5917: 
 5917:     /*
 5917:      * When firstArena is false, the index of arena in the chunk. When
 5917:      * firstArena is true, the index of a free arena holding JSGCChunkInfo or
 5917:      * NO_FREE_ARENAS if there are no free arenas in the chunk.
    1:      *
 5917:      * GET_ARENA_INDEX and GET_CHUNK_INFO_INDEX are convenience macros to
 5917:      * access either of indexes.
    1:      */
 5917:     jsuword         arenaIndex :        GC_ARENA_SHIFT - 1;
    1: 
12282:     /* Flag indicating if the arena is the first in the chunk. */
 5917:     jsuword         firstArena :        1;
12478: #endif
    1: 
12282:     union {
12282:         jsuword     untracedThings;     /* bitset for fast search of marked
12282:                                            but not yet traced things */
12282:         JSBool      hasMarkedDoubles;   /* the arena has marked doubles */
12282:     } u;
12478: 
12478: #if JS_GC_ARENA_PAD != 0
12478:     uint8           pad[JS_GC_ARENA_PAD];
12478: #endif
    1: };
    1: 
 5917: /*
 5917:  * Verify that the bit fields are indeed shared and JSGCArenaInfo is as small
 5917:  * as possible. The code does not rely on this check so if on a particular
 5917:  * platform this does not compile, then, as a workaround, comment the assert
 5917:  * out and submit a bug report.
 5917:  */
12478: JS_STATIC_ASSERT(offsetof(JSGCArenaInfo, u) == 3 * sizeof(jsuword));
    1: 
    1: /*
12478:  * Macros to convert between JSGCArenaInfo, the start address of the arena and
12478:  * arena's page defined as (start address) >> GC_ARENA_SHIFT.
12478:  */
12478: #define ARENA_INFO_OFFSET (GC_ARENA_SIZE - (uint32) sizeof(JSGCArenaInfo))
12478: 
12478: #define IS_ARENA_INFO_ADDRESS(arena)                                          \
12478:     (((jsuword) (arena) & GC_ARENA_MASK) == ARENA_INFO_OFFSET)
12478: 
12478: #define ARENA_START_TO_INFO(arenaStart)                                       \
12478:     (JS_ASSERT(((arenaStart) & (jsuword) GC_ARENA_MASK) == 0),                \
12478:      (JSGCArenaInfo *) ((arenaStart) + (jsuword) ARENA_INFO_OFFSET))
12478: 
12478: #define ARENA_INFO_TO_START(arena)                                            \
12478:     (JS_ASSERT(IS_ARENA_INFO_ADDRESS(arena)),                                 \
12478:      (jsuword) (arena) & ~(jsuword) GC_ARENA_MASK)
12478: 
12478: #define ARENA_PAGE_TO_INFO(arenaPage)                                         \
12478:     (JS_ASSERT(arenaPage != 0),                                               \
12478:      JS_ASSERT(!((jsuword)(arenaPage) >> (JS_BITS_PER_WORD-GC_ARENA_SHIFT))), \
12478:      ARENA_START_TO_INFO((arenaPage) << GC_ARENA_SHIFT))
12478: 
12478: #define ARENA_INFO_TO_PAGE(arena)                                             \
12478:     (JS_ASSERT(IS_ARENA_INFO_ADDRESS(arena)),                                 \
12478:      ((jsuword) (arena) >> GC_ARENA_SHIFT))
12478: 
12478: #define GET_ARENA_INFO(chunk, index)                                          \
12478:     (JS_ASSERT((index) < js_gcArenasPerChunk),                                \
12478:      ARENA_START_TO_INFO(chunk + ((index) << GC_ARENA_SHIFT)))
12478: 
12478: #if CHUNKED_ARENA_ALLOCATION
12478: /*
12478:  * Definitions for allocating arenas in chunks.
    1:  *
 5917:  * All chunks that have at least one free arena are put on the doubly-linked
 5917:  * list with the head stored in JSRuntime.gcChunkList. JSGCChunkInfo contains
 5917:  * the head of the chunk's free arena list together with the link fields for
 5917:  * gcChunkList.
    1:  *
12478:  * Structure stored in one of chunk's free arenas. GET_CHUNK_INFO_INDEX gives
12478:  * the index of this arena. When all arenas in the chunk are used, it is
12478:  * removed from the list and the index is set to NO_FREE_ARENAS indicating
12478:  * that the chunk is not on gcChunkList and has no JSGCChunkInfo available.
    1:  */
12478: 
 5917: struct JSGCChunkInfo {
 5917:     JSGCChunkInfo   **prevp;
 5917:     JSGCChunkInfo   *next;
 5917:     JSGCArenaInfo   *lastFreeArena;
 5917:     uint32          numFreeArenas;
 5917: };
 5917: 
12478: #define NO_FREE_ARENAS              JS_BITMASK(GC_ARENA_SHIFT - 1)
12478: 
12478: #ifdef js_gcArenasPerChunk
12478: JS_STATIC_ASSERT(1 <= js_gcArenasPerChunk &&
12478:                  js_gcArenasPerChunk <= NO_FREE_ARENAS);
 5917: #endif
 5917: 
 5917: #define GET_ARENA_CHUNK(arena, index)                                         \
 5917:     (JS_ASSERT(GET_ARENA_INDEX(arena) == index),                              \
 5917:      ARENA_INFO_TO_START(arena) - ((index) << GC_ARENA_SHIFT))
 5917: 
 5917: #define GET_ARENA_INDEX(arena)                                                \
 5917:     ((arena)->firstArena ? 0 : (uint32) (arena)->arenaIndex)
 5917: 
 5917: #define GET_CHUNK_INFO_INDEX(chunk)                                           \
 5917:     ((uint32) ARENA_START_TO_INFO(chunk)->arenaIndex)
 5917: 
 5917: #define SET_CHUNK_INFO_INDEX(chunk, index)                                    \
 5917:     (JS_ASSERT((index) < js_gcArenasPerChunk || (index) == NO_FREE_ARENAS),   \
 5917:      (void) (ARENA_START_TO_INFO(chunk)->arenaIndex = (jsuword) (index)))
 5917: 
 5917: #define GET_CHUNK_INFO(chunk, infoIndex)                                      \
 5917:     (JS_ASSERT(GET_CHUNK_INFO_INDEX(chunk) == (infoIndex)),                   \
 5917:      JS_ASSERT((uint32) (infoIndex) < js_gcArenasPerChunk),                   \
 5917:      (JSGCChunkInfo *) ((chunk) + ((infoIndex) << GC_ARENA_SHIFT)))
 5917: 
 5917: #define CHUNK_INFO_TO_INDEX(ci)                                               \
 5917:     GET_ARENA_INDEX(ARENA_START_TO_INFO((jsuword)ci))
 5917: 
12478: #endif
12478: 
 5917: /*
 5917:  * Macros for GC-thing operations.
 5917:  */
 5917: #define THINGS_PER_ARENA(thingSize)                                           \
 5917:     ((GC_ARENA_SIZE - (uint32) sizeof(JSGCArenaInfo)) / ((thingSize) + 1U))
 5917: 
 5917: #define THING_TO_ARENA(thing)                                                 \
 5917:     ((JSGCArenaInfo *)(((jsuword) (thing) | GC_ARENA_MASK) +                  \
 5917:                        1 - sizeof(JSGCArenaInfo)))
 5917: 
 5917: #define THING_TO_INDEX(thing, thingSize)                                      \
 5917:     ((uint32) ((jsuword) (thing) & GC_ARENA_MASK) / (uint32) (thingSize))
 5917: 
 5917: #define THING_FLAGS_END(arena) ((uint8 *)(arena))
 5917: 
 5917: #define THING_FLAGP(arena, thingIndex)                                        \
 5917:     (JS_ASSERT((jsuword) (thingIndex)                                         \
 5917:                < (jsuword) THINGS_PER_ARENA((arena)->list->thingSize)),       \
 5917:      (uint8 *)(arena) - 1 - (thingIndex))
 5917: 
 5917: #define THING_TO_FLAGP(thing, thingSize)                                      \
 5917:     THING_FLAGP(THING_TO_ARENA(thing), THING_TO_INDEX(thing, thingSize))
 5917: 
 5917: #define FLAGP_TO_ARENA(flagp) THING_TO_ARENA(flagp)
 5917: 
 5917: #define FLAGP_TO_INDEX(flagp)                                                 \
 5917:     (JS_ASSERT(((jsuword) (flagp) & GC_ARENA_MASK) < ARENA_INFO_OFFSET),      \
 5917:      (ARENA_INFO_OFFSET - 1 - (uint32) ((jsuword) (flagp) & GC_ARENA_MASK)))
 5917: 
 5917: #define FLAGP_TO_THING(flagp, thingSize)                                      \
 5917:     (JS_ASSERT(((jsuword) (flagp) & GC_ARENA_MASK) >=                         \
 5917:                (ARENA_INFO_OFFSET - THINGS_PER_ARENA(thingSize))),            \
 6155:      (JSGCThing *)(((jsuword) (flagp) & ~GC_ARENA_MASK) +                     \
 5917:                    (thingSize) * FLAGP_TO_INDEX(flagp)))
    1: 
12282: /*
12282:  * Macros for the specialized arena for doubles.
    1:  *
12282:  * DOUBLES_PER_ARENA defines the maximum number of doubles that the arena can
12282:  * hold. We find it as the following. Let n be the number of doubles in the
12282:  * arena. Together with the bitmap of flags and JSGCArenaInfo they should fit
12282:  * the arena. Hence DOUBLES_PER_ARENA or n_max is the maximum value of n for
12282:  * which the following holds:
12282:  *
12282:  *   n*s + ceil(n/B) <= M                                               (1)
12282:  *
12282:  * where "/" denotes normal real division,
12282:  *       ceil(r) gives the least integer not smaller than the number r,
13029:  *       s is the number of words in jsdouble,
13029:  *       B is number of bits per word or B == JS_BITS_PER_WORD
13029:  *       M is the number of words in the arena before JSGCArenaInfo or
13029:  *       M == (GC_ARENA_SIZE - sizeof(JSGCArenaInfo)) / sizeof(jsuword).
13029:  *       M == ARENA_INFO_OFFSET / sizeof(jsuword)
12282:  *
12282:  * We rewrite the inequality as
12282:  *
12282:  *   n*B*s/B + ceil(n/B) <= M,
12282:  *   ceil(n*B*s/B + n/B) <= M,
12282:  *   ceil(n*(B*s + 1)/B) <= M                                           (2)
12282:  *
12282:  * We define a helper function e(n, s, B),
12282:  *
12282:  *   e(n, s, B) := ceil(n*(B*s + 1)/B) - n*(B*s + 1)/B, 0 <= e(n, s, B) < 1.
12282:  *
12282:  * It gives:
12282:  *
12282:  *   n*(B*s + 1)/B + e(n, s, B) <= M,
12282:  *   n + e*B/(B*s + 1) <= M*B/(B*s + 1)
12282:  *
12282:  * We apply the floor function to both sides of the last equation, where
12282:  * floor(r) gives the biggest integer not greater than r. As a consequence we
12282:  * have:
12282:  *
12282:  *   floor(n + e*B/(B*s + 1)) <= floor(M*B/(B*s + 1)),
12282:  *   n + floor(e*B/(B*s + 1)) <= floor(M*B/(B*s + 1)),
12282:  *   n <= floor(M*B/(B*s + 1)),                                         (3)
12282:  *
12282:  * where floor(e*B/(B*s + 1)) is zero as e*B/(B*s + 1) < B/(B*s + 1) < 1.
12282:  * Thus any n that satisfies the original constraint (1) or its equivalent (2),
12282:  * must also satisfy (3). That is, we got an upper estimate for the maximum
12282:  * value of n. Lets show that this upper estimate,
12282:  *
12282:  *   floor(M*B/(B*s + 1)),                                              (4)
12282:  *
12282:  * also satisfies (1) and, as such, gives the required maximum value.
12282:  * Substituting it into (2) gives:
12282:  *
12282:  *   ceil(floor(M*B/(B*s + 1))*(B*s + 1)/B) == ceil(floor(M/X)*X)
12282:  *
12282:  * where X == (B*s + 1)/B > 1. But then floor(M/X)*X <= M/X*X == M and
12282:  *
12282:  *   ceil(floor(M/X)*X) <= ceil(M) == M.
12282:  *
12282:  * Thus the value of (4) gives the maximum n satisfying (1).
13029:  *
13029:  * For the final result we observe that in (4)
13029:  *
13029:  *    M*B == ARENA_INFO_OFFSET / sizeof(jsuword) * JS_BITS_PER_WORD
13029:  *        == ARENA_INFO_OFFSET * JS_BITS_PER_BYTE
13029:  *
13029:  *  and
13029:  *
13029:  *    B*s == JS_BITS_PER_WORD * sizeof(jsdouble) / sizeof(jsuword)
13029:  *        == JS_BITS_PER_DOUBLE.
    1:  */
12282: #define DOUBLES_PER_ARENA                                                     \
12282:     ((ARENA_INFO_OFFSET * JS_BITS_PER_BYTE) / (JS_BITS_PER_DOUBLE + 1))
    1: 
    1: /*
13029:  * Check that  ARENA_INFO_OFFSET and sizeof(jsdouble) divides sizeof(jsuword).
12282:  */
13029: JS_STATIC_ASSERT(ARENA_INFO_OFFSET % sizeof(jsuword) == 0);
13029: JS_STATIC_ASSERT(sizeof(jsdouble) % sizeof(jsuword) == 0);
13029: JS_STATIC_ASSERT(sizeof(jsbitmap) == sizeof(jsuword));
13029: 
13029: #define DOUBLES_ARENA_BITMAP_WORDS                                            \
13029:     (JS_HOWMANY(DOUBLES_PER_ARENA, JS_BITS_PER_WORD))
13029: 
13029: /* Check that DOUBLES_PER_ARENA indeed maximises (1). */
12282: JS_STATIC_ASSERT(DOUBLES_PER_ARENA * sizeof(jsdouble) +
13029:                  DOUBLES_ARENA_BITMAP_WORDS * sizeof(jsuword) <=
12282:                  ARENA_INFO_OFFSET);
12282: 
12282: JS_STATIC_ASSERT((DOUBLES_PER_ARENA + 1) * sizeof(jsdouble) +
13029:                  sizeof(jsuword) *
13029:                  JS_HOWMANY((DOUBLES_PER_ARENA + 1), JS_BITS_PER_WORD) >
12282:                  ARENA_INFO_OFFSET);
12282: 
12282: /*
13029:  * When DOUBLES_PER_ARENA % BITS_PER_DOUBLE_FLAG_UNIT != 0, some bits in the
13029:  * last byte of the occupation bitmap are unused.
12282:  */
12282: #define UNUSED_DOUBLE_BITMAP_BITS                                             \
13029:     (DOUBLES_ARENA_BITMAP_WORDS * JS_BITS_PER_WORD - DOUBLES_PER_ARENA)
13029: 
13029: JS_STATIC_ASSERT(UNUSED_DOUBLE_BITMAP_BITS < JS_BITS_PER_WORD);
12282: 
12282: #define DOUBLES_ARENA_BITMAP_OFFSET                                           \
13029:     (ARENA_INFO_OFFSET - DOUBLES_ARENA_BITMAP_WORDS * sizeof(jsuword))
12282: 
12282: #define CHECK_DOUBLE_ARENA_INFO(arenaInfo)                                    \
12282:     (JS_ASSERT(IS_ARENA_INFO_ADDRESS(arenaInfo)),                             \
12282:      JS_ASSERT(!(arenaInfo)->list))                                           \
12282: 
12282: /*
12282:  * Get the start of the bitmap area containing double mark flags in the arena.
12282:  * To access the flag the code uses
    1:  *
13029:  *   JS_TEST_BIT(bitmapStart, index)
12282:  *
12282:  * That is, compared with the case of arenas with non-double things, we count
12282:  * flags from the start of the bitmap area, not from the end.
    1:  */
12282: #define DOUBLE_ARENA_BITMAP(arenaInfo)                                        \
12282:     (CHECK_DOUBLE_ARENA_INFO(arenaInfo),                                      \
13029:      (jsbitmap *) arenaInfo - DOUBLES_ARENA_BITMAP_WORDS)
12282: 
12282: #define DOUBLE_THING_TO_INDEX(thing)                                          \
12282:     (CHECK_DOUBLE_ARENA_INFO(THING_TO_ARENA(thing)),                          \
12282:      JS_ASSERT(((jsuword) (thing) & GC_ARENA_MASK) <                          \
12282:                DOUBLES_ARENA_BITMAP_OFFSET),                                  \
12282:      ((uint32) (((jsuword) (thing) & GC_ARENA_MASK) / sizeof(jsdouble))))
12282: 
12282: static void
12282: ClearDoubleArenaFlags(JSGCArenaInfo *a)
12282: {
13029:     jsbitmap *bitmap, mask;
13029:     uintN nused;
12282: 
12282:     /*
12282:      * When some high bits in the last byte of the double occupation bitmap
12282:      * are unused, we must set them. Otherwise RefillDoubleFreeList will
12282:      * assume that they corresponds to some free cells and tries to allocate
12282:      * them.
12282:      *
12282:      * Note that the code works correctly with UNUSED_DOUBLE_BITMAP_BITS == 0.
12282:      */
12282:     bitmap = DOUBLE_ARENA_BITMAP(a);
13029:     memset(bitmap, 0, (DOUBLES_ARENA_BITMAP_WORDS - 1) * sizeof *bitmap);
13029:     mask = ((jsbitmap) 1 << UNUSED_DOUBLE_BITMAP_BITS) - 1;
13029:     nused = JS_BITS_PER_WORD - UNUSED_DOUBLE_BITMAP_BITS;
13029:     bitmap[DOUBLES_ARENA_BITMAP_WORDS - 1] = mask << nused;
12282: }
12282: 
16284: static JS_ALWAYS_INLINE JSBool
12282: IsMarkedDouble(JSGCArenaInfo *a, uint32 index)
12282: {
13029:     jsbitmap *bitmap;
12282: 
12282:     JS_ASSERT(a->u.hasMarkedDoubles);
13029:     bitmap = DOUBLE_ARENA_BITMAP(a);
13029:     return JS_TEST_BIT(bitmap, index);
12282: }
12282: 
12282: /*
12282:  * JSRuntime.gcDoubleArenaList.nextDoubleFlags points either to:
12282:  *
12282:  *   1. The next byte in the bitmap area for doubles to check for unmarked
12282:  *      (or free) doubles.
12282:  *   2. Or to the end of the bitmap area when all GC cells of the arena are
12282:  *      allocated.
12282:  *   3. Or to a special sentinel value indicating that there are no arenas
12282:  *      to check for unmarked doubles.
12282:  *
13029:  * We set the sentinel to ARENA_INFO_OFFSET so the single check
12282:  *
12282:  *   ((jsuword) nextDoubleFlags & GC_ARENA_MASK) == ARENA_INFO_OFFSET
12282:  *
12282:  * will cover both the second and the third cases.
12282:  */
13029: #define DOUBLE_BITMAP_SENTINEL  ((jsbitmap *) ARENA_INFO_OFFSET)
    1: 
    1: #ifdef JS_THREADSAFE
    1: /*
 5917:  * The maximum number of things to put on the local free list by taking
    1:  * several things from the global free list or from the tail of the last
    1:  * allocated arena to amortize the cost of rt->gcLock.
    1:  *
    1:  * We use number 8 based on benchmarks from bug 312238.
    1:  */
    1: #define MAX_THREAD_LOCAL_THINGS 8
    1: 
    1: #endif
    1: 
    1: JS_STATIC_ASSERT(sizeof(JSStackHeader) >= 2 * sizeof(jsval));
    1: 
    1: JS_STATIC_ASSERT(sizeof(JSGCThing) >= sizeof(JSString));
    1: JS_STATIC_ASSERT(sizeof(JSGCThing) >= sizeof(jsdouble));
    1: 
    1: /* We want to use all the available GC thing space for object's slots. */
    1: JS_STATIC_ASSERT(sizeof(JSObject) % sizeof(JSGCThing) == 0);
    1: 
    1: /*
 4529:  * Ensure that JSObject is allocated from a different GC-list rather than
 4529:  * jsdouble and JSString so we can easily finalize JSObject before these 2
 4529:  * types of GC things. See comments in js_GC.
  254:  */
 4529: JS_STATIC_ASSERT(GC_FREELIST_INDEX(sizeof(JSString)) !=
 4529:                  GC_FREELIST_INDEX(sizeof(JSObject)));
 4529: JS_STATIC_ASSERT(GC_FREELIST_INDEX(sizeof(jsdouble)) !=
  254:                  GC_FREELIST_INDEX(sizeof(JSObject)));
  254: 
  254: /*
    1:  * JSPtrTable capacity growth descriptor. The table grows by powers of two
    1:  * starting from capacity JSPtrTableInfo.minCapacity, but switching to linear
    1:  * growth when capacity reaches JSPtrTableInfo.linearGrowthThreshold.
    1:  */
    1: typedef struct JSPtrTableInfo {
    1:     uint16      minCapacity;
    1:     uint16      linearGrowthThreshold;
    1: } JSPtrTableInfo;
    1: 
    1: #define GC_ITERATOR_TABLE_MIN     4
    1: #define GC_ITERATOR_TABLE_LINEAR  1024
    1: 
    1: static const JSPtrTableInfo iteratorTableInfo = {
    1:     GC_ITERATOR_TABLE_MIN,
    1:     GC_ITERATOR_TABLE_LINEAR
    1: };
    1: 
    1: /* Calculate table capacity based on the current value of JSPtrTable.count. */
    1: static size_t
    1: PtrTableCapacity(size_t count, const JSPtrTableInfo *info)
    1: {
    1:     size_t linear, log, capacity;
    1: 
    1:     linear = info->linearGrowthThreshold;
    1:     JS_ASSERT(info->minCapacity <= linear);
    1: 
    1:     if (count == 0) {
    1:         capacity = 0;
    1:     } else if (count < linear) {
    1:         log = JS_CEILING_LOG2W(count);
    1:         JS_ASSERT(log != JS_BITS_PER_WORD);
    1:         capacity = (size_t)1 << log;
    1:         if (capacity < info->minCapacity)
    1:             capacity = info->minCapacity;
    1:     } else {
    1:         capacity = JS_ROUNDUP(count, linear);
    1:     }
    1: 
    1:     JS_ASSERT(capacity >= count);
    1:     return capacity;
    1: }
    1: 
    1: static void
    1: FreePtrTable(JSPtrTable *table, const JSPtrTableInfo *info)
    1: {
    1:     if (table->array) {
    1:         JS_ASSERT(table->count > 0);
    1:         free(table->array);
    1:         table->array = NULL;
    1:         table->count = 0;
    1:     }
    1:     JS_ASSERT(table->count == 0);
    1: }
    1: 
    1: static JSBool
    1: AddToPtrTable(JSContext *cx, JSPtrTable *table, const JSPtrTableInfo *info,
    1:               void *ptr)
    1: {
    1:     size_t count, capacity;
    1:     void **array;
    1: 
    1:     count = table->count;
    1:     capacity = PtrTableCapacity(count, info);
    1: 
    1:     if (count == capacity) {
    1:         if (capacity < info->minCapacity) {
    1:             JS_ASSERT(capacity == 0);
    1:             JS_ASSERT(!table->array);
    1:             capacity = info->minCapacity;
    1:         } else {
    1:             /*
    1:              * Simplify the overflow detection assuming pointer is bigger
    1:              * than byte.
    1:              */
    1:             JS_STATIC_ASSERT(2 <= sizeof table->array[0]);
    1:             capacity = (capacity < info->linearGrowthThreshold)
    1:                        ? 2 * capacity
    1:                        : capacity + info->linearGrowthThreshold;
    1:             if (capacity > (size_t)-1 / sizeof table->array[0])
    1:                 goto bad;
    1:         }
    1:         array = (void **) realloc(table->array,
    1:                                   capacity * sizeof table->array[0]);
    1:         if (!array)
    1:             goto bad;
    1: #ifdef DEBUG
    1:         memset(array + count, JS_FREE_PATTERN,
    1:                (capacity - count) * sizeof table->array[0]);
    1: #endif
    1:         table->array = array;
    1:     }
    1: 
    1:     table->array[count] = ptr;
    1:     table->count = count + 1;
    1: 
    1:     return JS_TRUE;
    1: 
    1:   bad:
    1:     JS_ReportOutOfMemory(cx);
    1:     return JS_FALSE;
    1: }
    1: 
    1: static void
    1: ShrinkPtrTable(JSPtrTable *table, const JSPtrTableInfo *info,
    1:                size_t newCount)
    1: {
    1:     size_t oldCapacity, capacity;
    1:     void **array;
    1: 
    1:     JS_ASSERT(newCount <= table->count);
    1:     if (newCount == table->count)
    1:         return;
    1: 
    1:     oldCapacity = PtrTableCapacity(table->count, info);
    1:     table->count = newCount;
    1:     capacity = PtrTableCapacity(newCount, info);
    1: 
    1:     if (oldCapacity != capacity) {
    1:         array = table->array;
    1:         JS_ASSERT(array);
    1:         if (capacity == 0) {
    1:             free(array);
    1:             table->array = NULL;
    1:             return;
    1:         }
    1:         array = (void **) realloc(array, capacity * sizeof array[0]);
    1:         if (array)
    1:             table->array = array;
    1:     }
    1: #ifdef DEBUG
    1:     memset(table->array + newCount, JS_FREE_PATTERN,
    1:            (capacity - newCount) * sizeof table->array[0]);
    1: #endif
    1: }
    1: 
    1: #ifdef JS_GCMETER
10954: # define METER(x)               ((void) (x))
10954: # define METER_IF(condition, x) ((void) ((condition) && (x)))
    1: #else
    1: # define METER(x)               ((void) 0)
10954: # define METER_IF(condition, x) ((void) 0)
    1: #endif
    1: 
10954: #define METER_UPDATE_MAX(maxLval, rval)                                       \
10954:     METER_IF((maxLval) < (rval), (maxLval) = (rval))
10954: 
12478: #if JS_GC_USE_MMAP || !HAS_POSIX_MEMALIGN
12478: 
 5917: /*
 5917:  * For chunks allocated via over-sized malloc, get a pointer to store the gap
 5917:  * between the malloc's result and the first arena in the chunk.
 5917:  */
 5917: static uint32 *
 5917: GetMallocedChunkGapPtr(jsuword chunk)
    1: {
 5917:     JS_ASSERT((chunk & GC_ARENA_MASK) == 0);
    1: 
 5917:     /* Use the memory after the chunk, see NewGCChunk for details. */
 5917:     return (uint32 *) (chunk + (js_gcArenasPerChunk << GC_ARENA_SHIFT));
 5917: }
    1: 
12478: #endif
12478: 
 5917: static jsuword
12478: NewGCChunk(void)
 5917: {
 5917:     void *p;
    1: 
 5917: #if JS_GC_USE_MMAP
 5917:     if (js_gcUseMmap) {
 5917: # if defined(XP_WIN)
 5917:         p = VirtualAlloc(NULL, js_gcArenasPerChunk << GC_ARENA_SHIFT,
 5917:                          MEM_COMMIT | MEM_RESERVE, PAGE_READWRITE);
 5917:         return (jsuword) p;
12478: # else
 5917:         p = mmap(NULL, js_gcArenasPerChunk << GC_ARENA_SHIFT,
 5917:                  PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
 5917:         return (p == MAP_FAILED) ? 0 : (jsuword) p;
 5917: # endif
 5917:     }
 5917: #endif
    1: 
12478: #if HAS_POSIX_MEMALIGN
12478:     if (0 != posix_memalign(&p, GC_ARENA_SIZE,
12478:                             GC_ARENA_SIZE * js_gcArenasPerChunk -
12478:                             JS_GC_ARENA_PAD)) {
12478:         return 0;
12478:     }
12478:     return (jsuword) p;
12478: #else
 5917:     /*
12478:      * Implement chunk allocation using oversized malloc if mmap and
12478:      * posix_memalign are not available.
 5917:      *
 5917:      * Since malloc allocates pointers aligned on the word boundary, to get
 5917:      * js_gcArenasPerChunk aligned arenas, we need to malloc only
12478:      *
 5917:      *   ((js_gcArenasPerChunk + 1) << GC_ARENA_SHIFT) - sizeof(size_t)
12478:      *
 5917:      * bytes. But since we stores the gap between the malloced pointer and the
 5917:      * first arena in the chunk after the chunk, we need to ask for
12478:      *
 5917:      *   ((js_gcArenasPerChunk + 1) << GC_ARENA_SHIFT)
12478:      *
 5917:      * bytes to ensure that we always have room to store the gap.
 5917:      */
 5917:     p = malloc((js_gcArenasPerChunk + 1) << GC_ARENA_SHIFT);
 5917:     if (!p)
 5917:         return 0;
12478: 
12478:     {
12478:         jsuword chunk;
12478: 
 5917:         chunk = ((jsuword) p + GC_ARENA_MASK) & ~GC_ARENA_MASK;
 5917:         *GetMallocedChunkGapPtr(chunk) = (uint32) (chunk - (jsuword) p);
 5917:         return chunk;
    1:     }
12478: #endif
12478: }
    1: 
    1: static void
 5917: DestroyGCChunk(jsuword chunk)
    1: {
 5917:     JS_ASSERT((chunk & GC_ARENA_MASK) == 0);
 5917: #if JS_GC_USE_MMAP
 5917:     if (js_gcUseMmap) {
 5917: # if defined(XP_WIN)
 5917:         VirtualFree((void *) chunk, 0, MEM_RELEASE);
18563: # elif defined(SOLARIS)
18563:         munmap((char *) chunk, js_gcArenasPerChunk << GC_ARENA_SHIFT);
12478: # else
 5917:         munmap((void *) chunk, js_gcArenasPerChunk << GC_ARENA_SHIFT);
 5917: # endif
 5917:         return;
 5917:     }
 5917: #endif
    1: 
12478: #if HAS_POSIX_MEMALIGN
12478:     free((void *) chunk);
12478: #else
 5917:     /* See comments in NewGCChunk. */
 5917:     JS_ASSERT(*GetMallocedChunkGapPtr(chunk) < GC_ARENA_SIZE);
 5917:     free((void *) (chunk - *GetMallocedChunkGapPtr(chunk)));
12478: #endif
 5917: }
 5917: 
12478: #if CHUNKED_ARENA_ALLOCATION
12478: 
 5917: static void
 5917: AddChunkToList(JSRuntime *rt, JSGCChunkInfo *ci)
 5917: {
 5917:     ci->prevp = &rt->gcChunkList;
 5917:     ci->next = rt->gcChunkList;
 5917:     if (rt->gcChunkList) {
 5917:         JS_ASSERT(rt->gcChunkList->prevp == &rt->gcChunkList);
 5917:         rt->gcChunkList->prevp = &ci->next;
 5917:     }
 5917:     rt->gcChunkList = ci;
 5917: }
 5917: 
 5917: static void
 5917: RemoveChunkFromList(JSRuntime *rt, JSGCChunkInfo *ci)
 5917: {
 5917:     *ci->prevp = ci->next;
 5917:     if (ci->next) {
 5917:         JS_ASSERT(ci->next->prevp == &ci->next);
 5917:         ci->next->prevp = ci->prevp;
 5917:     }
 5917: }
 5917: 
12478: #endif
12478: 
 5917: static JSGCArenaInfo *
 5917: NewGCArena(JSRuntime *rt)
 5917: {
 5917:     jsuword chunk;
12478:     JSGCArenaInfo *a;
 5917: 
    1:     if (rt->gcBytes >= rt->gcMaxBytes)
12282:         return NULL;
12282: 
12478: #if CHUNKED_ARENA_ALLOCATION
 5917:     if (js_gcArenasPerChunk == 1) {
12478: #endif
 5917:         chunk = NewGCChunk();
12282:         if (chunk == 0)
12282:             return NULL;
12282:         a = ARENA_START_TO_INFO(chunk);
12478: #if CHUNKED_ARENA_ALLOCATION
12282:     } else {
12478:         JSGCChunkInfo *ci;
12478:         uint32 i;
12478:         JSGCArenaInfo *aprev;
12478: 
 5917:         ci = rt->gcChunkList;
 5917:         if (!ci) {
 5917:             chunk = NewGCChunk();
 5917:             if (chunk == 0)
 5917:                 return NULL;
 5917:             JS_ASSERT((chunk & GC_ARENA_MASK) == 0);
 5917:             a = GET_ARENA_INFO(chunk, 0);
 5917:             a->firstArena = JS_TRUE;
 5917:             a->arenaIndex = 0;
 5917:             aprev = NULL;
 5917:             i = 0;
    1:             do {
 5917:                 a->prev = aprev;
 5917:                 aprev = a;
 5917:                 ++i;
 5917:                 a = GET_ARENA_INFO(chunk, i);
 5917:                 a->firstArena = JS_FALSE;
 5917:                 a->arenaIndex = i;
 5917:             } while (i != js_gcArenasPerChunk - 1);
 5917:             ci = GET_CHUNK_INFO(chunk, 0);
 5917:             ci->lastFreeArena = aprev;
 5917:             ci->numFreeArenas = js_gcArenasPerChunk - 1;
 5917:             AddChunkToList(rt, ci);
 5917:         } else {
 5917:             JS_ASSERT(ci->prevp == &rt->gcChunkList);
 5917:             a = ci->lastFreeArena;
 5917:             aprev = a->prev;
 5917:             if (!aprev) {
 5917:                 JS_ASSERT(ci->numFreeArenas == 1);
 5917:                 JS_ASSERT(ARENA_INFO_TO_START(a) == (jsuword) ci);
 5917:                 RemoveChunkFromList(rt, ci);
 5917:                 chunk = GET_ARENA_CHUNK(a, GET_ARENA_INDEX(a));
 5917:                 SET_CHUNK_INFO_INDEX(chunk, NO_FREE_ARENAS);
 5917:             } else {
 5917:                 JS_ASSERT(ci->numFreeArenas >= 2);
 5917:                 JS_ASSERT(ARENA_INFO_TO_START(a) != (jsuword) ci);
 5917:                 ci->lastFreeArena = aprev;
 5917:                 ci->numFreeArenas--;
 5917:             }
 5917:         }
12282:     }
12478: #endif
12282: 
    1:     rt->gcBytes += GC_ARENA_SIZE;
12282:     a->prevUntracedPage = 0;
12282:     memset(&a->u, 0, sizeof(a->u));
 5917: 
 5917:     return a;
    1: }
    1: 
    1: static void
12282: DestroyGCArenas(JSRuntime *rt, JSGCArenaInfo *last)
    1: {
12282:     JSGCArenaInfo *a;
12478: 
12478:     while (last) {
12478:         a = last;
12478:         last = last->prev;
12478: 
12478:         METER(rt->gcStats.afree++);
    1:         JS_ASSERT(rt->gcBytes >= GC_ARENA_SIZE);
    1:         rt->gcBytes -= GC_ARENA_SIZE;
12478: 
12478: #if CHUNKED_ARENA_ALLOCATION
12478:         if (js_gcArenasPerChunk == 1) {
12478: #endif
12478:             DestroyGCChunk(ARENA_INFO_TO_START(a));
12478: #if CHUNKED_ARENA_ALLOCATION
12478:         } else {
 5917:             uint32 arenaIndex;
 5917:             jsuword chunk;
 5917:             uint32 chunkInfoIndex;
 5917:             JSGCChunkInfo *ci;
    1: # ifdef DEBUG
12282:             jsuword firstArena;
 5917: 
 5917:             firstArena = a->firstArena;
 5917:             arenaIndex = a->arenaIndex;
 5917:             memset((void *) ARENA_INFO_TO_START(a), JS_FREE_PATTERN,
12478:                    GC_ARENA_SIZE - JS_GC_ARENA_PAD);
 5917:             a->firstArena = firstArena;
 5917:             a->arenaIndex = arenaIndex;
    1: # endif
 5917:             arenaIndex = GET_ARENA_INDEX(a);
 5917:             chunk = GET_ARENA_CHUNK(a, arenaIndex);
 5917:             chunkInfoIndex = GET_CHUNK_INFO_INDEX(chunk);
 5917:             if (chunkInfoIndex == NO_FREE_ARENAS) {
 5917:                 chunkInfoIndex = arenaIndex;
 5917:                 SET_CHUNK_INFO_INDEX(chunk, arenaIndex);
 5917:                 ci = GET_CHUNK_INFO(chunk, chunkInfoIndex);
 5917:                 a->prev = NULL;
 5917:                 ci->lastFreeArena = a;
 5917:                 ci->numFreeArenas = 1;
 5917:                 AddChunkToList(rt, ci);
 5917:             } else {
 5917:                 JS_ASSERT(chunkInfoIndex != arenaIndex);
 5917:                 ci = GET_CHUNK_INFO(chunk, chunkInfoIndex);
 5917:                 JS_ASSERT(ci->numFreeArenas != 0);
 5917:                 JS_ASSERT(ci->lastFreeArena);
 5917:                 JS_ASSERT(a != ci->lastFreeArena);
 5917:                 if (ci->numFreeArenas == js_gcArenasPerChunk - 1) {
 5917:                     RemoveChunkFromList(rt, ci);
 5917:                     DestroyGCChunk(chunk);
 5917:                 } else {
 5917:                     ++ci->numFreeArenas;
 5917:                     a->prev = ci->lastFreeArena;
 5917:                     ci->lastFreeArena = a;
 5917:                 }
 5917:             }
    1:         }
12478: # endif
12282:     }
    1: }
    1: 
    1: static void
    1: InitGCArenaLists(JSRuntime *rt)
    1: {
    1:     uintN i, thingSize;
    1:     JSGCArenaList *arenaList;
    1: 
    1:     for (i = 0; i < GC_NUM_FREELISTS; i++) {
    1:         arenaList = &rt->gcArenaList[i];
    1:         thingSize = GC_FREELIST_NBYTES(i);
    1:         JS_ASSERT((size_t)(uint16)thingSize == thingSize);
    1:         arenaList->last = NULL;
12689:         arenaList->lastCount = (uint16) THINGS_PER_ARENA(thingSize);
    1:         arenaList->thingSize = (uint16) thingSize;
    1:         arenaList->freeList = NULL;
    1:     }
12282:     rt->gcDoubleArenaList.first = NULL;
12282:     rt->gcDoubleArenaList.nextDoubleFlags = DOUBLE_BITMAP_SENTINEL;
    1: }
    1: 
    1: static void
    1: FinishGCArenaLists(JSRuntime *rt)
    1: {
    1:     uintN i;
    1:     JSGCArenaList *arenaList;
    1: 
    1:     for (i = 0; i < GC_NUM_FREELISTS; i++) {
    1:         arenaList = &rt->gcArenaList[i];
12282:         DestroyGCArenas(rt, arenaList->last);
 5917:         arenaList->last = NULL;
 5917:         arenaList->lastCount = THINGS_PER_ARENA(arenaList->thingSize);
    1:         arenaList->freeList = NULL;
    1:     }
12282:     DestroyGCArenas(rt, rt->gcDoubleArenaList.first);
12282:     rt->gcDoubleArenaList.first = NULL;
12282:     rt->gcDoubleArenaList.nextDoubleFlags = DOUBLE_BITMAP_SENTINEL;
12282: 
 5917:     rt->gcBytes = 0;
 5917:     JS_ASSERT(rt->gcChunkList == 0);
    1: }
    1: 
12282: /*
12282:  * This function must not be called when thing is jsdouble.
12282:  */
 8005: static uint8 *
 8005: GetGCThingFlags(void *thing)
    1: {
 5917:     JSGCArenaInfo *a;
 5917:     uint32 index;
    1: 
 5917:     a = THING_TO_ARENA(thing);
 5917:     index = THING_TO_INDEX(thing, a->list->thingSize);
 5917:     return THING_FLAGP(a, index);
    1: }
    1: 
12282: /*
12282:  * This function returns null when thing is jsdouble.
12282:  */
12282: static uint8 *
12282: GetGCThingFlagsOrNull(void *thing)
12282: {
12282:     JSGCArenaInfo *a;
12282:     uint32 index;
12282: 
12282:     a = THING_TO_ARENA(thing);
12282:     if (!a->list)
12282:         return NULL;
12282:     index = THING_TO_INDEX(thing, a->list->thingSize);
12282:     return THING_FLAGP(a, index);
12282: }
12282: 
 8005: intN
 8005: js_GetExternalStringGCType(JSString *str)
 8005: {
 8005:     uintN type;
 8005: 
 8005:     type = (uintN) *GetGCThingFlags(str) & GCF_TYPEMASK;
 8005:     JS_ASSERT(type == GCX_STRING || type >= GCX_EXTERNAL_STRING);
 8005:     return (type == GCX_STRING) ? -1 : (intN) (type - GCX_EXTERNAL_STRING);
 8005: }
 8005: 
 8005: static uint32
 8005: MapGCFlagsToTraceKind(uintN flags)
 8005: {
 8005:     uint32 type;
 8005: 
 8005:     type = flags & GCF_TYPEMASK;
12282:     JS_ASSERT(type != GCX_DOUBLE);
 8005:     JS_ASSERT(type < GCX_NTYPES);
 8005:     return (type < GCX_EXTERNAL_STRING) ? type : JSTRACE_STRING;
 8005: }
 8005: 
 8005: JS_FRIEND_API(uint32)
 8005: js_GetGCThingTraceKind(void *thing)
 8005: {
12282:     JSGCArenaInfo *a;
12282:     uint32 index;
12282: 
12282:     a = THING_TO_ARENA(thing);
12282:     if (!a->list)
12282:         return JSTRACE_DOUBLE;
12282: 
12282:     index = THING_TO_INDEX(thing, a->list->thingSize);
12282:     return MapGCFlagsToTraceKind(*THING_FLAGP(a, index));
    1: }
    1: 
    1: JSRuntime*
    1: js_GetGCStringRuntime(JSString *str)
    1: {
    1:     JSGCArenaList *list;
    1: 
 5917:     list = THING_TO_ARENA(str)->list;
    1: 
    1:     JS_ASSERT(list->thingSize == sizeof(JSGCThing));
    1:     JS_ASSERT(GC_FREELIST_INDEX(sizeof(JSGCThing)) == 0);
    1: 
    1:     return (JSRuntime *)((uint8 *)list - offsetof(JSRuntime, gcArenaList));
    1: }
    1: 
    1: JSBool
    1: js_IsAboutToBeFinalized(JSContext *cx, void *thing)
    1: {
12282:     JSGCArenaInfo *a;
12282:     uint32 index, flags;
12282: 
12282:     a = THING_TO_ARENA(thing);
12282:     if (!a->list) {
12282:         /*
12282:          * Check if arena has no marked doubles. In that case the bitmap with
12282:          * the mark flags contains all garbage as it is initialized only when
12282:          * marking the first double in the arena.
12282:          */
12282:         if (!a->u.hasMarkedDoubles)
12282:             return JS_TRUE;
12282:         index = DOUBLE_THING_TO_INDEX(thing);
12282:         return !IsMarkedDouble(a, index);
12282:     }
12282:     index = THING_TO_INDEX(thing, a->list->thingSize);
12282:     flags = *THING_FLAGP(a, index);
    1:     return !(flags & (GCF_MARK | GCF_LOCK | GCF_FINAL));
    1: }
    1: 
    1: /* This is compatible with JSDHashEntryStub. */
    1: typedef struct JSGCRootHashEntry {
    1:     JSDHashEntryHdr hdr;
    1:     void            *root;
    1:     const char      *name;
    1: } JSGCRootHashEntry;
    1: 
    1: /* Initial size of the gcRootsHash table (SWAG, small enough to amortize). */
    1: #define GC_ROOTS_SIZE   256
 5917: 
12478: #if CHUNKED_ARENA_ALLOCATION
12478: 
 5917: /*
 5917:  * For a CPU with extremely large pages using them for GC things wastes
 5917:  * too much memory.
 5917:  */
 5917: # define GC_ARENAS_PER_CPU_PAGE_LIMIT JS_BIT(18 - GC_ARENA_SHIFT)
 5917: 
 5917: JS_STATIC_ASSERT(GC_ARENAS_PER_CPU_PAGE_LIMIT <= NO_FREE_ARENAS);
    1: 
12478: #endif
    1: 
    1: JSBool
    1: js_InitGC(JSRuntime *rt, uint32 maxbytes)
    1: {
 5917: #if JS_GC_USE_MMAP
 5917:     if (js_gcArenasPerChunk == 0) {
 5917:         size_t cpuPageSize, arenasPerPage;
 5917: # if defined(XP_WIN)
 5917:         SYSTEM_INFO si;
 5917: 
 5917:         GetSystemInfo(&si);
 5917:         cpuPageSize = si.dwPageSize;
 5917: 
 5917: # elif defined(XP_UNIX) || defined(XP_BEOS)
 5917:         cpuPageSize = (size_t) sysconf(_SC_PAGESIZE);
 5917: # else
 5917: #  error "Not implemented"
 5917: # endif
 5917:         /* cpuPageSize is a power of 2. */
 5917:         JS_ASSERT((cpuPageSize & (cpuPageSize - 1)) == 0);
 5917:         arenasPerPage = cpuPageSize >> GC_ARENA_SHIFT;
 5917: #ifdef DEBUG
 5917:         if (arenasPerPage == 0) {
 5917:             fprintf(stderr,
 5917: "JS engine warning: the size of the CPU page, %u bytes, is too low to use\n"
 5917: "paged allocation for the garbage collector. Please report this.\n",
 5917:                     (unsigned) cpuPageSize);
 5917:         }
 5917: #endif
 5917:         if (arenasPerPage - 1 <= (size_t) (GC_ARENAS_PER_CPU_PAGE_LIMIT - 1)) {
 5917:             /*
 5917:              * Use at least 4 GC arenas per paged allocation chunk to minimize
 5917:              * the overhead of mmap/VirtualAlloc.
 5917:              */
 5917:             js_gcUseMmap = JS_TRUE;
 5917:             js_gcArenasPerChunk = JS_MAX((uint32) arenasPerPage, 4);
 5917:         } else {
 5917:             js_gcUseMmap = JS_FALSE;
 5917:             js_gcArenasPerChunk = 7;
 5917:         }
 5917:     }
 5917:     JS_ASSERT(1 <= js_gcArenasPerChunk &&
 5917:               js_gcArenasPerChunk <= NO_FREE_ARENAS);
12478: #endif
 5917: 
    1:     InitGCArenaLists(rt);
    1:     if (!JS_DHashTableInit(&rt->gcRootsHash, JS_DHashGetStubOps(), NULL,
    1:                            sizeof(JSGCRootHashEntry), GC_ROOTS_SIZE)) {
    1:         rt->gcRootsHash.ops = NULL;
    1:         return JS_FALSE;
    1:     }
    1:     rt->gcLocksHash = NULL;     /* create lazily */
    1: 
    1:     /*
    1:      * Separate gcMaxMallocBytes from gcMaxBytes but initialize to maxbytes
    1:      * for default backward API compatibility.
    1:      */
    1:     rt->gcMaxBytes = rt->gcMaxMallocBytes = maxbytes;
19196:     rt->gcEmptyArenaPoolLifespan = 30000;
    1: 
10954:     METER(memset(&rt->gcStats, 0, sizeof rt->gcStats));
    1:     return JS_TRUE;
    1: }
    1: 
    1: #ifdef JS_GCMETER
12282: 
12282: static void
12282: UpdateArenaStats(JSGCArenaStats *st, uint32 nlivearenas, uint32 nkilledArenas,
12282:                  uint32 nthings)
12282: {
12282:     size_t narenas;
12282: 
12282:     narenas = nlivearenas + nkilledArenas;
12282:     JS_ASSERT(narenas >= st->livearenas);
12282: 
12282:     st->newarenas = narenas - st->livearenas;
12282:     st->narenas = narenas;
12282:     st->livearenas = nlivearenas;
12282:     if (st->maxarenas < narenas)
12282:         st->maxarenas = narenas;
12282:     st->totalarenas += narenas;
12282: 
12282:     st->nthings = nthings;
12282:     if (st->maxthings < nthings)
12282:         st->maxthings = nthings;
12282:     st->totalthings += nthings;
12282: }
12282: 
    1: JS_FRIEND_API(void)
    1: js_DumpGCStats(JSRuntime *rt, FILE *fp)
    1: {
12282:     int i;
    1:     size_t sumArenas, sumTotalArenas;
12282:     size_t sumThings, sumMaxThings;
12282:     size_t sumThingSize, sumTotalThingSize;
12282:     size_t sumArenaCapacity, sumTotalArenaCapacity;
12282:     JSGCArenaStats *st;
12282:     size_t thingSize, thingsPerArena;
12282:     size_t sumAlloc, sumLocalAlloc, sumFail, sumRetry;
    1: 
    1:     fprintf(fp, "\nGC allocation statistics:\n");
    1: 
    1: #define UL(x)       ((unsigned long)(x))
    1: #define ULSTAT(x)   UL(rt->gcStats.x)
12282: #define PERCENT(x,y)  (100.0 * (double) (x) / (double) (y))
12282: 
    1:     sumArenas = 0;
    1:     sumTotalArenas = 0;
12282:     sumThings = 0;
12282:     sumMaxThings = 0;
12282:     sumThingSize = 0;
12282:     sumTotalThingSize = 0;
12282:     sumArenaCapacity = 0;
12282:     sumTotalArenaCapacity = 0;
12282:     sumAlloc = 0;
12282:     sumLocalAlloc = 0;
12282:     sumFail = 0;
12282:     sumRetry = 0;
12282:     for (i = -1; i < (int) GC_NUM_FREELISTS; i++) {
12282:         if (i == -1) {
12282:             thingSize = sizeof(jsdouble);
12282:             thingsPerArena = DOUBLES_PER_ARENA;
12282:             st = &rt->gcStats.doubleArenaStats;
12282:             fprintf(fp,
12282:                     "Arena list for double values (%lu doubles per arena):",
12282:                     UL(thingsPerArena));
12282:         } else {
12282:             thingSize = rt->gcArenaList[i].thingSize;
12282:             thingsPerArena = THINGS_PER_ARENA(thingSize);
12282:             st = &rt->gcStats.arenaStats[i];
12282:             fprintf(fp,
12282:                     "Arena list %d (thing size %lu, %lu things per arena):",
12282:                     i, UL(GC_FREELIST_NBYTES(i)), UL(thingsPerArena));
12282:         }
12282:         if (st->maxarenas == 0) {
12282:             fputs(" NEVER USED\n", fp);
    1:             continue;
    1:         }
12282:         putc('\n', fp);
12282:         fprintf(fp, "           arenas before GC: %lu\n", UL(st->narenas));
12282:         fprintf(fp, "       new arenas before GC: %lu (%.1f%%)\n",
12282:                 UL(st->newarenas), PERCENT(st->newarenas, st->narenas));
12282:         fprintf(fp, "            arenas after GC: %lu (%.1f%%)\n",
12282:                 UL(st->livearenas), PERCENT(st->livearenas, st->narenas));
12282:         fprintf(fp, "                 max arenas: %lu\n", UL(st->maxarenas));
12282:         fprintf(fp, "                     things: %lu\n", UL(st->nthings));
12282:         fprintf(fp, "        GC cell utilization: %.1f%%\n",
12282:                 PERCENT(st->nthings, thingsPerArena * st->narenas));
12282:         fprintf(fp, "   average cell utilization: %.1f%%\n",
12282:                 PERCENT(st->totalthings, thingsPerArena * st->totalarenas));
12282:         fprintf(fp, "                 max things: %lu\n", UL(st->maxthings));
12282:         fprintf(fp, "             alloc attempts: %lu\n", UL(st->alloc));
12282:         fprintf(fp, "        alloc without locks: %1u  (%.1f%%)\n",
12282:                 UL(st->localalloc), PERCENT(st->localalloc, st->alloc));
12282:         sumArenas += st->narenas;
12282:         sumTotalArenas += st->totalarenas;
12282:         sumThings += st->nthings;
12282:         sumMaxThings += st->maxthings;
12282:         sumThingSize += thingSize * st->nthings;
12282:         sumTotalThingSize += thingSize * st->totalthings;
12282:         sumArenaCapacity += thingSize * thingsPerArena * st->narenas;
12282:         sumTotalArenaCapacity += thingSize * thingsPerArena * st->totalarenas;
12282:         sumAlloc += st->alloc;
12282:         sumLocalAlloc += st->localalloc;
12282:         sumFail += st->fail;
12282:         sumRetry += st->retry;
    1:     }
    1:     fprintf(fp, "TOTAL STATS:\n");
    1:     fprintf(fp, "            bytes allocated: %lu\n", UL(rt->gcBytes));
    1:     fprintf(fp, "            total GC arenas: %lu\n", UL(sumArenas));
12282:     fprintf(fp, "            total GC things: %lu\n", UL(sumThings));
12282:     fprintf(fp, "        max total GC things: %lu\n", UL(sumMaxThings));
12282:     fprintf(fp, "        GC cell utilization: %.1f%%\n",
12282:             PERCENT(sumThingSize, sumArenaCapacity));
12282:     fprintf(fp, "   average cell utilization: %.1f%%\n",
12282:             PERCENT(sumTotalThingSize, sumTotalArenaCapacity));
12282:     fprintf(fp, "allocation retries after GC: %lu\n", UL(sumRetry));
12282:     fprintf(fp, "             alloc attempts: %lu\n", UL(sumAlloc));
12282:     fprintf(fp, "        alloc without locks: %1u  (%.1f%%)\n",
12282:             UL(sumLocalAlloc), PERCENT(sumLocalAlloc, sumAlloc));
12282:     fprintf(fp, "        allocation failures: %lu\n", UL(sumFail));
    1:     fprintf(fp, "         things born locked: %lu\n", ULSTAT(lockborn));
    1:     fprintf(fp, "           valid lock calls: %lu\n", ULSTAT(lock));
    1:     fprintf(fp, "         valid unlock calls: %lu\n", ULSTAT(unlock));
    1:     fprintf(fp, "       mark recursion depth: %lu\n", ULSTAT(depth));
    1:     fprintf(fp, "     maximum mark recursion: %lu\n", ULSTAT(maxdepth));
    1:     fprintf(fp, "     mark C recursion depth: %lu\n", ULSTAT(cdepth));
    1:     fprintf(fp, "   maximum mark C recursion: %lu\n", ULSTAT(maxcdepth));
 5917:     fprintf(fp, "      delayed tracing calls: %lu\n", ULSTAT(untraced));
    1: #ifdef DEBUG
 5917:     fprintf(fp, "      max trace later count: %lu\n", ULSTAT(maxuntraced));
    1: #endif
    1:     fprintf(fp, "   maximum GC nesting level: %lu\n", ULSTAT(maxlevel));
    1:     fprintf(fp, "potentially useful GC calls: %lu\n", ULSTAT(poke));
    1:     fprintf(fp, "  thing arenas freed so far: %lu\n", ULSTAT(afree));
    1:     fprintf(fp, "     stack segments scanned: %lu\n", ULSTAT(stackseg));
    1:     fprintf(fp, "stack segment slots scanned: %lu\n", ULSTAT(segslots));
    1:     fprintf(fp, "reachable closeable objects: %lu\n", ULSTAT(nclose));
    1:     fprintf(fp, "    max reachable closeable: %lu\n", ULSTAT(maxnclose));
    1:     fprintf(fp, "      scheduled close hooks: %lu\n", ULSTAT(closelater));
    1:     fprintf(fp, "  max scheduled close hooks: %lu\n", ULSTAT(maxcloselater));
12282: 
    1: #undef UL
12282: #undef ULSTAT
12282: #undef PERCENT
    1: 
    1: #ifdef JS_ARENAMETER
    1:     JS_DumpArenaStats(fp);
    1: #endif
    1: }
    1: #endif
    1: 
    1: #ifdef DEBUG
    1: static void
    1: CheckLeakedRoots(JSRuntime *rt);
    1: #endif
    1: 
19699: #ifdef JS_THREADSAFE
19699: static void
19699: TrimGCFreeListsPool(JSRuntime *rt, uintN keepCount);
19699: #endif
19699: 
    1: void
    1: js_FinishGC(JSRuntime *rt)
    1: {
    1: #ifdef JS_ARENAMETER
    1:     JS_DumpArenaStats(stdout);
    1: #endif
    1: #ifdef JS_GCMETER
    1:     js_DumpGCStats(rt, stdout);
    1: #endif
    1: 
    1:     FreePtrTable(&rt->gcIteratorTable, &iteratorTableInfo);
19699: #ifdef JS_THREADSAFE
19699:     TrimGCFreeListsPool(rt, 0);
19699:     JS_ASSERT(!rt->gcFreeListsPool);
19699: #endif
    1:     FinishGCArenaLists(rt);
    1: 
    1:     if (rt->gcRootsHash.ops) {
    1: #ifdef DEBUG
    1:         CheckLeakedRoots(rt);
    1: #endif
    1:         JS_DHashTableFinish(&rt->gcRootsHash);
    1:         rt->gcRootsHash.ops = NULL;
    1:     }
    1:     if (rt->gcLocksHash) {
    1:         JS_DHashTableDestroy(rt->gcLocksHash);
    1:         rt->gcLocksHash = NULL;
    1:     }
    1: }
    1: 
    1: JSBool
    1: js_AddRoot(JSContext *cx, void *rp, const char *name)
    1: {
    1:     JSBool ok = js_AddRootRT(cx->runtime, rp, name);
    1:     if (!ok)
    1:         JS_ReportOutOfMemory(cx);
    1:     return ok;
    1: }
    1: 
    1: JSBool
    1: js_AddRootRT(JSRuntime *rt, void *rp, const char *name)
    1: {
    1:     JSBool ok;
    1:     JSGCRootHashEntry *rhe;
    1: 
    1:     /*
    1:      * Due to the long-standing, but now removed, use of rt->gcLock across the
    1:      * bulk of js_GC, API users have come to depend on JS_AddRoot etc. locking
    1:      * properly with a racing GC, without calling JS_AddRoot from a request.
    1:      * We have to preserve API compatibility here, now that we avoid holding
    1:      * rt->gcLock across the mark phase (including the root hashtable mark).
    1:      *
    1:      * If the GC is running and we're called on another thread, wait for this
    1:      * GC activation to finish.  We can safely wait here (in the case where we
    1:      * are called within a request on another thread's context) without fear
    1:      * of deadlock because the GC doesn't set rt->gcRunning until after it has
    1:      * waited for all active requests to end.
    1:      */
    1:     JS_LOCK_GC(rt);
    1: #ifdef JS_THREADSAFE
    1:     JS_ASSERT(!rt->gcRunning || rt->gcLevel > 0);
    1:     if (rt->gcRunning && rt->gcThread->id != js_CurrentThreadId()) {
    1:         do {
    1:             JS_AWAIT_GC_DONE(rt);
    1:         } while (rt->gcLevel > 0);
    1:     }
    1: #endif
    1:     rhe = (JSGCRootHashEntry *)
    1:           JS_DHashTableOperate(&rt->gcRootsHash, rp, JS_DHASH_ADD);
    1:     if (rhe) {
    1:         rhe->root = rp;
    1:         rhe->name = name;
    1:         ok = JS_TRUE;
    1:     } else {
    1:         ok = JS_FALSE;
    1:     }
    1:     JS_UNLOCK_GC(rt);
    1:     return ok;
    1: }
    1: 
    1: JSBool
    1: js_RemoveRoot(JSRuntime *rt, void *rp)
    1: {
    1:     /*
    1:      * Due to the JS_RemoveRootRT API, we may be called outside of a request.
    1:      * Same synchronization drill as above in js_AddRoot.
    1:      */
    1:     JS_LOCK_GC(rt);
    1: #ifdef JS_THREADSAFE
    1:     JS_ASSERT(!rt->gcRunning || rt->gcLevel > 0);
    1:     if (rt->gcRunning && rt->gcThread->id != js_CurrentThreadId()) {
    1:         do {
    1:             JS_AWAIT_GC_DONE(rt);
    1:         } while (rt->gcLevel > 0);
    1:     }
    1: #endif
    1:     (void) JS_DHashTableOperate(&rt->gcRootsHash, rp, JS_DHASH_REMOVE);
    1:     rt->gcPoke = JS_TRUE;
    1:     JS_UNLOCK_GC(rt);
    1:     return JS_TRUE;
    1: }
    1: 
    1: #ifdef DEBUG
    1: 
18907: static JSDHashOperator
    1: js_root_printer(JSDHashTable *table, JSDHashEntryHdr *hdr, uint32 i, void *arg)
    1: {
    1:     uint32 *leakedroots = (uint32 *)arg;
    1:     JSGCRootHashEntry *rhe = (JSGCRootHashEntry *)hdr;
    1: 
    1:     (*leakedroots)++;
    1:     fprintf(stderr,
    1:             "JS engine warning: leaking GC root \'%s\' at %p\n",
    1:             rhe->name ? (char *)rhe->name : "", rhe->root);
    1: 
    1:     return JS_DHASH_NEXT;
    1: }
    1: 
    1: static void
    1: CheckLeakedRoots(JSRuntime *rt)
    1: {
    1:     uint32 leakedroots = 0;
    1: 
    1:     /* Warn (but don't assert) debug builds of any remaining roots. */
    1:     JS_DHashTableEnumerate(&rt->gcRootsHash, js_root_printer,
    1:                            &leakedroots);
    1:     if (leakedroots > 0) {
    1:         if (leakedroots == 1) {
    1:             fprintf(stderr,
11799: "JS engine warning: 1 GC root remains after destroying the JSRuntime at %p.\n"
    1: "                   This root may point to freed memory. Objects reachable\n"
12282: "                   through it have not been finalized.\n",
12282:                     (void *) rt);
    1:         } else {
    1:             fprintf(stderr,
11799: "JS engine warning: %lu GC roots remain after destroying the JSRuntime at %p.\n"
    1: "                   These roots may point to freed memory. Objects reachable\n"
    1: "                   through them have not been finalized.\n",
12282:                     (unsigned long) leakedroots, (void *) rt);
    1:         }
    1:     }
    1: }
    1: 
    1: typedef struct NamedRootDumpArgs {
    1:     void (*dump)(const char *name, void *rp, void *data);
    1:     void *data;
    1: } NamedRootDumpArgs;
    1: 
18907: static JSDHashOperator
    1: js_named_root_dumper(JSDHashTable *table, JSDHashEntryHdr *hdr, uint32 number,
    1:                      void *arg)
    1: {
    1:     NamedRootDumpArgs *args = (NamedRootDumpArgs *) arg;
    1:     JSGCRootHashEntry *rhe = (JSGCRootHashEntry *)hdr;
    1: 
    1:     if (rhe->name)
    1:         args->dump(rhe->name, rhe->root, args->data);
    1:     return JS_DHASH_NEXT;
    1: }
    1: 
18563: JS_BEGIN_EXTERN_C
    1: void
    1: js_DumpNamedRoots(JSRuntime *rt,
    1:                   void (*dump)(const char *name, void *rp, void *data),
    1:                   void *data)
    1: {
    1:     NamedRootDumpArgs args;
    1: 
    1:     args.dump = dump;
    1:     args.data = data;
    1:     JS_DHashTableEnumerate(&rt->gcRootsHash, js_named_root_dumper, &args);
    1: }
18563: JS_END_EXTERN_C
    1: 
    1: #endif /* DEBUG */
    1: 
    1: typedef struct GCRootMapArgs {
    1:     JSGCRootMapFun map;
    1:     void *data;
    1: } GCRootMapArgs;
    1: 
18907: static JSDHashOperator
    1: js_gcroot_mapper(JSDHashTable *table, JSDHashEntryHdr *hdr, uint32 number,
    1:                  void *arg)
    1: {
    1:     GCRootMapArgs *args = (GCRootMapArgs *) arg;
    1:     JSGCRootHashEntry *rhe = (JSGCRootHashEntry *)hdr;
    1:     intN mapflags;
 3164:     int op;
    1: 
    1:     mapflags = args->map(rhe->root, rhe->name, args->data);
    1: 
    1: #if JS_MAP_GCROOT_NEXT == JS_DHASH_NEXT &&                                     \
    1:     JS_MAP_GCROOT_STOP == JS_DHASH_STOP &&                                     \
    1:     JS_MAP_GCROOT_REMOVE == JS_DHASH_REMOVE
    1:     op = (JSDHashOperator)mapflags;
    1: #else
    1:     op = JS_DHASH_NEXT;
    1:     if (mapflags & JS_MAP_GCROOT_STOP)
    1:         op |= JS_DHASH_STOP;
    1:     if (mapflags & JS_MAP_GCROOT_REMOVE)
    1:         op |= JS_DHASH_REMOVE;
    1: #endif
    1: 
 3164:     return (JSDHashOperator) op;
    1: }
    1: 
    1: uint32
    1: js_MapGCRoots(JSRuntime *rt, JSGCRootMapFun map, void *data)
    1: {
    1:     GCRootMapArgs args;
    1:     uint32 rv;
    1: 
    1:     args.map = map;
    1:     args.data = data;
    1:     JS_LOCK_GC(rt);
    1:     rv = JS_DHashTableEnumerate(&rt->gcRootsHash, js_gcroot_mapper, &args);
    1:     JS_UNLOCK_GC(rt);
    1:     return rv;
    1: }
    1: 
    1: JSBool
    1: js_RegisterCloseableIterator(JSContext *cx, JSObject *obj)
    1: {
    1:     JSRuntime *rt;
    1:     JSBool ok;
    1: 
    1:     rt = cx->runtime;
    1:     JS_ASSERT(!rt->gcRunning);
    1: 
    1:     JS_LOCK_GC(rt);
    1:     ok = AddToPtrTable(cx, &rt->gcIteratorTable, &iteratorTableInfo, obj);
    1:     JS_UNLOCK_GC(rt);
    1:     return ok;
    1: }
    1: 
    1: static void
 3025: CloseNativeIterators(JSContext *cx)
    1: {
    1:     JSRuntime *rt;
    1:     size_t count, newCount, i;
    1:     void **array;
    1:     JSObject *obj;
    1: 
    1:     rt = cx->runtime;
    1:     count = rt->gcIteratorTable.count;
    1:     array = rt->gcIteratorTable.array;
    1: 
    1:     newCount = 0;
    1:     for (i = 0; i != count; ++i) {
    1:         obj = (JSObject *)array[i];
    1:         if (js_IsAboutToBeFinalized(cx, obj))
 3025:             js_CloseNativeIterator(cx, obj);
    1:         else
    1:             array[newCount++] = obj;
    1:     }
    1:     ShrinkPtrTable(&rt->gcIteratorTable, &iteratorTableInfo, newCount);
    1: }
    1: 
    1: #if defined(DEBUG_brendan) || defined(DEBUG_timeless)
    1: #define DEBUG_gchist
    1: #endif
    1: 
    1: #ifdef DEBUG_gchist
    1: #define NGCHIST 64
    1: 
    1: static struct GCHist {
    1:     JSBool      lastDitch;
    1:     JSGCThing   *freeList;
    1: } gchist[NGCHIST];
    1: 
10217: unsigned gchpos = 0;
    1: #endif
    1: 
19699: #ifdef JS_THREADSAFE
19699: 
19699: const JSGCFreeListSet js_GCEmptyFreeListSet = {
19699:     { NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL }, NULL
19699: };
19699: 
19699: static void
19699: TrimGCFreeListsPool(JSRuntime *rt, uintN keepCount)
19699: {
19699:     JSGCFreeListSet **cursor, *freeLists, *link;
19699: 
19699:     cursor = &rt->gcFreeListsPool;
19699:     while (keepCount != 0) {
19699:         --keepCount;
19699:         freeLists = *cursor;
19699:         if (!freeLists)
19699:             return;
19699:         memset(freeLists->array, 0, sizeof freeLists->array);
19699:         cursor = &freeLists->link;
19699:     }
19699:     freeLists = *cursor;
19699:     if (freeLists) {
19699:         *cursor = NULL;
19699:         do {
19699:             link = freeLists->link;
19699:             free(freeLists);
19699:         } while ((freeLists = link) != NULL);
19699:     }
19699: }
19699: 
19699: void
19699: js_RevokeGCLocalFreeLists(JSContext *cx)
19699: {
19699:     JS_ASSERT(!cx->gcLocalFreeLists->link);
19699:     if (cx->gcLocalFreeLists != &js_GCEmptyFreeListSet) {
19699:         cx->gcLocalFreeLists->link = cx->runtime->gcFreeListsPool;
19699:         cx->runtime->gcFreeListsPool = cx->gcLocalFreeLists;
19699:         cx->gcLocalFreeLists = (JSGCFreeListSet *) &js_GCEmptyFreeListSet;
19699:     }
19699: }
19699: 
19699: static JSGCFreeListSet *
19699: EnsureLocalFreeList(JSContext *cx)
19699: {
19699:     JSGCFreeListSet *freeLists;
19699: 
19699:     freeLists = cx->gcLocalFreeLists;
19699:     if (freeLists != &js_GCEmptyFreeListSet) {
19699:         JS_ASSERT(freeLists);
19699:         return freeLists;
19699:     }
19699: 
19699:     freeLists = cx->runtime->gcFreeListsPool;
19699:     if (freeLists) {
19699:         cx->runtime->gcFreeListsPool = freeLists->link;
19699:         freeLists->link = NULL;
19699:     } else {
19699:         /* JS_malloc is not used as the caller reports out-of-memory itself. */
19699:         freeLists = (JSGCFreeListSet *) calloc(1, sizeof *freeLists);
19699:         if (!freeLists)
19699:             return NULL;
19699:     }
19699:     cx->gcLocalFreeLists = freeLists;
19699:     return freeLists;
19699: }
19699: 
19699: #endif
19699: 
    1: void *
    1: js_NewGCThing(JSContext *cx, uintN flags, size_t nbytes)
    1: {
    1:     JSRuntime *rt;
    1:     uintN flindex;
    1:     JSBool doGC;
    1:     JSGCThing *thing;
 5917:     uint8 *flagp;
    1:     JSGCArenaList *arenaList;
 5917:     JSGCArenaInfo *a;
 5917:     uintN thingsLimit;
    1:     JSLocalRootStack *lrs;
12282: #ifdef JS_GCMETER
12282:     JSGCArenaStats *astats;
12282: #endif
    1: #ifdef JS_THREADSAFE
    1:     JSBool gcLocked;
    1:     uintN localMallocBytes;
19699:     JSGCFreeListSet *freeLists;
19699:     JSGCThing **lastptr;
    1:     JSGCThing *tmpthing;
    1:     uint8 *tmpflagp;
    1:     uintN maxFreeThings;         /* max to take from the global free list */
    1: #endif
    1: 
12282:     JS_ASSERT((flags & GCF_TYPEMASK) != GCX_DOUBLE);
    1:     rt = cx->runtime;
    1:     nbytes = JS_ROUNDUP(nbytes, sizeof(JSGCThing));
    1:     flindex = GC_FREELIST_INDEX(nbytes);
    1: 
12282:     /* Updates of metering counters here may not be thread-safe. */
12282:     METER(astats = &cx->runtime->gcStats.arenaStats[flindex]);
12282:     METER(astats->alloc++);
    1: 
    1: #ifdef JS_THREADSAFE
    1:     gcLocked = JS_FALSE;
    1:     JS_ASSERT(cx->thread);
19699:     freeLists = cx->gcLocalFreeLists;
19699:     thing = freeLists->array[flindex];
    1:     localMallocBytes = cx->thread->gcMallocBytes;
    1:     if (thing && rt->gcMaxMallocBytes - rt->gcMallocBytes > localMallocBytes) {
    1:         flagp = thing->flagp;
19699:         freeLists->array[flindex] = thing->next;
12282:         METER(astats->localalloc++);
    1:         goto success;
    1:     }
    1: 
    1:     JS_LOCK_GC(rt);
    1:     gcLocked = JS_TRUE;
    1: 
    1:     /* Transfer thread-local counter to global one. */
    1:     if (localMallocBytes != 0) {
    1:         cx->thread->gcMallocBytes = 0;
    1:         if (rt->gcMaxMallocBytes - rt->gcMallocBytes < localMallocBytes)
    1:             rt->gcMallocBytes = rt->gcMaxMallocBytes;
    1:         else
    1:             rt->gcMallocBytes += localMallocBytes;
    1:     }
    1: #endif
    1:     JS_ASSERT(!rt->gcRunning);
    1:     if (rt->gcRunning) {
    1:         METER(rt->gcStats.finalfail++);
    1:         JS_UNLOCK_GC(rt);
    1:         return NULL;
    1:     }
    1: 
 6038:     doGC = (rt->gcMallocBytes >= rt->gcMaxMallocBytes && rt->gcPoke);
 1492: #ifdef JS_GC_ZEAL
 6038:     doGC = doGC || rt->gcZeal >= 2 || (rt->gcZeal >= 1 && rt->gcPoke);
 6038: #endif
    1: 
    1:     arenaList = &rt->gcArenaList[flindex];
    1:     for (;;) {
18782:         if (doGC && !JS_ON_TRACE(cx)) {
    1:             /*
    1:              * Keep rt->gcLock across the call into js_GC so we don't starve
    1:              * and lose to racing threads who deplete the heap just after
    1:              * js_GC has replenished it (or has synchronized with a racing
    1:              * GC that collected a bunch of garbage).  This unfair scheduling
    1:              * can happen on certain operating systems. For the gory details,
    1:              * see bug 162779 at https://bugzilla.mozilla.org/.
    1:              */
    1:             js_GC(cx, GC_LAST_DITCH);
12282:             METER(astats->retry++);
    1:         }
    1: 
    1:         /* Try to get thing from the free list. */
    1:         thing = arenaList->freeList;
    1:         if (thing) {
    1:             arenaList->freeList = thing->next;
    1:             flagp = thing->flagp;
    1:             JS_ASSERT(*flagp & GCF_FINAL);
    1: 
    1: #ifdef JS_THREADSAFE
    1:             /*
    1:              * Refill the local free list by taking several things from the
    1:              * global free list unless we are still at rt->gcMaxMallocBytes
    1:              * barrier or the free list is already populated. The former
    1:              * happens when GC is canceled due to !gcCallback(cx, JSGC_BEGIN)
    1:              * or no gcPoke. The latter is caused via allocating new things
    1:              * in gcCallback(cx, JSGC_END).
    1:              */
19699:             if (rt->gcMallocBytes >= rt->gcMaxMallocBytes)
    1:                 break;
19699: 
19699:             freeLists = EnsureLocalFreeList(cx);
19699:             if (!freeLists)
19699:                 goto fail;
19699:             if (freeLists->array[flindex])
19699:                 break;
19699: 
    1:             tmpthing = arenaList->freeList;
    1:             if (tmpthing) {
    1:                 maxFreeThings = MAX_THREAD_LOCAL_THINGS;
    1:                 do {
    1:                     if (!tmpthing->next)
    1:                         break;
    1:                     tmpthing = tmpthing->next;
    1:                 } while (--maxFreeThings != 0);
    1: 
19699:                 freeLists->array[flindex] = arenaList->freeList;
    1:                 arenaList->freeList = tmpthing->next;
    1:                 tmpthing->next = NULL;
    1:             }
    1: #endif
    1:             break;
    1:         }
    1: 
    1:         /*
 5917:          * Try to allocate things from the last arena. If it is fully used,
 5917:          * check if we can allocate a new one and, if we cannot, consider
 5917:          * doing a "last ditch" GC unless already tried.
    1:          */
 5917:         thingsLimit = THINGS_PER_ARENA(nbytes);
 5917:         if (arenaList->lastCount != thingsLimit) {
 5917:             JS_ASSERT(arenaList->lastCount < thingsLimit);
 5917:             a = arenaList->last;
 5917:         } else {
12282:             a = NewGCArena(rt);
12282:             if (!a) {
18782:                 if (doGC || JS_ON_TRACE(cx))
 5917:                     goto fail;
 5917:                 doGC = JS_TRUE;
 5917:                 continue;
    1:             }
 5917:             a->list = arenaList;
 5917:             a->prev = arenaList->last;
 5917:             a->prevUntracedPage = 0;
12282:             a->u.untracedThings = 0;
 5917:             arenaList->last = a;
 5917:             arenaList->lastCount = 0;
    1:         }
 5917: 
 5917:         flagp = THING_FLAGP(a, arenaList->lastCount);
 6155:         thing = FLAGP_TO_THING(flagp, nbytes);
 5917:         arenaList->lastCount++;
    1: 
    1: #ifdef JS_THREADSAFE
    1:         /*
    1:          * Refill the local free list by taking free things from the last
    1:          * arena. Prefer to order free things by ascending address in the
    1:          * (unscientific) hope of better cache locality.
    1:          */
19699:         if (rt->gcMallocBytes >= rt->gcMaxMallocBytes)
    1:             break;
19699: 
19699:         freeLists = EnsureLocalFreeList(cx);
19699:         if (!freeLists)
19699:             goto fail;
19699:         if (freeLists->array[flindex])
19699:             break;
19699:         lastptr = &freeLists->array[flindex];
 5917:         maxFreeThings = thingsLimit - arenaList->lastCount;
 5917:         if (maxFreeThings > MAX_THREAD_LOCAL_THINGS)
    1:             maxFreeThings = MAX_THREAD_LOCAL_THINGS;
 5917:         while (maxFreeThings != 0) {
 5917:             --maxFreeThings;
    1: 
 5917:             tmpflagp = THING_FLAGP(a, arenaList->lastCount);
 6155:             tmpthing = FLAGP_TO_THING(tmpflagp, nbytes);
 5917:             arenaList->lastCount++;
    1:             tmpthing->flagp = tmpflagp;
    1:             *tmpflagp = GCF_FINAL;    /* signifying that thing is free */
    1: 
    1:             *lastptr = tmpthing;
    1:             lastptr = &tmpthing->next;
    1:         }
    1:         *lastptr = NULL;
    1: #endif
    1:         break;
    1:     }
    1: 
    1:     /* We successfully allocated the thing. */
    1: #ifdef JS_THREADSAFE
    1:   success:
    1: #endif
    1:     lrs = cx->localRootStack;
    1:     if (lrs) {
    1:         /*
    1:          * If we're in a local root scope, don't set newborn[type] at all, to
    1:          * avoid entraining garbage from it for an unbounded amount of time
    1:          * on this context.  A caller will leave the local root scope and pop
    1:          * this reference, allowing thing to be GC'd if it has no other refs.
    1:          * See JS_EnterLocalRootScope and related APIs.
    1:          */
    1:         if (js_PushLocalRoot(cx, lrs, (jsval) thing) < 0) {
    1:             /*
    1:              * When we fail for a thing allocated through the tail of the last
    1:              * arena, thing's flag byte is not initialized. So to prevent GC
    1:              * accessing the uninitialized flags during the finalization, we
    1:              * always mark the thing as final. See bug 337407.
    1:              */
    1:             *flagp = GCF_FINAL;
    1:             goto fail;
    1:         }
    1:     } else {
    1:         /*
    1:          * No local root scope, so we're stuck with the old, fragile model of
    1:          * depending on a pigeon-hole newborn per type per context.
    1:          */
    1:         cx->weakRoots.newborn[flags & GCF_TYPEMASK] = thing;
    1:     }
    1: 
    1:     /* We can't fail now, so update flags. */
    1:     *flagp = (uint8)flags;
    1: 
    1: #ifdef DEBUG_gchist
    1:     gchist[gchpos].lastDitch = doGC;
    1:     gchist[gchpos].freeList = rt->gcArenaList[flindex].freeList;
    1:     if (++gchpos == NGCHIST)
    1:         gchpos = 0;
    1: #endif
    1: 
    1:     /* This is not thread-safe for thread-local allocations. */
10954:     METER_IF(flags & GCF_LOCK, rt->gcStats.lockborn++);
10954: 
    1: #ifdef JS_THREADSAFE
    1:     if (gcLocked)
    1:         JS_UNLOCK_GC(rt);
    1: #endif
    1:     JS_COUNT_OPERATION(cx, JSOW_ALLOCATION);
    1:     return thing;
    1: 
    1: fail:
    1: #ifdef JS_THREADSAFE
    1:     if (gcLocked)
    1:         JS_UNLOCK_GC(rt);
    1: #endif
12282:     METER(astats->fail++);
18782:     if (!JS_ON_TRACE(cx))
    1:         JS_ReportOutOfMemory(cx);
    1:     return NULL;
    1: }
    1: 
12282: static JSGCDoubleCell *
12282: RefillDoubleFreeList(JSContext *cx)
    1: {
12282:     JSRuntime *rt;
13029:     jsbitmap *doubleFlags, usedBits;
19987:     JSBool didGC = JS_FALSE;
12282:     JSGCArenaInfo *a;
13029:     uintN bit, index;
12282:     JSGCDoubleCell *cell, *list, *lastcell;
12282: 
12282:     JS_ASSERT(!cx->doubleFreeList);
12282: 
12282:     rt = cx->runtime;
12282:     JS_LOCK_GC(rt);
12282: 
12282:     JS_ASSERT(!rt->gcRunning);
12282:     if (rt->gcRunning) {
12282:         METER(rt->gcStats.finalfail++);
12282:         JS_UNLOCK_GC(rt);
12282:         return NULL;
12282:     }
12282: 
19987:     if (rt->gcMallocBytes >= rt->gcMaxMallocBytes && rt->gcPoke
12282: #ifdef JS_GC_ZEAL
19987:         && (rt->gcZeal >= 2 || (rt->gcZeal >= 1 && rt->gcPoke))
12282: #endif
19987:         ) {
12282:         goto do_gc;
19987:     }
12282: 
12282:     /*
12282:      * Loop until we find a flag bitmap byte with unset bits indicating free
12282:      * double cells, then set all bits as used and put the cells to the free
12282:      * list for the current context.
12282:      */
12282:     doubleFlags = rt->gcDoubleArenaList.nextDoubleFlags;
12282:     for (;;) {
12282:         if (((jsuword) doubleFlags & GC_ARENA_MASK) ==
12282:             ARENA_INFO_OFFSET) {
12282:             if (doubleFlags == DOUBLE_BITMAP_SENTINEL ||
12282:                 !((JSGCArenaInfo *) doubleFlags)->prev) {
12282:                 a = NewGCArena(rt);
12282:                 if (!a) {
19987:                   do_gc:
19987:                     if (didGC || JS_ON_TRACE(cx)) {
12282:                         METER(rt->gcStats.doubleArenaStats.fail++);
12282:                         JS_UNLOCK_GC(rt);
19987:                         if (!JS_ON_TRACE(cx))
    1:                             JS_ReportOutOfMemory(cx);
12282:                         return NULL;
    1:                     }
12282:                     js_GC(cx, GC_LAST_DITCH);
12282:                     METER(rt->gcStats.doubleArenaStats.retry++);
12282:                     doubleFlags = rt->gcDoubleArenaList.nextDoubleFlags;
19987:                     didGC = JS_TRUE;
12282:                     continue;
12282:                 }
12282:                 a->list = NULL;
12282:                 a->prev = NULL;
12282:                 if (doubleFlags == DOUBLE_BITMAP_SENTINEL) {
12282:                     JS_ASSERT(!rt->gcDoubleArenaList.first);
12282:                     rt->gcDoubleArenaList.first = a;
12282:                 } else {
12282:                     JS_ASSERT(rt->gcDoubleArenaList.first);
12282:                     ((JSGCArenaInfo *) doubleFlags)->prev = a;
12282:                 }
12282:                 ClearDoubleArenaFlags(a);
12282:                 doubleFlags = DOUBLE_ARENA_BITMAP(a);
12282:                 break;
12282:             }
12282:             doubleFlags =
12282:                 DOUBLE_ARENA_BITMAP(((JSGCArenaInfo *) doubleFlags)->prev);
12282:         }
    1: 
    1:         /*
13029:          * When doubleFlags points the last bitmap's word in the arena, its
12282:          * high bits corresponds to non-existing cells. ClearDoubleArenaFlags
13029:          * sets such bits to 1. Thus even for this last word its bit is unset
13029:          * iff the corresponding cell exists and free.
12282:          */
13029:         if (*doubleFlags != (jsbitmap) -1)
12282:             break;
12282:         ++doubleFlags;
12282:     }
12282: 
12282:     rt->gcDoubleArenaList.nextDoubleFlags = doubleFlags + 1;
12282:     usedBits = *doubleFlags;
13029:     JS_ASSERT(usedBits != (jsbitmap) -1);
13029:     *doubleFlags = (jsbitmap) -1;
12282:     JS_UNLOCK_GC(rt);
12282: 
12282:     /*
12282:      * Find the index corresponding to the first bit in *doubleFlags. The last
13029:      * bit will have "index + JS_BITS_PER_WORD - 1".
12282:      */
12282:     index = ((uintN) ((jsuword) doubleFlags & GC_ARENA_MASK) -
12282:              DOUBLES_ARENA_BITMAP_OFFSET) * JS_BITS_PER_BYTE;
12282:     cell = (JSGCDoubleCell *) ((jsuword) doubleFlags & ~GC_ARENA_MASK) + index;
12282: 
12282:     if (usedBits == 0) {
12282:         /* The common case when all doubles from *doubleFlags are free. */
13029:         JS_ASSERT(index + JS_BITS_PER_WORD <= DOUBLES_PER_ARENA);
12282:         list = cell;
13029:         for (lastcell = cell + JS_BITS_PER_WORD - 1; cell != lastcell; ++cell)
12282:             cell->link = cell + 1;
12282:         lastcell->link = NULL;
12282:     } else {
12282:         /*
12282:          * Assemble the free list from free cells from *doubleFlags starting
12282:          * from the tail. In the loop
    1:          *
12282:          *   index + bit >= DOUBLES_PER_ARENA
12282:          *
12282:          * when bit is one of the unused bits. We do not check for such bits
12282:          * explicitly as they must be set and the "if" check filters them out.
    1:          */
13029:         JS_ASSERT(index + JS_BITS_PER_WORD <=
12282:                   DOUBLES_PER_ARENA + UNUSED_DOUBLE_BITMAP_BITS);
13029:         bit = JS_BITS_PER_WORD;
12282:         cell += bit;
12282:         list = NULL;
12282:         do {
12282:             --bit;
12282:             --cell;
13029:             if (!(((jsbitmap) 1 << bit) & usedBits)) {
12282:                 JS_ASSERT(index + bit < DOUBLES_PER_ARENA);
12282:                 JS_ASSERT_IF(index + bit == DOUBLES_PER_ARENA - 1, !list);
12282:                 cell->link = list;
12282:                 list = cell;
12282:             }
12282:         } while (bit != 0);
12282:     }
12282:     JS_ASSERT(list);
13029:     JS_COUNT_OPERATION(cx, JSOW_ALLOCATION * JS_BITS_PER_WORD);
12282: 
12282:     /*
12850:      * We delegate assigning cx->doubleFreeList to js_NewDoubleInRootedValue as
12282:      * it immediately consumes the head of the list.
12282:      */
12282:     return list;
12282: }
12282: 
    1: JSBool
12850: js_NewDoubleInRootedValue(JSContext *cx, jsdouble d, jsval *vp)
    1: {
12282: #ifdef JS_GCMETER
12282:     JSGCArenaStats *astats;
12282: #endif
12850:     JSGCDoubleCell *cell;
12282: 
12282:     /* Updates of metering counters here are not thread-safe. */
12282:     METER(astats = &cx->runtime->gcStats.doubleArenaStats);
12282:     METER(astats->alloc++);
12282:     cell = cx->doubleFreeList;
12282:     if (!cell) {
12282:         cell = RefillDoubleFreeList(cx);
12282:         if (!cell) {
12282:             METER(astats->fail++);
12850:             return JS_FALSE;
    1:         }
12282:     } else {
12282:         METER(astats->localalloc++);
12282:     }
12282:     cx->doubleFreeList = cell->link;
12850:     cell->number = d;
12850:     *vp = DOUBLE_TO_JSVAL(&cell->number);
12850:     return JS_TRUE;
12850: }
12850: 
12850: jsdouble *
12850: js_NewWeaklyRootedDouble(JSContext *cx, jsdouble d)
12850: {
12850:     jsval v;
12850:     jsdouble *dp;
12850: 
12850:     if (!js_NewDoubleInRootedValue(cx, d, &v))
12850:         return NULL;
12850: 
12850:     JS_ASSERT(JSVAL_IS_DOUBLE(v));
12850:     dp = JSVAL_TO_DOUBLE(v);
12282:     if (cx->localRootStack) {
12850:         if (js_PushLocalRoot(cx, cx->localRootStack, v) < 0)
12282:             return NULL;
12282:     } else {
12282:         cx->weakRoots.newborn[GCX_DOUBLE] = dp;
12282:     }
12282:     return dp;
12282: }
    1: 
17049: JSBool
17049: js_AddAsGCBytes(JSContext *cx, size_t sz)
17049: {
17049:     JSRuntime *rt;
17049: 
17049:     rt = cx->runtime;
17049:     if (rt->gcBytes >= rt->gcMaxBytes ||
17049:         sz > (size_t) (rt->gcMaxBytes - rt->gcBytes)
17049: #ifdef JS_GC_ZEAL
17049:         || rt->gcZeal >= 2 || (rt->gcZeal >= 1 && rt->gcPoke)
17049: #endif
17049:         ) {
19987:         if (JS_ON_TRACE(cx)) {
19987:             JS_UNLOCK_GC(rt);
19987:             return JS_FALSE;
19987:         }
17049:         js_GC(cx, GC_LAST_DITCH);
17049:         if (rt->gcBytes >= rt->gcMaxBytes ||
17049:             sz > (size_t) (rt->gcMaxBytes - rt->gcBytes)) {
17049:             JS_UNLOCK_GC(rt);
17049:             JS_ReportOutOfMemory(cx);
17049:             return JS_FALSE;
17049:         }
17049:     }
17049:     rt->gcBytes += (uint32) sz;
17049:     return JS_TRUE;
17049: }
17049: 
17049: void
17049: js_RemoveAsGCBytes(JSRuntime *rt, size_t sz)
17049: {
17049:     JS_ASSERT((size_t) rt->gcBytes >= sz);
17049:     rt->gcBytes -= (uint32) sz;
17049: }
17049: 
    1: /*
12282:  * Shallow GC-things can be locked just by setting the GCF_LOCK bit, because
12282:  * they have no descendants to mark during the GC. Currently the optimization
12282:  * is only used for non-dependant strings.
    1:  */
12282: #define GC_THING_IS_SHALLOW(flagp, thing)                                     \
12282:     ((flagp) &&                                                               \
12282:      ((*(flagp) & GCF_TYPEMASK) >= GCX_EXTERNAL_STRING ||                     \
12282:       ((*(flagp) & GCF_TYPEMASK) == GCX_STRING &&                             \
12282:        !JSSTRING_IS_DEPENDENT((JSString *) (thing)))))
    1: 
    1: /* This is compatible with JSDHashEntryStub. */
    1: typedef struct JSGCLockHashEntry {
    1:     JSDHashEntryHdr hdr;
12282:     const void      *thing;
    1:     uint32          count;
    1: } JSGCLockHashEntry;
    1: 
    1: JSBool
    1: js_LockGCThingRT(JSRuntime *rt, void *thing)
    1: {
12282:     JSBool shallow, ok;
    1:     uint8 *flagp;
    1:     JSGCLockHashEntry *lhe;
    1: 
    1:     if (!thing)
12282:         return JS_TRUE;
12282: 
12282:     flagp = GetGCThingFlagsOrNull(thing);
    1:     JS_LOCK_GC(rt);
12282:     shallow = GC_THING_IS_SHALLOW(flagp, thing);
    1: 
    1:     /*
    1:      * Avoid adding a rt->gcLocksHash entry for shallow things until someone
12282:      * nests a lock.
    1:      */
12282:     if (shallow && !(*flagp & GCF_LOCK)) {
12282:         *flagp |= GCF_LOCK;
12282:         METER(rt->gcStats.lock++);
12282:         ok = JS_TRUE;
12282:         goto out;
12282:     }
12282: 
    1:     if (!rt->gcLocksHash) {
12282:         rt->gcLocksHash = JS_NewDHashTable(JS_DHashGetStubOps(), NULL,
    1:                                            sizeof(JSGCLockHashEntry),
    1:                                            GC_ROOTS_SIZE);
    1:         if (!rt->gcLocksHash) {
    1:             ok = JS_FALSE;
12282:             goto out;
    1:         }
    1:     }
    1: 
    1:     lhe = (JSGCLockHashEntry *)
    1:           JS_DHashTableOperate(rt->gcLocksHash, thing, JS_DHASH_ADD);
    1:     if (!lhe) {
    1:         ok = JS_FALSE;
12282:         goto out;
    1:     }
    1:     if (!lhe->thing) {
12282:         lhe->thing = thing;
12282:         lhe->count = 1;
    1:     } else {
    1:         JS_ASSERT(lhe->count >= 1);
    1:         lhe->count++;
    1:     }
12282: 
    1:     METER(rt->gcStats.lock++);
    1:     ok = JS_TRUE;
12282:   out:
    1:     JS_UNLOCK_GC(rt);
    1:     return ok;
    1: }
    1: 
    1: JSBool
    1: js_UnlockGCThingRT(JSRuntime *rt, void *thing)
    1: {
12282:     uint8 *flagp;
12282:     JSBool shallow;
    1:     JSGCLockHashEntry *lhe;
    1: 
    1:     if (!thing)
    1:         return JS_TRUE;
    1: 
12282:     flagp = GetGCThingFlagsOrNull(thing);
    1:     JS_LOCK_GC(rt);
12282:     shallow = GC_THING_IS_SHALLOW(flagp, thing);
12282: 
12282:     if (shallow && !(*flagp & GCF_LOCK))
12282:         goto out;
    1:     if (!rt->gcLocksHash ||
    1:         (lhe = (JSGCLockHashEntry *)
    1:          JS_DHashTableOperate(rt->gcLocksHash, thing,
    1:                               JS_DHASH_LOOKUP),
    1:              JS_DHASH_ENTRY_IS_FREE(&lhe->hdr))) {
12282:         /* Shallow entry is not in the hash -> clear its lock bit. */
12282:         if (shallow)
12282:             *flagp &= ~GCF_LOCK;
12282:         else
12282:             goto out;
    1:     } else {
    1:         if (--lhe->count != 0)
    1:             goto out;
    1:         JS_DHashTableOperate(rt->gcLocksHash, thing, JS_DHASH_REMOVE);
    1:     }
    1: 
    1:     rt->gcPoke = JS_TRUE;
12282:     METER(rt->gcStats.unlock++);
    1:   out:
    1:     JS_UNLOCK_GC(rt);
    1:     return JS_TRUE;
    1: }
    1: 
  583: JS_PUBLIC_API(void)
  583: JS_TraceChildren(JSTracer *trc, void *thing, uint32 kind)
  583: {
    1:     JSObject *obj;
  583:     size_t nslots, i;
    1:     jsval v;
    1:     JSString *str;
  583: 
  583:     switch (kind) {
  583:       case JSTRACE_OBJECT:
    1:         /* If obj has no map, it must be a newborn. */
    1:         obj = (JSObject *) thing;
    1:         if (!obj->map)
    1:             break;
  583:         if (obj->map->ops->trace) {
  583:             obj->map->ops->trace(trc, obj);
  583:         } else {
  583:             nslots = STOBJ_NSLOTS(obj);
  583:             for (i = 0; i != nslots; ++i) {
    1:                 v = STOBJ_GET_SLOT(obj, i);
  583:                 if (JSVAL_IS_TRACEABLE(v)) {
  583:                     JS_SET_TRACING_INDEX(trc, "slot", i);
  583:                     JS_CallTracer(trc, JSVAL_TO_TRACEABLE(v),
  583:                                   JSVAL_TRACE_KIND(v));
  583:                 }
  583:             }
  583:         }
  583:         break;
  583: 
  583:       case JSTRACE_STRING:
  583:         str = (JSString *)thing;
  583:         if (JSSTRING_IS_DEPENDENT(str))
  583:             JS_CALL_STRING_TRACER(trc, JSSTRDEP_BASE(str), "base");
  583:         break;
  583: 
  583: #if JS_HAS_XML_SUPPORT
  583:       case JSTRACE_XML:
  583:         js_TraceXML(trc, (JSXML *)thing);
  583:         break;
    1: #endif
    1:     }
    1: }
    1: 
    1: /*
12282:  * Number of things covered by a single bit of JSGCArenaInfo.u.untracedThings.
    1:  */
 5917: #define THINGS_PER_UNTRACED_BIT(thingSize)                                    \
 5917:     JS_HOWMANY(THINGS_PER_ARENA(thingSize), JS_BITS_PER_WORD)
    1: 
    1: static void
 5917: DelayTracingChildren(JSRuntime *rt, uint8 *flagp)
    1: {
 5917:     JSGCArenaInfo *a;
 5917:     uint32 untracedBitIndex;
    1:     jsuword bit;
    1: 
 5917:     /*
 5917:      * Things with children to be traced later are marked with
 5917:      * GCF_MARK | GCF_FINAL flags.
 5917:      */
    1:     JS_ASSERT((*flagp & (GCF_MARK | GCF_FINAL)) == GCF_MARK);
    1:     *flagp |= GCF_FINAL;
    1: 
 5917:     METER(rt->gcStats.untraced++);
    1: #ifdef DEBUG
 5917:     ++rt->gcTraceLaterCount;
10954:     METER_UPDATE_MAX(rt->gcStats.maxuntraced, rt->gcTraceLaterCount);
    1: #endif
    1: 
 5917:     a = FLAGP_TO_ARENA(flagp);
 5917:     untracedBitIndex = FLAGP_TO_INDEX(flagp) /
 5917:                        THINGS_PER_UNTRACED_BIT(a->list->thingSize);
 5917:     JS_ASSERT(untracedBitIndex < JS_BITS_PER_WORD);
 5917:     bit = (jsuword)1 << untracedBitIndex;
12282:     if (a->u.untracedThings != 0) {
 5917:         JS_ASSERT(rt->gcUntracedArenaStackTop);
12282:         if (a->u.untracedThings & bit) {
 5917:             /* bit already covers things with children to trace later. */
    1:             return;
    1:         }
12282:         a->u.untracedThings |= bit;
    1:     } else {
    1:         /*
 5917:          * The thing is the first thing with not yet traced children in the
 5917:          * whole arena, so push the arena on the stack of arenas with things
 5917:          * to be traced later unless the arena has already been pushed. We
 5917:          * detect that through checking prevUntracedPage as the field is 0
 5917:          * only for not yet pushed arenas. To ensure that
 5917:          *   prevUntracedPage != 0
 5917:          * even when the stack contains one element, we make prevUntracedPage
 5917:          * for the arena at the bottom to point to itself.
 5917:          *
 5917:          * See comments in TraceDelayedChildren.
    1:          */
12282:         a->u.untracedThings = bit;
 5917:         if (a->prevUntracedPage == 0) {
 5917:             if (!rt->gcUntracedArenaStackTop) {
 5917:                 /* Stack was empty, mark the arena as the bottom element. */
 5917:                 a->prevUntracedPage = ARENA_INFO_TO_PAGE(a);
 5917:             } else {
 5917:                 JS_ASSERT(rt->gcUntracedArenaStackTop->prevUntracedPage != 0);
 5917:                 a->prevUntracedPage =
 5917:                     ARENA_INFO_TO_PAGE(rt->gcUntracedArenaStackTop);
    1:             }
 5917:             rt->gcUntracedArenaStackTop = a;
    1:         }
    1:     }
 5917:     JS_ASSERT(rt->gcUntracedArenaStackTop);
    1: }
    1: 
    1: static void
 5917: TraceDelayedChildren(JSTracer *trc)
    1: {
    1:     JSRuntime *rt;
 5917:     JSGCArenaInfo *a, *aprev;
 5917:     uint32 thingSize;
 5917:     uint32 thingsPerUntracedBit;
 5917:     uint32 untracedBitIndex, thingIndex, indexLimit, endIndex;
    1:     JSGCThing *thing;
    1:     uint8 *flagp;
    1: 
  583:     rt = trc->context->runtime;
 5917:     a = rt->gcUntracedArenaStackTop;
 5917:     if (!a) {
 5917:         JS_ASSERT(rt->gcTraceLaterCount == 0);
    1:         return;
    1:     }
    1: 
    1:     for (;;) {
    1:         /*
 5917:          * The following assert verifies that the current arena belongs to the
 5917:          * untraced stack, since DelayTracingChildren ensures that even for
 5917:          * stack's bottom prevUntracedPage != 0 but rather points to itself.
    1:          */
 5917:         JS_ASSERT(a->prevUntracedPage != 0);
 5917:         JS_ASSERT(rt->gcUntracedArenaStackTop->prevUntracedPage != 0);
 5917:         thingSize = a->list->thingSize;
 5917:         indexLimit = (a == a->list->last)
 5917:                      ? a->list->lastCount
 5917:                      : THINGS_PER_ARENA(thingSize);
 5917:         thingsPerUntracedBit = THINGS_PER_UNTRACED_BIT(thingSize);
 5917: 
    1:         /*
12282:          * We cannot use do-while loop here as a->u.untracedThings can be zero
 5917:          * before the loop as a leftover from the previous iterations. See
 5917:          * comments after the loop.
    1:          */
12282:         while (a->u.untracedThings != 0) {
12282:             untracedBitIndex = JS_FLOOR_LOG2W(a->u.untracedThings);
12282:             a->u.untracedThings &= ~((jsuword)1 << untracedBitIndex);
 5917:             thingIndex = untracedBitIndex * thingsPerUntracedBit;
 5917:             endIndex = thingIndex + thingsPerUntracedBit;
    1: 
    1:             /*
 5917:              * endIndex can go beyond the last allocated thing as the real
 5917:              * limit can be "inside" the bit.
    1:              */
 5917:             if (endIndex > indexLimit)
 5917:                 endIndex = indexLimit;
 5917:             JS_ASSERT(thingIndex < indexLimit);
 5917: 
 5917:             do {
    1:                 /*
 5917:                  * Skip free or already traced things that share the bit
 5917:                  * with untraced ones.
    1:                  */
 5917:                 flagp = THING_FLAGP(a, thingIndex);
    1:                 if ((*flagp & (GCF_MARK|GCF_FINAL)) != (GCF_MARK|GCF_FINAL))
    1:                     continue;
    1:                 *flagp &= ~GCF_FINAL;
    1: #ifdef DEBUG
 5917:                 JS_ASSERT(rt->gcTraceLaterCount != 0);
 5917:                 --rt->gcTraceLaterCount;
    1: #endif
 5917:                 thing = FLAGP_TO_THING(flagp, thingSize);
 8005:                 JS_TraceChildren(trc, thing, MapGCFlagsToTraceKind(*flagp));
 5917:             } while (++thingIndex != endIndex);
    1:         }
 5917: 
    1:         /*
 5917:          * We finished tracing of all things in the the arena but we can only
 5917:          * pop it from the stack if the arena is the stack's top.
    1:          *
 5917:          * When JS_TraceChildren from the above calls JS_CallTracer that in
 5917:          * turn on low C stack calls DelayTracingChildren and the latter
 5917:          * pushes new arenas to the untraced stack, we have to skip popping
 5917:          * of this arena until it becomes the top of the stack again.
    1:          */
 5917:         if (a == rt->gcUntracedArenaStackTop) {
 5917:             aprev = ARENA_PAGE_TO_INFO(a->prevUntracedPage);
 5917:             a->prevUntracedPage = 0;
 5917:             if (a == aprev) {
    1:                 /*
 5917:                  * prevUntracedPage points to itself and we reached the
 5917:                  * bottom of the stack.
    1:                  */
    1:                 break;
    1:             }
 5917:             rt->gcUntracedArenaStackTop = a = aprev;
    1:         } else {
 5917:             a = rt->gcUntracedArenaStackTop;
    1:         }
    1:     }
 5917:     JS_ASSERT(rt->gcUntracedArenaStackTop);
 5917:     JS_ASSERT(rt->gcUntracedArenaStackTop->prevUntracedPage == 0);
 5917:     rt->gcUntracedArenaStackTop = NULL;
 5917:     JS_ASSERT(rt->gcTraceLaterCount == 0);
    1: }
    1: 
  583: JS_PUBLIC_API(void)
  583: JS_CallTracer(JSTracer *trc, void *thing, uint32 kind)
    1: {
  583:     JSContext *cx;
  583:     JSRuntime *rt;
12282:     JSGCArenaInfo *a;
12282:     uintN index;
    1:     uint8 *flagp;
  583: 
  771:     JS_ASSERT(thing);
  583:     JS_ASSERT(JS_IS_VALID_TRACE_KIND(kind));
  583:     JS_ASSERT(trc->debugPrinter || trc->debugPrintArg);
  583: 
  583:     if (!IS_GC_MARKING_TRACER(trc)) {
  583:         trc->callback(trc, thing, kind);
  583:         goto out;
  583:     }
  583: 
  583:     cx = trc->context;
  583:     rt = cx->runtime;
  583:     JS_ASSERT(rt->gcMarkingTracer == trc);
  583:     JS_ASSERT(rt->gcLevel > 0);
  583: 
  583:     /*
 5917:      * Optimize for string and double as their size is known and their tracing
 5917:      * is not recursive.
  583:      */
 5917:     switch (kind) {
 5917:       case JSTRACE_DOUBLE:
12282:         a = THING_TO_ARENA(thing);
12282:         JS_ASSERT(!a->list);
12282:         if (!a->u.hasMarkedDoubles) {
12282:             ClearDoubleArenaFlags(a);
12282:             a->u.hasMarkedDoubles = JS_TRUE;
  583:         }
12282:         index = DOUBLE_THING_TO_INDEX(thing);
13029:         JS_SET_BIT(DOUBLE_ARENA_BITMAP(a), index);
 5917:         goto out;
 5917: 
 5917:       case JSTRACE_STRING:
 5917:         for (;;) {
 5917:             flagp = THING_TO_FLAGP(thing, sizeof(JSGCThing));
 5917:             JS_ASSERT((*flagp & GCF_FINAL) == 0);
 8005:             JS_ASSERT(kind == MapGCFlagsToTraceKind(*flagp));
 5917:             if (!JSSTRING_IS_DEPENDENT((JSString *) thing)) {
 5917:                 *flagp |= GCF_MARK;
  583:                 goto out;
  583:             }
    1:             if (*flagp & GCF_MARK)
  583:                 goto out;
    1:             *flagp |= GCF_MARK;
 5917:             thing = JSSTRDEP_BASE((JSString *) thing);
 5917:         }
 5917:         /* NOTREACHED */
 5917:     }
 5917: 
 8005:     flagp = GetGCThingFlags(thing);
 8005:     JS_ASSERT(kind == MapGCFlagsToTraceKind(*flagp));
    1:     if (*flagp & GCF_MARK)
  583:         goto out;
 5917: 
 5917:     /*
 5917:      * We check for non-final flag only if mark is unset as
 5917:      * DelayTracingChildren uses the flag. See comments in the function.
 5917:      */
 5917:     JS_ASSERT(*flagp != GCF_FINAL);
    1:     *flagp |= GCF_MARK;
    1:     if (!cx->insideGCMarkCallback) {
  583:         /*
  583:          * With JS_GC_ASSUME_LOW_C_STACK defined the mark phase of GC always
  583:          * uses the non-recursive code that otherwise would be called only on
  583:          * a low C stack condition.
  583:          */
  583: #ifdef JS_GC_ASSUME_LOW_C_STACK
  583: # define RECURSION_TOO_DEEP() JS_TRUE
  583: #else
  583:         int stackDummy;
  583: # define RECURSION_TOO_DEEP() (!JS_CHECK_STACK_SIZE(cx, stackDummy))
  583: #endif
  583:         if (RECURSION_TOO_DEEP())
 5917:             DelayTracingChildren(rt, flagp);
  583:         else
  583:             JS_TraceChildren(trc, thing, kind);
    1:     } else {
    1:         /*
    1:          * For API compatibility we allow for the callback to assume that
 5917:          * after it calls JS_MarkGCThing for the last time, the callback can
 5917:          * start to finalize its own objects that are only referenced by
 5917:          * unmarked GC things.
    1:          *
    1:          * Since we do not know which call from inside the callback is the
 5917:          * last, we ensure that children of all marked things are traced and
 5917:          * call TraceDelayedChildren(trc) after tracing the thing.
    1:          *
 5917:          * As TraceDelayedChildren unconditionally invokes JS_TraceChildren
 5917:          * for the things with untraced children, calling DelayTracingChildren
 5917:          * is useless here. Hence we always trace thing's children even with a
 5917:          * low native stack.
    1:          */
    1:         cx->insideGCMarkCallback = JS_FALSE;
  583:         JS_TraceChildren(trc, thing, kind);
 5917:         TraceDelayedChildren(trc);
    1:         cx->insideGCMarkCallback = JS_TRUE;
    1:     }
  583: 
  583:   out:
  583: #ifdef DEBUG
  583:     trc->debugPrinter = NULL;
  583:     trc->debugPrintArg = NULL;
  583: #endif
  583:     return;     /* to avoid out: right_curl when DEBUG is not defined */
    1: }
    1: 
  583: void
  771: js_CallValueTracerIfGCThing(JSTracer *trc, jsval v)
  583: {
  771:     void *thing;
  771:     uint32 kind;
  771: 
  771:     if (JSVAL_IS_DOUBLE(v) || JSVAL_IS_STRING(v)) {
  771:         thing = JSVAL_TO_TRACEABLE(v);
  771:         kind = JSVAL_TRACE_KIND(v);
 8005:         JS_ASSERT(kind == js_GetGCThingTraceKind(JSVAL_TO_GCTHING(v)));
  771:     } else if (JSVAL_IS_OBJECT(v) && v != JSVAL_NULL) {
  771:         /* v can be an arbitrary GC thing reinterpreted as an object. */
  771:         thing = JSVAL_TO_OBJECT(v);
 8005:         kind = js_GetGCThingTraceKind(thing);
  771:     } else {
  771:         return;
  583:     }
  771:     JS_CallTracer(trc, thing, kind);
  771: }
  583: 
18907: static JSDHashOperator
  583: gc_root_traversal(JSDHashTable *table, JSDHashEntryHdr *hdr, uint32 num,
  583:                   void *arg)
    1: {
    1:     JSGCRootHashEntry *rhe = (JSGCRootHashEntry *)hdr;
  583:     JSTracer *trc = (JSTracer *)arg;
    1:     jsval *rp = (jsval *)rhe->root;
    1:     jsval v = *rp;
    1: 
    1:     /* Ignore null object and scalar values. */
    1:     if (!JSVAL_IS_NULL(v) && JSVAL_IS_GCTHING(v)) {
    1: #ifdef DEBUG
    1:         JSBool root_points_to_gcArenaList = JS_FALSE;
    1:         jsuword thing = (jsuword) JSVAL_TO_GCTHING(v);
12282:         JSRuntime *rt;
    1:         uintN i;
    1:         JSGCArenaList *arenaList;
 5917:         uint32 thingSize;
 5917:         JSGCArenaInfo *a;
    1:         size_t limit;
    1: 
12282:         rt = trc->context->runtime;
    1:         for (i = 0; i < GC_NUM_FREELISTS; i++) {
12282:             arenaList = &rt->gcArenaList[i];
 5917:             thingSize = arenaList->thingSize;
 5917:             limit = (size_t) arenaList->lastCount * thingSize;
    1:             for (a = arenaList->last; a; a = a->prev) {
 5917:                 if (thing - ARENA_INFO_TO_START(a) < limit) {
    1:                     root_points_to_gcArenaList = JS_TRUE;
    1:                     break;
    1:                 }
 5917:                 limit = (size_t) THINGS_PER_ARENA(thingSize) * thingSize;
    1:             }
    1:         }
12282:         if (!root_points_to_gcArenaList) {
12282:             for (a = rt->gcDoubleArenaList.first; a; a = a->prev) {
12282:                 if (thing - ARENA_INFO_TO_START(a) <
12282:                     DOUBLES_PER_ARENA * sizeof(jsdouble)) {
12282:                     root_points_to_gcArenaList = JS_TRUE;
12282:                     break;
12282:                 }
    1:             }
    1:         }
    1:         if (!root_points_to_gcArenaList && rhe->name) {
    1:             fprintf(stderr,
    1: "JS API usage error: the address passed to JS_AddNamedRoot currently holds an\n"
    1: "invalid jsval.  This is usually caused by a missing call to JS_RemoveRoot.\n"
    1: "The root's name is \"%s\".\n",
    1:                     rhe->name);
    1:         }
    1:         JS_ASSERT(root_points_to_gcArenaList);
    1: #endif
  583:         JS_SET_TRACING_NAME(trc, rhe->name ? rhe->name : "root");
  771:         js_CallValueTracerIfGCThing(trc, v);
  583:     }
  583: 
  583:     return JS_DHASH_NEXT;
  583: }
  583: 
18907: static JSDHashOperator
  583: gc_lock_traversal(JSDHashTable *table, JSDHashEntryHdr *hdr, uint32 num,
  583:                   void *arg)
  583: {
  583:     JSGCLockHashEntry *lhe = (JSGCLockHashEntry *)hdr;
  583:     void *thing = (void *)lhe->thing;
  583:     JSTracer *trc = (JSTracer *)arg;
  583:     uint32 traceKind;
  583: 
  583:     JS_ASSERT(lhe->count >= 1);
 8005:     traceKind = js_GetGCThingTraceKind(thing);
  583:     JS_CALL_TRACER(trc, thing, traceKind, "locked object");
    1:     return JS_DHASH_NEXT;
    1: }
    1: 
  583: #define TRACE_JSVALS(trc, len, vec, name)                                     \
    1:     JS_BEGIN_MACRO                                                            \
    1:     jsval _v, *_vp, *_end;                                                    \
    1:                                                                               \
    1:         for (_vp = vec, _end = _vp + len; _vp < _end; _vp++) {                \
    1:             _v = *_vp;                                                        \
  583:             if (JSVAL_IS_TRACEABLE(_v)) {                                     \
  583:                 JS_SET_TRACING_INDEX(trc, name, _vp - (vec));                 \
  583:                 JS_CallTracer(trc, JSVAL_TO_TRACEABLE(_v),                    \
  583:                               JSVAL_TRACE_KIND(_v));                          \
  583:             }                                                                 \
    1:         }                                                                     \
    1:     JS_END_MACRO
    1: 
    1: void
  583: js_TraceStackFrame(JSTracer *trc, JSStackFrame *fp)
    1: {
13702:     uintN nslots, minargs, skip;
 4127: 
    1:     if (fp->callobj)
  786:         JS_CALL_OBJECT_TRACER(trc, fp->callobj, "call");
    1:     if (fp->argsobj)
  786:         JS_CALL_OBJECT_TRACER(trc, fp->argsobj, "arguments");
    1:     if (fp->varobj)
  786:         JS_CALL_OBJECT_TRACER(trc, fp->varobj, "variables");
    1:     if (fp->script) {
  583:         js_TraceScript(trc, fp->script);
16072:         if (fp->regs) {
    1:             /*
    1:              * Don't mark what has not been pushed yet, or what has been
    1:              * popped already.
    1:              */
16072:             nslots = (uintN) (fp->regs->sp - fp->slots);
16072:             TRACE_JSVALS(trc, nslots, fp->slots, "slot");
    1:         }
16072:     } else {
16072:         JS_ASSERT(!fp->slots);
16072:         JS_ASSERT(!fp->regs);
    1:     }
    1: 
    1:     /* Allow for primitive this parameter due to JSFUN_THISP_* flags. */
    1:     JS_ASSERT(JSVAL_IS_OBJECT((jsval)fp->thisp) ||
    1:               (fp->fun && JSFUN_THISP_FLAGS(fp->fun->flags)));
  583:     JS_CALL_VALUE_TRACER(trc, (jsval)fp->thisp, "this");
    1: 
 4127:     if (fp->callee)
 4127:         JS_CALL_OBJECT_TRACER(trc, fp->callee, "callee");
 4127: 
    1:     if (fp->argv) {
    1:         nslots = fp->argc;
 6040:         skip = 0;
    1:         if (fp->fun) {
13702:             minargs = FUN_MINARGS(fp->fun);
13702:             if (minargs > nslots)
13702:                 nslots = minargs;
13702:             if (!FUN_INTERPRETED(fp->fun)) {
13702:                 JS_ASSERT(!(fp->fun->flags & JSFUN_FAST_NATIVE));
    1:                 nslots += fp->fun->u.n.extra;
    1:             }
13702:             if (fp->fun->flags & JSFRAME_ROOTED_ARGV)
 6040:                 skip = 2 + fp->argc;
 4250:         }
 6040:         TRACE_JSVALS(trc, 2 + nslots - skip, fp->argv - 2 + skip, "operand");
    1:     }
17182: 
  583:     JS_CALL_VALUE_TRACER(trc, fp->rval, "rval");
  583:     if (fp->scopeChain)
  583:         JS_CALL_OBJECT_TRACER(trc, fp->scopeChain, "scope chain");
    1:     if (fp->sharpArray)
  583:         JS_CALL_OBJECT_TRACER(trc, fp->sharpArray, "sharp array");
    1: 
    1:     if (fp->xmlNamespace)
  583:         JS_CALL_OBJECT_TRACER(trc, fp->xmlNamespace, "xmlNamespace");
    1: }
    1: 
    1: static void
  583: TraceWeakRoots(JSTracer *trc, JSWeakRoots *wr)
    1: {
 8005:     uint32 i;
    1:     void *thing;
    1: 
 8005: #ifdef DEBUG
 8005:     static const char *weakRootNames[JSTRACE_LIMIT] = {
 8005:         "newborn object",
 8005:         "newborn double",
 8005:         "newborn string",
 8005:         "newborn xml"
 8005:     };
 8005: #endif
 8005: 
 8005:     for (i = 0; i != JSTRACE_LIMIT; i++) {
 8005:         thing = wr->newborn[i];
 8005:         if (thing)
 8005:             JS_CALL_TRACER(trc, thing, i, weakRootNames[i]);
 8005:     }
 8005:     JS_ASSERT(i == GCX_EXTERNAL_STRING);
 8005:     for (; i != GCX_NTYPES; ++i) {
  583:         thing = wr->newborn[i];
  771:         if (thing) {
 8005:             JS_SET_TRACING_INDEX(trc, "newborn external string",
 8005:                                  i - GCX_EXTERNAL_STRING);
 8005:             JS_CallTracer(trc, thing, JSTRACE_STRING);
  771:         }
  583:     }
 8005: 
 4529:     JS_CALL_VALUE_TRACER(trc, wr->lastAtom, "lastAtom");
  771:     JS_SET_TRACING_NAME(trc, "lastInternalResult");
  771:     js_CallValueTracerIfGCThing(trc, wr->lastInternalResult);
    1: }
  583: 
  583: JS_FRIEND_API(void)
  583: js_TraceContext(JSTracer *trc, JSContext *acx)
  583: {
 4521:     JSStackFrame *fp, *nextChain;
  583:     JSStackHeader *sh;
  583:     JSTempValueRooter *tvr;
  583: 
11145:     if (IS_GC_MARKING_TRACER(trc)) {
19196: 
19196: #define FREE_OLD_ARENAS(pool)                                                 \
19196:         JS_BEGIN_MACRO                                                        \
19196:             int64 _age;                                                       \
19196:             JSArena * _a = (pool).current;                                    \
19196:             if (_a == (pool).first.next &&                                    \
19196:                 _a->avail == _a->base + sizeof(int64)) {                      \
19196:                 _age = JS_Now() - *(int64 *) _a->base;                        \
19196:                 if (_age > (int64) acx->runtime->gcEmptyArenaPoolLifespan *   \
19196:                            1000)                                              \
19196:                     JS_FreeArenaPool(&(pool));                                \
19196:             }                                                                 \
19196:         JS_END_MACRO
19196: 
19699: #ifdef JS_THREADSAFE
19699:         js_RevokeGCLocalFreeLists(acx);
19699: #endif
19699: 
  583:         /*
19196:          * Release the stackPool's arenas if the stackPool has existed for
19196:          * longer than the limit specified by gcEmptyArenaPoolLifespan.
10985:          */
19196:         FREE_OLD_ARENAS(acx->stackPool);
19196: 
19196:         /*
19196:          * Release the regexpPool's arenas based on the same criterion as for
19196:          * the stackPool.
19196:          */
19196:         FREE_OLD_ARENAS(acx->regexpPool);
12282: 
12282:         /*
12282:          * Clear the double free list to release all the pre-allocated doubles.
12282:          */
12282:         acx->doubleFreeList = NULL;
11145:     }
10985: 
  583:     /*
 4521:      * Iterate frame chain and dormant chains.
  583:      *
  583:      * (NB: see comment on this whole "dormant" thing in js_Execute.)
  583:      */
 4521:     fp = acx->fp;
 4521:     nextChain = acx->dormantFrameChain;
 4521:     if (!fp)
 4521:         goto next_chain;
  583: 
 4521:     /* The top frame must not be dormant. */
 4521:     JS_ASSERT(!fp->dormantNext);
 4521:     for (;;) {
  583:         do {
  583:             js_TraceStackFrame(trc, fp);
  583:         } while ((fp = fp->down) != NULL);
 4521: 
 4521:       next_chain:
 4521:         if (!nextChain)
 4521:             break;
 4521:         fp = nextChain;
 4521:         nextChain = nextChain->dormantNext;
  583:     }
  583: 
  583:     /* Mark other roots-by-definition in acx. */
  583:     if (acx->globalObject)
  583:         JS_CALL_OBJECT_TRACER(trc, acx->globalObject, "global object");
  583:     TraceWeakRoots(trc, &acx->weakRoots);
  583:     if (acx->throwing) {
  583:         JS_CALL_VALUE_TRACER(trc, acx->exception, "exception");
  583:     } else {
  583:         /* Avoid keeping GC-ed junk stored in JSContext.exception. */
  583:         acx->exception = JSVAL_NULL;
  583:     }
  583: #if JS_HAS_LVALUE_RETURN
  583:     if (acx->rval2set)
  583:         JS_CALL_VALUE_TRACER(trc, acx->rval2, "rval2");
  583: #endif
  583: 
  583:     for (sh = acx->stackHeaders; sh; sh = sh->down) {
  583:         METER(trc->context->runtime->gcStats.stackseg++);
  583:         METER(trc->context->runtime->gcStats.segslots += sh->nslots);
  583:         TRACE_JSVALS(trc, sh->nslots, JS_STACK_SEGMENT(sh), "stack");
  583:     }
  583: 
  583:     if (acx->localRootStack)
  583:         js_TraceLocalRoots(trc, acx->localRootStack);
  583: 
  583:     for (tvr = acx->tempValueRooters; tvr; tvr = tvr->down) {
  583:         switch (tvr->count) {
  583:           case JSTVU_SINGLE:
  583:             JS_SET_TRACING_NAME(trc, "tvr->u.value");
  771:             js_CallValueTracerIfGCThing(trc, tvr->u.value);
  583:             break;
  583:           case JSTVU_TRACE:
  583:             tvr->u.trace(trc, tvr);
  583:             break;
  583:           case JSTVU_SPROP:
  583:             TRACE_SCOPE_PROPERTY(trc, tvr->u.sprop);
  583:             break;
  583:           case JSTVU_WEAK_ROOTS:
  583:             TraceWeakRoots(trc, tvr->u.weakRoots);
  583:             break;
 3235:           case JSTVU_PARSE_CONTEXT:
 3235:             js_TraceParseContext(trc, tvr->u.parseContext);
 3235:             break;
 7359:           case JSTVU_SCRIPT:
 7359:             js_TraceScript(trc, tvr->u.script);
 7359:             break;
  583:           default:
  583:             JS_ASSERT(tvr->count >= 0);
  583:             TRACE_JSVALS(trc, tvr->count, tvr->u.array, "tvr->u.array");
  583:         }
  583:     }
  583: 
  583:     if (acx->sharpObjectMap.depth > 0)
  583:         js_TraceSharpMap(trc, &acx->sharpObjectMap);
  583: }
  583: 
  786: void
17212: js_TraceTraceMonitor(JSTracer *trc, JSTraceMonitor *tm)
17212: {
21442:     if (IS_GC_MARKING_TRACER(trc)) {
19987:         tm->recoveryDoublePoolPtr = tm->recoveryDoublePool;
21442:         /* Make sure the global shape changes and will force a flush
21442:            of the code cache. */
21442:         tm->globalShape = -1; 
21442:     }
17212: }
17212: 
17212: void
  786: js_TraceRuntime(JSTracer *trc, JSBool allAtoms)
  583: {
  583:     JSRuntime *rt = trc->context->runtime;
  583:     JSContext *iter, *acx;
  583: 
  583:     JS_DHashTableEnumerate(&rt->gcRootsHash, gc_root_traversal, trc);
  583:     if (rt->gcLocksHash)
  583:         JS_DHashTableEnumerate(rt->gcLocksHash, gc_lock_traversal, trc);
 4529:     js_TraceAtomState(trc, allAtoms);
15613:     js_TraceNativeEnumerators(trc);
12282:     js_TraceRuntimeNumberState(trc);
  583: 
  583:     iter = NULL;
  583:     while ((acx = js_ContextIterator(rt, JS_TRUE, &iter)) != NULL)
  583:         js_TraceContext(trc, acx);
  958: 
  958:     if (rt->gcExtraRootsTraceOp)
  958:         rt->gcExtraRootsTraceOp(trc, rt->gcExtraRootsData);
17185: 
17187: #ifdef JS_THREADSAFE
17187:     /* Trace the loop table(s) which can contain pointers to code objects. */
17187:    while ((acx = js_ContextIterator(rt, JS_FALSE, &iter)) != NULL) {
17198:        if (!acx->thread)
17187:            continue;
17212:        js_TraceTraceMonitor(trc, &acx->thread->traceMonitor);
17187:    }
17187: #else
17212:    js_TraceTraceMonitor(trc, &rt->traceMonitor);
17186: #endif
  583: }
  583: 
11041: static void
11041: ProcessSetSlotRequest(JSContext *cx, JSSetSlotRequest *ssr)
11041: {
11041:     JSObject *obj, *pobj;
11041:     uint32 slot;
11041: 
11041:     obj = ssr->obj;
11041:     pobj = ssr->pobj;
11041:     slot = ssr->slot;
11041: 
11041:     while (pobj) {
12674:         pobj = js_GetWrappedObject(cx, pobj);
11041:         if (pobj == obj) {
11041:             ssr->errnum = JSMSG_CYCLIC_VALUE;
11041:             return;
11041:         }
11041:         pobj = JSVAL_TO_OBJECT(STOBJ_GET_SLOT(pobj, slot));
11041:     }
11041: 
11041:     pobj = ssr->pobj;
11041: 
11041:     if (slot == JSSLOT_PROTO && OBJ_IS_NATIVE(obj)) {
11041:         JSScope *scope, *newscope;
11041:         JSObject *oldproto;
11041: 
11041:         /* Check to see whether obj shares its prototype's scope. */
11041:         scope = OBJ_SCOPE(obj);
11041:         oldproto = STOBJ_GET_PROTO(obj);
11041:         if (oldproto && OBJ_SCOPE(oldproto) == scope) {
11041:             /* Either obj needs a new empty scope, or it should share pobj's. */
11041:             if (!pobj ||
11041:                 !OBJ_IS_NATIVE(pobj) ||
11041:                 OBJ_GET_CLASS(cx, pobj) != STOBJ_GET_CLASS(oldproto)) {
    1:                 /*
11041:                  * With no proto and no scope of its own, obj is truly empty.
11041:                  *
11041:                  * If pobj is not native, obj needs its own empty scope -- it
11041:                  * should not continue to share oldproto's scope once oldproto
11041:                  * is not on obj's prototype chain.  That would put properties
11041:                  * from oldproto's scope ahead of properties defined by pobj,
11041:                  * in lookup order.
11041:                  *
11041:                  * If pobj's class differs from oldproto's, we may need a new
11041:                  * scope to handle differences in private and reserved slots,
11041:                  * so we suboptimally but safely make one.
11041:                  */
11041:                 if (!js_GetMutableScope(cx, obj)) {
11041:                     ssr->errnum = JSMSG_OUT_OF_MEMORY;
11041:                     return;
11041:                 }
11041:             } else if (OBJ_SCOPE(pobj) != scope) {
11041:                 newscope = (JSScope *) js_HoldObjectMap(cx, pobj->map);
11041:                 obj->map = &newscope->map;
11041:                 js_DropObjectMap(cx, &scope->map, obj);
11041:                 JS_TRANSFER_SCOPE_LOCK(cx, scope, newscope);
11041:             }
11041:         }
11041: 
11041:         /*
11377:          * Regenerate property cache shape ids for all of the scopes along the
11041:          * old prototype chain, in case any property cache entries were filled
11041:          * by looking up starting from obj.
11041:          */
11041:         while (oldproto && OBJ_IS_NATIVE(oldproto)) {
11041:             scope = OBJ_SCOPE(oldproto);
12307:             SCOPE_MAKE_UNIQUE_SHAPE(cx, scope);
11041:             oldproto = STOBJ_GET_PROTO(scope->object);
11041:         }
11041:     }
11041: 
11041:     /* Finally, do the deed. */
11041:     STOBJ_SET_SLOT(obj, slot, OBJECT_TO_JSVAL(pobj));
11041: }
11041: 
18285: static void
18285: DestroyScriptsToGC(JSContext *cx, JSScript **listp)
18285: {
18285:     JSScript *script;
18285: 
18285:     while ((script = *listp) != NULL) {
18285:         *listp = script->u.nextToGC;
18285:         script->u.nextToGC = NULL;
18285:         js_DestroyScript(cx, script);
18285:     }
18285: }
18285: 
11041: /*
11041:  * The gckind flag bit GC_LOCK_HELD indicates a call from js_NewGCThing with
11041:  * rt->gcLock already held, so the lock should be kept on return.
    1:  */
    1: void
    1: js_GC(JSContext *cx, JSGCInvocationKind gckind)
    1: {
    1:     JSRuntime *rt;
    1:     JSBool keepAtoms;
11022:     JSGCCallback callback;
    1:     uintN i, type;
  583:     JSTracer trc;
 5917:     uint32 thingSize, indexLimit;
 6117:     JSGCArenaInfo *a, **ap, *emptyArenas;
 5917:     uint8 flags, *flagp;
    1:     JSGCThing *thing, *freeList;
    1:     JSGCArenaList *arenaList;
    1:     JSBool allClear;
    1: #ifdef JS_THREADSAFE
    1:     uint32 requestDebit;
  583:     JSContext *acx, *iter;
    1: #endif
10954: #ifdef JS_GCMETER
12282:     uint32 nlivearenas, nkilledarenas, nthings;
10954: #endif
    1: 
19987:     JS_ASSERT_IF(gckind == GC_LAST_DITCH, !JS_ON_TRACE(cx));
    1:     rt = cx->runtime;
    1: #ifdef JS_THREADSAFE
    1:     /* Avoid deadlock. */
    1:     JS_ASSERT(!JS_IS_RUNTIME_LOCKED(rt));
    1: #endif
    1: 
11041:     if (gckind & GC_KEEP_ATOMS) {
11041:         /*
11041:          * The set slot request and last ditch GC kinds preserve all atoms and
11041:          * weak roots.
11041:          */
    1:         keepAtoms = JS_TRUE;
    1:     } else {
    1:         /* Keep atoms when a suspended compile is running on another context. */
    1:         keepAtoms = (rt->gcKeepAtoms != 0);
 6038:         JS_CLEAR_WEAK_ROOTS(&cx->weakRoots);
    1:     }
    1: 
    1:     /*
    1:      * Don't collect garbage if the runtime isn't up, and cx is not the last
    1:      * context in the runtime.  The last context must force a GC, and nothing
    1:      * should suppress that final collection or there may be shutdown leaks,
    1:      * or runtime bloat until the next context is created.
    1:      */
    1:     if (rt->state != JSRTS_UP && gckind != GC_LAST_CONTEXT)
    1:         return;
    1: 
11057:   restart_at_beginning:
    1:     /*
    1:      * Let the API user decide to defer a GC if it wants to (unless this
11022:      * is the last context).  Invoke the callback regardless. Sample the
11022:      * callback in case we are freely racing with a JS_SetGCCallback{,RT} on
11022:      * another thread.
    1:      */
11057:     if (gckind != GC_SET_SLOT_REQUEST && (callback = rt->gcCallback)) {
11022:         JSBool ok;
11022: 
11041:         if (gckind & GC_LOCK_HELD)
11022:             JS_UNLOCK_GC(rt);
11022:         ok = callback(cx, JSGC_BEGIN);
11041:         if (gckind & GC_LOCK_HELD)
11022:             JS_LOCK_GC(rt);
15499:         if (!ok && gckind != GC_LAST_CONTEXT) {
15499:             /*
15499:              * It's possible that we've looped back to this code from the 'goto
15499:              * restart_at_beginning' below in the GC_SET_SLOT_REQUEST code and
15499:              * that rt->gcLevel is now 0. Don't return without notifying!
15499:              */
15499:             if (rt->gcLevel == 0 && (gckind & GC_LOCK_HELD))
15499:                 JS_NOTIFY_GC_DONE(rt);
    1:             return;
    1:         }
15499:     }
    1: 
    1:     /* Lock out other GC allocator and collector invocations. */
11041:     if (!(gckind & GC_LOCK_HELD))
    1:         JS_LOCK_GC(rt);
    1: 
    1:     METER(rt->gcStats.poke++);
    1:     rt->gcPoke = JS_FALSE;
    1: 
    1: #ifdef JS_THREADSAFE
    1:     JS_ASSERT(cx->thread->id == js_CurrentThreadId());
    1: 
    1:     /* Bump gcLevel and return rather than nest on this thread. */
    1:     if (rt->gcThread == cx->thread) {
    1:         JS_ASSERT(rt->gcLevel > 0);
    1:         rt->gcLevel++;
10954:         METER_UPDATE_MAX(rt->gcStats.maxlevel, rt->gcLevel);
11041:         if (!(gckind & GC_LOCK_HELD))
    1:             JS_UNLOCK_GC(rt);
    1:         return;
    1:     }
    1: 
    1:     /*
    1:      * If we're in one or more requests (possibly on more than one context)
    1:      * running on the current thread, indicate, temporarily, that all these
    1:      * requests are inactive.  If cx->thread is NULL, then cx is not using
    1:      * the request model, and does not contribute to rt->requestCount.
    1:      */
    1:     requestDebit = 0;
    1:     if (cx->thread) {
    1:         JSCList *head, *link;
    1: 
    1:         /*
    1:          * Check all contexts on cx->thread->contextList for active requests,
    1:          * counting each such context against requestDebit.
    1:          */
    1:         head = &cx->thread->contextList;
    1:         for (link = head->next; link != head; link = link->next) {
    1:             acx = CX_FROM_THREAD_LINKS(link);
    1:             JS_ASSERT(acx->thread == cx->thread);
    1:             if (acx->requestDepth)
    1:                 requestDebit++;
    1:         }
    1:     } else {
    1:         /*
    1:          * We assert, but check anyway, in case someone is misusing the API.
    1:          * Avoiding the loop over all of rt's contexts is a win in the event
    1:          * that the GC runs only on request-less contexts with null threads,
    1:          * in a special thread such as might be used by the UI/DOM/Layout
    1:          * "mozilla" or "main" thread in Mozilla-the-browser.
    1:          */
    1:         JS_ASSERT(cx->requestDepth == 0);
    1:         if (cx->requestDepth)
    1:             requestDebit = 1;
    1:     }
    1:     if (requestDebit) {
    1:         JS_ASSERT(requestDebit <= rt->requestCount);
    1:         rt->requestCount -= requestDebit;
    1:         if (rt->requestCount == 0)
    1:             JS_NOTIFY_REQUEST_DONE(rt);
    1:     }
    1: 
    1:     /* If another thread is already in GC, don't attempt GC; wait instead. */
    1:     if (rt->gcLevel > 0) {
    1:         /* Bump gcLevel to restart the current GC, so it finds new garbage. */
    1:         rt->gcLevel++;
10954:         METER_UPDATE_MAX(rt->gcStats.maxlevel, rt->gcLevel);
    1: 
    1:         /* Wait for the other thread to finish, then resume our request. */
    1:         while (rt->gcLevel > 0)
    1:             JS_AWAIT_GC_DONE(rt);
    1:         if (requestDebit)
    1:             rt->requestCount += requestDebit;
11041:         if (!(gckind & GC_LOCK_HELD))
    1:             JS_UNLOCK_GC(rt);
    1:         return;
    1:     }
    1: 
    1:     /* No other thread is in GC, so indicate that we're now in GC. */
    1:     rt->gcLevel = 1;
    1:     rt->gcThread = cx->thread;
    1: 
    1:     /* Wait for all other requests to finish. */
    1:     while (rt->requestCount > 0)
    1:         JS_AWAIT_REQUEST_DONE(rt);
    1: 
    1: #else  /* !JS_THREADSAFE */
    1: 
    1:     /* Bump gcLevel and return rather than nest; the outer gc will restart. */
    1:     rt->gcLevel++;
10954:     METER_UPDATE_MAX(rt->gcStats.maxlevel, rt->gcLevel);
    1:     if (rt->gcLevel > 1)
    1:         return;
    1: 
    1: #endif /* !JS_THREADSAFE */
    1: 
    1:     /*
    1:      * Set rt->gcRunning here within the GC lock, and after waiting for any
    1:      * active requests to end, so that new requests that try to JS_AddRoot,
    1:      * JS_RemoveRoot, or JS_RemoveRootRT block in JS_BeginRequest waiting for
    1:      * rt->gcLevel to drop to zero, while request-less calls to the *Root*
    1:      * APIs block in js_AddRoot or js_RemoveRoot (see above in this file),
    1:      * waiting for GC to finish.
    1:      */
    1:     rt->gcRunning = JS_TRUE;
11041: 
11041:     if (gckind == GC_SET_SLOT_REQUEST) {
11041:         JSSetSlotRequest *ssr;
11041: 
11041:         while ((ssr = rt->setSlotRequests) != NULL) {
11041:             rt->setSlotRequests = ssr->next;
11041:             JS_UNLOCK_GC(rt);
11041:             ssr->next = NULL;
11041:             ProcessSetSlotRequest(cx, ssr);
11041:             JS_LOCK_GC(rt);
11041:         }
11041: 
11041:         /*
11041:          * We assume here that killing links to parent and prototype objects
11041:          * does not create garbage (such objects typically are long-lived and
11041:          * widely shared, e.g. global objects, Function.prototype, etc.). We
11041:          * collect garbage only if a racing thread attempted GC and is waiting
11041:          * for us to finish (gcLevel > 1) or if someone already poked us.
11041:          */
11041:         if (rt->gcLevel == 1 && !rt->gcPoke)
11041:             goto done_running;
11689: 
11623:         rt->gcLevel = 0;
11041:         rt->gcPoke = JS_FALSE;
11623:         rt->gcRunning = JS_FALSE;
11623: #ifdef JS_THREADSAFE
11623:         rt->gcThread = NULL;
11623:         rt->requestCount += requestDebit;
11623: #endif
11057:         gckind = GC_LOCK_HELD;
11057:         goto restart_at_beginning;
11041:     }
11041: 
    1:     JS_UNLOCK_GC(rt);
    1: 
19575: #ifdef JS_TRACER
19575:     if (JS_ON_TRACE(cx))
19575:         goto out;
19575: #endif
19575: 
    1:     /* Reset malloc counter. */
    1:     rt->gcMallocBytes = 0;
    1: 
10217: #ifdef JS_DUMP_SCOPE_METERS
    1:   { extern void js_DumpScopeMeters(JSRuntime *rt);
    1:     js_DumpScopeMeters(rt);
    1:   }
    1: #endif
    1: 
21442:     /* Clear property and JIT oracle caches (only for cx->thread if JS_THREADSAFE). */
11041:     js_FlushPropertyCache(cx);
18092: #ifdef JS_TRACER
18277:     js_FlushJITOracle(cx);
18092: #endif
10572: 
18285:     /* Destroy eval'ed scripts. */
18285:     DestroyScriptsToGC(cx, &JS_SCRIPTS_TO_GC(cx));
18285: 
    1: #ifdef JS_THREADSAFE
    1:     /*
19699:      * Clear thread-based caches. To avoid redundant clearing we unroll the
15500:      * current thread's step.
15500:      *
19699:      * In case a JSScript wrapped within an object was finalized, we null
19699:      * acx->thread->gsnCache.script and finish the cache's hashtable. Note
19699:      * that js_DestroyScript, called from script_finalize, will have already
19699:      * cleared cx->thread->gsnCache above during finalization, so we don't
19699:      * have to here.
    1:      */
    1:     iter = NULL;
    1:     while ((acx = js_ContextIterator(rt, JS_FALSE, &iter)) != NULL) {
    1:         if (!acx->thread || acx->thread == cx->thread)
    1:             continue;
    1:         GSN_CACHE_CLEAR(&acx->thread->gsnCache);
11041:         js_FlushPropertyCache(acx);
18098: #ifdef JS_TRACER
21442:         js_FlushJITOracle(acx);
18098: #endif
18285:         DestroyScriptsToGC(cx, &acx->thread->scriptsToGC);
    1:     }
    1: #else
    1:     /* The thread-unsafe case just has to clear the runtime's GSN cache. */
    1:     GSN_CACHE_CLEAR(&rt->gsnCache);
    1: #endif
    1: 
    1:   restart:
    1:     rt->gcNumber++;
 5917:     JS_ASSERT(!rt->gcUntracedArenaStackTop);
 5917:     JS_ASSERT(rt->gcTraceLaterCount == 0);
    1: 
11041:     /* Reset the property cache's type id generator so we can compress ids. */
11377:     rt->shapeGen = 0;
    1: 
    1:     /*
    1:      * Mark phase.
    1:      */
  583:     JS_TRACER_INIT(&trc, cx, NULL);
  583:     rt->gcMarkingTracer = &trc;
  583:     JS_ASSERT(IS_GC_MARKING_TRACER(&trc));
12282: 
12282:     for (a = rt->gcDoubleArenaList.first; a; a = a->prev)
12282:         a->u.hasMarkedDoubles = JS_FALSE;
12282: 
  786:     js_TraceRuntime(&trc, keepAtoms);
    1:     js_MarkScriptFilenames(rt, keepAtoms);
  583: 
    1:     /*
  583:      * Mark children of things that caused too deep recursion during the above
  583:      * tracing.
    1:      */
 5917:     TraceDelayedChildren(&trc);
    1: 
    1:     JS_ASSERT(!cx->insideGCMarkCallback);
    1:     if (rt->gcCallback) {
    1:         cx->insideGCMarkCallback = JS_TRUE;
    1:         (void) rt->gcCallback(cx, JSGC_MARK_END);
    1:         JS_ASSERT(cx->insideGCMarkCallback);
    1:         cx->insideGCMarkCallback = JS_FALSE;
    1:     }
 5917:     JS_ASSERT(rt->gcTraceLaterCount == 0);
    1: 
  583:     rt->gcMarkingTracer = NULL;
  583: 
    1:     /*
    1:      * Sweep phase.
    1:      *
    1:      * Finalize as we sweep, outside of rt->gcLock but with rt->gcRunning set
    1:      * so that any attempt to allocate a GC-thing from a finalizer will fail,
    1:      * rather than nest badly and leave the unmarked newborn to be swept.
    1:      *
 4529:      * We first sweep atom state so we can use js_IsAboutToBeFinalized on
 4529:      * JSString or jsdouble held in a hashtable to check if the hashtable
 4529:      * entry can be freed. Note that even after the entry is freed, JSObject
 4529:      * finalizers can continue to access the corresponding jsdouble* and
 4529:      * JSString* assuming that they are unique. This works since the
 4529:      * atomization API must not be called during GC.
 4529:      */
 4529:     js_SweepAtomState(cx);
 4529: 
 5816:     /* Finalize iterator states before the objects they iterate over. */
 5816:     CloseNativeIterators(cx);
 5816: 
 5816:     /* Finalize watch points associated with unreachable objects. */
 5816:     js_SweepWatchPoints(cx);
 5816: 
10217: #ifdef DEBUG
10217:     /* Save the pre-sweep count of scope-mapped properties. */
10217:     rt->liveScopePropsPreSweep = rt->liveScopeProps;
10217: #endif
10217: 
 4529:     /*
  254:      * Here we need to ensure that JSObject instances are finalized before GC-
 4529:      * allocated JSString and jsdouble instances so object's finalizer can
 4529:      * access them even if they will be freed. For that we simply finalize the
 4529:      * list containing JSObject first since the static assert at the beginning
 4529:      * of the file guarantees that JSString and jsdouble instances are
 4529:      * allocated from a different list.
    1:      */
 6117:     emptyArenas = NULL;
    1:     for (i = 0; i < GC_NUM_FREELISTS; i++) {
    1:         arenaList = &rt->gcArenaList[i == 0
    1:                                      ? GC_FREELIST_INDEX(sizeof(JSObject))
    1:                                      : i == GC_FREELIST_INDEX(sizeof(JSObject))
    1:                                      ? 0
    1:                                      : i];
 6117:         ap = &arenaList->last;
 6117:         if (!(a = *ap))
 5917:             continue;
 5917: 
 6117:         JS_ASSERT(arenaList->lastCount > 0);
 6117:         arenaList->freeList = NULL;
 6117:         freeList = NULL;
 5917:         thingSize = arenaList->thingSize;
 5917:         indexLimit = THINGS_PER_ARENA(thingSize);
 5917:         flagp = THING_FLAGP(a, arenaList->lastCount - 1);
12282:         METER((nlivearenas = 0, nkilledarenas = 0, nthings = 0));
 5917:         for (;;) {
 5917:             JS_ASSERT(a->prevUntracedPage == 0);
12282:             JS_ASSERT(a->u.untracedThings == 0);
 6117:             allClear = JS_TRUE;
 5917:             do {
    1:                 flags = *flagp;
 6117:                 if (flags & (GCF_MARK | GCF_LOCK)) {
    1:                     *flagp &= ~GCF_MARK;
 6117:                     allClear = JS_FALSE;
12282:                     METER(nthings++);
10572:                 } else {
 6155:                     thing = FLAGP_TO_THING(flagp, thingSize);
10572:                     if (!(flags & GCF_FINAL)) {
10572:                         /*
10572:                          * Call the finalizer with GCF_FINAL ORed into flags.
10572:                          */
    1:                         *flagp = (uint8)(flags | GCF_FINAL);
 6117:                         type = flags & GCF_TYPEMASK;
 6117:                         switch (type) {
 6117:                           case GCX_OBJECT:
 6117:                             js_FinalizeObject(cx, (JSObject *) thing);
 6117:                             break;
 6117:                           case GCX_DOUBLE:
 6117:                             /* Do nothing. */
 6117:                             break;
 6117: #if JS_HAS_XML_SUPPORT
 6117:                           case GCX_XML:
 6117:                             js_FinalizeXML(cx, (JSXML *) thing);
 6117:                             break;
 6117: #endif
 6117:                           default:
 6117:                             JS_ASSERT(type == GCX_STRING ||
 6117:                                       type - GCX_EXTERNAL_STRING <
 6117:                                       GCX_NTYPES - GCX_EXTERNAL_STRING);
 8005:                             js_FinalizeStringRT(rt, (JSString *) thing,
10572:                                                 (intN) (type -
10572:                                                         GCX_EXTERNAL_STRING),
 8005:                                                 cx);
 6117:                             break;
    1:                         }
13705: #ifdef DEBUG
13705:                         memset(thing, JS_FREE_PATTERN, thingSize);
13705: #endif
    1:                     }
 6117:                     thing->flagp = flagp;
 6117:                     thing->next = freeList;
 6117:                     freeList = thing;
    1:                 }
 5917:             } while (++flagp != THING_FLAGS_END(a));
 6117: 
 6117:             if (allClear) {
 6117:                 /*
 6117:                  * Forget just assembled free list head for the arena and
 6117:                  * add the arena itself to the destroy list.
 6117:                  */
 6117:                 freeList = arenaList->freeList;
 6117:                 if (a == arenaList->last)
12689:                     arenaList->lastCount = (uint16) indexLimit;
 6117:                 *ap = a->prev;
 6117:                 a->prev = emptyArenas;
 6117:                 emptyArenas = a;
12282:                 METER(nkilledarenas++);
 6117:             } else {
 6117:                 arenaList->freeList = freeList;
 6117:                 ap = &a->prev;
12282:                 METER(nlivearenas++);
    1:             }
 6117:             if (!(a = *ap))
 5917:                 break;
 5917:             flagp = THING_FLAGP(a, indexLimit - 1);
    1:         }
12282: 
12282:         /*
12282:          * We use arenaList - &rt->gcArenaList[0], not i, as the stat index
12282:          * due to the enumeration reorder at the beginning of the loop.
12282:          */
12282:         METER(UpdateArenaStats(&rt->gcStats.arenaStats[arenaList -
12282:                                                        &rt->gcArenaList[0]],
12282:                                nlivearenas, nkilledarenas, nthings));
    1:     }
    1: 
19699: #ifdef JS_THREADSAFE
19699:     /*
19699:      * Release all but two free list sets to avoid allocating a new set in
19699:      * js_NewGCThing.
19699:      */
19699:     TrimGCFreeListsPool(rt, 2);
19699: #endif
19699: 
12282:     ap = &rt->gcDoubleArenaList.first;
12282:     METER((nlivearenas = 0, nkilledarenas = 0, nthings = 0));
12282:     while ((a = *ap) != NULL) {
12282:         if (!a->u.hasMarkedDoubles) {
12282:             /* No marked double values in the arena. */
12282:             *ap = a->prev;
12282:             a->prev = emptyArenas;
12282:             emptyArenas = a;
12282:             METER(nkilledarenas++);
12282:         } else {
12282:             ap = &a->prev;
12282: #ifdef JS_GCMETER
12282:             for (i = 0; i != DOUBLES_PER_ARENA; ++i) {
12282:                 if (IsMarkedDouble(a, index))
12282:                     METER(nthings++);
12282:             }
12282:             METER(nlivearenas++);
12282: #endif
12282:         }
12282:     }
12282:     METER(UpdateArenaStats(&rt->gcStats.doubleArenaStats,
12282:                            nlivearenas, nkilledarenas, nthings));
12282:     rt->gcDoubleArenaList.nextDoubleFlags =
12282:         rt->gcDoubleArenaList.first
12282:         ? DOUBLE_ARENA_BITMAP(rt->gcDoubleArenaList.first)
12282:         : DOUBLE_BITMAP_SENTINEL;
12282: 
    1:     /*
    1:      * Sweep the runtime's property tree after finalizing objects, in case any
 4529:      * had watchpoints referencing tree nodes.
    1:      */
    1:     js_SweepScopeProperties(cx);
    1: 
    1:     /*
    1:      * Sweep script filenames after sweeping functions in the generic loop
    1:      * above. In this way when a scripted function's finalizer destroys the
    1:      * script and calls rt->destroyScriptHook, the hook can still access the
    1:      * script's filename. See bug 323267.
    1:      */
    1:     js_SweepScriptFilenames(rt);
    1: 
    1:     /*
 6117:      * Destroy arenas after we finished the sweeping sofinalizers can safely
 6117:      * use js_IsAboutToBeFinalized().
    1:      */
12282:     DestroyGCArenas(rt, emptyArenas);
    1: 
    1:     if (rt->gcCallback)
    1:         (void) rt->gcCallback(cx, JSGC_FINALIZE_END);
    1: #ifdef DEBUG_srcnotesize
    1:   { extern void DumpSrcNoteSizeHist();
    1:     DumpSrcNoteSizeHist();
    1:     printf("GC HEAP SIZE %lu\n", (unsigned long)rt->gcBytes);
    1:   }
    1: #endif
    1: 
10217: #ifdef JS_SCOPE_DEPTH_METER
10217:   { static FILE *fp;
10217:     if (!fp)
10217:         fp = fopen("/tmp/scopedepth.stats", "w");
10217: 
10217:     if (fp) {
10217:         JS_DumpBasicStats(&rt->protoLookupDepthStats, "proto-lookup depth", fp);
10217:         JS_DumpBasicStats(&rt->scopeSearchDepthStats, "scope-search depth", fp);
10217:         JS_DumpBasicStats(&rt->hostenvScopeDepthStats, "hostenv scope depth", fp);
10217:         JS_DumpBasicStats(&rt->lexicalScopeDepthStats, "lexical scope depth", fp);
10217: 
10217:         putc('\n', fp);
10217:         fflush(fp);
10217:     }
10217:   }
10217: #endif /* JS_SCOPE_DEPTH_METER */
10217: 
17182: #ifdef JS_DUMP_LOOP_STATS
17182:   { static FILE *lsfp;
17182:     if (!lsfp)
17182:         lsfp = fopen("/tmp/loopstats", "w");
17182:     if (lsfp) {
17182:         JS_DumpBasicStats(&rt->loopStats, "loops", lsfp);
17182:         fflush(lsfp);
17182:     }
17182:   }
17182: #endif /* JS_DUMP_LOOP_STATS */
17182: 
19575: #ifdef JS_TRACER
19575: out:
19575: #endif
    1:     JS_LOCK_GC(rt);
    1: 
    1:     /*
    1:      * We want to restart GC if js_GC was called recursively or if any of the
    1:      * finalizers called js_RemoveRoot or js_UnlockGCThingRT.
    1:      */
    1:     if (rt->gcLevel > 1 || rt->gcPoke) {
    1:         rt->gcLevel = 1;
    1:         rt->gcPoke = JS_FALSE;
    1:         JS_UNLOCK_GC(rt);
    1:         goto restart;
    1:     }
11041: 
21744:     if (rt->shapeGen >= SHAPE_OVERFLOW_BIT - 1) {
15505:         /*
15505:          * FIXME bug 440834: The shape id space has overflowed. Currently we
15505:          * cope badly with this. Every call to js_GenerateShape does GC, and
15505:          * we never re-enable the property cache.
15505:          */
15505:         js_DisablePropertyCache(cx);
11377: #ifdef JS_THREADSAFE
11377:         iter = NULL;
11377:         while ((acx = js_ContextIterator(rt, JS_FALSE, &iter)) != NULL) {
11377:             if (!acx->thread || acx->thread == cx->thread)
11377:                 continue;
15505:             js_DisablePropertyCache(acx);
11377:         }
11041: #endif
11377:     }
11041: 
11041:     rt->gcLastBytes = rt->gcBytes;
11041:   done_running:
    1:     rt->gcLevel = 0;
    1:     rt->gcRunning = JS_FALSE;
    1: 
    1: #ifdef JS_THREADSAFE
    1:     /* If we were invoked during a request, pay back the temporary debit. */
    1:     if (requestDebit)
    1:         rt->requestCount += requestDebit;
    1:     rt->gcThread = NULL;
    1:     JS_NOTIFY_GC_DONE(rt);
    1: 
    1:     /*
11041:      * Unlock unless we have GC_LOCK_HELD which requires locked GC on return.
    1:      */
11041:     if (!(gckind & GC_LOCK_HELD))
    1:         JS_UNLOCK_GC(rt);
    1: #endif
    1: 
11022:     /*
11022:      * Execute JSGC_END callback outside the lock. Again, sample the callback
11022:      * pointer in case it changes, since we are outside of the GC vs. requests
11022:      * interlock mechanism here.
11022:      */
11057:     if (gckind != GC_SET_SLOT_REQUEST && (callback = rt->gcCallback)) {
    1:         JSWeakRoots savedWeakRoots;
    1:         JSTempValueRooter tvr;
    1: 
11041:         if (gckind & GC_KEEP_ATOMS) {
    1:             /*
    1:              * We allow JSGC_END implementation to force a full GC or allocate
11041:              * new GC things. Thus we must protect the weak roots from garbage
11041:              * collection and overwrites.
    1:              */
    1:             savedWeakRoots = cx->weakRoots;
    1:             JS_PUSH_TEMP_ROOT_WEAK_COPY(cx, &savedWeakRoots, &tvr);
    1:             JS_KEEP_ATOMS(rt);
    1:             JS_UNLOCK_GC(rt);
    1:         }
    1: 
11022:         (void) callback(cx, JSGC_END);
    1: 
11041:         if (gckind & GC_KEEP_ATOMS) {
    1:             JS_LOCK_GC(rt);
    1:             JS_UNKEEP_ATOMS(rt);
    1:             JS_POP_TEMP_ROOT(cx, &tvr);
    1:         } else if (gckind == GC_LAST_CONTEXT && rt->gcPoke) {
    1:             /*
    1:              * On shutdown iterate until JSGC_END callback stops creating
    1:              * garbage.
    1:              */
11057:             goto restart_at_beginning;
    1:         }
    1:     }
    1: }
    1: 
    1: void
    1: js_UpdateMallocCounter(JSContext *cx, size_t nbytes)
    1: {
    1:     uint32 *pbytes, bytes;
    1: 
    1: #ifdef JS_THREADSAFE
    1:     pbytes = &cx->thread->gcMallocBytes;
    1: #else
    1:     pbytes = &cx->runtime->gcMallocBytes;
    1: #endif
    1:     bytes = *pbytes;
    1:     *pbytes = ((uint32)-1 - bytes <= nbytes) ? (uint32)-1 : bytes + nbytes;
    1: }
