22705: /* -*- Mode: C; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*-
17487:  * vim: set ts=8 sw=4 et tw=99 ft=cpp:
17186:  *
17186:  * ***** BEGIN LICENSE BLOCK *****
17186:  * Version: MPL 1.1/GPL 2.0/LGPL 2.1
17186:  *
17186:  * The contents of this file are subject to the Mozilla Public License Version
17186:  * 1.1 (the "License"); you may not use this file except in compliance with
17186:  * the License. You may obtain a copy of the License at
17186:  * http://www.mozilla.org/MPL/
17186:  *
17186:  * Software distributed under the License is distributed on an "AS IS" basis,
17186:  * WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
17186:  * for the specific language governing rights and limitations under the
17186:  * License.
17186:  *
17186:  * The Original Code is Mozilla SpiderMonkey JavaScript 1.9 code, released
17186:  * May 28, 2008.
17186:  *
17186:  * The Initial Developer of the Original Code is
17339:  *   Brendan Eich <brendan@mozilla.org>
17186:  *
17186:  * Contributor(s):
17339:  *   Andreas Gal <gal@mozilla.com>
17671:  *   Mike Shaver <shaver@mozilla.org>
17671:  *   David Anderson <danderson@mozilla.com>
17186:  *
17186:  * Alternatively, the contents of this file may be used under the terms of
17186:  * either of the GNU General Public License Version 2 or later (the "GPL"),
17186:  * or the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
17186:  * in which case the provisions of the GPL or the LGPL are applicable instead
17186:  * of those above. If you wish to allow use of your version of this file only
17186:  * under the terms of either the GPL or the LGPL, and not to allow others to
17186:  * use your version of this file under the terms of the MPL, indicate your
17186:  * decision by deleting the provisions above and replace them with the notice
17186:  * and other provisions required by the GPL or the LGPL. If you do not delete
17186:  * the provisions above, a recipient may use your version of this file under
17186:  * the terms of any one of the MPL, the GPL or the LGPL.
17186:  *
17186:  * ***** END LICENSE BLOCK ***** */
17186: 
17196: #ifndef jstracer_h___
17196: #define jstracer_h___
17186: 
19171: #ifdef JS_TRACER
18091: 
21521: #include "jscntxt.h"
17811: #include "jstypes.h"
17186: #include "jslock.h"
17393: #include "jsnum.h"
17407: #include "jsinterp.h"
20408: #include "jsbuiltins.h"
17434: 
21459: #if defined(DEBUG) && !defined(JS_JIT_SPEW)
21459: #define JS_JIT_SPEW
21459: #endif
21459: 
17886: template <typename T>
21523: class Queue : public avmplus::GCObject {
17886:     T* _data;
17886:     unsigned _len;
17886:     unsigned _max;
17891: 
17891:     void ensure(unsigned size) {
17891:         while (_max < size)
17891:             _max <<= 1;
17891:         _data = (T*)realloc(_data, _max * sizeof(T));
24246: #if defined(DEBUG)
24246:         memset(&_data[_len], 0xcd, _max - _len);
24246: #endif
17891:     }
17886: public:
17886:     Queue(unsigned max = 16) {
17886:         this->_max = max;
17886:         this->_len = 0;
17886:         this->_data = (T*)malloc(max * sizeof(T));
17886:     }
17886: 
17886:     ~Queue() {
17886:         free(_data);
17886:     }
17886: 
17973:     bool contains(T a) {
23709:         for (unsigned n = 0; n < _len; ++n) {
17973:             if (_data[n] == a)
17973:                 return true;
23709:         }
17973:         return false;
17973:     }
17973: 
17886:     void add(T a) {
18621:         ensure(_len + 1);
17886:         JS_ASSERT(_len <= _max);
17891:         _data[_len++] = a;
17886:     }
17891: 
18621:     void add(T* chunk, unsigned size) {
18621:         ensure(_len + size);
18621:         JS_ASSERT(_len <= _max);
18621:         memcpy(&_data[_len], chunk, size * sizeof(T));
18621:         _len += size;
18621:     }
18621: 
18051:     void addUnique(T a) {
18051:         if (!contains(a))
18051:             add(a);
18051:     }
18051: 
17891:     void setLength(unsigned len) {
17891:         ensure(len + 1);
17891:         _len = len;
17886:     }
17886: 
17886:     void clear() {
17886:         _len = 0;
17886:     }
17886: 
22609:     const T & get(unsigned i) const {
22609:         return _data[i];
22609:     }
22609: 
17886:     unsigned length() const {
17886:         return _len;
17886:     }
17886: 
17886:     T* data() const {
17886:         return _data;
17886:     }
17886: };
17886: 
17434: /*
17487:  * Tracker is used to keep track of values being manipulated by the interpreter
17487:  * during trace recording.
17293:  */
17487: class Tracker {
17293:     struct Page {
17293:         struct Page*    next;
17424:         jsuword         base;
17596:         nanojit::LIns*  map[1];
17293:     };
17293:     struct Page* pagelist;
17293: 
17424:     jsuword         getPageBase(const void* v) const;
17293:     struct Page*    findPage(const void* v) const;
17293:     struct Page*    addPage(const void* v);
17293: public:
17293:     Tracker();
17293:     ~Tracker();
17293: 
17773:     bool            has(const void* v) const;
17596:     nanojit::LIns*  get(const void* v) const;
17596:     void            set(const void* v, nanojit::LIns* ins);
17293:     void            clear();
17293: };
17293: 
23450: #ifdef JS_JIT_SPEW
23450: extern bool js_verboseDebug;
23450: #define debug_only_v(x) if (js_verboseDebug) { x; }
23450: #else
23450: #define debug_only_v(x)
23450: #endif
23450: 
17981: /*
24290:  * The oracle keeps track of hit counts for program counter locations, as
24290:  * well as slots that should not be demoted to int because we know them to
24290:  * overflow or they result in type-unstable traces. We are using simple
24290:  * hash tables.  Collisions lead to loss of optimization (demotable slots
24290:  * are not demoted, etc.) but have no correctness implications.
17981:  */
17981: #define ORACLE_SIZE 4096
17981: 
17981: class Oracle {
25484:     uint32_t hits[ORACLE_SIZE];
25484:     uint32_t blacklistLevels[ORACLE_SIZE];
22613:     avmplus::BitSet _stackDontDemote;
22613:     avmplus::BitSet _globalDontDemote;
17981: public:
24290:     Oracle();
25484:     int32_t hit(const void* ip);
25484:     int32_t getHits(const void* ip);
25484:     void resetHits(const void* ip);
25484:     void blacklist(const void* ip);
24290: 
23456:     JS_REQUIRES_STACK void markGlobalSlotUndemotable(JSContext* cx, unsigned slot);
23456:     JS_REQUIRES_STACK bool isGlobalSlotUndemotable(JSContext* cx, unsigned slot) const;
23456:     JS_REQUIRES_STACK void markStackSlotUndemotable(JSContext* cx, unsigned slot);
23456:     JS_REQUIRES_STACK bool isStackSlotUndemotable(JSContext* cx, unsigned slot) const;
24290:     void clearHitCounts();
24290:     void clearDemotability();
24290:     void clear() {
24290:         clearDemotability();
24290:         clearHitCounts();
24290:     }
17981: };
17981: 
17992: typedef Queue<uint16> SlotList;
17992: 
17981: class TypeMap : public Queue<uint8> {
17981: public:
24246:     JS_REQUIRES_STACK void captureTypes(JSContext* cx, SlotList& slots, unsigned callDepth);
24246:     JS_REQUIRES_STACK void captureMissingGlobalTypes(JSContext* cx,
24246:                                                      SlotList& slots,
24246:                                                      unsigned stackSlots);
18239:     bool matches(TypeMap& other) const;
17981: };
17981: 
21521: enum ExitType {
25478:     /*
25478:      * An exit at a possible branch-point in the trace at which to attach a
25478:      * future secondary trace. Therefore the recorder must generate different
25478:      * code to handle the other outcome of the branch condition from the
25478:      * primary trace's outcome.
25478:      */
21521:     BRANCH_EXIT,
25478: 
25478:     /*
25478:      * Exit at a tableswitch via a numbered case.
25478:      */
25478:     CASE_EXIT,
25478: 
25478:     /*
25478:      * Exit at a tableswitch via the default case.
25478:      */
25478:     DEFAULT_EXIT,
25478: 
21521:     LOOP_EXIT,
21521:     NESTED_EXIT,
25478: 
25478:     /*
25478:      * An exit from a trace because a condition relied upon at recording time
25478:      * no longer holds, where the alternate path of execution is so rare or
25478:      * difficult to address in native code that it is not traced at all, e.g.
25478:      * negative array index accesses, which differ from positive indexes in
25478:      * that they require a string-based property lookup rather than a simple
25478:      * memory access.
25478:      */
21521:     MISMATCH_EXIT,
25478: 
25478:     /*
25478:      * A specialization of MISMATCH_EXIT to handle allocation failures.
25478:      */
21521:     OOM_EXIT,
21521:     OVERFLOW_EXIT,
21521:     UNSTABLE_LOOP_EXIT,
24612:     TIMEOUT_EXIT,
24612:     DEEP_BAIL_EXIT,
24612:     STATUS_EXIT
21521: };
21521: 
21521: struct VMSideExit : public nanojit::SideExit
21521: {
22925:     JSObject* block;
25111:     jsbytecode* pc;
25111:     jsbytecode* imacpc;
21521:     intptr_t sp_adj;
21521:     intptr_t rp_adj;
21521:     int32_t calldepth;
21521:     uint32 numGlobalSlots;
21521:     uint32 numStackSlots;
21521:     uint32 numStackSlotsBelowCurrentFrame;
21521:     ExitType exitType;
21521: };
21521: 
24246: static inline uint8* getStackTypeMap(nanojit::SideExit* exit)
21521: {
21521:     return (uint8*)(((VMSideExit*)exit) + 1);
21521: }
21521: 
24246: static inline uint8* getGlobalTypeMap(nanojit::SideExit* exit)
24246: {
24246:     return getStackTypeMap(exit) + ((VMSideExit*)exit)->numStackSlots;
24246: }
24246: 
24246: static inline uint8* getFullTypeMap(nanojit::SideExit* exit)
24246: {
24246:     return getStackTypeMap(exit);
24246: }
24246: 
24612: struct FrameInfo {
24612:     JSObject*       callee;     // callee function object
24612:     JSObject*       block;      // caller block chain head
25111:     jsbytecode*     pc;         // caller fp->regs->pc
25111:     jsbytecode*     imacpc;     // caller fp->imacpc
24612:     union {
24612:         struct {
24612:             uint16  spdist;     // distance from fp->slots to fp->regs->sp at JSOP_CALL
24612:             uint16  argc;       // actual argument count, may be < fun->nargs
24612:         } s;
24612:         uint32      word;       // for spdist/argc LIR store in record_JSOP_CALL
24612:     };
21521: };
21521: 
21433: struct UnstableExit
21433: {
21433:     nanojit::Fragment* fragment;
21521:     VMSideExit* exit;
21433:     UnstableExit* next;
21433: };
21433: 
18056: class TreeInfo MMGC_SUBCLASS_DECL {
17974:     nanojit::Fragment*      fragment;
17701: public:
18595:     JSScript*               script;
17815:     unsigned                maxNativeStackSlots;
17811:     ptrdiff_t               nativeStackBase;
17659:     unsigned                maxCallDepth;
24246:     TypeMap                 typeMap;
24491:     unsigned                nStackTypes;
24491:     uint32                  globalShape;
24491:     SlotList*               globalSlots;
25491:     /* Dependent trees must be trashed if this tree dies, and updated on missing global types */
18650:     Queue<nanojit::Fragment*> dependentTrees;
25491:     /* Linked trees must be updated on missing global types, but are not dependent */
25491:     Queue<nanojit::Fragment*> linkedTrees;
18781:     unsigned                branchCount;
21521:     Queue<VMSideExit*>      sideExits;
21433:     UnstableExit*           unstableExits;
17974: 
24491:     TreeInfo(nanojit::Fragment* _fragment,
24491:              uint32 _globalShape,
24491:              SlotList* _globalSlots)
24491:       : fragment(_fragment),
24491:         script(NULL),
24491:         maxNativeStackSlots(0),
24491:         nativeStackBase(0),
24491:         maxCallDepth(0),
24491:         nStackTypes(0),
24491:         globalShape(_globalShape),
24491:         globalSlots(_globalSlots),
24491:         branchCount(0),
24491:         unstableExits(NULL)
24491:             {}
25102:     ~TreeInfo();
24246: 
24491:     inline unsigned nGlobalTypes() {
24491:         return typeMap.length() - nStackTypes;
24246:     }
24246:     inline uint8* globalTypeMap() {
24491:         return typeMap.data() + nStackTypes;
24246:     }
24246:     inline uint8* stackTypeMap() {
24246:         return typeMap.data();
24246:     }
17413: };
17413: 
24612: #if defined(JS_JIT_SPEW) && (defined(NANOJIT_IA32) || (defined(NANOJIT_AMD64) && defined(__GNUC__)))
24612: # define EXECUTE_TREE_TIMER
24612: #endif
24612: 
24612: struct InterpState
24612: {
24612:     double        *sp;                  // native stack pointer, stack[0] is spbase[0]
24612:     void          *rp;                  // call stack pointer
24612:     double        *global;              // global frame pointer
24612:     JSContext     *cx;                  // current VM context handle
24612:     double        *eos;                 // first unusable word after the native stack
24612:     void          *eor;                 // first unusable word after the call stack
24612:     VMSideExit*    lastTreeExitGuard;   // guard we exited on during a tree call
24612:     VMSideExit*    lastTreeCallGuard;   // guard we want to grow from if the tree
24612:                                         // call exit guard mismatched
24612:     void*          rpAtLastTreeCall;    // value of rp at innermost tree call guard
24612:     TreeInfo*      outermostTree;       // the outermost tree we initially invoked
24612:     JSObject*      globalObj;           // pointer to the global object
24612:     double*        stackBase;           // native stack base
24612:     FrameInfo**    callstackBase;       // call stack base
24612:     uintN*         inlineCallCountp;    // inline call count counter
24612:     VMSideExit** innermostNestedGuardp;
24612:     void*          stackMark;
24612:     VMSideExit*    innermost;
24612: #ifdef EXECUTE_TREE_TIMER
24612:     uint64         startTime;
24612: #endif
24612: #ifdef DEBUG
24612:     bool           jsframe_pop_blocks_set_on_entry;
24612: #endif
20429: };
20429: 
23111: enum JSMonitorRecordingStatus {
23111:     JSMRS_CONTINUE,
23111:     JSMRS_STOP,
23111:     JSMRS_IMACRO
23111: };
23111: 
21523: class TraceRecorder : public avmplus::GCObject {
17351:     JSContext*              cx;
18239:     JSTraceMonitor*         traceMonitor;
17657:     JSObject*               globalObj;
17596:     Tracker                 tracker;
17815:     Tracker                 nativeFrameTracker;
17393:     char*                   entryTypeMap;
17789:     unsigned                callDepth;
17611:     JSAtom**                atoms;
21521:     VMSideExit*             anchor;
17334:     nanojit::Fragment*      fragment;
17785:     TreeInfo*               treeInfo;
17334:     nanojit::LirBuffer*     lirbuf;
17334:     nanojit::LirWriter*     lir;
17381:     nanojit::LirBufWriter*  lir_buf_writer;
17370:     nanojit::LirWriter*     verbose_filter;
17370:     nanojit::LirWriter*     cse_filter;
17370:     nanojit::LirWriter*     expr_filter;
17453:     nanojit::LirWriter*     func_filter;
18776: #ifdef NJ_SOFTFLOAT
18773:     nanojit::LirWriter*     float_filter;
18773: #endif
17393:     nanojit::LIns*          cx_ins;
17815:     nanojit::LIns*          gp_ins;
18118:     nanojit::LIns*          eos_ins;
18133:     nanojit::LIns*          eor_ins;
24282:     nanojit::LIns*          globalObj_ins;
17818:     nanojit::LIns*          rval_ins;
18241:     nanojit::LIns*          inner_sp_ins;
19068:     bool                    deepAborted;
22609:     bool                    trashSelf;
22609:     Queue<nanojit::Fragment*> whichTreesToTrash;
18694:     Queue<jsbytecode*>      cfgMerges;
19653:     jsval*                  global_dslots;
20405:     JSTraceableNative*      pendingTraceableNative;
20416:     bool                    terminate;
25111:     jsbytecode*             terminate_pc;
25111:     jsbytecode*             terminate_imacpc;
25484:     nanojit::Fragment*      outerToBlacklist;
21723:     TraceRecorder*          nextRecorderToAbort;
21723:     bool                    wasRootFragment;
17323: 
17815:     bool isGlobal(jsval* p) const;
18193:     ptrdiff_t nativeGlobalOffset(jsval* p) const;
22652:     JS_REQUIRES_STACK ptrdiff_t nativeStackOffset(jsval* p) const;
22652:     JS_REQUIRES_STACK void import(nanojit::LIns* base, ptrdiff_t offset, jsval* p, uint8& t,
18045:                                   const char *prefix, uintN index, JSStackFrame *fp);
24246:     JS_REQUIRES_STACK void import(TreeInfo* treeInfo, nanojit::LIns* sp, unsigned stackSlots,
24246:                                   unsigned callDepth, unsigned ngslots, uint8* typeMap);
17815:     void trackNativeStackUse(unsigned slots);
17381: 
22652:     JS_REQUIRES_STACK bool lazilyImportGlobalSlot(unsigned slot);
17891: 
22652:     JS_REQUIRES_STACK nanojit::LIns* guard(bool expected, nanojit::LIns* cond,
22652:                                            ExitType exitType);
21083:     nanojit::LIns* guard(bool expected, nanojit::LIns* cond, nanojit::LIns* exit);
23111: 
17721:     nanojit::LIns* addName(nanojit::LIns* ins, const char* name);
17346: 
18197:     nanojit::LIns* writeBack(nanojit::LIns* i, nanojit::LIns* base, ptrdiff_t offset);
22652:     JS_REQUIRES_STACK void set(jsval* p, nanojit::LIns* l, bool initializing = false);
24381:     JS_REQUIRES_STACK nanojit::LIns* get(jsval* p);
24381:     JS_REQUIRES_STACK bool known(jsval* p);
24381:     JS_REQUIRES_STACK void checkForGlobalObjectReallocation();
17320: 
22652:     JS_REQUIRES_STACK bool checkType(jsval& v, uint8 t, jsval*& stage_val,
22652:                                      nanojit::LIns*& stage_ins, unsigned& stage_count);
22652:     JS_REQUIRES_STACK bool deduceTypeStability(nanojit::Fragment* root_peer,
24246:                                                nanojit::Fragment** stable_peer,
24246:                                                bool& demote);
17410: 
22652:     JS_REQUIRES_STACK jsval& argval(unsigned n) const;
22652:     JS_REQUIRES_STACK jsval& varval(unsigned n) const;
22652:     JS_REQUIRES_STACK jsval& stackval(int n) const;
17412: 
22652:     JS_REQUIRES_STACK nanojit::LIns* scopeChain() const;
22652:     JS_REQUIRES_STACK bool activeCallOrGlobalSlot(JSObject* obj, jsval*& vp);
18286: 
22652:     JS_REQUIRES_STACK nanojit::LIns* arg(unsigned n);
22652:     JS_REQUIRES_STACK void arg(unsigned n, nanojit::LIns* i);
22652:     JS_REQUIRES_STACK nanojit::LIns* var(unsigned n);
22652:     JS_REQUIRES_STACK void var(unsigned n, nanojit::LIns* i);
22652:     JS_REQUIRES_STACK nanojit::LIns* stack(int n);
22652:     JS_REQUIRES_STACK void stack(int n, nanojit::LIns* i);
17412: 
23456:     JS_REQUIRES_STACK nanojit::LIns* alu(nanojit::LOpcode op, jsdouble v0, jsdouble v1,
21799:                                          nanojit::LIns* s0, nanojit::LIns* s1);
17469:     nanojit::LIns* f2i(nanojit::LIns* f);
22652:     JS_REQUIRES_STACK nanojit::LIns* makeNumberInt32(nanojit::LIns* f);
23456:     JS_REQUIRES_STACK nanojit::LIns* stringify(jsval& v);
21685: 
23456:     JS_REQUIRES_STACK bool call_imacro(jsbytecode* imacro);
17469: 
22652:     JS_REQUIRES_STACK bool ifop();
22652:     JS_REQUIRES_STACK bool switchop();
25099: #ifdef NANOJIT_IA32
25099:     JS_REQUIRES_STACK nanojit::LIns* tableswitch();
25099: #endif
22652:     JS_REQUIRES_STACK bool inc(jsval& v, jsint incr, bool pre = true);
22652:     JS_REQUIRES_STACK bool inc(jsval& v, nanojit::LIns*& v_ins, jsint incr, bool pre = true);
22652:     JS_REQUIRES_STACK bool incProp(jsint incr, bool pre = true);
22652:     JS_REQUIRES_STACK bool incElem(jsint incr, bool pre = true);
22652:     JS_REQUIRES_STACK bool incName(jsint incr, bool pre = true);
18687: 
23093:     JS_REQUIRES_STACK void strictEquality(bool equal, bool cmpCase);
23093:     JS_REQUIRES_STACK bool equality(bool negate, bool tryBranchAfterCond);
23223:     JS_REQUIRES_STACK bool equalityHelper(jsval l, jsval r,
23223:                                           nanojit::LIns* l_ins, nanojit::LIns* r_ins,
23223:                                           bool negate, bool tryBranchAfterCond,
23223:                                           jsval& rval);
23093:     JS_REQUIRES_STACK bool relational(nanojit::LOpcode op, bool tryBranchAfterCond);
17467: 
22652:     JS_REQUIRES_STACK bool unary(nanojit::LOpcode op);
22652:     JS_REQUIRES_STACK bool binary(nanojit::LOpcode op);
17467: 
17450:     bool ibinary(nanojit::LOpcode op);
17450:     bool iunary(nanojit::LOpcode op);
17436:     bool bbinary(nanojit::LOpcode op);
17453:     void demote(jsval& v, jsdouble result);
17417: 
22652:     JS_REQUIRES_STACK bool map_is_native(JSObjectMap* map, nanojit::LIns* map_ins,
22652:                                          nanojit::LIns*& ops_ins, size_t op_offset = 0);
22652:     JS_REQUIRES_STACK bool test_property_cache(JSObject* obj, nanojit::LIns* obj_ins,
22652:                                                JSObject*& obj2, jsuword& pcval);
22652:     JS_REQUIRES_STACK bool test_property_cache_direct_slot(JSObject* obj, nanojit::LIns* obj_ins,
22652:                                                            uint32& slot);
22626:     void stobj_set_slot(nanojit::LIns* obj_ins, unsigned slot, nanojit::LIns*& dslots_ins,
22626:                         nanojit::LIns* v_ins);
22626:     void stobj_set_dslot(nanojit::LIns *obj_ins, unsigned slot, nanojit::LIns*& dslots_ins,
22626:                          nanojit::LIns* v_ins, const char *name);
22626: 
17899:     nanojit::LIns* stobj_get_fslot(nanojit::LIns* obj_ins, unsigned slot);
17459:     nanojit::LIns* stobj_get_slot(nanojit::LIns* obj_ins, unsigned slot,
17459:                                   nanojit::LIns*& dslots_ins);
17429:     bool native_set(nanojit::LIns* obj_ins, JSScopeProperty* sprop,
17429:                     nanojit::LIns*& dslots_ins, nanojit::LIns* v_ins);
17429:     bool native_get(nanojit::LIns* obj_ins, nanojit::LIns* pobj_ins, JSScopeProperty* sprop,
17429:                     nanojit::LIns*& dslots_ins, nanojit::LIns*& v_ins);
17429: 
22652:     JS_REQUIRES_STACK bool name(jsval*& vp);
22652:     JS_REQUIRES_STACK bool prop(JSObject* obj, nanojit::LIns* obj_ins, uint32& slot,
22652:                                 nanojit::LIns*& v_ins);
22652:     JS_REQUIRES_STACK bool elem(jsval& oval, jsval& idx, jsval*& vp, nanojit::LIns*& v_ins,
22652:                                 nanojit::LIns*& addr_ins);
22652:     JS_REQUIRES_STACK bool getProp(JSObject* obj, nanojit::LIns* obj_ins);
22652:     JS_REQUIRES_STACK bool getProp(jsval& v);
22652:     JS_REQUIRES_STACK bool getThis(nanojit::LIns*& this_ins);
17758: 
23708:     JS_REQUIRES_STACK void box_jsval(jsval v, nanojit::LIns*& v_ins);
23710:     JS_REQUIRES_STACK void unbox_jsval(jsval v, nanojit::LIns*& v_ins);
22652:     JS_REQUIRES_STACK bool guardClass(JSObject* obj, nanojit::LIns* obj_ins, JSClass* clasp,
21685:                                       ExitType exitType = MISMATCH_EXIT);
22652:     JS_REQUIRES_STACK bool guardDenseArray(JSObject* obj, nanojit::LIns* obj_ins,
21685:                                            ExitType exitType = MISMATCH_EXIT);
22652:     JS_REQUIRES_STACK bool guardDenseArrayIndex(JSObject* obj, jsint idx, nanojit::LIns* obj_ins,
20972:                                                 nanojit::LIns* dslots_ins, nanojit::LIns* idx_ins,
21521:                                                 ExitType exitType);
22652:     JS_REQUIRES_STACK bool guardElemOp(JSObject* obj, nanojit::LIns* obj_ins, jsid id,
22652:                                        size_t op_offset, jsval* vp);
17818:     void clearFrameSlotsFromCache();
22652:     JS_REQUIRES_STACK bool guardCallee(jsval& callee);
22652:     JS_REQUIRES_STACK bool getClassPrototype(JSObject* ctor, nanojit::LIns*& proto_ins);
22652:     JS_REQUIRES_STACK bool newArray(JSObject* ctor, uint32 argc, jsval* argv, jsval* vp);
22652:     JS_REQUIRES_STACK bool interpretedFunctionCall(jsval& fval, JSFunction* fun, uintN argc,
22652:                                                    bool constructing);
22652:     JS_REQUIRES_STACK bool functionCall(bool constructing, uintN argc);
17921: 
22652:     JS_REQUIRES_STACK void trackCfgMerges(jsbytecode* pc);
22652:     JS_REQUIRES_STACK void flipIf(jsbytecode* pc, bool& cond);
22652:     JS_REQUIRES_STACK void fuseIf(jsbytecode* pc, bool cond, nanojit::LIns* x);
19068: 
21685:     bool hasMethod(JSObject* obj, jsid id);
24299:     JS_REQUIRES_STACK bool hasIteratorMethod(JSObject* obj);
21685: 
17409: public:
22652:     JS_REQUIRES_STACK
21521:     TraceRecorder(JSContext* cx, VMSideExit*, nanojit::Fragment*, TreeInfo*,
24246:                   unsigned stackSlots, unsigned ngslots, uint8* typeMap,
25484:                   VMSideExit* expectedInnerExit, nanojit::Fragment* outerToBlacklist);
17409:     ~TraceRecorder();
17409: 
23440:     static JS_REQUIRES_STACK JSMonitorRecordingStatus monitorRecording(JSContext* cx, TraceRecorder* tr, JSOp op);
23111: 
24381:     JS_REQUIRES_STACK uint8 determineSlotType(jsval* vp);
22652:     JS_REQUIRES_STACK nanojit::LIns* snapshot(ExitType exitType);
17718:     nanojit::Fragment* getFragment() const { return fragment; }
22652:     JS_REQUIRES_STACK bool isLoopHeader(JSContext* cx) const;
24307:     JS_REQUIRES_STACK void compile(JSTraceMonitor* tm);
24307:     JS_REQUIRES_STACK bool closeLoop(JSTraceMonitor* tm, bool& demote);
24307:     JS_REQUIRES_STACK void endLoop(JSTraceMonitor* tm);
22652:     JS_REQUIRES_STACK void joinEdgesToEntry(nanojit::Fragmento* fragmento,
22652:                                             nanojit::Fragment* peer_root);
17980:     void blacklist() { fragment->blacklist(); }
24246:     JS_REQUIRES_STACK bool adjustCallerTypes(nanojit::Fragment* f);
22652:     JS_REQUIRES_STACK nanojit::Fragment* findNestedCompatiblePeer(nanojit::Fragment* f,
22652:                                                                   nanojit::Fragment** empty);
22652:     JS_REQUIRES_STACK void prepareTreeCall(nanojit::Fragment* inner);
22652:     JS_REQUIRES_STACK void emitTreeCall(nanojit::Fragment* inner, VMSideExit* exit);
17988:     unsigned getCallDepth() const;
21723:     void pushAbortStack();
21723:     void popAbortStack();
21723:     void removeFragmentoReferences();
17409: 
22652:     JS_REQUIRES_STACK bool record_EnterFrame();
22652:     JS_REQUIRES_STACK bool record_LeaveFrame();
22652:     JS_REQUIRES_STACK bool record_SetPropHit(JSPropCacheEntry* entry, JSScopeProperty* sprop);
22652:     JS_REQUIRES_STACK bool record_SetPropMiss(JSPropCacheEntry* entry);
22652:     JS_REQUIRES_STACK bool record_DefLocalFunSetSlot(uint32 slot, JSObject* obj);
22652:     JS_REQUIRES_STACK bool record_FastNativeCallComplete();
17695: 
25484:     nanojit::Fragment* getOuterToBlacklist() { return outerToBlacklist; }
18706:     void deepAbort() { deepAborted = true; }
18706:     bool wasDeepAborted() { return deepAborted; }
20416:     bool walkedOutOfLoop() { return terminate; }
21433:     TreeInfo* getTreeInfo() { return treeInfo; }
18706: 
17484: #define OPDEF(op,val,name,token,length,nuses,ndefs,prec,format)               \
22652:     JS_REQUIRES_STACK bool record_##op();
17484: # include "jsopcode.tbl"
17484: #undef OPDEF
17296: };
17331: #define TRACING_ENABLED(cx)       JS_HAS_OPTION(cx, JSOPTION_JIT)
19093: #define TRACE_RECORDER(cx)        (JS_TRACE_MONITOR(cx).recorder)
19093: #define SET_TRACE_RECORDER(cx,tr) (JS_TRACE_MONITOR(cx).recorder = (tr))
17186: 
21685: #define JSOP_IS_BINARY(op) ((uintN)((op) - JSOP_BITOR) <= (uintN)(JSOP_MOD - JSOP_BITOR))
23106: #define JSOP_IS_UNARY(op) ((uintN)((op) - JSOP_NEG) <= (uintN)(JSOP_POS - JSOP_NEG))
23223: #define JSOP_IS_EQUALITY(op) ((uintN)((op) - JSOP_EQ) <= (uintN)(JSOP_NE - JSOP_EQ))
21685: 
23111: #define TRACE_ARGS_(x,args)                                                   \
23110:     JS_BEGIN_MACRO                                                            \
23111:         TraceRecorder* tr_ = TRACE_RECORDER(cx);                              \
23111:         if (tr_ && !tr_->record_##x args)                                     \
23111:             js_AbortRecording(cx, #x);                                        \
23110:     JS_END_MACRO
23110: 
23111: #define TRACE_ARGS(x,args)      TRACE_ARGS_(x, args)
19582: #define TRACE_0(x)              TRACE_ARGS(x, ())
19093: #define TRACE_1(x,a)            TRACE_ARGS(x, (a))
19093: #define TRACE_2(x,a,b)          TRACE_ARGS(x, (a, b))
19093: 
22652: extern JS_REQUIRES_STACK bool
20422: js_MonitorLoopEdge(JSContext* cx, uintN& inlineCallCount);
18683: 
23111: #ifdef DEBUG
23111: # define js_AbortRecording(cx, reason) js_AbortRecordingImpl(cx, reason)
23111: #else
23111: # define js_AbortRecording(cx, reason) js_AbortRecordingImpl(cx)
23111: #endif
17257: 
22652: extern JS_REQUIRES_STACK void
20422: js_AbortRecording(JSContext* cx, const char* reason);
17350: 
17442: extern void
18068: js_InitJIT(JSTraceMonitor *tm);
17442: 
17726: extern void
18075: js_FinishJIT(JSTraceMonitor *tm);
17726: 
17976: extern void
24879: js_FlushScriptFragments(JSContext* cx, JSScript* script);
24879: 
24879: extern void
17976: js_FlushJITCache(JSContext* cx);
17976: 
18277: extern void
18277: js_FlushJITOracle(JSContext* cx);
18277: 
24384: extern JSObject *
24384: js_GetBuiltinFunction(JSContext *cx, uintN index);
24384: 
19171: #else  /* !JS_TRACER */
19171: 
19599: #define TRACE_0(x)              ((void)0)
19171: #define TRACE_1(x,a)            ((void)0)
19171: #define TRACE_2(x,a,b)          ((void)0)
19171: 
19171: #endif /* !JS_TRACER */
18091: 
17196: #endif /* jstracer_h___ */
