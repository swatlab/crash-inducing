29366: /* -*- Mode: C++; tab-width: 8; indent-tabs-mode: nil; c-basic-offset: 4 -*-
    1:  * vim: set ts=8 sw=4 et tw=78:
    1:  *
    1:  * ***** BEGIN LICENSE BLOCK *****
    1:  * Version: MPL 1.1/GPL 2.0/LGPL 2.1
    1:  *
    1:  * The contents of this file are subject to the Mozilla Public License Version
    1:  * 1.1 (the "License"); you may not use this file except in compliance with
    1:  * the License. You may obtain a copy of the License at
    1:  * http://www.mozilla.org/MPL/
    1:  *
    1:  * Software distributed under the License is distributed on an "AS IS" basis,
    1:  * WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
    1:  * for the specific language governing rights and limitations under the
    1:  * License.
    1:  *
    1:  * The Original Code is Mozilla Communicator client code, released
    1:  * March 31, 1998.
    1:  *
    1:  * The Initial Developer of the Original Code is
    1:  * Netscape Communications Corporation.
    1:  * Portions created by the Initial Developer are Copyright (C) 1998
    1:  * the Initial Developer. All Rights Reserved.
    1:  *
    1:  * Contributor(s):
    1:  *
    1:  * Alternatively, the contents of this file may be used under the terms of
    1:  * either of the GNU General Public License Version 2 or later (the "GPL"),
    1:  * or the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
    1:  * in which case the provisions of the GPL or the LGPL are applicable instead
    1:  * of those above. If you wish to allow use of your version of this file only
    1:  * under the terms of either the GPL or the LGPL, and not to allow others to
    1:  * use your version of this file under the terms of the MPL, indicate your
    1:  * decision by deleting the provisions above and replace them with the notice
    1:  * and other provisions required by the GPL or the LGPL. If you do not delete
    1:  * the provisions above, a recipient may use your version of this file under
    1:  * the terms of any one of the MPL, the GPL or the LGPL.
    1:  *
    1:  * ***** END LICENSE BLOCK ***** */
    1: 
    1: /*
    1:  * JS Mark-and-Sweep Garbage Collector.
    1:  *
    1:  * This GC allocates fixed-sized things with sizes up to GC_NBYTES_MAX (see
    1:  * jsgc.h). It allocates from a special GC arena pool with each arena allocated
    1:  * using malloc. It uses an ideally parallel array of flag bytes to hold the
    1:  * mark bit, finalizer type index, etc.
    1:  *
    1:  * XXX swizzle page to freelist for better locality of reference
    1:  */
    1: #include <stdlib.h>     /* for free */
17182: #include <math.h>
    1: #include <string.h>     /* for memset used when DEBUG */
    1: #include "jstypes.h"
26316: #include "jsstdint.h"
    1: #include "jsutil.h" /* Added by JSIFY */
    1: #include "jshash.h" /* Added by JSIFY */
17182: #include "jsbit.h"
17182: #include "jsclist.h"
17182: #include "jsprf.h"
    1: #include "jsapi.h"
    1: #include "jsatom.h"
    1: #include "jscntxt.h"
18863: #include "jsversion.h"
    1: #include "jsdbgapi.h"
    1: #include "jsexn.h"
    1: #include "jsfun.h"
    1: #include "jsgc.h"
40876: #include "jsgcchunk.h"
    1: #include "jsinterp.h"
    1: #include "jsiter.h"
    1: #include "jslock.h"
    1: #include "jsnum.h"
    1: #include "jsobj.h"
 3235: #include "jsparse.h"
42733: #include "jsproxy.h"
    1: #include "jsscope.h"
    1: #include "jsscript.h"
24499: #include "jsstaticcheck.h"
    1: #include "jsstr.h"
30851: #include "jstask.h"
17976: #include "jstracer.h"
    1: 
    1: #if JS_HAS_XML_SUPPORT
    1: #include "jsxml.h"
    1: #endif
    1: 
30285: #include "jsdtracef.h"
40383: #include "jscntxtinlines.h"
36997: #include "jsobjinlines.h"
42755: #include "jshashtable.h"
36997: 
37741: using namespace js;
37741: 
32823: /*
25901:  * Check that JSTRACE_XML follows JSTRACE_OBJECT, JSTRACE_DOUBLE and
25901:  * JSTRACE_STRING.
25901:  */
25901: JS_STATIC_ASSERT(JSTRACE_OBJECT == 0);
25901: JS_STATIC_ASSERT(JSTRACE_DOUBLE == 1);
25901: JS_STATIC_ASSERT(JSTRACE_STRING == 2);
25901: JS_STATIC_ASSERT(JSTRACE_XML    == 3);
25901: 
25901: /*
25901:  * JS_IS_VALID_TRACE_KIND assumes that JSTRACE_STRING is the last non-xml
25901:  * trace kind when JS_HAS_XML_SUPPORT is false.
25901:  */
25901: JS_STATIC_ASSERT(JSTRACE_STRING + 1 == JSTRACE_XML);
25901: 
25901: /*
25901:  * Check that we can use memset(p, 0, ...) to implement JS_CLEAR_WEAK_ROOTS.
25901:  */
25901: JS_STATIC_ASSERT(JSVAL_NULL == 0);
25901: 
33582: /*
33582:  * Check consistency of external string constants from JSFinalizeGCThingKind.
33582:  */
33582: JS_STATIC_ASSERT(FINALIZE_EXTERNAL_STRING_LAST - FINALIZE_EXTERNAL_STRING0 ==
33582:                  JS_EXTERNAL_STRING_LIMIT - 1);
25901: 
25901: /*
40876:  * GC memory is allocated in chunks. The size of each chunk is GC_CHUNK_SIZE.
40876:  * The chunk contains an array of GC arenas holding GC things, an array of
40876:  * the mark bitmaps for each arena, an array of JSGCArenaInfo arena
40876:  * descriptors, an array of JSGCMarkingDelay descriptors, the JSGCChunkInfo
40876:  * chunk descriptor and a bitmap indicating free arenas in the chunk. The
40876:  * following picture demonstrates the layout:
    1:  *
40876:  *  +--------+--------------+-------+--------+------------+-----------------+
40876:  *  | arenas | mark bitmaps | infos | delays | chunk info | free arena bits |
40876:  *  +--------+--------------+-------+--------+------------+-----------------+
    1:  *
40876:  * To ensure fast O(1) lookup of mark bits and arena descriptors each chunk is
40876:  * allocated on GC_CHUNK_SIZE boundary. This way a simple mask and shift
40876:  * operation gives an arena index into the mark and JSGCArenaInfo arrays.
    1:  *
40876:  * All chunks that have at least one free arena are put on the doubly-linked
40876:  * list with the head stored in JSRuntime.gcChunkList. JSGCChunkInfo contains
40876:  * the head of the chunk's free arena list together with the link fields for
40876:  * gcChunkList.
    1:  *
40876:  * A GC arena contains GC_ARENA_SIZE bytes aligned on GC_ARENA_SIZE boundary
40876:  * and holds things of the same size and kind. The size of each thing in the
40876:  * arena must be divisible by GC_CELL_SIZE, the minimal allocation unit, and
40876:  * the size of the mark bitmap is fixed and is independent of the thing's
40876:  * size with one bit per each GC_CELL_SIZE bytes. For thing sizes that exceed
40876:  * GC_CELL_SIZE this implies that we waste space in the mark bitmap. The
40876:  * advantage is that we can find the mark bit for the thing using just
40876:  * integer shifts avoiding an expensive integer division. We trade some space
40876:  * for speed here.
12282:  *
40876:  * The number of arenas in the chunk is given by GC_ARENAS_PER_CHUNK. We find
40876:  * that number as follows. Suppose chunk contains n arenas. Together with the
40876:  * word-aligned free arena bitmap and JSGCChunkInfo they should fit into the
40876:  * chunk. Hence GC_ARENAS_PER_CHUNK or n_max is the maximum value of n for
40876:  * which the following holds:
12282:   *
12282:  *   n*s + ceil(n/B) <= M                                               (1)
12282:  *
12282:  * where "/" denotes normal real division,
12282:  *       ceil(r) gives the least integer not smaller than the number r,
40876:  *       s is the number of words in the GC arena, arena's mark bitmap,
40876:  *         JSGCArenaInfo and JSGCMarkingDelay or GC_ARENA_ALL_WORDS.
13029:  *       B is number of bits per word or B == JS_BITS_PER_WORD
40876:  *       M is the number of words in the chunk without JSGCChunkInfo or
40876:  *       M == (GC_CHUNK_SIZE - sizeof(JSGCArenaInfo)) / sizeof(jsuword).
12282:  *
12282:  * We rewrite the inequality as
12282:  *
12282:  *   n*B*s/B + ceil(n/B) <= M,
12282:  *   ceil(n*B*s/B + n/B) <= M,
12282:  *   ceil(n*(B*s + 1)/B) <= M                                           (2)
12282:  *
12282:  * We define a helper function e(n, s, B),
12282:  *
12282:  *   e(n, s, B) := ceil(n*(B*s + 1)/B) - n*(B*s + 1)/B, 0 <= e(n, s, B) < 1.
12282:  *
12282:  * It gives:
12282:  *
12282:  *   n*(B*s + 1)/B + e(n, s, B) <= M,
12282:  *   n + e*B/(B*s + 1) <= M*B/(B*s + 1)
12282:  *
12282:  * We apply the floor function to both sides of the last equation, where
12282:  * floor(r) gives the biggest integer not greater than r. As a consequence we
12282:  * have:
12282:  *
12282:  *   floor(n + e*B/(B*s + 1)) <= floor(M*B/(B*s + 1)),
12282:  *   n + floor(e*B/(B*s + 1)) <= floor(M*B/(B*s + 1)),
12282:  *   n <= floor(M*B/(B*s + 1)),                                         (3)
12282:  *
12282:  * where floor(e*B/(B*s + 1)) is zero as e*B/(B*s + 1) < B/(B*s + 1) < 1.
12282:  * Thus any n that satisfies the original constraint (1) or its equivalent (2),
12282:  * must also satisfy (3). That is, we got an upper estimate for the maximum
12282:  * value of n. Lets show that this upper estimate,
12282:  *
12282:  *   floor(M*B/(B*s + 1)),                                              (4)
12282:  *
12282:  * also satisfies (1) and, as such, gives the required maximum value.
12282:  * Substituting it into (2) gives:
12282:  *
12282:  *   ceil(floor(M*B/(B*s + 1))*(B*s + 1)/B) == ceil(floor(M/X)*X)
12282:  *
12282:  * where X == (B*s + 1)/B > 1. But then floor(M/X)*X <= M/X*X == M and
12282:  *
12282:  *   ceil(floor(M/X)*X) <= ceil(M) == M.
12282:  *
12282:  * Thus the value of (4) gives the maximum n satisfying (1).
13029:  *
13029:  * For the final result we observe that in (4)
13029:  *
40876:  *    M*B == (GC_CHUNK_SIZE - sizeof(JSGCChunkInfo)) / sizeof(jsuword) *
40876:  *           JS_BITS_PER_WORD
40876:  *        == (GC_CHUNK_SIZE - sizeof(JSGCChunkInfo)) * JS_BITS_PER_BYTE
13029:  *
40876:  * since GC_CHUNK_SIZE and sizeof(JSGCChunkInfo) are at least word-aligned.
    1:  */
37684: 
41286: const jsuword GC_ARENA_SHIFT = 12;
41286: const jsuword GC_ARENA_MASK = JS_BITMASK(GC_ARENA_SHIFT);
41286: const jsuword GC_ARENA_SIZE = JS_BIT(GC_ARENA_SHIFT);
41286: 
41286: const jsuword GC_MAX_CHUNK_AGE = 3;
40886: 
37684: const size_t GC_CELL_SHIFT = 3;
37684: const size_t GC_CELL_SIZE = size_t(1) << GC_CELL_SHIFT;
37684: const size_t GC_CELL_MASK = GC_CELL_SIZE - 1;
37684: 
37684: const size_t BITS_PER_GC_CELL = GC_CELL_SIZE * JS_BITS_PER_BYTE;
37684: 
40876: const size_t GC_CELLS_PER_ARENA = size_t(1) << (GC_ARENA_SHIFT - GC_CELL_SHIFT);
40876: const size_t GC_MARK_BITMAP_SIZE = GC_CELLS_PER_ARENA / JS_BITS_PER_BYTE;
40876: const size_t GC_MARK_BITMAP_WORDS = GC_CELLS_PER_ARENA / JS_BITS_PER_WORD;
40876: 
40876: JS_STATIC_ASSERT(sizeof(jsbitmap) == sizeof(jsuword));
40876: 
40876: JS_STATIC_ASSERT(sizeof(JSString) % GC_CELL_SIZE == 0);
40876: JS_STATIC_ASSERT(sizeof(JSObject) % GC_CELL_SIZE == 0);
40876: JS_STATIC_ASSERT(sizeof(JSFunction) % GC_CELL_SIZE == 0);
40876: #ifdef JSXML
40876: JS_STATIC_ASSERT(sizeof(JSXML) % GC_CELL_SIZE == 0);
40876: #endif
40876: 
40876: JS_STATIC_ASSERT(GC_CELL_SIZE == sizeof(jsdouble));
40876: const size_t DOUBLES_PER_ARENA = GC_CELLS_PER_ARENA;
40876: 
37684: struct JSGCArenaInfo {
    1:     /*
37684:      * Allocation list for the arena or NULL if the arena holds double values.
12282:      */
37684:     JSGCArenaList   *list;
37684: 
37684:     /*
37684:      * Pointer to the previous arena in a linked list. The arena can either
37684:      * belong to one of JSContext.gcArenaList lists or, when it does not have
37684:      * any allocated GC things, to the list of free arenas in the chunk with
37684:      * head stored in JSGCChunkInfo.lastFreeArena.
37684:      */
37684:     JSGCArena       *prev;
37684: 
37684:     JSGCThing       *freeList;
37684: 
40876:     static inline JSGCArenaInfo *fromGCThing(void* thing);
37684: };
40876: 
40876: /* See comments before ThingsPerUnmarkedBit below. */
40876: struct JSGCMarkingDelay {
40876:     JSGCArena       *link;
40876:     jsuword         unmarkedChildren;
37684: };
37684: 
37684: struct JSGCArena {
40876:     uint8 data[GC_ARENA_SIZE];
37684: 
37684:     void checkAddress() const {
37684:         JS_ASSERT(!(reinterpret_cast<jsuword>(this) & GC_ARENA_MASK));
37684:     }
37684: 
37684:     jsuword toPageStart() const {
37684:         checkAddress();
37684:         return reinterpret_cast<jsuword>(this);
37684:     }
37684: 
40876:     static inline JSGCArena *fromGCThing(void* thing);
40876: 
40876:     static inline JSGCArena *fromChunkAndIndex(jsuword chunk, size_t index);
40876: 
40876:     jsuword getChunk() {
40876:         return toPageStart() & ~GC_CHUNK_MASK;
37684:     }
37684: 
40876:     jsuword getIndex() {
40876:         return (toPageStart() & GC_CHUNK_MASK) >> GC_ARENA_SHIFT;
37684:     }
37684: 
40876:     inline JSGCArenaInfo *getInfo();
40876: 
40876:     inline JSGCMarkingDelay *getMarkingDelay();
40876: 
40876:     inline jsbitmap *getMarkBitmap();
40876: };
40876: 
40876: struct JSGCChunkInfo {
41286:     JSRuntime       *runtime;
40876:     size_t          numFreeArenas;
40886:     size_t          gcChunkAge;
40876: 
40876:     inline void init(JSRuntime *rt);
40876: 
40876:     inline jsbitmap *getFreeArenaBitmap();
40876: 
40876:     inline jsuword getChunk();
40876: 
42725:     inline void clearMarkBitmap();
42725: 
40876:     static inline JSGCChunkInfo *fromChunk(jsuword chunk);
40876: };
40876: 
40876: /* Check that all chunk arrays at least word-aligned. */
40876: JS_STATIC_ASSERT(sizeof(JSGCArena) == GC_ARENA_SIZE);
40876: JS_STATIC_ASSERT(GC_MARK_BITMAP_WORDS % sizeof(jsuword) == 0);
40876: JS_STATIC_ASSERT(sizeof(JSGCArenaInfo) % sizeof(jsuword) == 0);
40876: JS_STATIC_ASSERT(sizeof(JSGCMarkingDelay) % sizeof(jsuword) == 0);
40876: 
40876: const size_t GC_ARENA_ALL_WORDS = (GC_ARENA_SIZE + GC_MARK_BITMAP_SIZE +
40876:                                    sizeof(JSGCArenaInfo) +
40876:                                    sizeof(JSGCMarkingDelay)) / sizeof(jsuword);
40876: 
40876: /* The value according (4) above. */
40876: const size_t GC_ARENAS_PER_CHUNK =
40876:     (GC_CHUNK_SIZE - sizeof(JSGCChunkInfo)) * JS_BITS_PER_BYTE /
40876:     (JS_BITS_PER_WORD * GC_ARENA_ALL_WORDS + 1);
40876: 
40876: const size_t GC_FREE_ARENA_BITMAP_WORDS = (GC_ARENAS_PER_CHUNK +
40876:                                            JS_BITS_PER_WORD - 1) /
40876:                                           JS_BITS_PER_WORD;
40876: 
40876: const size_t GC_FREE_ARENA_BITMAP_SIZE = GC_FREE_ARENA_BITMAP_WORDS *
40876:                                          sizeof(jsuword);
40876: 
40876: /* Check that GC_ARENAS_PER_CHUNK indeed maximises (1). */
40876: JS_STATIC_ASSERT(GC_ARENAS_PER_CHUNK * GC_ARENA_ALL_WORDS +
40876:                  GC_FREE_ARENA_BITMAP_WORDS <=
40876:                  (GC_CHUNK_SIZE - sizeof(JSGCChunkInfo)) / sizeof(jsuword));
40876: 
40876: JS_STATIC_ASSERT((GC_ARENAS_PER_CHUNK + 1) * GC_ARENA_ALL_WORDS +
40876:                  (GC_ARENAS_PER_CHUNK + 1 + JS_BITS_PER_WORD - 1) /
40876:                  JS_BITS_PER_WORD >
40876:                  (GC_CHUNK_SIZE - sizeof(JSGCChunkInfo)) / sizeof(jsuword));
40876: 
40876: 
40876: const size_t GC_MARK_BITMAP_ARRAY_OFFSET = GC_ARENAS_PER_CHUNK
40876:                                            << GC_ARENA_SHIFT;
40876: 
40876: const size_t GC_ARENA_INFO_ARRAY_OFFSET =
40876:     GC_MARK_BITMAP_ARRAY_OFFSET + GC_MARK_BITMAP_SIZE * GC_ARENAS_PER_CHUNK;
40876: 
40876: const size_t GC_MARKING_DELAY_ARRAY_OFFSET =
40876:     GC_ARENA_INFO_ARRAY_OFFSET + sizeof(JSGCArenaInfo) * GC_ARENAS_PER_CHUNK;
40876: 
40876: const size_t GC_CHUNK_INFO_OFFSET = GC_CHUNK_SIZE - GC_FREE_ARENA_BITMAP_SIZE -
40876:                                     sizeof(JSGCChunkInfo);
40876: 
40876: inline jsuword
40876: JSGCChunkInfo::getChunk() {
40876:     jsuword addr = reinterpret_cast<jsuword>(this);
40876:     JS_ASSERT((addr & GC_CHUNK_MASK) == GC_CHUNK_INFO_OFFSET);
40876:     jsuword chunk = addr & ~GC_CHUNK_MASK;
40876:     return chunk;
40876: }
40876: 
42725: inline void
42725: JSGCChunkInfo::clearMarkBitmap()
42725: {
42725:     PodZero(reinterpret_cast<jsbitmap *>(getChunk() + GC_MARK_BITMAP_ARRAY_OFFSET),
42725:             GC_MARK_BITMAP_WORDS * GC_ARENAS_PER_CHUNK);
42725: }
42725: 
40876: /* static */
40876: inline JSGCChunkInfo *
40876: JSGCChunkInfo::fromChunk(jsuword chunk) {
40876:     JS_ASSERT(!(chunk & GC_CHUNK_MASK));
40876:     jsuword addr = chunk | GC_CHUNK_INFO_OFFSET;
40876:     return reinterpret_cast<JSGCChunkInfo *>(addr);
40876: }
40876: 
40876: inline jsbitmap *
40876: JSGCChunkInfo::getFreeArenaBitmap()
40876: {
40876:     jsuword addr = reinterpret_cast<jsuword>(this);
40876:     return reinterpret_cast<jsbitmap *>(addr + sizeof(JSGCChunkInfo));
40876: }
40876: 
40876: inline void
40876: JSGCChunkInfo::init(JSRuntime *rt)
40876: {
41286:     runtime = rt;
40876:     numFreeArenas = GC_ARENAS_PER_CHUNK;
41286:     gcChunkAge = 0;
40876: 
40876:     /*
40876:      * For simplicity we set all bits to 1 including the high bits in the
42587:      * last word that corresponds to nonexistent arenas. This is fine since
40876:      * the arena scans the bitmap words from lowest to highest bits and the
40876:      * allocation checks numFreeArenas before doing the search.
40876:      */
41286:     memset(getFreeArenaBitmap(), 0xFF, GC_FREE_ARENA_BITMAP_SIZE);
40876: }
40876: 
40876: inline void
40876: CheckValidGCThingPtr(void *thing)
40876: {
40876: #ifdef DEBUG
40874:     JS_ASSERT(!JSString::isStatic(thing));
40876:     jsuword addr = reinterpret_cast<jsuword>(thing);
40876:     JS_ASSERT(!(addr & GC_CELL_MASK));
40876:     JS_ASSERT((addr & GC_CHUNK_MASK) < GC_MARK_BITMAP_ARRAY_OFFSET);
40876: #endif
40874: }
40874: 
40876: /* static */
40876: inline JSGCArenaInfo *
40876: JSGCArenaInfo::fromGCThing(void* thing)
40876: {
40876:     CheckValidGCThingPtr(thing);
40876:     jsuword addr = reinterpret_cast<jsuword>(thing);
40876:     jsuword chunk = addr & ~GC_CHUNK_MASK;
40876:     JSGCArenaInfo *array =
40876:         reinterpret_cast<JSGCArenaInfo *>(chunk | GC_ARENA_INFO_ARRAY_OFFSET);
40876:     size_t arenaIndex = (addr & GC_CHUNK_MASK) >> GC_ARENA_SHIFT;
40876:     return array + arenaIndex;
40874: }
40874: 
40876: /* static */
40876: inline JSGCArena *
40876: JSGCArena::fromGCThing(void* thing)
40876: {
40876:     CheckValidGCThingPtr(thing);
40876:     jsuword addr = reinterpret_cast<jsuword>(thing);
40876:     return reinterpret_cast<JSGCArena *>(addr & ~GC_ARENA_MASK);
40874: }
40876: 
40876: /* static */
40876: inline JSGCArena *
40876: JSGCArena::fromChunkAndIndex(jsuword chunk, size_t index) {
40876:     JS_ASSERT(chunk);
40876:     JS_ASSERT(!(chunk & GC_CHUNK_MASK));
40876:     JS_ASSERT(index < GC_ARENAS_PER_CHUNK);
40876:     return reinterpret_cast<JSGCArena *>(chunk | (index << GC_ARENA_SHIFT));
40876: }
40876: 
40876: inline JSGCArenaInfo *
40876: JSGCArena::getInfo()
40876: {
40876:     jsuword chunk = getChunk();
40876:     jsuword index = getIndex();
40876:     jsuword offset = GC_ARENA_INFO_ARRAY_OFFSET + index * sizeof(JSGCArenaInfo);
40876:     return reinterpret_cast<JSGCArenaInfo *>(chunk | offset);
40876: }
40876: 
40876: inline JSGCMarkingDelay *
40876: JSGCArena::getMarkingDelay()
40876: {
40876:     jsuword chunk = getChunk();
40876:     jsuword index = getIndex();
40876:     jsuword offset = GC_MARKING_DELAY_ARRAY_OFFSET +
40876:                      index * sizeof(JSGCMarkingDelay);
40876:     return reinterpret_cast<JSGCMarkingDelay *>(chunk | offset);
40876: }
40876: 
40876: inline jsbitmap *
40876: JSGCArena::getMarkBitmap()
40876: {
40876:     jsuword chunk = getChunk();
40876:     jsuword index = getIndex();
40876:     jsuword offset = GC_MARK_BITMAP_ARRAY_OFFSET + index * GC_MARK_BITMAP_SIZE;
40876:     return reinterpret_cast<jsbitmap *>(chunk | offset);
40876: }
40876: 
40876: /*
40876:  * Helpers for GC-thing operations.
40876:  */
40876: 
40876: inline jsbitmap *
40876: GetGCThingMarkBit(void *thing, size_t &bitIndex)
40876: {
40876:     CheckValidGCThingPtr(thing);
40876:     jsuword addr = reinterpret_cast<jsuword>(thing);
40876:     jsuword chunk = addr & ~GC_CHUNK_MASK;
40876:     bitIndex = (addr & GC_CHUNK_MASK) >> GC_CELL_SHIFT;
40876:     return reinterpret_cast<jsbitmap *>(chunk | GC_MARK_BITMAP_ARRAY_OFFSET);
40876: }
40876: 
40876: inline bool
40876: IsMarkedGCThing(void *thing)
40876: {
40876:     size_t index;
40876:     jsbitmap *markBitmap = GetGCThingMarkBit(thing, index);
40876:     return !!JS_TEST_BIT(markBitmap, index);
40876: }
40876: 
40876: inline bool
40876: MarkIfUnmarkedGCThing(void *thing)
40876: {
40876:     size_t index;
40876:     jsbitmap *markBitmap = GetGCThingMarkBit(thing, index);
40876:     if (JS_TEST_BIT(markBitmap, index))
40876:         return false;
40876:     JS_SET_BIT(markBitmap, index);
40876:     return true;
40876: }
40876: 
40876: inline size_t
40876: ThingsPerArena(size_t thingSize)
40876: {
40876:     JS_ASSERT(!(thingSize & GC_CELL_MASK));
40876:     JS_ASSERT(thingSize <= GC_ARENA_SIZE);
40876:     return GC_ARENA_SIZE / thingSize;
40876: }
40876: 
40876: /* Can only be called if thing belongs to an arena where a->list is not null. */
40876: inline size_t
40876: GCThingToArenaIndex(void *thing)
40876: {
40876:     CheckValidGCThingPtr(thing);
40876:     jsuword addr = reinterpret_cast<jsuword>(thing);
40876:     jsuword offsetInArena = addr & GC_ARENA_MASK;
40876:     JSGCArenaInfo *a = JSGCArenaInfo::fromGCThing(thing);
40876:     JS_ASSERT(a->list);
40876:     JS_ASSERT(offsetInArena % a->list->thingSize == 0);
40876:     return offsetInArena / a->list->thingSize;
40876: }
40876: 
40876: /* Can only be applicable to arena where a->list is not null. */
40876: inline uint8 *
40876: GCArenaIndexToThing(JSGCArena *a, JSGCArenaInfo *ainfo, size_t index)
40876: {
40876:     JS_ASSERT(a->getInfo() == ainfo);
40876: 
40876:     /*
40876:      * We use "<=" and not "<" in the assert so index can mean the limit.
40876:      * For the same reason we use "+", not "|" when finding the thing address
40876:      * as the limit address can start at the next arena.
40876:      */
40876:     JS_ASSERT(index <= ThingsPerArena(ainfo->list->thingSize));
40876:     jsuword offsetInArena = index * ainfo->list->thingSize;
40876:     return reinterpret_cast<uint8 *>(a->toPageStart() + offsetInArena);
40876: }
12282: 
12282: /*
37684:  * The private JSGCThing struct, which describes a JSRuntime.gcFreeList element.
37684:  */
42771: union JSGCThing {
37684:     JSGCThing   *link;
42771:     double      asDouble;
37684: };
37684: 
37684: static inline JSGCThing *
37684: MakeNewArenaFreeList(JSGCArena *a, size_t thingSize)
37684: {
37684:     jsuword thingsStart = a->toPageStart();
40876:     jsuword lastThingMinAddr = thingsStart + GC_ARENA_SIZE - thingSize * 2 + 1;
37684:     jsuword thingPtr = thingsStart;
37684:     do {
37684:         jsuword nextPtr = thingPtr + thingSize;
40876:         JS_ASSERT((nextPtr & GC_ARENA_MASK) + thingSize <= GC_ARENA_SIZE);
37684:         JSGCThing *thing = reinterpret_cast<JSGCThing *>(thingPtr);
37684:         thing->link = reinterpret_cast<JSGCThing *>(nextPtr);
37684:         thingPtr = nextPtr;
37684:     } while (thingPtr < lastThingMinAddr);
37684: 
37684:     JSGCThing *lastThing = reinterpret_cast<JSGCThing *>(thingPtr);
37684:     lastThing->link = NULL;
37684:     return reinterpret_cast<JSGCThing *>(thingsStart);
37684: }
    1: 
    1: #ifdef JS_GCMETER
10954: # define METER(x)               ((void) (x))
10954: # define METER_IF(condition, x) ((void) ((condition) && (x)))
    1: #else
    1: # define METER(x)               ((void) 0)
10954: # define METER_IF(condition, x) ((void) 0)
    1: #endif
    1: 
10954: #define METER_UPDATE_MAX(maxLval, rval)                                       \
10954:     METER_IF((maxLval) < (rval), (maxLval) = (rval))
10954: 
40373: #ifdef MOZ_GCTIMER
40373: static jsrefcount newChunkCount = 0;
40373: static jsrefcount destroyChunkCount = 0;
40373: #endif
40373: 
40876: inline void *
40876: GetGCChunk(JSRuntime *rt)
32823: {
47400:     void *p = rt->gcChunkAllocator->alloc();
40373: #ifdef MOZ_GCTIMER
40876:     if (p)
40373:         JS_ATOMIC_INCREMENT(&newChunkCount);
40373: #endif
40876:     METER_IF(p, rt->gcStats.nchunks++);
40876:     METER_UPDATE_MAX(rt->gcStats.maxnchunks, rt->gcStats.nchunks);
40876:     return p;
34613: }
40876: 
40876: inline void
41286: ReleaseGCChunk(JSRuntime *rt, jsuword chunk)
32823: {
41286:     void *p = reinterpret_cast<void *>(chunk);
40876:     JS_ASSERT(p);
40373: #ifdef MOZ_GCTIMER
40373:     JS_ATOMIC_INCREMENT(&destroyChunkCount);
40373: #endif
40876:     JS_ASSERT(rt->gcStats.nchunks != 0);
40876:     METER(rt->gcStats.nchunks--);
47400:     rt->gcChunkAllocator->free(p);
32823: }
32823: 
37684: static JSGCArena *
34329: NewGCArena(JSContext *cx)
 5917: {
34329:     JSRuntime *rt = cx->runtime;
35078:     if (!JS_THREAD_DATA(cx)->waiveGCQuota && rt->gcBytes >= rt->gcMaxBytes) {
34329:         /*
34329:          * FIXME bug 524051 We cannot run a last-ditch GC on trace for now, so
35078:          * just pretend we are out of memory which will throw us off trace and
35078:          * we will re-try this code path from the interpreter.
34329:          */
34329:         if (!JS_ON_TRACE(cx))
32823:             return NULL;
34329:         js_TriggerGC(cx, true);
34329:     }
32823: 
41286:     size_t nchunks = rt->gcChunks.length();
41286: 
41286:     JSGCChunkInfo *ci;
41286:     for (;; ++rt->gcChunkCursor) {
41286:         if (rt->gcChunkCursor == nchunks) {
41286:             ci = NULL;
41286:             break;
41286:         }
41286:         ci = rt->gcChunks[rt->gcChunkCursor];
41286:         if (ci->numFreeArenas != 0)
41286:             break;
41286:     }
32823:     if (!ci) {
41286:         if (!rt->gcChunks.reserve(nchunks + 1))
41286:             return NULL;
40876:         void *chunkptr = GetGCChunk(rt);
40876:         if (!chunkptr)
12282:             return NULL;
41286:         ci = JSGCChunkInfo::fromChunk(reinterpret_cast<jsuword>(chunkptr));
41286:         ci->init(rt);
41286:         JS_ALWAYS_TRUE(rt->gcChunks.append(ci));
40886:     }
40876: 
40876:     /* Scan the bitmap for the first non-zero bit. */
40876:     jsbitmap *freeArenas = ci->getFreeArenaBitmap();
40876:     size_t arenaIndex = 0;
40876:     while (!*freeArenas) {
40876:         arenaIndex += JS_BITS_PER_WORD;
40876:         freeArenas++;
40872:     }
40876:     size_t bit = CountTrailingZeros(*freeArenas);
40876:     arenaIndex += bit;
40876:     JS_ASSERT(arenaIndex < GC_ARENAS_PER_CHUNK);
40876:     JS_ASSERT(*freeArenas & (jsuword(1) << bit));
40876:     *freeArenas &= ~(jsuword(1) << bit);
40876:     --ci->numFreeArenas;
40876: 
40876:     rt->gcBytes += GC_ARENA_SIZE;
40876:     METER(rt->gcStats.nallarenas++);
40876:     METER_UPDATE_MAX(rt->gcStats.maxnallarenas, rt->gcStats.nallarenas);
40876: 
41286:     return JSGCArena::fromChunkAndIndex(ci->getChunk(), arenaIndex);
32823: }
40872: 
40876: /*
41286:  * This function does not touch the arena or release its memory so code can
40876:  * still refer into it.
40876:  */
41286: static void
41286: ReleaseGCArena(JSRuntime *rt, JSGCArena *a)
41286: {
32823:     METER(rt->gcStats.afree++);
32823:     JS_ASSERT(rt->gcBytes >= GC_ARENA_SIZE);
32823:     rt->gcBytes -= GC_ARENA_SIZE;
40876:     JS_ASSERT(rt->gcStats.nallarenas != 0);
40876:     METER(rt->gcStats.nallarenas--);
40876: 
40876:     jsuword chunk = a->getChunk();
40876:     JSGCChunkInfo *ci = JSGCChunkInfo::fromChunk(chunk);
40876:     JS_ASSERT(ci->numFreeArenas <= GC_ARENAS_PER_CHUNK - 1);
40876:     jsbitmap *freeArenas = ci->getFreeArenaBitmap();
40876:     JS_ASSERT(!JS_TEST_BIT(freeArenas, a->getIndex()));
40876:     JS_SET_BIT(freeArenas, a->getIndex());
40876:     ci->numFreeArenas++;
41286:     if (ci->numFreeArenas == GC_ARENAS_PER_CHUNK)
40886:         ci->gcChunkAge = 0;
40876: 
40874: #ifdef DEBUG
41286:     a->getInfo()->prev = rt->gcEmptyArenaList;
41286:     rt->gcEmptyArenaList = a;
40874: #endif
40872: }
40876: 
41286: static void
41286: FreeGCChunks(JSRuntime *rt)
41286: {
40876: #ifdef DEBUG
41286:     while (rt->gcEmptyArenaList) {
41286:         JSGCArena *next = rt->gcEmptyArenaList->getInfo()->prev;
41286:         memset(rt->gcEmptyArenaList, JS_FREE_PATTERN, GC_ARENA_SIZE);
41286:         rt->gcEmptyArenaList = next;
32823:     }
40876: #endif
41286: 
41286:     /* Remove unused chunks. */
41286:     size_t available = 0;
41286:     for (JSGCChunkInfo **i = rt->gcChunks.begin(); i != rt->gcChunks.end(); ++i) {
41286:         JSGCChunkInfo *ci = *i;
41286:         JS_ASSERT(ci->runtime == rt);
41286:         if (ci->numFreeArenas == GC_ARENAS_PER_CHUNK) {
41286:             if (ci->gcChunkAge > GC_MAX_CHUNK_AGE) {
41286:                 ReleaseGCChunk(rt, ci->getChunk());
41286:                 continue;
40876:             }
41286:             ci->gcChunkAge++;
41286:         }
41286:         rt->gcChunks[available++] = ci;
41286:     }
41286:     rt->gcChunks.resize(available);
41286:     rt->gcChunkCursor = 0;
41286: }
32823: 
33582: static inline size_t
33582: GetFinalizableThingSize(unsigned thingKind)
33582: {
33582:     JS_STATIC_ASSERT(JS_EXTERNAL_STRING_LIMIT == 8);
33582: 
33582:     static const uint8 map[FINALIZE_LIMIT] = {
33582:         sizeof(JSObject),   /* FINALIZE_OBJECT */
33582:         sizeof(JSFunction), /* FINALIZE_FUNCTION */
33582: #if JS_HAS_XML_SUPPORT
33582:         sizeof(JSXML),      /* FINALIZE_XML */
33582: #endif
33582:         sizeof(JSString),   /* FINALIZE_STRING */
33582:         sizeof(JSString),   /* FINALIZE_EXTERNAL_STRING0 */
33582:         sizeof(JSString),   /* FINALIZE_EXTERNAL_STRING1 */
33582:         sizeof(JSString),   /* FINALIZE_EXTERNAL_STRING2 */
33582:         sizeof(JSString),   /* FINALIZE_EXTERNAL_STRING3 */
33582:         sizeof(JSString),   /* FINALIZE_EXTERNAL_STRING4 */
33582:         sizeof(JSString),   /* FINALIZE_EXTERNAL_STRING5 */
33582:         sizeof(JSString),   /* FINALIZE_EXTERNAL_STRING6 */
33582:         sizeof(JSString),   /* FINALIZE_EXTERNAL_STRING7 */
33582:     };
33582: 
33582:     JS_ASSERT(thingKind < FINALIZE_LIMIT);
33582:     return map[thingKind];
33582: }
33582: 
33582: static inline size_t
33952: GetFinalizableTraceKind(size_t thingKind)
33582: {
33582:     JS_STATIC_ASSERT(JS_EXTERNAL_STRING_LIMIT == 8);
33582: 
33582:     static const uint8 map[FINALIZE_LIMIT] = {
33582:         JSTRACE_OBJECT,     /* FINALIZE_OBJECT */
33582:         JSTRACE_OBJECT,     /* FINALIZE_FUNCTION */
33582: #if JS_HAS_XML_SUPPORT      /* FINALIZE_XML */
33582:         JSTRACE_XML,
33582: #endif                      /* FINALIZE_STRING */
33582:         JSTRACE_STRING,
33582:         JSTRACE_STRING,     /* FINALIZE_EXTERNAL_STRING0 */
33582:         JSTRACE_STRING,     /* FINALIZE_EXTERNAL_STRING1 */
33582:         JSTRACE_STRING,     /* FINALIZE_EXTERNAL_STRING2 */
33582:         JSTRACE_STRING,     /* FINALIZE_EXTERNAL_STRING3 */
33582:         JSTRACE_STRING,     /* FINALIZE_EXTERNAL_STRING4 */
33582:         JSTRACE_STRING,     /* FINALIZE_EXTERNAL_STRING5 */
33582:         JSTRACE_STRING,     /* FINALIZE_EXTERNAL_STRING6 */
33582:         JSTRACE_STRING,     /* FINALIZE_EXTERNAL_STRING7 */
33582:     };
33582: 
33952:     JS_ASSERT(thingKind < FINALIZE_LIMIT);
33952:     return map[thingKind];
33952: }
33952: 
33952: static inline size_t
40876: GetFinalizableArenaTraceKind(JSGCArenaInfo *ainfo)
33952: {
40876:     JS_ASSERT(ainfo->list);
40876:     return GetFinalizableTraceKind(ainfo->list->thingKind);
40876: }
40876: 
40876: static inline size_t
47439: GetArenaTraceKind(JSGCArenaInfo *ainfo)
47439: {
47439:     if (!ainfo->list)
47439:         return JSTRACE_DOUBLE;
47439:     return GetFinalizableArenaTraceKind(ainfo);
47439: }
47439: 
47439: static inline size_t
40876: GetFinalizableThingTraceKind(void *thing)
40876: {
40876:     JSGCArenaInfo *ainfo = JSGCArenaInfo::fromGCThing(thing);
40876:     return GetFinalizableArenaTraceKind(ainfo);
33582: }
33582: 
32823: static void
    1: InitGCArenaLists(JSRuntime *rt)
    1: {
33582:     for (unsigned i = 0; i != FINALIZE_LIMIT; ++i) {
33582:         JSGCArenaList *arenaList = &rt->gcArenaList[i];
33952:         arenaList->head = NULL;
33952:         arenaList->cursor = NULL;
33952:         arenaList->thingKind = i;
33947:         arenaList->thingSize = GetFinalizableThingSize(i);
    1:     }
33952:     rt->gcDoubleArenaList.head = NULL;
33747:     rt->gcDoubleArenaList.cursor = NULL;
    1: }
    1: 
    1: static void
    1: FinishGCArenaLists(JSRuntime *rt)
    1: {
33582:     for (unsigned i = 0; i < FINALIZE_LIMIT; i++) {
41286:         rt->gcArenaList[i].head = NULL;
41286:         rt->gcArenaList[i].cursor = NULL;
    1:     }
33952:     rt->gcDoubleArenaList.head = NULL;
33747:     rt->gcDoubleArenaList.cursor = NULL;
12282: 
 5917:     rt->gcBytes = 0;
41286: 
41286:     for (JSGCChunkInfo **i = rt->gcChunks.begin(); i != rt->gcChunks.end(); ++i)
41286:         ReleaseGCChunk(rt, (*i)->getChunk());
41286:     rt->gcChunks.clear();
41286:     rt->gcChunkCursor = 0;
    1: }
    1: 
 8005: intN
 8005: js_GetExternalStringGCType(JSString *str)
 8005: {
33582:     JS_STATIC_ASSERT(FINALIZE_STRING + 1 == FINALIZE_EXTERNAL_STRING0);
32686:     JS_ASSERT(!JSString::isStatic(str));
32686: 
40876:     unsigned thingKind = JSGCArenaInfo::fromGCThing(str)->list->thingKind;
33582:     JS_ASSERT(IsFinalizableStringKind(thingKind));
33582:     return intN(thingKind) - intN(FINALIZE_EXTERNAL_STRING0);
 8005: }
 8005: 
 8005: JS_FRIEND_API(uint32)
 8005: js_GetGCThingTraceKind(void *thing)
 8005: {
32734:     if (JSString::isStatic(thing))
32734:         return JSTRACE_STRING;
32734: 
40876:     JSGCArenaInfo *ainfo = JSGCArenaInfo::fromGCThing(thing);
47439:     return GetArenaTraceKind(ainfo);
    1: }
    1: 
    1: JSRuntime *
41286: js_GetGCThingRuntime(void *thing)
    1: {
41286:     jsuword chunk = JSGCArena::fromGCThing(thing)->getChunk();
41286:     return JSGCChunkInfo::fromChunk(chunk)->runtime;
    1: }
    1: 
36410: bool
36410: js_IsAboutToBeFinalized(void *thing)
    1: {
36660:     if (JSString::isStatic(thing))
36660:         return false;
36660: 
40876:     return !IsMarkedGCThing(thing);
    1: }
    1: 
    1: JSBool
32823: js_InitGC(JSRuntime *rt, uint32 maxbytes)
    1: {
    1:     InitGCArenaLists(rt);
42755: 
42755:     if (!rt->gcRootsHash.init(256))
36680:         return false;
42755: 
42755:     if (!rt->gcLocksHash.init(256))
36680:         return false;
    1: 
41801: #ifdef JS_THREADSAFE
41796:     if (!rt->gcHelperThread.init())
41796:         return false;
41801: #endif
41796: 
32553:     /*
32553:      * Separate gcMaxMallocBytes from gcMaxBytes but initialize to maxbytes
32553:      * for default backward API compatibility.
32553:      */
34288:     rt->gcMaxBytes = maxbytes;
34288:     rt->setGCMaxMallocBytes(maxbytes);
34288: 
32543:     rt->gcEmptyArenaPoolLifespan = 30000;
32543: 
31888:     /*
32553:      * By default the trigger factor gets maximum possible value. This
32553:      * means that GC will not be triggered by growth of GC memory (gcBytes).
31888:      */
32553:     rt->setGCTriggerFactor((uint32) -1);
32553: 
32553:     /*
32553:      * The assigned value prevents GC from running when GC memory is too low
32553:      * (during JS engine start).
32553:      */
32553:     rt->setGCLastBytes(8192);
24313: 
40229:     METER(PodZero(&rt->gcStats));
36680:     return true;
    1: }
    1: 
47439: namespace js {
47439: 
47439: struct GCChunkHasher
47439: {
47439:     typedef jsuword Lookup;
47439:     static HashNumber hash(jsuword chunk) {
47439:         /*
47439:          * Strip zeros for better distribution after multiplying by the golden
47439:          * ratio.
47439:          */
47439:         JS_ASSERT(!(chunk & GC_CHUNK_MASK));
47439:         return HashNumber(chunk >> GC_CHUNK_SHIFT);
47439:     }
47439:     static bool match(jsuword k, jsuword l) {
47439:         JS_ASSERT(!(k & GC_CHUNK_MASK));
47439:         JS_ASSERT(!(l & GC_CHUNK_MASK));
47439:         return k == l;
47439:     }
47439: };
47439: 
47439: class ConservativeGCStackMarker {
47439:   public:
47439:     ConservativeGCStackMarker(JSTracer *trc);
47439: 
47439:     ~ConservativeGCStackMarker() {
47439: #ifdef JS_DUMP_CONSERVATIVE_GC_ROOTS
47439:         dumpConservativeRoots();
47439: #endif
47439: #ifdef JS_GCMETER
47439:         JSConservativeGCStats *total = &trc->context->runtime->gcStats.conservative;
47439:         total->words        += stats.words;
47439:         total->oddaddress   += stats.oddaddress;
47510:         total->special      += stats.special;
47439:         total->notarena     += stats.notarena;
47439:         total->notchunk     += stats.notchunk;
47439:         total->freearena    += stats.freearena;
47439:         total->wrongtag     += stats.wrongtag;
47439:         total->notlive      += stats.notlive;
47439:         total->gcthings     += stats.gcthings;
47439:         total->raw          += stats.raw;
47439:         total->unmarked     += stats.unmarked;
47439: #endif
47439:     }
47439: 
47439:     void markRoots();
47439: 
47439:   private:
47439:     void markRange(jsuword *begin, jsuword *end);
47439:     void markWord(jsuword w);
47439: 
47439:     JSTracer *trc;
47439:     HashSet<jsuword, GCChunkHasher, SystemAllocPolicy> chunkSet;
47439: 
47439: #if defined(JS_DUMP_CONSERVATIVE_GC_ROOTS) || defined(JS_GCMETER)
47439:     JSConservativeGCStats stats;
47439: 
47439:   public:
47439:     static void dumpStats(FILE *fp, JSConservativeGCStats *stats);
47439: 
47439: # define CONSERVATIVE_METER(x)  ((void) (x))
47439: # define CONSERVATIVE_METER_IF(condition, x) ((void) ((condition) && (x)))
47439: 
47439: #else
47439: 
47439: # define CONSERVATIVE_METER(x)                  ((void) 0)
47439: # define CONSERVATIVE_METER_IF(condition, x)    ((void) 0)
47439: 
47439: #endif
47439: 
47439: #ifdef JS_DUMP_CONSERVATIVE_GC_ROOTS
47439:   private:
47439:     struct ConservativeRoot { void *thing; uint32 traceKind; };
47439:     Vector<ConservativeRoot, 0, SystemAllocPolicy> conservativeRoots;
47439:     const char *dumpFileName;
47439: 
47439:     void dumpConservativeRoots();
47439: #endif
47439: };
47439: 
47439: ConservativeGCStackMarker::ConservativeGCStackMarker(JSTracer *trc)
47439:   : trc(trc)
47439: {
47439:     /*
47439:      * If initializing fails because we are out of memory, stack scanning
47439:      * slows down but is otherwise unaffected.
47439:      */
47439:     JSRuntime *rt = trc->context->runtime;
47510:     if (chunkSet.init(rt->gcChunks.length())) {
47439:         for (JSGCChunkInfo **i = rt->gcChunks.begin(); i != rt->gcChunks.end(); ++i) {
47439:             jsuword chunk = (*i)->getChunk();
47439:             JS_ASSERT(!chunkSet.has(chunk));
47439:             JS_ALWAYS_TRUE(chunkSet.put(chunk));
47439:         }
47510:     }
47439: 
47439: #ifdef JS_DUMP_CONSERVATIVE_GC_ROOTS
47439:     dumpFileName = getenv("JS_DUMP_CONSERVATIVE_GC_ROOTS");
47439:     memset(&stats, 0, sizeof(stats));
47439: #endif
47439: }
47439: 
47439: #if defined(JS_DUMP_CONSERVATIVE_GC_ROOTS) || defined(JS_GCMETER)
47439: /* static */
47439: void
47439: ConservativeGCStackMarker::dumpStats(FILE *fp, JSConservativeGCStats *stats)
47439: {
47439: #define ULSTAT(x)       ((unsigned long)(stats->x))
47439:     fprintf(fp, "CONSERVATIVE STACK SCANNING:\n");
47439:     fprintf(fp, "      number of stack words: %lu\n", ULSTAT(words));
47439:     fprintf(fp, "      excluded, low bit set: %lu\n", ULSTAT(oddaddress));
47510:     fprintf(fp, "          excluded, special: %lu\n", ULSTAT(special));
47510:     fprintf(fp, "        not withing a chunk: %lu\n", ULSTAT(notchunk));
47439:     fprintf(fp, "     not within arena range: %lu\n", ULSTAT(notarena));
47439:     fprintf(fp, "       points to free arena: %lu\n", ULSTAT(freearena));
47439:     fprintf(fp, "        excluded, wrong tag: %lu\n", ULSTAT(wrongtag));
47439:     fprintf(fp, "         excluded, not live: %lu\n", ULSTAT(notlive));
47439:     fprintf(fp, "              things marked: %lu\n", ULSTAT(gcthings));
47439:     fprintf(fp, "        raw pointers marked: %lu\n", ULSTAT(raw));
47439:     fprintf(fp, "         conservative roots: %lu\n", ULSTAT(unmarked));
47439: #undef ULSTAT
47439: }
47439: #endif
47439: 
47439: #ifdef JS_DUMP_CONSERVATIVE_GC_ROOTS
47439: void
47439: ConservativeGCStackMarker::dumpConservativeRoots()
47439: {
47439:     if (!dumpFileName)
47439:         return;
47439: 
47439:     JS_ASSERT(stats.unmarked == conservativeRoots.length());
47439: 
47439:     FILE *fp;
47439:     if (!strcmp(dumpFileName, "stdout")) {
47439:         fp = stdout;
47439:     } else if (!strcmp(dumpFileName, "stderr")) {
47439:         fp = stderr;
47439:     } else if (!(fp = fopen(dumpFileName, "aw"))) {
47439:         fprintf(stderr,
47439:                 "Warning: cannot open %s to dump the conservative roots\n",
47439:                 dumpFileName);
47439:         return;
47439:     }
47439: 
47439:     dumpStats(fp, &stats);
47439:     for (ConservativeRoot *i = conservativeRoots.begin();
47439:          i != conservativeRoots.end();
47439:          ++i) {
47439:         fprintf(fp, "  %p: ", i->thing);
47439:         switch (i->traceKind) {
47439:           default:
47439:             JS_NOT_REACHED("Unknown trace kind");
47439: 
47439:           case JSTRACE_OBJECT: {
47439:             JSObject *obj = (JSObject *) i->thing;
47439:             fprintf(fp, "object %s", obj->getClass()->name);
47439:             break;
47439:           }
47439:           case JSTRACE_STRING: {
47439:             JSString *str = (JSString *) i->thing;
47439:             char buf[50];
47439:             js_PutEscapedString(buf, sizeof buf, str, '"');
47439:             fprintf(fp, "string %s", buf);
47439:             break;
47439:           }
47439:           case JSTRACE_DOUBLE: {
47439:             jsdouble *dp = (jsdouble *) i->thing;
47439:             fprintf(fp, "double %e", *dp);
47439:             break;
47439:           }
47439: # if JS_HAS_XML_SUPPORT
47439:           case JSTRACE_XML: {
47439:             JSXML *xml = (JSXML *) i->thing;
47439:             fprintf(fp, "xml %u", xml->xml_class);
47439:             break;
47439:           }
47439: # endif
47439:         }
47439:         fputc('\n', fp);
47439:     }
47439:     fputc('\n', fp);
47439: 
47439:     if (fp != stdout && fp != stderr)
47439:         fclose(fp);
47439: }
47439: #endif /* JS_DUMP_CONSERVATIVE_GC_ROOTS */
47439: 
47439: void
47439: ConservativeGCStackMarker::markWord(jsuword w)
47439: {
47439: #define RETURN(x) do { CONSERVATIVE_METER(stats.x++); return; } while (0)
47491:     /*
47491:      * We assume that the compiler never uses sub-word alignment to store
47491:      * pointers and does not tag pointers on its own. Thus we exclude words
47491:      * with JSVAL_INT (any odd words) or JSVAL_SPECIAL tags as they never
47491:      * point to GC things. We also exclude words with a double tag that point
47491:      * into a non-double. But, for example, on 32-bit platforms we cannot
47491:      * exclude a pointer into an object arena tagged with JSVAL_STRING. The
47491:      * latter is 4 and a compiler can store a pointer not to the object but
47491:      * rather a pointer to its second field.
47491:      */
47491:     JS_STATIC_ASSERT(JSVAL_INT == 1);
47491:     JS_STATIC_ASSERT(JSVAL_DOUBLE == 2);
47491:     JS_STATIC_ASSERT(JSVAL_STRING == 4);
47491:     JS_STATIC_ASSERT(JSVAL_SPECIAL == 6);
47491: 
47439:     if (w & 1)
47439:         RETURN(oddaddress);
47439: 
47439:     /* Strip off the tag bits. */
47439:     jsuword tag = w & JSVAL_TAGMASK;
47439: 
47439:     if (tag == JSVAL_SPECIAL)
47510:         RETURN(special);
47510: 
47510:     jsuword chunk = w & ~GC_CHUNK_MASK;
47439:     JSGCChunkInfo *ci;
47439:     if (JS_LIKELY(chunkSet.initialized())) {
47439:         if (!chunkSet.has(chunk))
47439:             RETURN(notchunk);
47439:         ci = JSGCChunkInfo::fromChunk(chunk);
47439:     } else {
47439:         ci = JSGCChunkInfo::fromChunk(chunk);
47510:         for (JSGCChunkInfo **i = trc->context->runtime->gcChunks.begin(); ; ++i) {
47510:             if (i == trc->context->runtime->gcChunks.end())
47439:                 RETURN(notchunk);
47439:             if (*i == ci)
47439:                 break;
47439:         }
47439:     }
47439: 
47510:     if ((w & GC_CHUNK_MASK) >= GC_MARK_BITMAP_ARRAY_OFFSET)
47510:         RETURN(notarena);
47510: 
47510:     size_t arenaIndex = (w & GC_CHUNK_MASK) >> GC_ARENA_SHIFT;
47439:     if (JS_TEST_BIT(ci->getFreeArenaBitmap(), arenaIndex))
47439:         RETURN(freearena);
47439: 
47439:     JSGCArena *a = JSGCArena::fromChunkAndIndex(chunk, arenaIndex);
47439:     JSGCArenaInfo *ainfo = a->getInfo();
47439: 
47439:     JSGCThing *thing;
47491:     uint32 traceKind;
47439:     if (!ainfo->list) { /* doubles */
47439:         if (tag && tag != JSVAL_DOUBLE)
47439:             RETURN(wrongtag);
47439:         JS_STATIC_ASSERT(JSVAL_TAGMASK == 7 && (sizeof(double) - 1) == 7);
47510:         thing = (JSGCThing *) (w & ~JSVAL_TAGMASK);
47491:         traceKind = JSTRACE_DOUBLE;
47439:     } else {
47439:         if (tag == JSVAL_DOUBLE)
47439:             RETURN(wrongtag);
47491:         traceKind = GetFinalizableArenaTraceKind(ainfo);
47491: #if JS_BYTES_PER_WORD == 8
47511:         if (tag == JSVAL_STRING && traceKind != JSTRACE_STRING)
47491:             RETURN(wrongtag);
47491: #endif
47491: 
47439:         jsuword start = a->toPageStart();
47510:         jsuword offset = w - start;
47439:         size_t thingSize = ainfo->list->thingSize;
47491:         offset -= offset % thingSize;
47491: 
47491:         /*
47491:          * If GC_ARENA_SIZE % thingSize != 0 or when thingSize is not a power
47491:          * of two, thingSize-aligned pointer may point at the end of the last
47491:          * thing yet be inside the arena.
47491:          */
47491:         if (offset + thingSize > GC_ARENA_SIZE) {
47491:             JS_ASSERT(thingSize & (thingSize - 1));
47491:             RETURN(notarena);
47491:         }
47491:         thing = (JSGCThing *) (start + offset);
47439: 
47439:         /* Make sure the thing is not on the freelist of the arena. */
47439:         JSGCThing *cursor = ainfo->freeList;
47439:         while (cursor) {
47456:             JS_ASSERT((((jsuword) cursor) & GC_ARENA_MASK) % thingSize == 0);
47456:             JS_ASSERT(!IsMarkedGCThing(cursor));
47491: 
47439:             /* If the cursor moves past the thing, it's not in the freelist. */
47439:             if (thing < cursor)
47439:                 break;
47439: 
47439:             /* If we find it on the freelist, it's dead. */
47439:             if (thing == cursor)
47439:                 RETURN(notlive);
47456:             JS_ASSERT_IF(cursor->link, cursor < cursor->link);
47439:             cursor = cursor->link;
47439:         }
47439:     }
47439: 
47439:     CONSERVATIVE_METER(stats.gcthings++);
47439:     CONSERVATIVE_METER_IF(!tag, stats.raw++);
47439: 
47439:     /*
47439:      * We have now a valid pointer, that is either raw or tagged properly.
47439:      * Since we do not rely on the conservative scanning yet and assume that
47439:      * all the roots are precisely reported, any unmarked GC things here mean
47439:      * those things leaked.
47439:      */
47439:     if (IS_GC_MARKING_TRACER(trc)) {
47439:         if (!js_IsAboutToBeFinalized(thing))
47439:             return;
47439:         CONSERVATIVE_METER(stats.unmarked++);
47439:     }
47439: 
47439: #ifdef JS_DUMP_CONSERVATIVE_GC_ROOTS
47439:     if (IS_GC_MARKING_TRACER(trc) && dumpFileName) {
47439:         ConservativeRoot root = {thing, traceKind};
47439:         conservativeRoots.append(root);
47439:     }
47439: #endif
47439:     JS_SET_TRACING_NAME(trc, "machine stack");
47439:     js_CallGCMarker(trc, thing, traceKind);
47439: 
47439: #undef RETURN
47439: }
47439: 
47439: void
47439: ConservativeGCStackMarker::markRange(jsuword *begin, jsuword *end)
47439: {
47439:     JS_ASSERT(begin <= end);
47439:     for (jsuword *i = begin; i != end; ++i) {
47439:         CONSERVATIVE_METER(stats.words++);
47439:         markWord(*i);
47439:     }
47439: }
47439: 
47439: void
47439: ConservativeGCStackMarker::markRoots()
47439: {
47456:     /* Do conservative scanning of the stack and registers. */
47439:     for (ThreadDataIter i(trc->context->runtime); !i.empty(); i.popFront()) {
47439:         JSThreadData *td = i.threadData();
47439:         ConservativeGCThreadData *ctd = &td->conservativeGC;
47439:         if (ctd->isEnabled()) {
47456:             jsuword *stackMin, *stackEnd;
47439: #if JS_STACK_GROWTH_DIRECTION > 0
47456:             stackMin = td->nativeStackBase;
47456:             stackEnd = ctd->nativeStackTop;
47439: #else
47456:             stackMin = ctd->nativeStackTop + 1;
47456:             stackEnd = td->nativeStackBase;
47439: #endif
47456:             JS_ASSERT(stackMin <= stackEnd);
47456:             markRange(stackMin, stackEnd);
47439:             markRange(ctd->registerSnapshot.words,
47439:                       JS_ARRAY_END(ctd->registerSnapshot.words));
47439:         }
47439:     }
47439: }
47439: 
47439: /* static */
47439: JS_NEVER_INLINE JS_FRIEND_API(void)
47439: ConservativeGCThreadData::enable(bool knownStackBoundary)
47439: {
47439:     ++enableCount;
47439:     if (enableCount <= 0)
47439:         return;
47439: 
47439:     /* Update the native stack pointer if it points to a bigger stack. */
47439: #if JS_STACK_GROWTH_DIRECTION > 0
47439: # define CMP >
47439: #else
47439: # define CMP <
47439: #endif
47439:     jsuword dummy;
47439:     if (knownStackBoundary || enableCount == 1 || &dummy CMP nativeStackTop)
47439:         nativeStackTop = &dummy;
47439: #undef CMP
47439: 
47439:     /* Update the register snapshot with the latest values. */
47439: #if defined(_MSC_VER)
47439: # pragma warning(push)
47439: # pragma warning(disable: 4611)
47439: #endif
47439:     setjmp(registerSnapshot.jmpbuf);
47439: #if defined(_MSC_VER)
47439: # pragma warning(pop)
47439: #endif
47439: 
47439: }
47439: 
47439: JS_NEVER_INLINE JS_FRIEND_API(void)
47439: ConservativeGCThreadData::disable()
47439: {
47439:     --enableCount;
47439: #ifdef DEBUG
47439:     if (enableCount == 0)
47439:         nativeStackTop = NULL;
47439: #endif
47439: }
47439: 
47439: } /* namespace js */
47439: 
47439: 
    1: #ifdef JS_GCMETER
12282: 
12282: static void
12282: UpdateArenaStats(JSGCArenaStats *st, uint32 nlivearenas, uint32 nkilledArenas,
12282:                  uint32 nthings)
12282: {
12282:     size_t narenas;
12282: 
12282:     narenas = nlivearenas + nkilledArenas;
12282:     JS_ASSERT(narenas >= st->livearenas);
12282: 
12282:     st->newarenas = narenas - st->livearenas;
12282:     st->narenas = narenas;
12282:     st->livearenas = nlivearenas;
12282:     if (st->maxarenas < narenas)
12282:         st->maxarenas = narenas;
12282:     st->totalarenas += narenas;
12282: 
12282:     st->nthings = nthings;
12282:     if (st->maxthings < nthings)
12282:         st->maxthings = nthings;
12282:     st->totalthings += nthings;
12282: }
12282: 
    1: JS_FRIEND_API(void)
    1: js_DumpGCStats(JSRuntime *rt, FILE *fp)
    1: {
36680:     static const char *const GC_ARENA_NAMES[] = {
36680:         "double",
36680:         "object",
36680:         "function",
36680: #if JS_HAS_XML_SUPPORT
36680:         "xml",
36680: #endif
36680:         "string",
36680:         "external_string_0",
36680:         "external_string_1",
36680:         "external_string_2",
36680:         "external_string_3",
36680:         "external_string_4",
36680:         "external_string_5",
36680:         "external_string_6",
36680:         "external_string_7",
36680:     };
36680:     JS_STATIC_ASSERT(JS_ARRAY_LENGTH(GC_ARENA_NAMES) == FINALIZE_LIMIT + 1);
36680: 
36680:     fprintf(fp, "\nGC allocation statistics:\n\n");
    1: 
    1: #define UL(x)       ((unsigned long)(x))
    1: #define ULSTAT(x)   UL(rt->gcStats.x)
12282: #define PERCENT(x,y)  (100.0 * (double) (x) / (double) (y))
12282: 
36680:     size_t sumArenas = 0;
36680:     size_t sumTotalArenas = 0;
36680:     size_t sumThings = 0;
36680:     size_t sumMaxThings = 0;
36680:     size_t sumThingSize = 0;
36680:     size_t sumTotalThingSize = 0;
36680:     size_t sumArenaCapacity = 0;
36680:     size_t sumTotalArenaCapacity = 0;
36680:     size_t sumAlloc = 0;
36680:     size_t sumLocalAlloc = 0;
36680:     size_t sumFail = 0;
36680:     size_t sumRetry = 0;
36680:     for (int i = -1; i < (int) FINALIZE_LIMIT; i++) {
36680:         size_t thingSize, thingsPerArena;
36680:         JSGCArenaStats *st;
12282:         if (i == -1) {
12282:             thingSize = sizeof(jsdouble);
12282:             thingsPerArena = DOUBLES_PER_ARENA;
12282:             st = &rt->gcStats.doubleArenaStats;
12282:         } else {
12282:             thingSize = rt->gcArenaList[i].thingSize;
37684:             thingsPerArena = ThingsPerArena(thingSize);
12282:             st = &rt->gcStats.arenaStats[i];
36680:         }
36680:         if (st->maxarenas == 0)
36680:             continue;
12282:         fprintf(fp,
36680:                 "%s arenas (thing size %lu, %lu things per arena):",
36680:                 GC_ARENA_NAMES[i + 1], UL(thingSize), UL(thingsPerArena));
12282:         putc('\n', fp);
12282:         fprintf(fp, "           arenas before GC: %lu\n", UL(st->narenas));
12282:         fprintf(fp, "       new arenas before GC: %lu (%.1f%%)\n",
12282:                 UL(st->newarenas), PERCENT(st->newarenas, st->narenas));
12282:         fprintf(fp, "            arenas after GC: %lu (%.1f%%)\n",
12282:                 UL(st->livearenas), PERCENT(st->livearenas, st->narenas));
12282:         fprintf(fp, "                 max arenas: %lu\n", UL(st->maxarenas));
12282:         fprintf(fp, "                     things: %lu\n", UL(st->nthings));
12282:         fprintf(fp, "        GC cell utilization: %.1f%%\n",
12282:                 PERCENT(st->nthings, thingsPerArena * st->narenas));
12282:         fprintf(fp, "   average cell utilization: %.1f%%\n",
12282:                 PERCENT(st->totalthings, thingsPerArena * st->totalarenas));
12282:         fprintf(fp, "                 max things: %lu\n", UL(st->maxthings));
12282:         fprintf(fp, "             alloc attempts: %lu\n", UL(st->alloc));
36680:         fprintf(fp, "        alloc without locks: %lu  (%.1f%%)\n",
12282:                 UL(st->localalloc), PERCENT(st->localalloc, st->alloc));
12282:         sumArenas += st->narenas;
12282:         sumTotalArenas += st->totalarenas;
12282:         sumThings += st->nthings;
12282:         sumMaxThings += st->maxthings;
12282:         sumThingSize += thingSize * st->nthings;
39928:         sumTotalThingSize += size_t(thingSize * st->totalthings);
12282:         sumArenaCapacity += thingSize * thingsPerArena * st->narenas;
12282:         sumTotalArenaCapacity += thingSize * thingsPerArena * st->totalarenas;
12282:         sumAlloc += st->alloc;
12282:         sumLocalAlloc += st->localalloc;
12282:         sumFail += st->fail;
12282:         sumRetry += st->retry;
36680:         putc('\n', fp);
    1:     }
36680: 
36680:     fputs("Never used arenas:\n", fp);
36680:     for (int i = -1; i < (int) FINALIZE_LIMIT; i++) {
36680:         size_t thingSize, thingsPerArena;
36680:         JSGCArenaStats *st;
36680:         if (i == -1) {
36680:             thingSize = sizeof(jsdouble);
36680:             thingsPerArena = DOUBLES_PER_ARENA;
36680:             st = &rt->gcStats.doubleArenaStats;
36680:         } else {
36680:             thingSize = rt->gcArenaList[i].thingSize;
37684:             thingsPerArena = ThingsPerArena(thingSize);
36680:             st = &rt->gcStats.arenaStats[i];
36680:         }
36680:         if (st->maxarenas != 0)
36680:             continue;
36680:         fprintf(fp,
36680:                 "%s (thing size %lu, %lu things per arena)\n",
36680:                 GC_ARENA_NAMES[i + 1], UL(thingSize), UL(thingsPerArena));
36680:     }
36680:     fprintf(fp, "\nTOTAL STATS:\n");
    1:     fprintf(fp, "            bytes allocated: %lu\n", UL(rt->gcBytes));
    1:     fprintf(fp, "            total GC arenas: %lu\n", UL(sumArenas));
12282:     fprintf(fp, "            total GC things: %lu\n", UL(sumThings));
12282:     fprintf(fp, "        max total GC things: %lu\n", UL(sumMaxThings));
12282:     fprintf(fp, "        GC cell utilization: %.1f%%\n",
12282:             PERCENT(sumThingSize, sumArenaCapacity));
12282:     fprintf(fp, "   average cell utilization: %.1f%%\n",
12282:             PERCENT(sumTotalThingSize, sumTotalArenaCapacity));
12282:     fprintf(fp, "allocation retries after GC: %lu\n", UL(sumRetry));
12282:     fprintf(fp, "             alloc attempts: %lu\n", UL(sumAlloc));
36680:     fprintf(fp, "        alloc without locks: %lu  (%.1f%%)\n",
12282:             UL(sumLocalAlloc), PERCENT(sumLocalAlloc, sumAlloc));
12282:     fprintf(fp, "        allocation failures: %lu\n", UL(sumFail));
    1:     fprintf(fp, "         things born locked: %lu\n", ULSTAT(lockborn));
    1:     fprintf(fp, "           valid lock calls: %lu\n", ULSTAT(lock));
    1:     fprintf(fp, "         valid unlock calls: %lu\n", ULSTAT(unlock));
    1:     fprintf(fp, "       mark recursion depth: %lu\n", ULSTAT(depth));
    1:     fprintf(fp, "     maximum mark recursion: %lu\n", ULSTAT(maxdepth));
    1:     fprintf(fp, "     mark C recursion depth: %lu\n", ULSTAT(cdepth));
    1:     fprintf(fp, "   maximum mark C recursion: %lu\n", ULSTAT(maxcdepth));
36410:     fprintf(fp, "      delayed tracing calls: %lu\n", ULSTAT(unmarked));
    1: #ifdef DEBUG
36410:     fprintf(fp, "      max trace later count: %lu\n", ULSTAT(maxunmarked));
    1: #endif
    1:     fprintf(fp, "potentially useful GC calls: %lu\n", ULSTAT(poke));
32823:     fprintf(fp, "  thing arenas freed so far: %lu\n", ULSTAT(afree));
    1:     fprintf(fp, "     stack segments scanned: %lu\n", ULSTAT(stackseg));
    1:     fprintf(fp, "stack segment slots scanned: %lu\n", ULSTAT(segslots));
    1:     fprintf(fp, "reachable closeable objects: %lu\n", ULSTAT(nclose));
    1:     fprintf(fp, "    max reachable closeable: %lu\n", ULSTAT(maxnclose));
    1:     fprintf(fp, "      scheduled close hooks: %lu\n", ULSTAT(closelater));
    1:     fprintf(fp, "  max scheduled close hooks: %lu\n", ULSTAT(maxcloselater));
12282: 
47439:     ConservativeGCStackMarker::dumpStats(fp, &rt->gcStats.conservative);
47439: 
    1: #undef UL
12282: #undef ULSTAT
12282: #undef PERCENT
    1: }
    1: #endif
    1: 
    1: #ifdef DEBUG
    1: static void
    1: CheckLeakedRoots(JSRuntime *rt);
    1: #endif
    1: 
    1: void
    1: js_FinishGC(JSRuntime *rt)
    1: {
    1: #ifdef JS_ARENAMETER
    1:     JS_DumpArenaStats(stdout);
    1: #endif
    1: #ifdef JS_GCMETER
36680:     if (JS_WANT_GC_METER_PRINT)
    1:         js_DumpGCStats(rt, stdout);
    1: #endif
    1: 
41801: #ifdef JS_THREADSAFE
41796:     rt->gcHelperThread.cancel();
41801: #endif
    1:     FinishGCArenaLists(rt);
    1: 
    1: #ifdef DEBUG
42755:     if (!rt->gcRootsHash.empty())
    1:         CheckLeakedRoots(rt);
    1: #endif
42755:     rt->gcRootsHash.clear();
42755:     rt->gcLocksHash.clear();
    1: }
    1: 
    1: JSBool
47403: js_AddRoot(JSContext *cx, jsval *vp, const char *name)
    1: {
47403:     JSBool ok = js_AddRootRT(cx->runtime, vp, name);
    1:     if (!ok)
    1:         JS_ReportOutOfMemory(cx);
    1:     return ok;
    1: }
    1: 
    1: JSBool
47403: js_AddGCThingRoot(JSContext *cx, void **rp, const char *name)
47403: {
47403:     JSBool ok = js_AddGCThingRootRT(cx->runtime, rp, name);
47403:     if (!ok)
47403:         JS_ReportOutOfMemory(cx);
47403:     return ok;
47403: }
47403: 
47403: static JSBool
47403: AddRoot(JSRuntime *rt, void *rp, const char *name)
    1: {
42755:     js::GCRoots *roots = &rt->gcRootsHash;
    1: 
    1:     /*
    1:      * Due to the long-standing, but now removed, use of rt->gcLock across the
    1:      * bulk of js_GC, API users have come to depend on JS_AddRoot etc. locking
    1:      * properly with a racing GC, without calling JS_AddRoot from a request.
    1:      * We have to preserve API compatibility here, now that we avoid holding
    1:      * rt->gcLock across the mark phase (including the root hashtable mark).
    1:      */
40840:     AutoLockGC lock(rt);
24871:     js_WaitForGC(rt);
42755:     return !!roots->put(rp, name);
    1: }
    1: 
47403: JS_FRIEND_API(JSBool)
47403: js_AddRootRT(JSRuntime *rt, jsval *vp, const char *name)
47403: {
47403:     return AddRoot(rt, vp, name);
47403: }
47403: 
47403: JS_FRIEND_API(JSBool)
47403: js_AddGCThingRootRT(JSRuntime *rt, void **rp, const char *name)
47403: {
47403:     return AddRoot(rt, rp, name);
47403: }
47403: 
47403: JS_FRIEND_API(JSBool)
    1: js_RemoveRoot(JSRuntime *rt, void *rp)
    1: {
    1:     /*
47403:      * Due to the JS_RemoveRoot API, we may be called outside of a request.
    1:      * Same synchronization drill as above in js_AddRoot.
    1:      */
40840:     AutoLockGC lock(rt);
24871:     js_WaitForGC(rt);
42755:     rt->gcRootsHash.remove(rp);
    1:     rt->gcPoke = JS_TRUE;
    1:     return JS_TRUE;
    1: }
    1: 
    1: #ifdef DEBUG
    1: 
    1: static void
    1: CheckLeakedRoots(JSRuntime *rt)
    1: {
    1:     uint32 leakedroots = 0;
    1: 
    1:     /* Warn (but don't assert) debug builds of any remaining roots. */
42755:     for (GCRoots::Range r = rt->gcRootsHash.all(); !r.empty(); r.popFront()) {
42755:         leakedroots++;
42755:         fprintf(stderr,
42755:                 "JS engine warning: leaking GC root \'%s\' at %p\n",
42755:                 r.front().value ? r.front().value : "", r.front().key);
42755:     }
    1:     if (leakedroots > 0) {
    1:         if (leakedroots == 1) {
    1:             fprintf(stderr,
11799: "JS engine warning: 1 GC root remains after destroying the JSRuntime at %p.\n"
    1: "                   This root may point to freed memory. Objects reachable\n"
12282: "                   through it have not been finalized.\n",
12282:                     (void *) rt);
    1:         } else {
    1:             fprintf(stderr,
11799: "JS engine warning: %lu GC roots remain after destroying the JSRuntime at %p.\n"
    1: "                   These roots may point to freed memory. Objects reachable\n"
    1: "                   through them have not been finalized.\n",
12282:                     (unsigned long) leakedroots, (void *) rt);
    1:         }
    1:     }
    1: }
    1: 
    1: void
    1: js_DumpNamedRoots(JSRuntime *rt,
    1:                   void (*dump)(const char *name, void *rp, void *data),
    1:                   void *data)
    1: {
42755:     for (GCRoots::Range r = rt->gcRootsHash.all(); !r.empty(); r.popFront()) {
42755:         if (r.front().value)
42755:             dump(r.front().value, r.front().key, data);
    1:     }
42755: }
    1: 
    1: #endif /* DEBUG */
    1: 
    1: uint32
    1: js_MapGCRoots(JSRuntime *rt, JSGCRootMapFun map, void *data)
    1: {
40840:     AutoLockGC lock(rt);
42755:     intN mapflags;
42755:     uint32 i = 0;
42755: 
42755:     for (GCRoots::Enum e(rt->gcRootsHash); !e.empty(); e.popFront()) {
42755:         mapflags = map(e.front().key, e.front().value, data);
42755:         i++;
42755:         if (mapflags & JS_MAP_GCROOT_REMOVE)
42755:             e.removeFront();
42755:         if (mapflags & JS_MAP_GCROOT_STOP)
42755:             break;
42755:     }
42755:     return i;
    1: }
    1: 
32553: void
32553: JSRuntime::setGCTriggerFactor(uint32 factor)
32553: {
32553:     JS_ASSERT(factor >= 100);
32553: 
32553:     gcTriggerFactor = factor;
32553:     setGCLastBytes(gcLastBytes);
32553: }
32553: 
32553: void
32553: JSRuntime::setGCLastBytes(size_t lastBytes)
32553: {
32553:     gcLastBytes = lastBytes;
32553:     uint64 triggerBytes = uint64(lastBytes) * uint64(gcTriggerFactor / 100);
32553:     if (triggerBytes != size_t(triggerBytes))
32553:         triggerBytes = size_t(-1);
32553:     gcTriggerBytes = size_t(triggerBytes);
32553: }
32553: 
33952: void
33952: JSGCFreeLists::purge()
33952: {
33952:     /*
33952:      * Return the free list back to the arena so the GC finalization will not
33952:      * run the finalizers over unitialized bytes from free things.
33952:      */
33952:     for (JSGCThing **p = finalizables; p != JS_ARRAY_END(finalizables); ++p) {
34288:         JSGCThing *freeListHead = *p;
34288:         if (freeListHead) {
40876:             JSGCArenaInfo *ainfo = JSGCArenaInfo::fromGCThing(freeListHead);
40876:             JS_ASSERT(!ainfo->freeList);
40876:             ainfo->freeList = freeListHead;
34288:             *p = NULL;
34288:         }
33952:     }
33952:     doubles = NULL;
33952: }
33952: 
35075: void
35075: JSGCFreeLists::moveTo(JSGCFreeLists *another)
35075: {
35075:     *another = *this;
35075:     doubles = NULL;
40229:     PodArrayZero(finalizables);
35075:     JS_ASSERT(isEmpty());
35075: }
35075: 
33952: static inline bool
32553: IsGCThresholdReached(JSRuntime *rt)
32553: {
32553: #ifdef JS_GC_ZEAL
32553:     if (rt->gcZeal >= 1)
32553:         return true;
32553: #endif
32553: 
32553:     /*
32553:      * Since the initial value of the gcLastBytes parameter is not equal to
32553:      * zero (see the js_InitGC function) the return value is false when
32553:      * the gcBytes value is close to zero at the JS engine start.
32553:      */
34288:     return rt->isGCMallocLimitReached() || rt->gcBytes >= rt->gcTriggerBytes;
32553: }
32553: 
41294: static void
41294: LastDitchGC(JSContext *cx)
41294: {
41294:     JS_ASSERT(!JS_ON_TRACE(cx));
41294: 
41294:     /* The last ditch GC preserves weak roots and all atoms. */
41820:     AutoPreserveWeakRoots save(cx);
41294:     AutoKeepAtoms keep(cx->runtime);
41294: 
41294:     /*
41294:      * Keep rt->gcLock across the call into the GC so we don't starve and
41294:      * lose to racing threads who deplete the heap just after the GC has
41294:      * replenished it (or has synchronized with a racing GC that collected a
41294:      * bunch of garbage).  This unfair scheduling can happen on certain
41294:      * operating systems. For the gory details, see bug 162779.
41294:      */
41294:     js_GC(cx, GC_LOCK_HELD);
41294: }
41294: 
33952: static JSGCThing *
33952: RefillFinalizableFreeList(JSContext *cx, unsigned thingKind)
    1: {
47494:     JS_ASSERT(!JS_THREAD_DATA(cx)->gcFreeLists.finalizables[thingKind]);
33952:     JSRuntime *rt = cx->runtime;
40840:     JSGCArenaList *arenaList;
40840:     JSGCArena *a;
40840: 
40840:     {
40840:         AutoLockGC lock(rt);
33952:         JS_ASSERT(!rt->gcRunning);
33952:         if (rt->gcRunning) {
33952:             METER(rt->gcStats.finalfail++);
33952:             return NULL;
33952:         }
33952: 
35105:         bool canGC = !JS_ON_TRACE(cx) && !JS_THREAD_DATA(cx)->waiveGCQuota;
33952:         bool doGC = canGC && IsGCThresholdReached(rt);
40840:         arenaList = &rt->gcArenaList[thingKind];
    1:         for (;;) {
33952:             if (doGC) {
41294:                 LastDitchGC(cx);
36680:                 METER(cx->runtime->gcStats.arenaStats[thingKind].retry++);
33952:                 canGC = false;
33995: 
33995:                 /*
40840:                  * The JSGC_END callback can legitimately allocate new GC
40840:                  * things and populate the free list. If that happens, just
40840:                  * return that list head.
33995:                  */
47494:                 JSGCThing *freeList = JS_THREAD_DATA(cx)->gcFreeLists.finalizables[thingKind];
40840:                 if (freeList)
33995:                     return freeList;
33995:             }
    1: 
33952:             while ((a = arenaList->cursor) != NULL) {
40876:                 JSGCArenaInfo *ainfo = a->getInfo();
40876:                 arenaList->cursor = ainfo->prev;
40876:                 JSGCThing *freeList = ainfo->freeList;
33952:                 if (freeList) {
40876:                     ainfo->freeList = NULL;
33952:                     return freeList;
33952:                 }
33952:             }
33952: 
34329:             a = NewGCArena(cx);
33952:             if (a)
33952:                 break;
33952:             if (!canGC) {
36680:                 METER(cx->runtime->gcStats.arenaStats[thingKind].fail++);
33952:                 return NULL;
33952:             }
33952:             doGC = true;
33952:         }
33952: 
33952:         /*
33952:          * Do only minimal initialization of the arena inside the GC lock. We
33952:          * can do the rest outside the lock because no other threads will see
33952:          * the arena until the GC is run.
33952:          */
40876:         JSGCArenaInfo *ainfo = a->getInfo();
40876:         ainfo->list = arenaList;
40876:         ainfo->prev = arenaList->head;
40876:         ainfo->freeList = NULL;
33952:         arenaList->head = a;
40840:     }
33952: 
40876:     JSGCMarkingDelay *markingDelay = a->getMarkingDelay();
40876:     markingDelay->link = NULL;
40876:     markingDelay->unmarkedChildren = 0;
40876: 
37684:     return MakeNewArenaFreeList(a, arenaList->thingSize);
33952: }
33952: 
35075: static inline void
35075: CheckGCFreeListLink(JSGCThing *thing)
35075: {
35075:     /*
35075:      * The GC things on the free lists come from one arena and the things on
35075:      * the free list are linked in ascending address order.
35075:      */
35075:     JS_ASSERT_IF(thing->link,
37684:                  JSGCArena::fromGCThing(thing) ==
37684:                  JSGCArena::fromGCThing(thing->link));
35075:     JS_ASSERT_IF(thing->link, thing < thing->link);
35075: }
35075: 
33952: void *
35075: js_NewFinalizableGCThing(JSContext *cx, unsigned thingKind)
33952: {
33952:     JS_ASSERT(thingKind < FINALIZE_LIMIT);
33952: #ifdef JS_THREADSAFE
33952:     JS_ASSERT(cx->thread);
33952: #endif
33952: 
33952:     /* Updates of metering counters here may not be thread-safe. */
33952:     METER(cx->runtime->gcStats.arenaStats[thingKind].alloc++);
33952: 
33952:     JSGCThing **freeListp =
33952:         JS_THREAD_DATA(cx)->gcFreeLists.finalizables + thingKind;
33952:     JSGCThing *thing = *freeListp;
35075:     if (thing) {
35075:         *freeListp = thing->link;
35075:         cx->weakRoots.finalizableNewborns[thingKind] = thing;
35075:         CheckGCFreeListLink(thing);
36680:         METER(cx->runtime->gcStats.arenaStats[thingKind].localalloc++);
35075:         return thing;
35075:     }
35075: 
33952:     thing = RefillFinalizableFreeList(cx, thingKind);
47494:     if (!thing) {
47494:         js_ReportOutOfMemory(cx);
47494:         return NULL;
47494:     }
47494: 
33995:     /*
33995:      * See comments in RefillFinalizableFreeList about a possibility
33995:      * of *freeListp == thing.
33995:      */
33995:     JS_ASSERT(!*freeListp || *freeListp == thing);
33952:     *freeListp = thing->link;
33952: 
35075:     CheckGCFreeListLink(thing);
47494: 
33952:     cx->weakRoots.finalizableNewborns[thingKind] = thing;
    1: 
33952:     return thing;
    1: }
    1: 
33952: static JSGCThing *
37684: TurnUsedArenaIntoDoubleList(JSGCArena *a)
33747: {
33952:     JSGCThing *head;
33952:     JSGCThing **tailp = &head;
37684:     jsuword thing = a->toPageStart();
40876:     jsbitmap *markBitmap = a->getMarkBitmap();
40876:     jsbitmap *lastMarkWord = markBitmap + GC_MARK_BITMAP_WORDS - 1;
40876: 
40876:     for (jsbitmap *m = markBitmap; m <= lastMarkWord; ++m) {
40876:         JS_ASSERT(thing < a->toPageStart() + GC_ARENA_SIZE);
37684:         JS_ASSERT((thing - a->toPageStart()) %
33747:                   (JS_BITS_PER_WORD * sizeof(jsdouble)) == 0);
33747: 
33747:         jsbitmap bits = *m;
33747:         if (bits == jsbitmap(-1)) {
33952:             thing += JS_BITS_PER_WORD * sizeof(jsdouble);
33747:         } else {
33747:             /*
33747:              * We have some zero bits. Turn corresponding cells into a list
33747:              * unrolling the loop for better performance.
33747:              */
33747:             const unsigned unroll = 4;
33747:             const jsbitmap unrollMask = (jsbitmap(1) << unroll) - 1;
33747:             JS_STATIC_ASSERT((JS_BITS_PER_WORD & unrollMask) == 0);
33747: 
33747:             for (unsigned n = 0; n != JS_BITS_PER_WORD; n += unroll) {
33747:                 jsbitmap bitsChunk = bits & unrollMask;
33747:                 bits >>= unroll;
33747:                 if (bitsChunk == unrollMask) {
33952:                     thing += unroll * sizeof(jsdouble);
33747:                 } else {
33747: #define DO_BIT(bit)                                                           \
33747:                     if (!(bitsChunk & (jsbitmap(1) << (bit)))) {              \
37684:                         JS_ASSERT(thing - a->toPageStart() <=                 \
33747:                                   (DOUBLES_PER_ARENA - 1) * sizeof(jsdouble));\
33952:                         JSGCThing *t = reinterpret_cast<JSGCThing *>(thing);  \
33952:                         *tailp = t;                                           \
33952:                         tailp = &t->link;                                     \
33747:                     }                                                         \
33952:                     thing += sizeof(jsdouble);
33747:                     DO_BIT(0);
33747:                     DO_BIT(1);
33747:                     DO_BIT(2);
33747:                     DO_BIT(3);
33747: #undef DO_BIT
33747:                 }
33747:             }
33747:         }
33747:     }
33747:     *tailp = NULL;
33747:     return head;
33747: }
33747: 
33952: static JSGCThing *
12282: RefillDoubleFreeList(JSContext *cx)
    1: {
47494:     JS_ASSERT(!JS_THREAD_DATA(cx)->gcFreeLists.doubles);
33747: 
33747:     JSRuntime *rt = cx->runtime;
33747:     JS_ASSERT(!rt->gcRunning);
33747: 
33747:     JS_LOCK_GC(rt);
33747: 
35105:     bool canGC = !JS_ON_TRACE(cx) && !JS_THREAD_DATA(cx)->waiveGCQuota;
33747:     bool doGC = canGC && IsGCThresholdReached(rt);
40876:     JSGCArena *a;
33747:     for (;;) {
33747:         if (doGC) {
41294:             LastDitchGC(cx);
33747:             METER(rt->gcStats.doubleArenaStats.retry++);
33747:             canGC = false;
35075: 
35075:             /* See comments in RefillFinalizableFreeList. */
47494:             JSGCThing *freeList = JS_THREAD_DATA(cx)->gcFreeLists.doubles;
35075:             if (freeList) {
35075:                 JS_UNLOCK_GC(rt);
35075:                 return freeList;
35075:             }
33747:         }
33747: 
33747:         /*
33747:          * Loop until we find arena with some free doubles. We turn arenas
33747:          * into free lists outside the lock to minimize contention between
33747:          * threads.
33747:          */
33747:         while (!!(a = rt->gcDoubleArenaList.cursor)) {
40876:             rt->gcDoubleArenaList.cursor = a->getInfo()->prev;
33747:             JS_UNLOCK_GC(rt);
33952:             JSGCThing *list = TurnUsedArenaIntoDoubleList(a);
33747:             if (list)
33747:                 return list;
12282:             JS_LOCK_GC(rt);
12282:         }
34329:         a = NewGCArena(cx);
33747:         if (a)
33747:             break;
33747:         if (!canGC) {
12282:             METER(rt->gcStats.doubleArenaStats.fail++);
12282:             JS_UNLOCK_GC(rt);
12282:             return NULL;
    1:         }
33747:         doGC = true;
12282:     }
33747: 
40876:     JSGCArenaInfo *ainfo = a->getInfo();
40876:     ainfo->list = NULL;
40876:     ainfo->freeList = NULL;
40876:     ainfo->prev = rt->gcDoubleArenaList.head;
33952:     rt->gcDoubleArenaList.head = a;
12282:     JS_UNLOCK_GC(rt);
37684: 
37684:     return MakeNewArenaFreeList(a, sizeof(jsdouble));
12282: }
12282: 
    1: JSBool
12850: js_NewDoubleInRootedValue(JSContext *cx, jsdouble d, jsval *vp)
    1: {
12282:     /* Updates of metering counters here are not thread-safe. */
36680:     METER(cx->runtime->gcStats.doubleArenaStats.alloc++);
33747: 
35075:     JSGCThing **freeListp = &JS_THREAD_DATA(cx)->gcFreeLists.doubles;
35075:     JSGCThing *thing = *freeListp;
35075:     if (thing) {
36680:         METER(cx->runtime->gcStats.doubleArenaStats.localalloc++);
35075:         CheckGCFreeListLink(thing);
35075:         *freeListp = thing->link;
35075: 
42771:         jsdouble *dp = &thing->asDouble;
35075:         *dp = d;
35075:         *vp = DOUBLE_TO_JSVAL(dp);
35075:         return true;
35075:     }
35075: 
33952:     thing = RefillDoubleFreeList(cx);
47494:     if (!thing) {
35075:         if (!JS_ON_TRACE(cx)) {
35075:             /* Trace code handle this on its own. */
35075:             js_ReportOutOfMemory(cx);
36680:             METER(cx->runtime->gcStats.doubleArenaStats.fail++);
35075:         }
33747:         return false;
    1:     }
35075: 
47494:     JS_ASSERT(!*freeListp || *freeListp == thing);
47494: 
35075:     CheckGCFreeListLink(thing);
35075:     *freeListp = thing->link;
35075: 
33952:     jsdouble *dp = reinterpret_cast<jsdouble *>(thing);
33952:     *dp = d;
33952:     *vp = DOUBLE_TO_JSVAL(dp);
47494:     return true;
12850: }
12850: 
12850: jsdouble *
12850: js_NewWeaklyRootedDouble(JSContext *cx, jsdouble d)
12850: {
12850:     jsval v;
12850:     if (!js_NewDoubleInRootedValue(cx, d, &v))
12850:         return NULL;
12850: 
35075:     jsdouble *dp = JSVAL_TO_DOUBLE(v);
33581:     cx->weakRoots.newbornDouble = dp;
12282:     return dp;
12282: }
    1: 
    1: JSBool
    1: js_LockGCThingRT(JSRuntime *rt, void *thing)
    1: {
42755:     GCLocks *locks;
42755: 
32734:     if (!thing)
36680:         return true;
42755:     locks = &rt->gcLocksHash;
40840:     AutoLockGC lock(rt);
42755:     GCLocks::AddPtr p = locks->lookupForAdd(thing);
42755: 
42755:     if (!p) {
42755:         if (!locks->add(p, thing, 1))
42755:             return false;
    1:     } else {
42755:         JS_ASSERT(p->value >= 1);
42755:         p->value++;
    1:     }
42755: 
    1:     METER(rt->gcStats.lock++);
42755:     return true;
    1: }
    1: 
36410: void
    1: js_UnlockGCThingRT(JSRuntime *rt, void *thing)
    1: {
32734:     if (!thing)
36410:         return;
    1: 
40840:     AutoLockGC lock(rt);
42755:     GCLocks::Ptr p = rt->gcLocksHash.lookup(thing);
42755: 
42755:     if (p) {
36680:         rt->gcPoke = true;
42755:         if (--p->value == 0)
42755:             rt->gcLocksHash.remove(p);
42755: 
36680:         METER(rt->gcStats.unlock++);
    1:     }
    1: }
    1: 
  583: JS_PUBLIC_API(void)
  583: JS_TraceChildren(JSTracer *trc, void *thing, uint32 kind)
  583: {
  583:     switch (kind) {
32603:       case JSTRACE_OBJECT: {
    1:         /* If obj has no map, it must be a newborn. */
32603:         JSObject *obj = (JSObject *) thing;
    1:         if (!obj->map)
    1:             break;
  583:         obj->map->ops->trace(trc, obj);
32603:         break;
  583:       }
32603: 
32603:       case JSTRACE_STRING: {
32603:         JSString *str = (JSString *) thing;
29366:         if (str->isDependent())
29366:             JS_CALL_STRING_TRACER(trc, str->dependentBase(), "base");
  583:         break;
32603:       }
  583: 
  583: #if JS_HAS_XML_SUPPORT
  583:       case JSTRACE_XML:
  583:         js_TraceXML(trc, (JSXML *)thing);
  583:         break;
    1: #endif
    1:     }
    1: }
    1: 
    1: /*
36410:  * When the native stack is low, the GC does not call JS_TraceChildren to mark
36410:  * the reachable "children" of the thing. Rather the thing is put aside and
36410:  * JS_TraceChildren is called later with more space on the C stack.
36410:  *
36410:  * To implement such delayed marking of the children with minimal overhead for
40876:  * the normal case of sufficient native stack, the code uses two fields per
40876:  * arena stored in JSGCMarkingDelay. The first field, JSGCMarkingDelay::link,
40876:  * links all arenas with delayed things into a stack list with the pointer to
40876:  * stack top in JSRuntime::gcUnmarkedArenaStackTop. DelayMarkingChildren adds
40876:  * arenas to the stack as necessary while MarkDelayedChildren pops the arenas
40876:  * from the stack until it empties.
36410:  *
40876:  * The second field, JSGCMarkingDelay::unmarkedChildren, is a bitmap that
40876:  * tells for which things the GC should call JS_TraceChildren later. The
40876:  * bitmap is a single word. As such it does not pinpoint the delayed things
40876:  * in the arena but rather tells the intervals containing
40876:  * ThingsPerUnmarkedBit(thingSize) things. Later the code in
40876:  * MarkDelayedChildren discovers such intervals and calls JS_TraceChildren on
40876:  * any marked thing in the interval. This implies that JS_TraceChildren can be
40876:  * called many times for a single thing if the thing shares the same interval
40876:  * with some delayed things. This should be fine as any GC graph
40876:  * marking/traversing hooks must allow repeated calls during the same GC cycle.
40876:  * In particular, xpcom cycle collector relies on this.
36410:  *
36410:  * Note that such repeated scanning may slow down the GC. In particular, it is
36410:  * possible to construct an object graph where the GC calls JS_TraceChildren
36410:  * ThingsPerUnmarkedBit(thingSize) for almost all things in the graph. We
36410:  * tolerate this as the max value for ThingsPerUnmarkedBit(thingSize) is 4.
36410:  * This is archived for JSObject on 32 bit system as it is exactly JSObject
36410:  * that has the smallest size among the GC things that can be delayed. On 32
36410:  * bit CPU we have less than 128 objects per 4K GC arena so each bit in
36410:  * unmarkedChildren covers 4 objects.
    1:  */
36410: inline unsigned
36410: ThingsPerUnmarkedBit(unsigned thingSize)
36410: {
37684:     return JS_HOWMANY(ThingsPerArena(thingSize), JS_BITS_PER_WORD);
36410: }
    1: 
    1: static void
37684: DelayMarkingChildren(JSRuntime *rt, void *thing)
    1: {
40876:     JS_ASSERT(IsMarkedGCThing(thing));
36410:     METER(rt->gcStats.unmarked++);
40876: 
37684:     JSGCArena *a = JSGCArena::fromGCThing(thing);
40876:     JSGCArenaInfo *ainfo = a->getInfo();
40876:     JSGCMarkingDelay *markingDelay = a->getMarkingDelay();
40876: 
40876:     size_t thingArenaIndex = GCThingToArenaIndex(thing);
40876:     size_t unmarkedBitIndex = thingArenaIndex /
40876:                               ThingsPerUnmarkedBit(ainfo->list->thingSize);
36410:     JS_ASSERT(unmarkedBitIndex < JS_BITS_PER_WORD);
37684: 
37684:     jsuword bit = jsuword(1) << unmarkedBitIndex;
40876:     if (markingDelay->unmarkedChildren != 0) {
36410:         JS_ASSERT(rt->gcUnmarkedArenaStackTop);
40876:         if (markingDelay->unmarkedChildren & bit) {
36410:             /* bit already covers things with children to mark later. */
    1:             return;
    1:         }
40876:         markingDelay->unmarkedChildren |= bit;
    1:     } else {
    1:         /*
36410:          * The thing is the first thing with not yet marked children in the
 5917:          * whole arena, so push the arena on the stack of arenas with things
36410:          * to be marked later unless the arena has already been pushed. We
40876:          * detect that through checking prevUnmarked as the field is 0
 5917:          * only for not yet pushed arenas. To ensure that
40876:          *   prevUnmarked != 0
40876:          * even when the stack contains one element, we make prevUnmarked
 5917:          * for the arena at the bottom to point to itself.
 5917:          *
36410:          * See comments in MarkDelayedChildren.
    1:          */
40876:         markingDelay->unmarkedChildren = bit;
40876:         if (!markingDelay->link) {
36410:             if (!rt->gcUnmarkedArenaStackTop) {
 5917:                 /* Stack was empty, mark the arena as the bottom element. */
40876:                 markingDelay->link = a;
 5917:             } else {
40876:                 JS_ASSERT(rt->gcUnmarkedArenaStackTop->getMarkingDelay()->link);
40876:                 markingDelay->link = rt->gcUnmarkedArenaStackTop;
    1:             }
36410:             rt->gcUnmarkedArenaStackTop = a;
    1:         }
36410:         JS_ASSERT(rt->gcUnmarkedArenaStackTop);
    1:     }
36410: #ifdef DEBUG
40876:     rt->gcMarkLaterCount += ThingsPerUnmarkedBit(ainfo->list->thingSize);
36410:     METER_UPDATE_MAX(rt->gcStats.maxunmarked, rt->gcMarkLaterCount);
36410: #endif
    1: }
    1: 
    1: static void
36410: MarkDelayedChildren(JSTracer *trc)
    1: {
    1:     JSRuntime *rt;
37684:     JSGCArena *a, *aprev;
37684:     unsigned thingSize, traceKind;
37684:     unsigned thingsPerUnmarkedBit;
37684:     unsigned unmarkedBitIndex, thingIndex, indexLimit, endIndex;
    1: 
  583:     rt = trc->context->runtime;
36410:     a = rt->gcUnmarkedArenaStackTop;
 5917:     if (!a) {
36410:         JS_ASSERT(rt->gcMarkLaterCount == 0);
    1:         return;
    1:     }
    1: 
    1:     for (;;) {
    1:         /*
 5917:          * The following assert verifies that the current arena belongs to the
36410:          * unmarked stack, since DelayMarkingChildren ensures that even for
40876:          * the stack's bottom, prevUnmarked != 0 but rather points to
36410:          * itself.
    1:          */
40876:         JSGCArenaInfo *ainfo = a->getInfo();
40876:         JSGCMarkingDelay *markingDelay = a->getMarkingDelay();
40876:         JS_ASSERT(markingDelay->link);
40876:         JS_ASSERT(rt->gcUnmarkedArenaStackTop->getMarkingDelay()->link);
40876:         thingSize = ainfo->list->thingSize;
40876:         traceKind = GetFinalizableArenaTraceKind(ainfo);
37684:         indexLimit = ThingsPerArena(thingSize);
36410:         thingsPerUnmarkedBit = ThingsPerUnmarkedBit(thingSize);
 5917: 
    1:         /*
36410:          * We cannot use do-while loop here as a->unmarkedChildren can be zero
 5917:          * before the loop as a leftover from the previous iterations. See
 5917:          * comments after the loop.
    1:          */
40876:         while (markingDelay->unmarkedChildren != 0) {
40876:             unmarkedBitIndex = JS_FLOOR_LOG2W(markingDelay->unmarkedChildren);
40876:             markingDelay->unmarkedChildren &= ~(jsuword(1) << unmarkedBitIndex);
36410: #ifdef DEBUG
36410:             JS_ASSERT(rt->gcMarkLaterCount >= thingsPerUnmarkedBit);
36410:             rt->gcMarkLaterCount -= thingsPerUnmarkedBit;
36410: #endif
36410:             thingIndex = unmarkedBitIndex * thingsPerUnmarkedBit;
36410:             endIndex = thingIndex + thingsPerUnmarkedBit;
    1: 
    1:             /*
 5917:              * endIndex can go beyond the last allocated thing as the real
 5917:              * limit can be "inside" the bit.
    1:              */
 5917:             if (endIndex > indexLimit)
 5917:                 endIndex = indexLimit;
40876:             uint8 *thing = GCArenaIndexToThing(a, ainfo, thingIndex);
40876:             uint8 *end = GCArenaIndexToThing(a, ainfo, endIndex);
 5917:             do {
40876:                 JS_ASSERT(thing < end);
40876:                 if (IsMarkedGCThing(thing))
33582:                     JS_TraceChildren(trc, thing, traceKind);
40876:                 thing += thingSize;
40876:             } while (thing != end);
    1:         }
 5917: 
    1:         /*
 5917:          * We finished tracing of all things in the the arena but we can only
 5917:          * pop it from the stack if the arena is the stack's top.
    1:          *
 5917:          * When JS_TraceChildren from the above calls JS_CallTracer that in
36410:          * turn on low C stack calls DelayMarkingChildren and the latter
36410:          * pushes new arenas to the unmarked stack, we have to skip popping
 5917:          * of this arena until it becomes the top of the stack again.
    1:          */
36410:         if (a == rt->gcUnmarkedArenaStackTop) {
40876:             aprev = markingDelay->link;
40876:             markingDelay->link = NULL;
 5917:             if (a == aprev) {
    1:                 /*
40876:                  * prevUnmarked points to itself and we reached the bottom of
40876:                  * the stack.
    1:                  */
    1:                 break;
    1:             }
36410:             rt->gcUnmarkedArenaStackTop = a = aprev;
    1:         } else {
36410:             a = rt->gcUnmarkedArenaStackTop;
    1:         }
    1:     }
36410:     JS_ASSERT(rt->gcUnmarkedArenaStackTop);
40876:     JS_ASSERT(!rt->gcUnmarkedArenaStackTop->getMarkingDelay()->link);
36410:     rt->gcUnmarkedArenaStackTop = NULL;
36410:     JS_ASSERT(rt->gcMarkLaterCount == 0);
    1: }
    1: 
38595: void
38595: js_CallGCMarker(JSTracer *trc, void *thing, uint32 kind)
    1: {
  583:     JSContext *cx;
  583:     JSRuntime *rt;
  583: 
  771:     JS_ASSERT(thing);
  583:     JS_ASSERT(JS_IS_VALID_TRACE_KIND(kind));
  583:     JS_ASSERT(trc->debugPrinter || trc->debugPrintArg);
  583: 
  583:     if (!IS_GC_MARKING_TRACER(trc)) {
  583:         trc->callback(trc, thing, kind);
  583:         goto out;
  583:     }
  583: 
  583:     cx = trc->context;
  583:     rt = cx->runtime;
  583:     JS_ASSERT(rt->gcMarkingTracer == trc);
42715:     JS_ASSERT(rt->gcRunning);
  583: 
  583:     /*
 5917:      * Optimize for string and double as their size is known and their tracing
 5917:      * is not recursive.
  583:      */
 5917:     switch (kind) {
40876:       case JSTRACE_DOUBLE: {
40876:         MarkIfUnmarkedGCThing(thing);
 5917:         goto out;
40876:       }
 5917: 
 5917:       case JSTRACE_STRING:
 5917:         for (;;) {
32734:             if (JSString::isStatic(thing))
32674:                 goto out;
40876:             JS_ASSERT(kind == GetFinalizableThingTraceKind(thing));
40876:             if (!MarkIfUnmarkedGCThing(thing))
  583:                 goto out;
37684:             if (!((JSString *) thing)->isDependent())
  583:                 goto out;
29366:             thing = ((JSString *) thing)->dependentBase();
 5917:         }
 5917:         /* NOTREACHED */
 5917:     }
 5917: 
40876:     JS_ASSERT(kind == GetFinalizableThingTraceKind(thing));
40876:     if (!MarkIfUnmarkedGCThing(thing))
  583:         goto out;
 5917: 
    1:     if (!cx->insideGCMarkCallback) {
  583:         /*
  583:          * With JS_GC_ASSUME_LOW_C_STACK defined the mark phase of GC always
  583:          * uses the non-recursive code that otherwise would be called only on
  583:          * a low C stack condition.
  583:          */
  583: #ifdef JS_GC_ASSUME_LOW_C_STACK
  583: # define RECURSION_TOO_DEEP() JS_TRUE
  583: #else
  583:         int stackDummy;
  583: # define RECURSION_TOO_DEEP() (!JS_CHECK_STACK_SIZE(cx, stackDummy))
  583: #endif
  583:         if (RECURSION_TOO_DEEP())
37684:             DelayMarkingChildren(rt, thing);
  583:         else
  583:             JS_TraceChildren(trc, thing, kind);
    1:     } else {
    1:         /*
    1:          * For API compatibility we allow for the callback to assume that
 5917:          * after it calls JS_MarkGCThing for the last time, the callback can
 5917:          * start to finalize its own objects that are only referenced by
 5917:          * unmarked GC things.
    1:          *
    1:          * Since we do not know which call from inside the callback is the
 5917:          * last, we ensure that children of all marked things are traced and
36410:          * call MarkDelayedChildren(trc) after tracing the thing.
    1:          *
36410:          * As MarkDelayedChildren unconditionally invokes JS_TraceChildren
36410:          * for the things with unmarked children, calling DelayMarkingChildren
 5917:          * is useless here. Hence we always trace thing's children even with a
 5917:          * low native stack.
    1:          */
37684:         cx->insideGCMarkCallback = false;
  583:         JS_TraceChildren(trc, thing, kind);
36410:         MarkDelayedChildren(trc);
37684:         cx->insideGCMarkCallback = true;
    1:     }
  583: 
  583:   out:
  583: #ifdef DEBUG
  583:     trc->debugPrinter = NULL;
  583:     trc->debugPrintArg = NULL;
  583: #endif
  583:     return;     /* to avoid out: right_curl when DEBUG is not defined */
    1: }
    1: 
  583: void
  771: js_CallValueTracerIfGCThing(JSTracer *trc, jsval v)
  583: {
  771:     void *thing;
  771:     uint32 kind;
  771: 
  771:     if (JSVAL_IS_DOUBLE(v) || JSVAL_IS_STRING(v)) {
  771:         thing = JSVAL_TO_TRACEABLE(v);
  771:         kind = JSVAL_TRACE_KIND(v);
36410:         JS_ASSERT(kind == js_GetGCThingTraceKind(thing));
  771:     } else if (JSVAL_IS_OBJECT(v) && v != JSVAL_NULL) {
  771:         /* v can be an arbitrary GC thing reinterpreted as an object. */
  771:         thing = JSVAL_TO_OBJECT(v);
 8005:         kind = js_GetGCThingTraceKind(thing);
  771:     } else {
  771:         return;
  583:     }
38595:     js_CallGCMarker(trc, thing, kind);
  771: }
  583: 
42755: static void
42755: gc_root_traversal(const GCRoots::Entry &entry, JSTracer *trc)
    1: {
42755:     jsval *rp = (jsval *)entry.key;
    1:     jsval v = *rp;
    1: 
32694:     /* Ignore null reference, scalar values, and static strings. */
36410:     if (JSVAL_IS_TRACEABLE(v)) {
    1: #ifdef DEBUG
36410:         if (!JSString::isStatic(JSVAL_TO_GCTHING(v))) {
33582:             bool root_points_to_gcArenaList = false;
    1:             jsuword thing = (jsuword) JSVAL_TO_GCTHING(v);
33582:             JSRuntime *rt = trc->context->runtime;
33582:             for (unsigned i = 0; i != FINALIZE_LIMIT; i++) {
33582:                 JSGCArenaList *arenaList = &rt->gcArenaList[i];
33582:                 size_t thingSize = arenaList->thingSize;
37684:                 size_t limit = ThingsPerArena(thingSize) * thingSize;
40876:                 for (JSGCArena *a = arenaList->head;
40876:                      a;
40876:                      a = a->getInfo()->prev) {
37684:                     if (thing - a->toPageStart() < limit) {
33582:                         root_points_to_gcArenaList = true;
    1:                         break;
    1:                     }
    1:                 }
    1:             }
12282:             if (!root_points_to_gcArenaList) {
37684:                 for (JSGCArena *a = rt->gcDoubleArenaList.head;
36410:                      a;
40876:                      a = a->getInfo()->prev) {
37684:                     if (thing - a->toPageStart() <
12282:                         DOUBLES_PER_ARENA * sizeof(jsdouble)) {
33582:                         root_points_to_gcArenaList = true;
12282:                         break;
12282:                     }
    1:                 }
    1:             }
42755:             if (!root_points_to_gcArenaList && entry.value) {
    1:                 fprintf(stderr,
    1: "JS API usage error: the address passed to JS_AddNamedRoot currently holds an\n"
    1: "invalid jsval.  This is usually caused by a missing call to JS_RemoveRoot.\n"
    1: "The root's name is \"%s\".\n",
42755:                         entry.value);
    1:             }
    1:             JS_ASSERT(root_points_to_gcArenaList);
36410:         }
    1: #endif
42755:         JS_SET_TRACING_NAME(trc, entry.value ? entry.value : "root");
  771:         js_CallValueTracerIfGCThing(trc, v);
  583:     }
  583: }
  583: 
42755: static void
42755: gc_lock_traversal(const GCLocks::Entry &entry, JSTracer *trc)
  583: {
  583:     uint32 traceKind;
  583: 
42755:     JS_ASSERT(entry.value >= 1);
42755:     traceKind = js_GetGCThingTraceKind(entry.key);
42755:     JS_CALL_TRACER(trc, entry.key, traceKind, "locked object");
    1: }
    1: 
40383: namespace js {
40383: 
40383: void
40383: TraceObjectVector(JSTracer *trc, JSObject **vec, uint32 len)
40383: {
40383:     for (uint32 i = 0; i < len; i++) {
40383:         if (JSObject *obj = vec[i]) {
40383:             JS_SET_TRACING_INDEX(trc, "vector", i);
40383:             js_CallGCMarker(trc, obj, JSTRACE_OBJECT);
40383:         }
40383:     }
40383: }
40383: 
40383: }
40383: 
    1: void
  583: js_TraceStackFrame(JSTracer *trc, JSStackFrame *fp)
    1: {
 4127: 
    1:     if (fp->callobj)
  786:         JS_CALL_OBJECT_TRACER(trc, fp->callobj, "call");
    1:     if (fp->argsobj)
30248:         JS_CALL_OBJECT_TRACER(trc, JSVAL_TO_OBJECT(fp->argsobj), "arguments");
42714:     if (fp->script)
  583:         js_TraceScript(trc, fp->script);
21992: 
    1:     /* Allow for primitive this parameter due to JSFUN_THISP_* flags. */
32774:     JS_CALL_VALUE_TRACER(trc, fp->thisv, "this");
    1: 
  583:     JS_CALL_VALUE_TRACER(trc, fp->rval, "rval");
  583:     if (fp->scopeChain)
  583:         JS_CALL_OBJECT_TRACER(trc, fp->scopeChain, "scope chain");
    1: }
    1: 
33952: void
33952: JSWeakRoots::mark(JSTracer *trc)
    1: {
33952: #ifdef DEBUG
33952:     const char * const newbornNames[] = {
33952:         "newborn_object",             /* FINALIZE_OBJECT */
33952:         "newborn_function",           /* FINALIZE_FUNCTION */
33581: #if JS_HAS_XML_SUPPORT
33952:         "newborn_xml",                /* FINALIZE_XML */
 8005: #endif
33952:         "newborn_string",             /* FINALIZE_STRING */
33952:         "newborn_external_string0",   /* FINALIZE_EXTERNAL_STRING0 */
33952:         "newborn_external_string1",   /* FINALIZE_EXTERNAL_STRING1 */
33952:         "newborn_external_string2",   /* FINALIZE_EXTERNAL_STRING2 */
33952:         "newborn_external_string3",   /* FINALIZE_EXTERNAL_STRING3 */
33952:         "newborn_external_string4",   /* FINALIZE_EXTERNAL_STRING4 */
33952:         "newborn_external_string5",   /* FINALIZE_EXTERNAL_STRING5 */
33952:         "newborn_external_string6",   /* FINALIZE_EXTERNAL_STRING6 */
33952:         "newborn_external_string7",   /* FINALIZE_EXTERNAL_STRING7 */
33952:     };
33952: #endif
33952:     for (size_t i = 0; i != JS_ARRAY_LENGTH(finalizableNewborns); ++i) {
33952:         void *newborn = finalizableNewborns[i];
33952:         if (newborn) {
33952:             JS_CALL_TRACER(trc, newborn, GetFinalizableTraceKind(i),
33952:                            newbornNames[i]);
 8005:         }
33952:     }
33952:     if (newbornDouble)
33952:         JS_CALL_DOUBLE_TRACER(trc, newbornDouble, "newborn_double");
33952:     JS_CALL_VALUE_TRACER(trc, lastAtom, "lastAtom");
  771:     JS_SET_TRACING_NAME(trc, "lastInternalResult");
33952:     js_CallValueTracerIfGCThing(trc, lastInternalResult);
    1: }
  583: 
47447: inline void
47447: AutoGCRooter::trace(JSTracer *trc)
47447: {
47447:     switch (tag) {
47447:       case JSVAL:
47447:         JS_SET_TRACING_NAME(trc, "js::AutoValueRooter.val");
47447:         js_CallValueTracerIfGCThing(trc, static_cast<AutoValueRooter *>(this)->val);
47447:         return;
47447: 
47447:       case SPROP:
47447:         static_cast<AutoScopePropertyRooter *>(this)->sprop->trace(trc);
47447:         return;
47447: 
47447:       case WEAKROOTS:
47447:         static_cast<AutoPreserveWeakRoots *>(this)->savedRoots.mark(trc);
47447:         return;
47447: 
47447:       case PARSER:
47447:         static_cast<Parser *>(this)->trace(trc);
47447:         return;
47447: 
47447:       case SCRIPT:
47447:         if (JSScript *script = static_cast<AutoScriptRooter *>(this)->script)
47447:             js_TraceScript(trc, script);
47447:         return;
47447: 
47447:       case ENUMERATOR:
47447:         static_cast<AutoEnumStateRooter *>(this)->trace(trc);
47447:         return;
47447: 
47447:       case IDARRAY: {
47447:         JSIdArray *ida = static_cast<AutoIdArray *>(this)->idArray;
47447:         TraceValues(trc, ida->length, ida->vector, "js::AutoIdArray.idArray");
47447:         return;
47447:       }
47447: 
47447:       case DESCRIPTORS: {
47447:         PropertyDescriptorArray &descriptors =
47447:             static_cast<AutoDescriptorArray *>(this)->descriptors;
47447:         for (size_t i = 0, len = descriptors.length(); i < len; i++) {
47447:             PropertyDescriptor &desc = descriptors[i];
47447: 
47447:             JS_CALL_VALUE_TRACER(trc, desc.pd, "PropertyDescriptor::pd");
47447:             JS_CALL_VALUE_TRACER(trc, desc.value, "PropertyDescriptor::value");
47447:             JS_CALL_VALUE_TRACER(trc, desc.get, "PropertyDescriptor::get");
47447:             JS_CALL_VALUE_TRACER(trc, desc.set, "PropertyDescriptor::set");
47447:             js_TraceId(trc, desc.id);
47447:         }
47447:         return;
47447:       }
47447: 
47447:       case DESCRIPTOR : {
47447:         AutoDescriptor &desc = *static_cast<AutoDescriptor *>(this);
47447:         if (desc.obj)
47447:             JS_CALL_OBJECT_TRACER(trc, desc.obj, "Descriptor::obj");
47447:         JS_CALL_VALUE_TRACER(trc, desc.value, "Descriptor::value");
47447:         if (desc.attrs & JSPROP_GETTER)
47447:             JS_CALL_VALUE_TRACER(trc, jsval(desc.getter), "Descriptor::get");
47447:         if (desc.attrs & JSPROP_SETTER)
47447:             JS_CALL_VALUE_TRACER(trc, jsval(desc.setter), "Descriptor::set");
47447:         return;
47447:       }
47447: 
47447:       case NAMESPACES: {
47478:         JSXMLArray &array = static_cast<AutoNamespaceArray *>(this)->array;
47447:         TraceObjectVector(trc, reinterpret_cast<JSObject **>(array.vector), array.length);
47447:         array.cursors->trace(trc);
47447:         return;
47447:       }
47447: 
47447:       case XML:
47447:         js_TraceXML(trc, static_cast<AutoXMLRooter *>(this)->xml);
47447:         return;
47447: 
47447:       case OBJECT:
47447:         if (JSObject *obj = static_cast<AutoObjectRooter *>(this)->obj) {
47447:             JS_SET_TRACING_NAME(trc, "js::AutoObjectRooter.obj");
47447:             js_CallGCMarker(trc, obj, JSTRACE_OBJECT);
47447:         }
47447:         return;
47447: 
47447:       case ID:
47447:         JS_SET_TRACING_NAME(trc, "js::AutoIdRooter.val");
47447:         js_CallValueTracerIfGCThing(trc, static_cast<AutoIdRooter *>(this)->idval);
47447:         return;
47447: 
47447:       case VECTOR: {
47447:         js::Vector<jsval, 8> &vector = static_cast<js::AutoValueVector *>(this)->vector;
47447:         js::TraceValues(trc, vector.length(), vector.begin(), "js::AutoValueVector.vector");
47447:         return;
47447:       }
47447:     }
47447: 
47447:     JS_ASSERT(tag >= 0);
47447:     TraceValues(trc, tag, static_cast<AutoArrayRooter *>(this)->array, "js::AutoArrayRooter.array");
47447: }
47447: 
42714: void
  583: js_TraceContext(JSTracer *trc, JSContext *acx)
  583: {
42714:     /* Stack frames and slots are traced by StackSpace::mark. */
37777: 
  583:     /* Mark other roots-by-definition in acx. */
22795:     if (acx->globalObject && !JS_HAS_OPTION(acx, JSOPTION_UNROOTED_GLOBAL))
  583:         JS_CALL_OBJECT_TRACER(trc, acx->globalObject, "global object");
33952:     acx->weakRoots.mark(trc);
  583:     if (acx->throwing) {
  583:         JS_CALL_VALUE_TRACER(trc, acx->exception, "exception");
  583:     } else {
  583:         /* Avoid keeping GC-ed junk stored in JSContext.exception. */
  583:         acx->exception = JSVAL_NULL;
  583:     }
  583: 
40383:     for (js::AutoGCRooter *gcr = acx->autoGCRooters; gcr; gcr = gcr->down)
40383:         gcr->trace(trc);
40383: 
  583:     if (acx->sharpObjectMap.depth > 0)
  583:         js_TraceSharpMap(trc, &acx->sharpObjectMap);
23094: 
23094:     js_TraceRegExpStatics(trc, acx);
28086: 
42641:     JS_CALL_VALUE_TRACER(trc, acx->iterValue, "iterValue");
42641: 
28086: #ifdef JS_TRACER
41276:     TracerState* state = acx->tracerState;
32726:     while (state) {
32726:         if (state->nativeVp)
40397:             TraceValues(trc, state->nativeVpLen, state->nativeVp, "nativeVp");
32726:         state = state->prev;
32726:     }
28086: #endif
  583: }
  583: 
24499: JS_REQUIRES_STACK void
41294: js_TraceRuntime(JSTracer *trc)
  583: {
  583:     JSRuntime *rt = trc->context->runtime;
  583: 
42755:     for (GCRoots::Range r = rt->gcRootsHash.all(); !r.empty(); r.popFront())
42755:         gc_root_traversal(r.front(), trc);
42755: 
42755:     for (GCLocks::Range r = rt->gcLocksHash.all(); !r.empty(); r.popFront())
42755:         gc_lock_traversal(r.front(), trc);
42755: 
41294:     js_TraceAtomState(trc);
12282:     js_TraceRuntimeNumberState(trc);
35076:     js_MarkTraps(trc);
  583: 
47439:     JSContext *iter = NULL;
47439:     while (JSContext *acx = js_ContextIterator(rt, JS_TRUE, &iter))
  583:         js_TraceContext(trc, acx);
  958: 
42712:     for (ThreadDataIter i(rt); !i.empty(); i.popFront())
42712:         i.threadData()->mark(trc);
31843: 
  958:     if (rt->gcExtraRootsTraceOp)
  958:         rt->gcExtraRootsTraceOp(trc, rt->gcExtraRootsData);
47439: 
47439:     /*
47439:      * For now we use the conservative stack scanner only as a sanity check,
47439:      * so we mark conservatively after marking all other roots to detect
47439:      * conservative leaks.
47439:      */
47439:     if (rt->state != JSRTS_LANDING)
47439:         ConservativeGCStackMarker(trc).markRoots();
  583: }
  583: 
27546: void
27546: js_TriggerGC(JSContext *cx, JSBool gcLocked)
27546: {
27546:     JSRuntime *rt = cx->runtime;
27546: 
27546: #ifdef JS_THREADSAFE
27546:     JS_ASSERT(cx->requestDepth > 0);
27546: #endif
27546:     JS_ASSERT(!rt->gcRunning);
27546:     if (rt->gcIsNeeded)
27546:         return;
27546: 
40808:     /*
40808:      * Trigger the GC when it is safe to call an operation callback on any
40808:      * thread.
40808:      */
40808:     rt->gcIsNeeded = JS_TRUE;
27546:     js_TriggerAllOperationCallbacks(rt, gcLocked);
27546: }
27546: 
26569: void
26569: js_DestroyScriptsToGC(JSContext *cx, JSThreadData *data)
18285: {
26569:     JSScript **listp, *script;
26569: 
26569:     for (size_t i = 0; i != JS_ARRAY_LENGTH(data->scriptsToGC); ++i) {
26569:         listp = &data->scriptsToGC[i];
18285:         while ((script = *listp) != NULL) {
18285:             *listp = script->u.nextToGC;
18285:             script->u.nextToGC = NULL;
18285:             js_DestroyScript(cx, script);
18285:         }
18285:     }
26569: }
18285: 
36997: inline void
36997: FinalizeObject(JSContext *cx, JSObject *obj, unsigned thingKind)
30274: {
40857:     JS_ASSERT(thingKind == FINALIZE_OBJECT ||
40857:               thingKind == FINALIZE_FUNCTION);
33582: 
30274:     /* Cope with stillborn objects that have no map. */
30274:     if (!obj->map)
30274:         return;
30274: 
30274:     /* Finalize obj first, in case it needs map and slots. */
36997:     JSClass *clasp = obj->getClass();
30654:     if (clasp->finalize)
30654:         clasp->finalize(cx, obj);
30274: 
41862:     DTrace::finalizeObject(obj);
30274: 
40430:     if (JS_LIKELY(obj->isNative())) {
40847:         JSScope *scope = obj->scope();
37766:         if (scope->isSharedEmpty())
37766:             static_cast<JSEmptyScope *>(scope)->dropFromGC(cx);
37766:         else
37766:             scope->destroy(cx);
37766:     }
36997:     if (obj->hasSlotsArray())
36997:         obj->freeSlotsArray(cx);
30274: }
30274: 
36997: inline void
36997: FinalizeFunction(JSContext *cx, JSFunction *fun, unsigned thingKind)
33582: {
36997:     FinalizeObject(cx, FUN_OBJECT(fun), thingKind);
33582: }
33582: 
36997: inline void
36997: FinalizeHookedObject(JSContext *cx, JSObject *obj, unsigned thingKind)
36997: {
36997:     if (!obj->map)
36997:         return;
36997: 
36997:     if (cx->debugHooks->objectHook) {
36997:         cx->debugHooks->objectHook(cx, obj, JS_FALSE,
36997:                                    cx->debugHooks->objectHookData);
36997:     }
36997:     FinalizeObject(cx, obj, thingKind);
36997: }
36997: 
36997: inline void
36997: FinalizeHookedFunction(JSContext *cx, JSFunction *fun, unsigned thingKind)
36997: {
36997:     FinalizeHookedObject(cx, FUN_OBJECT(fun), thingKind);
36997: }
36997: 
33582: #if JS_HAS_XML_SUPPORT
36997: inline void
36997: FinalizeXML(JSContext *cx, JSXML *xml, unsigned thingKind)
33582: {
33582:     js_FinalizeXML(cx, xml);
33582: }
33582: #endif
33582: 
33581: JS_STATIC_ASSERT(JS_EXTERNAL_STRING_LIMIT == 8);
33581: static JSStringFinalizeOp str_finalizers[JS_EXTERNAL_STRING_LIMIT] = {
30275:     NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL
30275: };
30275: 
30275: intN
30275: js_ChangeExternalStringFinalizer(JSStringFinalizeOp oldop,
30275:                                  JSStringFinalizeOp newop)
30275: {
33581:     for (uintN i = 0; i != JS_ARRAY_LENGTH(str_finalizers); i++) {
30275:         if (str_finalizers[i] == oldop) {
30275:             str_finalizers[i] = newop;
33581:             return intN(i);
30275:         }
30275:     }
30275:     return -1;
30275: }
30275: 
36997: inline void
36997: FinalizeString(JSContext *cx, JSString *str, unsigned thingKind)
36997: {
36997:     JS_ASSERT(FINALIZE_STRING == thingKind);
36997:     JS_ASSERT(!JSString::isStatic(str));
36997:     JS_RUNTIME_UNMETER(cx->runtime, liveStrings);
36997:     if (str->isDependent()) {
36997:         JS_ASSERT(str->dependentBase());
36997:         JS_RUNTIME_UNMETER(cx->runtime, liveDependentStrings);
36997:     } else {
30275:         /*
36997:          * flatChars for stillborn string is null, but cx->free would checks
36997:          * for a null pointer on its own.
30275:          */
36997:         cx->free(str->flatChars());
36997:     }
36997: }
36997: 
36997: inline void
36997: FinalizeExternalString(JSContext *cx, JSString *str, unsigned thingKind)
30275: {
36997:     unsigned type = thingKind - FINALIZE_EXTERNAL_STRING0;
36997:     JS_ASSERT(type < JS_ARRAY_LENGTH(str_finalizers));
36997:     JS_ASSERT(!JSString::isStatic(str));
36997:     JS_ASSERT(!str->isDependent());
36997: 
36997:     JS_RUNTIME_UNMETER(cx->runtime, liveStrings);
36997: 
36997:     /* A stillborn string has null chars. */
36997:     jschar *chars = str->flatChars();
36997:     if (!chars)
36997:         return;
36997:     JSStringFinalizeOp finalizer = str_finalizers[type];
36997:     if (finalizer)
36997:         finalizer(cx, str);
36997: }
36997: 
36997: /*
36997:  * This function is called from js_FinishAtomState to force the finalization
36997:  * of the permanently interned strings when cx is not available.
36997:  */
36997: void
36997: js_FinalizeStringRT(JSRuntime *rt, JSString *str)
36997: {
30275:     JS_RUNTIME_UNMETER(rt, liveStrings);
32674:     JS_ASSERT(!JSString::isStatic(str));
36997: 
30275:     if (str->isDependent()) {
30275:         /* A dependent string can not be external and must be valid. */
40876:         JS_ASSERT(JSGCArenaInfo::fromGCThing(str)->list->thingKind ==
37684:                   FINALIZE_STRING);
30275:         JS_ASSERT(str->dependentBase());
30275:         JS_RUNTIME_UNMETER(rt, liveDependentStrings);
30275:     } else {
40876:         unsigned thingKind = JSGCArenaInfo::fromGCThing(str)->list->thingKind;
37684:         JS_ASSERT(IsFinalizableStringKind(thingKind));
37684: 
30275:         /* A stillborn string has null chars, so is not valid. */
36997:         jschar *chars = str->flatChars();
36997:         if (!chars)
36997:             return;
33582:         if (thingKind == FINALIZE_STRING) {
30851:             rt->free(chars);
30275:         } else {
33582:             unsigned type = thingKind - FINALIZE_EXTERNAL_STRING0;
33582:             JS_ASSERT(type < JS_ARRAY_LENGTH(str_finalizers));
36997:             JSStringFinalizeOp finalizer = str_finalizers[type];
30275:             if (finalizer) {
30275:                 /*
30275:                  * Assume that the finalizer for the permanently interned
30275:                  * string knows how to deal with null context.
30275:                  */
36997:                 finalizer(NULL, str);
30275:             }
30275:         }
30275:     }
30275: }
30275: 
36997: template<typename T,
36997:          void finalizer(JSContext *cx, T *thing, unsigned thingKind)>
33582: static void
41286: FinalizeArenaList(JSContext *cx, unsigned thingKind)
33582: {
37684:     JS_STATIC_ASSERT(!(sizeof(T) & GC_CELL_MASK));
33582:     JSGCArenaList *arenaList = &cx->runtime->gcArenaList[thingKind];
33582:     JS_ASSERT(sizeof(T) == arenaList->thingSize);
33582: 
37684:     JSGCArena **ap = &arenaList->head;
37684:     JSGCArena *a = *ap;
33582:     if (!a)
33582:         return;
33582: 
33582: #ifdef JS_GCMETER
33582:     uint32 nlivearenas = 0, nkilledarenas = 0, nthings = 0;
33582: #endif
33582:     for (;;) {
40876:         JSGCArenaInfo *ainfo = a->getInfo();
40876:         JS_ASSERT(ainfo->list == arenaList);
40876:         JS_ASSERT(!a->getMarkingDelay()->link);
40876:         JS_ASSERT(a->getMarkingDelay()->unmarkedChildren == 0);
33952: 
33952:         JSGCThing *freeList = NULL;
33952:         JSGCThing **tailp = &freeList;
33582:         bool allClear = true;
37684: 
40876:         jsuword thing = a->toPageStart();
40876:         jsuword thingsEnd = thing + GC_ARENA_SIZE / sizeof(T) * sizeof(T);
40876: 
40876:         jsuword nextFree = reinterpret_cast<jsuword>(ainfo->freeList);
37684:         if (!nextFree) {
37684:             nextFree = thingsEnd;
37684:         } else {
37684:             JS_ASSERT(thing <= nextFree);
37684:             JS_ASSERT(nextFree < thingsEnd);
37684:         }
37684: 
37684:         jsuword gcCellIndex = 0;
40876:         jsbitmap *bitmap = a->getMarkBitmap();
40876:         for (;; thing += sizeof(T), gcCellIndex += sizeof(T) >> GC_CELL_SHIFT) {
33952:             if (thing == nextFree) {
33952:                 if (thing == thingsEnd)
33952:                     break;
40876:                 nextFree = reinterpret_cast<jsuword>(
40876:                     reinterpret_cast<JSGCThing *>(nextFree)->link);
33952:                 if (!nextFree) {
33952:                     nextFree = thingsEnd;
33952:                 } else {
33952:                     JS_ASSERT(thing < nextFree);
33952:                     JS_ASSERT(nextFree < thingsEnd);
33952:                 }
37684:             } else if (JS_TEST_BIT(bitmap, gcCellIndex)) {
33582:                 allClear = false;
33582:                 METER(nthings++);
33952:                 continue;
37684:             } else {
40876:                 T *t = reinterpret_cast<T *>(thing);
40876:                 finalizer(cx, t, thingKind);
33582: #ifdef DEBUG
40876:                 memset(t, JS_FREE_PATTERN, sizeof(T));
33582: #endif
33582:             }
40876:             JSGCThing *t = reinterpret_cast<JSGCThing *>(thing);
40876:             *tailp = t;
40876:             tailp = &t->link;
33582:         }
33952: 
33952: #ifdef DEBUG
33952:         /* Check that the free list is consistent. */
33952:         unsigned nfree = 0;
33952:         if (freeList) {
33952:             JS_ASSERT(tailp != &freeList);
40876:             JSGCThing *t = freeList;
33952:             for (;;) {
33952:                 ++nfree;
40876:                 if (&t->link == tailp)
33952:                     break;
40876:                 JS_ASSERT(t < t->link);
40876:                 t = t->link;
33952:             }
33952:         }
33952: #endif
33582:         if (allClear) {
33582:             /*
33582:              * Forget just assembled free list head for the arena and
33582:              * add the arena itself to the destroy list.
33582:              */
37684:             JS_ASSERT(nfree == ThingsPerArena(sizeof(T)));
40876:             *ap = ainfo->prev;
41286:             ReleaseGCArena(cx->runtime, a);
33582:             METER(nkilledarenas++);
33582:         } else {
37684:             JS_ASSERT(nfree < ThingsPerArena(sizeof(T)));
33952:             *tailp = NULL;
40876:             ainfo->freeList = freeList;
40876:             ap = &ainfo->prev;
33582:             METER(nlivearenas++);
33582:         }
33582:         if (!(a = *ap))
33582:             break;
33582:     }
33952:     arenaList->cursor = arenaList->head;
33582: 
36680:     METER(UpdateArenaStats(&cx->runtime->gcStats.arenaStats[thingKind],
33582:                            nlivearenas, nkilledarenas, nthings));
33582: }
33582: 
40373: #ifdef MOZ_GCTIMER
42716: 
42716: const bool JS_WANT_GC_SUITE_PRINT = true;  //false for gnuplot output
42716: 
40373: struct GCTimer {
40373:     uint64 enter;
40373:     uint64 startMark;
40373:     uint64 startSweep;
40373:     uint64 sweepObjectEnd;
40373:     uint64 sweepStringEnd;
40373:     uint64 sweepDoubleEnd;
40373:     uint64 sweepDestroyEnd;
40373:     uint64 end;
41273: 
41273:     GCTimer() {
41273:         getFirstEnter();
41273:         memset(this, 0, sizeof(GCTimer));
41273:         enter = rdtsc();
41273:     }
41273: 
41273:     static uint64 getFirstEnter() {
41273:         static uint64 firstEnter = rdtsc();
41273:         return firstEnter;
41273:     }
41273: 
41273:     void finish(bool lastGC) {
41273:         end = rdtsc();
41273: 
41273:         if (startMark > 0) {
42716:             if (JS_WANT_GC_SUITE_PRINT) {
42716:                 fprintf(stderr, "%f %f %f\n",
42716:                         (double)(end - enter) / 1e6,
42716:                         (double)(startSweep - startMark) / 1e6,
42716:                         (double)(sweepDestroyEnd - startSweep) / 1e6);
42716:             } else {
40373:                 static FILE *gcFile;
40373: 
40373:                 if (!gcFile) {
40373:                     gcFile = fopen("gcTimer.dat", "w");
40373: 
42716:                     fprintf(gcFile, "     AppTime,  Total,   Mark,  Sweep,");
42716:                     fprintf(gcFile, " FinObj, FinStr, FinDbl,");
42716:                     fprintf(gcFile, " Destroy,  newChunks, destoyChunks\n");
40373:                 }
41102:                 JS_ASSERT(gcFile);
42716:                 fprintf(gcFile, "%12.1f, %6.1f, %6.1f, %6.1f, %6.1f, %6.1f,"\
42716:                                  " %6.1f, %7.1f, ",
41273:                         (double)(enter - getFirstEnter()) / 1e6,
41273:                         (double)(end - enter) / 1e6,
41273:                         (double)(startSweep - startMark) / 1e6,
41273:                         (double)(sweepDestroyEnd - startSweep) / 1e6,
41273:                         (double)(sweepObjectEnd - startSweep) / 1e6,
41273:                         (double)(sweepStringEnd - sweepObjectEnd) / 1e6,
41273:                         (double)(sweepDoubleEnd - sweepStringEnd) / 1e6,
41273:                         (double)(sweepDestroyEnd - sweepDoubleEnd) / 1e6);
42716:                 fprintf(gcFile, "%10d, %10d \n", newChunkCount,
42716:                         destroyChunkCount);
40373:                 fflush(gcFile);
40373: 
41102:                 if (lastGC) {
40373:                     fclose(gcFile);
41102:                     gcFile = NULL;
41102:                 }
40373:             }
42716:         }
41273:         newChunkCount = 0;
41273:         destroyChunkCount = 0;
41273:     }
41273: };
40836: 
40836: # define GCTIMER_PARAM      , GCTimer &gcTimer
40836: # define GCTIMER_ARG        , gcTimer
41273: # define TIMESTAMP(x)       (gcTimer.x = rdtsc())
41273: # define GCTIMER_BEGIN()    GCTimer gcTimer
41273: # define GCTIMER_END(last)  (gcTimer.finish(last))
40836: #else
40836: # define GCTIMER_PARAM
40836: # define GCTIMER_ARG
40836: # define TIMESTAMP(x)       ((void) 0)
41273: # define GCTIMER_BEGIN()    ((void) 0)
41273: # define GCTIMER_END(last)  ((void) 0)
40373: #endif
40373: 
42725: static inline bool
42725: HasMarkedDoubles(JSGCArena *a)
42725: {
42725:     JS_STATIC_ASSERT(GC_MARK_BITMAP_SIZE == 8 * sizeof(uint64));
42725:     uint64 *markBitmap = (uint64 *) a->getMarkBitmap();
42725:     return !!(markBitmap[0] | markBitmap[1] | markBitmap[2] | markBitmap[3] |
42725:               markBitmap[4] | markBitmap[5] | markBitmap[6] | markBitmap[7]);
42725: }
42725: 
40876: static void
41286: SweepDoubles(JSRuntime *rt)
40876: {
40876: #ifdef JS_GCMETER
40876:     uint32 nlivearenas = 0, nkilledarenas = 0, nthings = 0;
40876: #endif
40876:     JSGCArena **ap = &rt->gcDoubleArenaList.head;
40876:     while (JSGCArena *a = *ap) {
40876:         JSGCArenaInfo *ainfo = a->getInfo();
42725:         if (!HasMarkedDoubles(a)) {
40876:             /* No marked double values in the arena. */
40876:             *ap = ainfo->prev;
41286:             ReleaseGCArena(rt, a);
40876:             METER(nkilledarenas++);
40876:         } else {
40876: #ifdef JS_GCMETER
40876:             jsdouble *thing = reinterpret_cast<jsdouble *>(a->toPageStart());
40876:             jsdouble *end = thing + DOUBLES_PER_ARENA;
40876:             for (; thing != end; ++thing) {
40876:                 if (IsMarkedGCThing(thing))
40876:                     METER(nthings++);
40876:             }
40876:             METER(nlivearenas++);
40876: #endif
40876:             ap = &ainfo->prev;
40876:         }
40876:     }
40876:     METER(UpdateArenaStats(&rt->gcStats.doubleArenaStats,
40876:                            nlivearenas, nkilledarenas, nthings));
40876:     rt->gcDoubleArenaList.cursor = rt->gcDoubleArenaList.head;
40876: }
40876: 
41796: #ifdef JS_THREADSAFE
41796: 
41796: namespace js {
41796: 
41837: JS_FRIEND_API(void)
41796: BackgroundSweepTask::replenishAndFreeLater(void *ptr)
41796: {
41796:     JS_ASSERT(freeCursor == freeCursorEnd);
41796:     do {
41796:         if (freeCursor && !freeVector.append(freeCursorEnd - FREE_ARRAY_LENGTH))
41796:             break;
41796:         freeCursor = (void **) js_malloc(FREE_ARRAY_SIZE);
41796:         if (!freeCursor) {
41796:             freeCursorEnd = NULL;
41796:             break;
41796:         }
41796:         freeCursorEnd = freeCursor + FREE_ARRAY_LENGTH;
41796:         *freeCursor++ = ptr;
41796:         return;
41796:     } while (false);
41796:     js_free(ptr);
41796: }
41796: 
41796: void
41796: BackgroundSweepTask::run()
41796: {
41796:     if (freeCursor) {
41796:         void **array = freeCursorEnd - FREE_ARRAY_LENGTH;
41796:         freeElementsAndArray(array, freeCursor);
41796:         freeCursor = freeCursorEnd = NULL;
41796:     } else {
41796:         JS_ASSERT(!freeCursorEnd);
41796:     }
41796:     for (void ***iter = freeVector.begin(); iter != freeVector.end(); ++iter) {
41796:         void **array = *iter;
41796:         freeElementsAndArray(array, array + FREE_ARRAY_LENGTH);
41796:     }
41796: }
41796: 
41796: }
41796: 
41796: #endif /* JS_THREADSAFE */
41796: 
47498: static void
47498: SweepCompartments(JSContext *cx)
47498: {
47498:     JSRuntime *rt = cx->runtime;
47498:     JSCompartment **read = rt->compartments.begin();
47498:     JSCompartment **end = rt->compartments.end();
47498:     JSCompartment **write = read;
47498:     while (read < end) {
47498:         JSCompartment *compartment = (*read++);
47498:         if (compartment->marked) {
47498:             compartment->marked = false;
47498:             *write++ = compartment;
47498:             /* Remove dead wrappers from the compartment map. */
47498:             compartment->sweep(cx);
47498:         } else {
47498:             delete compartment;
47498:         }
47498:     }
47498:     rt->compartments.resize(write - rt->compartments.begin());
47498: }
47498: 
11041: /*
40838:  * Common cache invalidation and so forth that must be done before GC. Even if
40841:  * GCUntilDone calls GC several times, this work only needs to be done once.
40838:  */
40838: static void
40838: PreGCCleanup(JSContext *cx, JSGCInvocationKind gckind)
40838: {
40838:     JSRuntime *rt = cx->runtime;
40838: 
40838:     /* Clear gcIsNeeded now, when we are about to start a normal GC cycle. */
40838:     rt->gcIsNeeded = JS_FALSE;
40838: 
40838:     /* Reset malloc counter. */
40838:     rt->resetGCMallocBytes();
40838: 
40838: #ifdef JS_DUMP_SCOPE_METERS
40838:     {
40838:         extern void js_DumpScopeMeters(JSRuntime *rt);
40838:         js_DumpScopeMeters(rt);
40838:     }
40838: #endif
40838: 
40838:     /*
40838:      * Reset the property cache's type id generator so we can compress ids.
40838:      * Same for the protoHazardShape proxy-shape standing in for all object
40838:      * prototypes having readonly or setter properties.
40838:      */
40838:     if (rt->shapeGen & SHAPE_OVERFLOW_BIT
40838: #ifdef JS_GC_ZEAL
40838:         || rt->gcZeal >= 1
40838: #endif
40838:         ) {
40838:         rt->gcRegenShapes = true;
40838:         rt->gcRegenShapesScopeFlag ^= JSScope::SHAPE_REGEN;
42754:         rt->shapeGen = JSScope::LAST_RESERVED_SHAPE;
40838:         rt->protoHazardShape = 0;
40838:     }
40838: 
40838:     js_PurgeThreads(cx);
40838:     {
40838:         JSContext *iter = NULL;
40838:         while (JSContext *acx = js_ContextIterator(rt, JS_TRUE, &iter))
40838:             acx->purge();
40838:     }
40838: 
40838:     JS_CLEAR_WEAK_ROOTS(&cx->weakRoots);
40838: }
40838: 
40838: /*
40836:  * Perform mark-and-sweep GC.
40836:  *
40836:  * In a JS_THREADSAFE build, the calling thread must be rt->gcThread and each
40836:  * other thread must be either outside all requests or blocked waiting for GC
40836:  * to finish. Note that the caller does not hold rt->gcLock.
40836:  */
40836: static void
41274: GC(JSContext *cx  GCTIMER_PARAM)
40836: {
40836:     JSRuntime *rt = cx->runtime;
40836:     rt->gcNumber++;
40836:     JS_ASSERT(!rt->gcUnmarkedArenaStackTop);
40836:     JS_ASSERT(rt->gcMarkLaterCount == 0);
40836: 
40836:     /*
40836:      * Mark phase.
40836:      */
40837:     JSTracer trc;
40836:     JS_TRACER_INIT(&trc, cx, NULL);
40836:     rt->gcMarkingTracer = &trc;
40836:     JS_ASSERT(IS_GC_MARKING_TRACER(&trc));
40836: 
42725:     for (JSGCChunkInfo **i = rt->gcChunks.begin(); i != rt->gcChunks.end(); ++i)
42725:         (*i)->clearMarkBitmap();
40836: 
41294:     js_TraceRuntime(&trc);
41294:     js_MarkScriptFilenames(rt);
40836: 
40836:     /*
40836:      * Mark children of things that caused too deep recursion during the above
40836:      * tracing.
40836:      */
40836:     MarkDelayedChildren(&trc);
40836: 
40836:     JS_ASSERT(!cx->insideGCMarkCallback);
40836:     if (rt->gcCallback) {
40836:         cx->insideGCMarkCallback = JS_TRUE;
40836:         (void) rt->gcCallback(cx, JSGC_MARK_END);
40836:         JS_ASSERT(cx->insideGCMarkCallback);
40836:         cx->insideGCMarkCallback = JS_FALSE;
40836:     }
40836:     JS_ASSERT(rt->gcMarkLaterCount == 0);
40836: 
40836:     rt->gcMarkingTracer = NULL;
40836: 
40836: #ifdef JS_THREADSAFE
41796:     JS_ASSERT(!cx->gcSweepTask);
41796:     if (!rt->gcHelperThread.busy())
41796:         cx->gcSweepTask = new js::BackgroundSweepTask();
40836: #endif
40836: 
40836:     /*
40836:      * Sweep phase.
40836:      *
40836:      * Finalize as we sweep, outside of rt->gcLock but with rt->gcRunning set
40836:      * so that any attempt to allocate a GC-thing from a finalizer will fail,
40836:      * rather than nest badly and leave the unmarked newborn to be swept.
40836:      *
40836:      * We first sweep atom state so we can use js_IsAboutToBeFinalized on
40836:      * JSString or jsdouble held in a hashtable to check if the hashtable
40836:      * entry can be freed. Note that even after the entry is freed, JSObject
40836:      * finalizers can continue to access the corresponding jsdouble* and
40836:      * JSString* assuming that they are unique. This works since the
40836:      * atomization API must not be called during GC.
40836:      */
41273:     TIMESTAMP(startSweep);
40836:     js_SweepAtomState(cx);
40836: 
40836:     /* Finalize watch points associated with unreachable objects. */
40836:     js_SweepWatchPoints(cx);
40836: 
40836: #ifdef DEBUG
40836:     /* Save the pre-sweep count of scope-mapped properties. */
40836:     rt->liveScopePropsPreSweep = rt->liveScopeProps;
40836: #endif
40836: 
40836:     /*
41286:      * We finalize iterators before other objects so the iterator can use the
41286:      * object which properties it enumerates over to finalize the enumeration
41286:      * state. We finalize objects before string, double and other GC things
40836:      * things to ensure that object's finalizer can access them even if they
40836:      * will be freed.
40836:      *
40836:      * To minimize the number of checks per each to be freed object and
40836:      * function we use separated list finalizers when a debug hook is
40836:      * installed.
40836:      */
41286:     JS_ASSERT(!rt->gcEmptyArenaList);
40836:     if (!cx->debugHooks->objectHook) {
41286:         FinalizeArenaList<JSObject, FinalizeObject>(cx, FINALIZE_OBJECT);
41286:         FinalizeArenaList<JSFunction, FinalizeFunction>(cx, FINALIZE_FUNCTION);
40836:     } else {
41286:         FinalizeArenaList<JSObject, FinalizeHookedObject>(cx, FINALIZE_OBJECT);
41286:         FinalizeArenaList<JSFunction, FinalizeHookedFunction>(cx, FINALIZE_FUNCTION);
40836:     }
40836: #if JS_HAS_XML_SUPPORT
41286:     FinalizeArenaList<JSXML, FinalizeXML>(cx, FINALIZE_XML);
40836: #endif
41273:     TIMESTAMP(sweepObjectEnd);
40836: 
40836:     /*
40836:      * We sweep the deflated cache before we finalize the strings so the
40836:      * cache can safely use js_IsAboutToBeFinalized..
40836:      */
40836:     rt->deflatedStringCache->sweep(cx);
40836: 
41286:     FinalizeArenaList<JSString, FinalizeString>(cx, FINALIZE_STRING);
40836:     for (unsigned i = FINALIZE_EXTERNAL_STRING0;
40836:          i <= FINALIZE_EXTERNAL_STRING_LAST;
40836:          ++i) {
41286:         FinalizeArenaList<JSString, FinalizeExternalString>(cx, i);
40836:     }
41273:     TIMESTAMP(sweepStringEnd);
40836: 
41286:     SweepDoubles(rt);
41273:     TIMESTAMP(sweepDoubleEnd);
40876: 
47498:     SweepCompartments(cx);
43286: 
40836:     /*
40836:      * Sweep the runtime's property tree after finalizing objects, in case any
40836:      * had watchpoints referencing tree nodes.
40836:      */
40836:     js::SweepScopeProperties(cx);
40836: 
40836:     /*
40836:      * Sweep script filenames after sweeping functions in the generic loop
40836:      * above. In this way when a scripted function's finalizer destroys the
40836:      * script and calls rt->destroyScriptHook, the hook can still access the
40836:      * script's filename. See bug 323267.
40836:      */
40836:     js_SweepScriptFilenames(rt);
40836: 
40836:     /*
40836:      * Destroy arenas after we finished the sweeping so finalizers can safely
40836:      * use js_IsAboutToBeFinalized().
40836:      */
41286:     FreeGCChunks(rt);
41273:     TIMESTAMP(sweepDestroyEnd);
40836: 
40836: #ifdef JS_THREADSAFE
41796:     if (cx->gcSweepTask) {
41796:         rt->gcHelperThread.schedule(cx->gcSweepTask);
41796:         cx->gcSweepTask = NULL;
41796:     }
40836: #endif
40836: 
40836:     if (rt->gcCallback)
40836:         (void) rt->gcCallback(cx, JSGC_FINALIZE_END);
40836: #ifdef DEBUG_srcnotesize
40836:   { extern void DumpSrcNoteSizeHist();
40836:     DumpSrcNoteSizeHist();
40836:     printf("GC HEAP SIZE %lu\n", (unsigned long)rt->gcBytes);
40836:   }
40836: #endif
40836: 
40836: #ifdef JS_SCOPE_DEPTH_METER
40836:   { static FILE *fp;
40836:     if (!fp)
40836:         fp = fopen("/tmp/scopedepth.stats", "w");
40836: 
40836:     if (fp) {
40836:         JS_DumpBasicStats(&rt->protoLookupDepthStats, "proto-lookup depth", fp);
40836:         JS_DumpBasicStats(&rt->scopeSearchDepthStats, "scope-search depth", fp);
40836:         JS_DumpBasicStats(&rt->hostenvScopeDepthStats, "hostenv scope depth", fp);
40836:         JS_DumpBasicStats(&rt->lexicalScopeDepthStats, "lexical scope depth", fp);
40836: 
40836:         putc('\n', fp);
40836:         fflush(fp);
40836:     }
40836:   }
40836: #endif /* JS_SCOPE_DEPTH_METER */
40836: 
40836: #ifdef JS_DUMP_LOOP_STATS
40836:   { static FILE *lsfp;
40836:     if (!lsfp)
40836:         lsfp = fopen("/tmp/loopstats", "w");
40836:     if (lsfp) {
40836:         JS_DumpBasicStats(&rt->loopStats, "loops", lsfp);
40836:         fflush(lsfp);
40836:     }
40836:   }
40836: #endif /* JS_DUMP_LOOP_STATS */
40836: }
40836: 
42715: #ifdef JS_THREADSAFE
43188: 
43188: /*
43188:  * If the GC is running and we're called on another thread, wait for this GC
43188:  * activation to finish. We can safely wait here without fear of deadlock (in
43188:  * the case where we are called within a request on another thread's context)
43188:  * because the GC doesn't set rt->gcRunning until after it has waited for all
43188:  * active requests to end.
43188:  *
43188:  * We call here js_CurrentThreadId() after checking for rt->gcState to avoid
43188:  * an expensive call when the GC is not running.
43188:  */
43188: void
43188: js_WaitForGC(JSRuntime *rt)
43188: {
43188:     if (rt->gcRunning && rt->gcThread->id != js_CurrentThreadId()) {
43188:         do {
43188:             JS_AWAIT_GC_DONE(rt);
43188:         } while (rt->gcRunning);
43188:     }
43188: }
43188: 
40836: /*
42715:  * GC is running on another thread. Temporarily suspend all requests running
42715:  * on the current thread and wait until the GC is done.
40841:  */
40841: static void
42770: LetOtherGCFinish(JSContext *cx)
41269: {
41269:     JSRuntime *rt = cx->runtime;
41269:     JS_ASSERT(rt->gcThread);
42715:     JS_ASSERT(cx->thread != rt->gcThread);
42715: 
43188:     size_t requestDebit = cx->thread->contextsInRequests;
41269:     JS_ASSERT(requestDebit <= rt->requestCount);
41269: #ifdef JS_TRACER
41269:     JS_ASSERT_IF(requestDebit == 0, !JS_ON_TRACE(cx));
41269: #endif
41269:     if (requestDebit != 0) {
41269: #ifdef JS_TRACER
41269:         if (JS_ON_TRACE(cx)) {
41269:             /*
41269:              * Leave trace before we decrease rt->requestCount and notify the
41269:              * GC. Otherwise the GC may start immediately after we unlock while
41269:              * this thread is still on trace.
41269:              */
41269:             AutoUnlockGC unlock(rt);
41269:             LeaveTrace(cx);
41269:         }
41269: #endif
41269:         rt->requestCount -= requestDebit;
41269:         if (rt->requestCount == 0)
41269:             JS_NOTIFY_REQUEST_DONE(rt);
42770:     }
41269: 
41269:     /* See comments before another call to js_ShareWaitingTitles below. */
41269:     cx->thread->gcWaiting = true;
41269:     js_ShareWaitingTitles(cx);
41269: 
41269:     /*
41269:      * Check that we did not release the GC lock above and let the GC to
41269:      * finish before we wait.
41269:      */
42715:     JS_ASSERT(rt->gcThread);
47439:     JS_THREAD_DATA(cx)->conservativeGC.enable(true);
42770: 
42770:     /*
42770:      * Wait for GC to finish on the other thread, even if requestDebit is 0
42770:      * and even if GC has not started yet because the gcThread is waiting in
42770:      * BeginGCSession. This ensures that js_GC never returns without a full GC
42770:      * cycle happening.
42770:      */
41269:     do {
41269:         JS_AWAIT_GC_DONE(rt);
42715:     } while (rt->gcThread);
41269: 
47439:     JS_THREAD_DATA(cx)->conservativeGC.disable();
41269:     cx->thread->gcWaiting = false;
41269:     rt->requestCount += requestDebit;
41269: }
41269: 
41269: #endif
41269: 
41269: /*
42715:  * Start a new GC session assuming no GC is running on this or other threads.
42770:  * Together with LetOtherGCFinish this function contains the rendezvous
42715:  * algorithm by which we stop the world for GC.
41269:  *
42715:  * This thread becomes the GC thread. Wait for all other threads to quiesce.
42715:  * Then set rt->gcRunning and return. The caller must call EndGCSession when
42715:  * GC work is done.
41269:  */
42715: static void
41274: BeginGCSession(JSContext *cx)
41269: {
41269:     JSRuntime *rt = cx->runtime;
42715:     JS_ASSERT(!rt->gcRunning);
41269: 
41269: #ifdef JS_THREADSAFE
42715:     /* No other thread is in GC, so indicate that we're now in GC. */
42715:     JS_ASSERT(!rt->gcThread);
42715:     rt->gcThread = cx->thread;
42715: 
41269:     /*
42715:      * Notify all operation callbacks, which will give them a chance to yield
42715:      * their current request. Contexts that are not currently executing will
42715:      * perform their callback at some later point, which then will be
42715:      * unnecessary, but harmless.
41269:      */
41269:     js_NudgeOtherContexts(cx);
41269: 
41269:     /*
42715:      * Discount all the requests on the current thread from contributing to
42715:      * rt->requestCount before we wait for all other requests to finish.
41269:      * JS_NOTIFY_REQUEST_DONE, which will wake us up, is only called on
41269:      * rt->requestCount transitions to 0.
41269:      */
43188:     size_t requestDebit = cx->thread->contextsInRequests;
41269:     JS_ASSERT_IF(cx->requestDepth != 0, requestDebit >= 1);
41269:     JS_ASSERT(requestDebit <= rt->requestCount);
41269:     if (requestDebit != rt->requestCount) {
41269:         rt->requestCount -= requestDebit;
41269: 
41269:         /*
41269:          * Share any title that is owned by the GC thread before we wait, to
41269:          * avoid a deadlock with ClaimTitle. We also set the gcWaiting flag so
41269:          * that ClaimTitle can claim the title ownership from the GC thread if
41269:          * that function is called while the GC is waiting.
41269:          */
41269:         cx->thread->gcWaiting = true;
41269:         js_ShareWaitingTitles(cx);
41269:         do {
41269:             JS_AWAIT_REQUEST_DONE(rt);
41269:         } while (rt->requestCount > 0);
41269:         cx->thread->gcWaiting = false;
41269:         rt->requestCount += requestDebit;
41269:     }
41269: 
42715: #endif /* JS_THREADSAFE */
41269: 
41269:     /*
41269:      * Set rt->gcRunning here within the GC lock, and after waiting for any
42715:      * active requests to end. This way js_WaitForGC called outside a request
42715:      * would not block on the GC that is waiting for other requests to finish
42715:      * with rt->gcThread set while JS_BeginRequest would do such wait.
41269:      */
42715:     rt->gcRunning = true;
41269: }
41269: 
41269: /* End the current GC session and allow other threads to proceed. */
41269: static void
41269: EndGCSession(JSContext *cx)
41269: {
41269:     JSRuntime *rt = cx->runtime;
41269: 
42715:     rt->gcRunning = false;
41269: #ifdef JS_THREADSAFE
41269:     JS_ASSERT(rt->gcThread == cx->thread);
41269:     rt->gcThread = NULL;
41269:     JS_NOTIFY_GC_DONE(rt);
41269: #endif
41269: }
41269: 
40841: /*
42715:  * GC, repeatedly if necessary, until we think we have not created any new
42715:  * garbage and no other threads are demanding more GC.
40839:  */
42715: static void
42715: GCUntilDone(JSContext *cx, JSGCInvocationKind gckind  GCTIMER_PARAM)
40839: {
42715:     if (JS_ON_TRACE(cx))
42715:         return;
42715: 
40839:     JSRuntime *rt = cx->runtime;
42715: 
42715:     /* Recursive GC or a call from another thread restarts the GC cycle. */
42715: #ifndef JS_THREADSAFE
42715:     if (rt->gcRunning) {
42715:         rt->gcPoke = true;
42715:         return;
42715:     }
42715: #else /* JS_THREADSAFE */
42715:     if (rt->gcThread) {
42715:         rt->gcPoke = true;
42715:         if (cx->thread == rt->gcThread) {
42715:             JS_ASSERT(rt->gcRunning);
42715:             return;
42715:         }
42770:         LetOtherGCFinish(cx);
40839: 
40839:         /*
42715:          * Check if the GC on another thread have collected the garbage and
42715:          * it was not a set slot request.
40839:          */
42715:         if (!rt->gcPoke)
42715:             return;
40839:     }
42715: #endif /* JS_THREADSAFE */
42715: 
42715:     BeginGCSession(cx);
42715: 
42715:     METER(rt->gcStats.poke++);
42715: 
47439:     /*
47439:      * Do not scan the current thread on the shutdown or when the GC is called
47439:      * outside a request.
47439:      */
47439:     bool scanGCThreadStack =
47439: #ifdef JS_THREADSAFE
47439:                              (cx->thread->contextsInRequests != 0) &&
47439: #endif
47439:                              (rt->state != JSRTS_LANDING);
47439:     if (scanGCThreadStack)
47439:         JS_THREAD_DATA(cx)->conservativeGC.enable(true);
42715:     bool firstRun = true;
42715:     do {
42715:         rt->gcPoke = false;
42715: 
42715:         AutoUnlockGC unlock(rt);
42715:         if (firstRun) {
42715:             PreGCCleanup(cx, gckind);
42715:             TIMESTAMP(startMark);
42715:             firstRun = false;
40839:         }
42715:         GC(cx  GCTIMER_ARG);
42715: 
42715:         // GC again if:
42715:         //   - another thread, not in a request, called js_GC
42715:         //   - js_GC was called recursively
42715:         //   - a finalizer called js_RemoveRoot or js_UnlockGCThingRT.
42715:     } while (rt->gcPoke);
42715: 
47439:     if (scanGCThreadStack)
47439:         JS_THREAD_DATA(cx)->conservativeGC.disable();
47439: 
42715:     rt->gcRegenShapes = false;
42715:     rt->setGCLastBytes(rt->gcBytes);
42715: 
42715:     EndGCSession(cx);
41271: }
41271: 
41271: /*
41271:  * The gckind flag bit GC_LOCK_HELD indicates a call from js_NewGCThing with
41271:  * rt->gcLock already held, so the lock should be kept on return.
41271:  */
41271: void
41271: js_GC(JSContext *cx, JSGCInvocationKind gckind)
41271: {
41271:     JSRuntime *rt = cx->runtime;
41271: 
41271:     /*
41271:      * Don't collect garbage if the runtime isn't up, and cx is not the last
41271:      * context in the runtime.  The last context must force a GC, and nothing
41271:      * should suppress that final collection or there may be shutdown leaks,
41271:      * or runtime bloat until the next context is created.
41271:      */
41271:     if (rt->state != JSRTS_UP && gckind != GC_LAST_CONTEXT)
41271:         return;
41271: 
41273:     GCTIMER_BEGIN();
41271: 
42715:     do {
42715:         /*
42715:          * Let the API user decide to defer a GC if it wants to (unless this
42715:          * is the last context).  Invoke the callback regardless. Sample the
42715:          * callback in case we are freely racing with a JS_SetGCCallback{,RT}
42715:          * on another thread.
42715:          */
42715:         if (JSGCCallback callback = rt->gcCallback) {
42715:             Conditionally<AutoUnlockGC> unlockIf(!!(gckind & GC_LOCK_HELD), rt);
42715:             if (!callback(cx, JSGC_BEGIN) && gckind != GC_LAST_CONTEXT)
41271:                 return;
42715:         }
41271: 
41272:         {
41271:             /* Lock out other GC allocator and collector invocations. */
41272:             Conditionally<AutoLockGC> lockIf(!(gckind & GC_LOCK_HELD), rt);
41271: 
42715:             GCUntilDone(cx, gckind  GCTIMER_ARG);
11041:         }
41271: 
42715:         /* We re-sample the callback again as the finalizers can change it. */
42715:         if (JSGCCallback callback = rt->gcCallback) {
42715:             Conditionally<AutoUnlockGC> unlockIf(gckind & GC_LOCK_HELD, rt);
42715: 
42715:             (void) callback(cx, JSGC_END);
41270:         }
41270: 
42715:         /*
42715:          * On shutdown, iterate until the JSGC_END callback stops creating
42715:          * garbage.
42715:          */
42715:     } while (gckind == GC_LAST_CONTEXT && rt->gcPoke);
42715: 
42715:     GCTIMER_END(gckind == GC_LAST_CONTEXT);
41272: }
41272: 
47465: namespace js {
47465: 
42715: bool
47465: SetProtoCheckingForCycles(JSContext *cx, JSObject *obj, JSObject *proto)
42715: {
42715:     JSRuntime *rt = cx->runtime;
42715: 
42715:     /*
42715:      * This function cannot be called during the GC and always requires a
42715:      * request.
42715:      */
42715: #ifdef JS_THREADSAFE
42715:     JS_ASSERT(cx->requestDepth);
42715: #endif
42715: 
42715:     AutoLockGC lock(rt);
42715: 
42715:     /*
42715:      * The set slot request cannot be called recursively and must not be
42715:      * called during a normal GC. So if at this point JSRuntime::gcThread is
42715:      * set it must be a GC or a set slot request from another thread.
42715:      */
42715: #ifdef JS_THREADSAFE
42715:     if (rt->gcThread) {
42715:         JS_ASSERT(cx->thread != rt->gcThread);
42770:         LetOtherGCFinish(cx);
42715:     }
42715: #endif
42715: 
42715:     BeginGCSession(cx);
42715: 
42715:     bool cycle;
42715:     {
42715:         AutoUnlockGC unlock(rt);
42715: 
42715:         cycle = false;
47465:         for (JSObject *obj2 = proto; obj2;) {
43261:             obj2 = obj2->wrappedObject(cx);
42715:             if (obj2 == obj) {
42715:                 cycle = true;
41272:                 break;
41272:             }
47465:             obj2 = obj2->getProto();
47465:         }
47465:         if (!cycle)
47465:             obj->setProto(proto);
42715:     }
42715: 
42715:     EndGCSession(cx);
42715: 
42715:     return !cycle;
42715: }
43286: 
43286: JSCompartment *
43286: NewCompartment(JSContext *cx)
43286: {
43286:     JSRuntime *rt = cx->runtime;
43286:     JSCompartment *compartment = new JSCompartment(rt);
47498:     if (!compartment || !compartment->init()) {
43286:         JS_ReportOutOfMemory(cx);
43286:         return false;
43286:     }
43286: 
43286:     AutoLockGC lock(rt);
43286: 
43286:     if (!rt->compartments.append(compartment)) {
43286:         AutoUnlockGC unlock(rt);
43286:         JS_ReportOutOfMemory(cx);
43286:         return false;
43286:     }
43286: 
43286:     return compartment;
43286: }
43286: 
47498: }
