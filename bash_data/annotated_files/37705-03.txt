30266: /* -*- Mode: C++; c-basic-offset: 4; indent-tabs-mode: nil; tab-width: 4 -*- */
30266: /* vi: set ts=4 sw=4 expandtab: (add to ~/.vimrc: set modeline modelines=5) */
18774: /* ***** BEGIN LICENSE BLOCK *****
18774:  * Version: MPL 1.1/GPL 2.0/LGPL 2.1
18774:  *
18774:  * The contents of this file are subject to the Mozilla Public License Version
18774:  * 1.1 (the "License"); you may not use this file except in compliance with
18774:  * the License. You may obtain a copy of the License at
18774:  * http://www.mozilla.org/MPL/
18774:  *
18774:  * Software distributed under the License is distributed on an "AS IS" basis,
18774:  * WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
18774:  * for the specific language governing rights and limitations under the
18774:  * License.
18774:  *
18774:  * The Original Code is [Open Source Virtual Machine].
18774:  *
18774:  * The Initial Developer of the Original Code is
18774:  * Adobe System Incorporated.
18774:  * Portions created by the Initial Developer are Copyright (C) 2004-2007
18774:  * the Initial Developer. All Rights Reserved.
18774:  *
18774:  * Contributor(s):
18774:  *   Adobe AS3 Team
18775:  *   Vladimir Vukicevic <vladimir@pobox.com>
18774:  *
18774:  * Alternatively, the contents of this file may be used under the terms of
18774:  * either the GNU General Public License Version 2 or later (the "GPL"), or
18774:  * the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
18774:  * in which case the provisions of the GPL or the LGPL are applicable instead
18774:  * of those above. If you wish to allow use of your version of this file only
18774:  * under the terms of either the GPL or the LGPL, and not to allow others to
18774:  * use your version of this file under the terms of the MPL, indicate your
18774:  * decision by deleting the provisions above and replace them with the notice
18774:  * and other provisions required by the GPL or the LGPL. If you do not delete
18774:  * the provisions above, a recipient may use your version of this file under
18774:  * the terms of any one of the MPL, the GPL or the LGPL.
18774:  *
18774:  * ***** END LICENSE BLOCK ***** */
18774: 
18774: #include "nanojit.h"
18774: 
18774: #ifdef UNDER_CE
18774: #include <cmnintrin.h>
34298: extern "C" bool blx_lr_broken();
18774: #endif
18774: 
34333: #if defined(FEATURE_NANOJIT) && defined(NANOJIT_ARM)
18776: 
18774: namespace nanojit
18774: {
18774: 
18774: #ifdef NJ_VERBOSE
26536: const char* regNames[] = {"r0","r1","r2","r3","r4","r5","r6","r7","r8","r9","r10","fp","ip","sp","lr","pc",
18776:                           "d0","d1","d2","d3","d4","d5","d6","d7","s14"};
26536: const char* condNames[] = {"eq","ne","cs","cc","mi","pl","vs","vc","hi","ls","ge","lt","gt","le",""/*al*/,"nv"};
26536: const char* shiftNames[] = { "lsl", "lsl", "lsr", "lsr", "asr", "asr", "ror", "ror" };
18775: #endif
18774: 
18774: const Register Assembler::argRegs[] = { R0, R1, R2, R3 };
18774: const Register Assembler::retRegs[] = { R0, R1 };
20919: const Register Assembler::savedRegs[] = { R4, R5, R6, R7, R8, R9, R10 };
18774: 
29861: // --------------------------------
29861: // ARM-specific utility functions.
29861: // --------------------------------
29861: 
30460: #ifdef DEBUG
30460: // Return true if enc is a valid Operand 2 encoding and thus can be used as-is
30460: // in an ARM arithmetic operation that accepts such encoding.
30460: //
30460: // This utility does not know (or determine) the actual value that the encoded
30460: // value represents, and thus cannot be used to ensure the correct operation of
30460: // encOp2Imm, but it does ensure that the encoded value can be used to encode a
30460: // valid ARM instruction. decOp2Imm can be used if you also need to check that
30460: // a literal is correctly encoded (and thus that encOp2Imm is working
30460: // correctly).
30460: inline bool
30460: Assembler::isOp2Imm(uint32_t enc)
30460: {
30460:     return ((enc & 0xfff) == enc);
30460: }
30460: 
30460: // Decodes operand 2 immediate values (for debug output and assertions).
30460: inline uint32_t
30460: Assembler::decOp2Imm(uint32_t enc)
30460: {
30460:     NanoAssert(isOp2Imm(enc));
30460: 
30460:     uint32_t    imm8 = enc & 0xff;
30460:     uint32_t    rot = 32 - ((enc >> 7) & 0x1e);
30460: 
30460:     return imm8 << (rot & 0x1f);
30460: }
30460: #endif
30460: 
29861: // Calculate the number of leading zeroes in data.
30460: inline uint32_t
29861: Assembler::CountLeadingZeroes(uint32_t data)
29861: {
29861:     uint32_t    leading_zeroes;
30460: 
30460:     // We can't do CLZ on anything earlier than ARMv5. Architectures as early
30460:     // as that aren't supported, but assert that we aren't running on one
30460:     // anyway.
33993:     // If ARMv4 support is required in the future for some reason, we can do a
33993:     // run-time check on config.arch and fall back to the C routine, but for
33993:     // now we can avoid the cost of the check as we don't intend to support
33993:     // ARMv4 anyway.
33942:     NanoAssert(ARM_ARCH >= 5);
30460: 
29861: #if defined(__ARMCC__)
29861:     // ARMCC can do this with an intrinsic.
29861:     leading_zeroes = __clz(data);
33993: 
33993: // current Android GCC compiler incorrectly refuses to compile 'clz' for armv5
33993: // (even though this is a legal instruction there). Since we currently only compile for ARMv5
33993: // for emulation, we don't care too much (but we DO care for ARMv6+ since those are "real"
33993: // devices).
33993: #elif defined(__GNUC__) && !(defined(ANDROID) && __ARM_ARCH__ <= 5)
29861:     // GCC can use inline assembler to insert a CLZ instruction.
29861:     __asm (
29861:         "   clz     %0, %1  \n"
29861:         :   "=r"    (leading_zeroes)
29861:         :   "r"     (data)
29861:     );
37671: #elif defined(UNDER_CE)
29861:     // WinCE can do this with an intrinsic.
29861:     leading_zeroes = _CountLeadingZeros(data);
29861: #else
29861:     // Other platforms must fall back to a C routine. This won't be as
29861:     // efficient as the CLZ instruction, but it is functional.
29861:     uint32_t    try_shift;
29861: 
29861:     leading_zeroes = 0;
29861: 
29861:     // This loop does a bisection search rather than the obvious rotation loop.
29861:     // This should be faster, though it will still be no match for CLZ.
29861:     for (try_shift = 16; try_shift != 0; try_shift /= 2) {
29861:         uint32_t    shift = leading_zeroes + try_shift;
29861:         if (((data << shift) >> shift) == data) {
29861:             leading_zeroes = shift;
29861:         }
29861:     }
29861: #endif
29861: 
30460:     // Assert that the operation worked!
30460:     NanoAssert(((0xffffffff >> leading_zeroes) & data) == data);
30460: 
29861:     return leading_zeroes;
29861: }
29861: 
30460: // The ARM instruction set allows some flexibility to the second operand of
30460: // most arithmetic operations. When operand 2 is an immediate value, it takes
30460: // the form of an 8-bit value rotated by an even value in the range 0-30.
30460: //
30460: // Some values that can be encoded this scheme — such as 0xf000000f — are
30460: // probably fairly rare in practice and require extra code to detect, so this
30460: // function implements a fast CLZ-based heuristic to detect any value that can
30460: // be encoded using just a shift, and not a full rotation. For example,
30460: // 0xff000000 and 0x000000ff are both detected, but 0xf000000f is not.
30460: //
30460: // This function will return true to indicate that the encoding was successful,
30460: // or false to indicate that the literal could not be encoded as an operand 2
30460: // immediate. If successful, the encoded value will be written to *enc.
30460: inline bool
30460: Assembler::encOp2Imm(uint32_t literal, uint32_t * enc)
30460: {
30460:     // The number of leading zeroes in the literal. This is used to calculate
30460:     // the rotation component of the encoding.
30460:     uint32_t    leading_zeroes;
30460: 
30460:     // Components of the operand 2 encoding.
34303:     int32_t    rot;
30460:     uint32_t    imm8;
30460: 
30460:     // Check the literal to see if it is a simple 8-bit value. I suspect that
30460:     // most literals are in fact small values, so doing this check early should
30460:     // give a decent speed-up.
30460:     if (literal < 256)
30460:     {
30460:         *enc = literal;
30460:         return true;
30460:     }
30460: 
30460:     // Determine the number of leading zeroes in the literal. This is used to
30460:     // calculate the required rotation.
30460:     leading_zeroes = CountLeadingZeroes(literal);
30460: 
30460:     // We've already done a check to see if the literal is an 8-bit value, so
30460:     // leading_zeroes must be less than (and not equal to) (32-8)=24. However,
30460:     // if it is greater than 24, this algorithm will break, so debug code
30460:     // should use an assertion here to check that we have a value that we
30460:     // expect.
30460:     NanoAssert(leading_zeroes < 24);
30460: 
30460:     // Assuming that we have a field of no more than 8 bits for a valid
30460:     // literal, we can calculate the required rotation by subtracting
30460:     // leading_zeroes from (32-8):
30460:     //
30460:     // Example:
30460:     //      0: Known to be zero.
30460:     //      1: Known to be one.
30460:     //      X: Either zero or one.
30460:     //      .: Zero in a valid operand 2 literal.
30460:     //
30460:     //  Literal:     [ 1XXXXXXX ........ ........ ........ ]
30460:     //  leading_zeroes = 0
30460:     //  Therefore rot (left) = 24.
30460:     //  Encoded 8-bit literal:                  [ 1XXXXXXX ]
30460:     //
30460:     //  Literal:     [ ........ ..1XXXXX XX...... ........ ]
30460:     //  leading_zeroes = 10
30460:     //  Therefore rot (left) = 14.
30460:     //  Encoded 8-bit literal:                  [ 1XXXXXXX ]
30460:     //
30460:     // Note, however, that we can only encode even shifts, and so
30460:     // "rot=24-leading_zeroes" is not sufficient by itself. By ignoring
30460:     // zero-bits in odd bit positions, we can ensure that we get a valid
30460:     // encoding.
30460:     //
30460:     // Example:
30460:     //  Literal:     [ 01XXXXXX ........ ........ ........ ]
30460:     //  leading_zeroes = 1
30460:     //  Therefore rot (left) = round_up(23) = 24.
30460:     //  Encoded 8-bit literal:                  [ 01XXXXXX ]
30460:     rot = 24 - (leading_zeroes & ~1);
30460: 
30460:     // The imm8 component of the operand 2 encoding can be calculated from the
30460:     // rot value.
30460:     imm8 = literal >> rot;
30460: 
30460:     // The validity of the literal can be checked by reversing the
30460:     // calculation. It is much easier to decode the immediate than it is to
30460:     // encode it!
30460:     if (literal != (imm8 << rot)) {
30460:         // The encoding is not valid, so report the failure. Calling code
30460:         // should use some other method of loading the value (such as LDR).
30460:         return false;
30460:     }
30460: 
30460:     // The operand is valid, so encode it.
30460:     // Note that the ARM encoding is actually described by a rotate to the
30460:     // _right_, so rot must be negated here. Calculating a left shift (rather
30460:     // than calculating a right rotation) simplifies the above code.
30460:     *enc = ((-rot << 7) & 0xf00) | imm8;
30460: 
30460:     // Assert that the operand was properly encoded.
30460:     NanoAssert(decOp2Imm(*enc) == literal);
30460: 
30460:     return true;
30460: }
30460: 
30460: // Encode "rd = rn + imm" using an appropriate instruction sequence.
30460: // Set stat to 1 to update the status flags. Otherwise, set it to 0 or omit it.
30460: // (The declaration in NativeARM.h defines the default value of stat as 0.)
30460: //
30460: // It is not valid to call this function if:
30460: //   (rd == IP) AND (rn == IP) AND !encOp2Imm(imm) AND !encOp2Imm(-imm)
30460: // Where: if (encOp2Imm(imm)), imm can be encoded as an ARM operand 2 using the
30460: // encOp2Imm method.
30460: void
30460: Assembler::asm_add_imm(Register rd, Register rn, int32_t imm, int stat /* =0 */)
30460: {
30460:     // Operand 2 encoding of the immediate.
30460:     uint32_t    op2imm;
30460: 
30460:     NanoAssert(IsGpReg(rd));
30460:     NanoAssert(IsGpReg(rn));
30460:     NanoAssert((stat & 1) == stat);
30460: 
30460:     // Try to encode the value directly as an operand 2 immediate value, then
30460:     // fall back to loading the value into a register.
30460:     if (encOp2Imm(imm, &op2imm)) {
30460:         ADDis(rd, rn, op2imm, stat);
30460:     } else if (encOp2Imm(-imm, &op2imm)) {
30460:         // We could not encode the value for ADD, so try to encode it for SUB.
30460:         // Note that this is valid even if stat is set, _unless_ imm is 0, but
30460:         // that case is caught above.
30460:         NanoAssert(imm != 0);
30460:         SUBis(rd, rn, op2imm, stat);
30460:     } else {
30460:         // We couldn't encode the value directly, so use an intermediate
30460:         // register to encode the value. We will use IP to do this unless rn is
30460:         // IP; in that case we can reuse rd. This allows every case other than
30460:         // "ADD IP, IP, =#imm".
30460:         Register    rm = (rn == IP) ? (rd) : (IP);
30460:         NanoAssert(rn != rm);
30460: 
30460:         ADDs(rd, rn, rm, stat);
30460:         asm_ld_imm(rm, imm);
30460:     }
30460: }
30460: 
30460: // Encode "rd = rn - imm" using an appropriate instruction sequence.
30460: // Set stat to 1 to update the status flags. Otherwise, set it to 0 or omit it.
30460: // (The declaration in NativeARM.h defines the default value of stat as 0.)
30460: //
30460: // It is not valid to call this function if:
30460: //   (rd == IP) AND (rn == IP) AND !encOp2Imm(imm) AND !encOp2Imm(-imm)
30460: // Where: if (encOp2Imm(imm)), imm can be encoded as an ARM operand 2 using the
30460: // encOp2Imm method.
30460: void
30460: Assembler::asm_sub_imm(Register rd, Register rn, int32_t imm, int stat /* =0 */)
30460: {
30460:     // Operand 2 encoding of the immediate.
30460:     uint32_t    op2imm;
30460: 
30460:     NanoAssert(IsGpReg(rd));
30460:     NanoAssert(IsGpReg(rn));
30460:     NanoAssert((stat & 1) == stat);
30460: 
30460:     // Try to encode the value directly as an operand 2 immediate value, then
30460:     // fall back to loading the value into a register.
30460:     if (encOp2Imm(imm, &op2imm)) {
30460:         SUBis(rd, rn, op2imm, stat);
30460:     } else if (encOp2Imm(-imm, &op2imm)) {
30460:         // We could not encode the value for SUB, so try to encode it for ADD.
30460:         // Note that this is valid even if stat is set, _unless_ imm is 0, but
30460:         // that case is caught above.
30460:         NanoAssert(imm != 0);
30460:         ADDis(rd, rn, op2imm, stat);
30460:     } else {
30460:         // We couldn't encode the value directly, so use an intermediate
30460:         // register to encode the value. We will use IP to do this unless rn is
30460:         // IP; in that case we can reuse rd. This allows every case other than
30460:         // "SUB IP, IP, =#imm".
30460:         Register    rm = (rn == IP) ? (rd) : (IP);
30460:         NanoAssert(rn != rm);
30460: 
30460:         SUBs(rd, rn, rm, stat);
30460:         asm_ld_imm(rm, imm);
30460:     }
30460: }
30460: 
30460: // Encode "rd = rn & imm" using an appropriate instruction sequence.
30460: // Set stat to 1 to update the status flags. Otherwise, set it to 0 or omit it.
30460: // (The declaration in NativeARM.h defines the default value of stat as 0.)
30460: //
30460: // It is not valid to call this function if:
30460: //   (rd == IP) AND (rn == IP) AND !encOp2Imm(imm) AND !encOp2Imm(~imm)
30460: // Where: if (encOp2Imm(imm)), imm can be encoded as an ARM operand 2 using the
30460: // encOp2Imm method.
30460: void
30460: Assembler::asm_and_imm(Register rd, Register rn, int32_t imm, int stat /* =0 */)
30460: {
30460:     // Operand 2 encoding of the immediate.
30460:     uint32_t    op2imm;
30460: 
30460:     NanoAssert(IsGpReg(rd));
30460:     NanoAssert(IsGpReg(rn));
30460:     NanoAssert((stat & 1) == stat);
30460: 
30460:     // Try to encode the value directly as an operand 2 immediate value, then
30460:     // fall back to loading the value into a register.
30460:     if (encOp2Imm(imm, &op2imm)) {
30460:         ANDis(rd, rn, op2imm, stat);
30460:     } else if (encOp2Imm(~imm, &op2imm)) {
30460:         // Use BIC with the inverted immediate.
30460:         BICis(rd, rn, op2imm, stat);
30460:     } else {
30460:         // We couldn't encode the value directly, so use an intermediate
30460:         // register to encode the value. We will use IP to do this unless rn is
30460:         // IP; in that case we can reuse rd. This allows every case other than
30460:         // "AND IP, IP, =#imm".
30460:         Register    rm = (rn == IP) ? (rd) : (IP);
30460:         NanoAssert(rn != rm);
30460: 
30460:         ANDs(rd, rn, rm, stat);
30460:         asm_ld_imm(rm, imm);
30460:     }
30460: }
30460: 
30460: // Encode "rd = rn | imm" using an appropriate instruction sequence.
30460: // Set stat to 1 to update the status flags. Otherwise, set it to 0 or omit it.
30460: // (The declaration in NativeARM.h defines the default value of stat as 0.)
30460: //
30460: // It is not valid to call this function if:
30460: //   (rd == IP) AND (rn == IP) AND !encOp2Imm(imm)
30460: // Where: if (encOp2Imm(imm)), imm can be encoded as an ARM operand 2 using the
30460: // encOp2Imm method.
30460: void
30460: Assembler::asm_orr_imm(Register rd, Register rn, int32_t imm, int stat /* =0 */)
30460: {
30460:     // Operand 2 encoding of the immediate.
30460:     uint32_t    op2imm;
30460: 
30460:     NanoAssert(IsGpReg(rd));
30460:     NanoAssert(IsGpReg(rn));
30460:     NanoAssert((stat & 1) == stat);
30460: 
30460:     // Try to encode the value directly as an operand 2 immediate value, then
30460:     // fall back to loading the value into a register.
30460:     if (encOp2Imm(imm, &op2imm)) {
30460:         ORRis(rd, rn, op2imm, stat);
30460:     } else {
30460:         // We couldn't encode the value directly, so use an intermediate
30460:         // register to encode the value. We will use IP to do this unless rn is
30460:         // IP; in that case we can reuse rd. This allows every case other than
30460:         // "ORR IP, IP, =#imm".
30460:         Register    rm = (rn == IP) ? (rd) : (IP);
30460:         NanoAssert(rn != rm);
30460: 
30460:         ORRs(rd, rn, rm, stat);
30460:         asm_ld_imm(rm, imm);
30460:     }
30460: }
30460: 
30460: // Encode "rd = rn ^ imm" using an appropriate instruction sequence.
30460: // Set stat to 1 to update the status flags. Otherwise, set it to 0 or omit it.
30460: // (The declaration in NativeARM.h defines the default value of stat as 0.)
30460: //
30460: // It is not valid to call this function if:
30460: //   (rd == IP) AND (rn == IP) AND !encOp2Imm(imm)
30460: // Where: if (encOp2Imm(imm)), imm can be encoded as an ARM operand 2 using the
30460: // encOp2Imm method.
30460: void
30460: Assembler::asm_eor_imm(Register rd, Register rn, int32_t imm, int stat /* =0 */)
30460: {
30460:     // Operand 2 encoding of the immediate.
30460:     uint32_t    op2imm;
30460: 
30460:     NanoAssert(IsGpReg(rd));
30460:     NanoAssert(IsGpReg(rn));
30460:     NanoAssert((stat & 1) == stat);
30460: 
30460:     // Try to encode the value directly as an operand 2 immediate value, then
30460:     // fall back to loading the value into a register.
30460:     if (encOp2Imm(imm, &op2imm)) {
30460:         EORis(rd, rn, op2imm, stat);
30460:     } else {
30460:         // We couldn't encoder the value directly, so use an intermediate
30460:         // register to encode the value. We will use IP to do this unless rn is
30460:         // IP; in that case we can reuse rd. This allows every case other than
30460:         // "EOR IP, IP, =#imm".
30460:         Register    rm = (rn == IP) ? (rd) : (IP);
30460:         NanoAssert(rn != rm);
30460: 
30460:         EORs(rd, rn, rm, stat);
30460:         asm_ld_imm(rm, imm);
30460:     }
30460: }
30460: 
29861: // --------------------------------
29861: // Assembler functions.
29861: // --------------------------------
29861: 
18775: void
18775: Assembler::nInit(AvmCore*)
18774: {
33943: #ifdef UNDER_CE
33943:     blx_lr_bug = blx_lr_broken();
33943: #else
33943:     blx_lr_bug = 0;
33943: #endif
18774: }
18774: 
32635: void Assembler::nBeginAssembly()
32635: {
32719:     max_out_args = 0;
32635: }
32635: 
18775: NIns*
20919: Assembler::genPrologue()
18774: {
18774:     /**
18774:      * Prologue
18774:      */
18774: 
18774:     // NJ_RESV_OFFSET is space at the top of the stack for us
18774:     // to use for parameter passing (8 bytes at the moment)
36664:     uint32_t stackNeeded = max_out_args + STACK_GRANULARITY * _activation.stackSlotsNeeded();
26536:     uint32_t savingCount = 2;
18774: 
32711:     uint32_t savingMask = rmask(FP) | rmask(LR);
32711: 
20926:     // so for alignment purposes we've pushed return addr and fp
20919:     uint32_t stackPushed = STACK_GRANULARITY * savingCount;
18774:     uint32_t aligned = alignUp(stackNeeded + stackPushed, NJ_ALIGN_STACK);
18774:     int32_t amt = aligned - stackPushed;
18774: 
18774:     // Make room on stack for what we are doing
18774:     if (amt)
30460:         asm_sub_imm(SP, SP, amt);
18774: 
29883:     verbose_only( asm_output("## %p:",(void*)_nIns); )
29883:     verbose_only( asm_output("## patch entry"); )
18774:     NIns *patchEntry = _nIns;
18774: 
26537:     MOV(FP, SP);
32711:     PUSH_mask(savingMask);
18774:     return patchEntry;
18774: }
18774: 
18775: void
18775: Assembler::nFragExit(LInsp guard)
18774: {
20940:     SideExit *  exit = guard->record()->exit;
18774:     Fragment *  frag = exit->target;
18775: 
29876:     bool        target_is_known = frag && frag->fragEntry;
29876: 
29876:     if (target_is_known) {
29876:         // The target exists so we can simply emit a branch to its location.
21548:         JMP_far(frag->fragEntry);
18775:     } else {
29876:         // The target doesn't exit yet, so emit a jump to the epilogue. If the
29876:         // target is created later on, the jump will be patched.
18774: 
29876:         GuardRecord *gr = guard->record();
29876: 
32634:         if (!_epilogue)
32634:             _epilogue = genEpilogue();
32634: 
29876:         // Jump to the epilogue. This may get patched later, but JMP_far always
29876:         // emits two instructions even when only one is required, so patching
29876:         // will work correctly.
20937:         JMP_far(_epilogue);
18774: 
34576:         // In the future you may want to move this further down so that we can
34576:         // overwrite the r0 guard record load during a patch to a different
34576:         // fragment with some assumed input-register state. Not today though.
34576:         gr->jmp = _nIns;
29876: 
34576:         // NB: this is a workaround for the fact that, by patching a
34576:         // fragment-exit jump, we could be changing the *meaning* of the R0
34576:         // register we're passing to the jump target. If we jump to the
34576:         // epilogue, ideally R0 means "return value when exiting fragment".
34576:         // If we patch this to jump to another fragment however, R0 means
34576:         // "incoming 0th parameter". This is just a quirk of ARM ABI. So
34576:         // we compromise by passing "return value" to the epilogue in IP,
34576:         // not R0, and have the epilogue MOV(R0, IP) first thing.
34576: 
34576:         asm_ld_imm(IP, int(gr));
18774:     }
18774: 
18774: #ifdef NJ_VERBOSE
31475:     if (config.show_stats) {
18774:         // load R1 with Fragment *fromFrag, target fragment
18774:         // will make use of this when calling fragenter().
18774:         int fromfrag = int((Fragment*)_thisfrag);
30460:         asm_ld_imm(argRegs[1], fromfrag);
18774:     }
18774: #endif
18774: 
35091:     // profiling for the exit
35091:     verbose_only(
35091:        if (_logc->lcbits & LC_FragProfile) {
35091:            asm_inc_m32( &guard->record()->profCount );
35091:        }
35091:     )
35091: 
29876:     // Pop the stack frame.
29876:     MOV(SP, FP);
18774: }
18774: 
18775: NIns*
20919: Assembler::genEpilogue()
18774: {
28544:     // On ARMv5+, loading directly to PC correctly handles interworking.
28544:     // Note that we don't support anything older than ARMv5.
33942:     NanoAssert(ARM_ARCH >= 5);
20919: 
32711:     RegisterMask savingMask = rmask(FP) | rmask(PC);
32711: 
32711:     POP_mask(savingMask); // regs
32711: 
34576:     // NB: this is the later half of the dual-nature patchable exit branch
34576:     // workaround noted above in nFragExit. IP has the "return value"
34576:     // incoming, we need to move it to R0.
34576:     MOV(R0, IP);
34576: 
18774:     return _nIns;
18774: }
18774: 
32555: /*
32555:  * asm_arg will encode the specified argument according to the current ABI, and
32555:  * will update r and stkd as appropriate so that the next argument can be
32555:  * encoded.
32555:  *
32555:  * Linux has used ARM's EABI for some time. Windows CE uses the legacy ABI.
25562:  *
26678:  * Under EABI:
26678:  * - doubles are 64-bit aligned both in registers and on the stack.
26678:  *   If the next available argument register is R1, it is skipped
26678:  *   and the double is placed in R2:R3.  If R0:R1 or R2:R3 are not
26678:  *   available, the double is placed on the stack, 64-bit aligned.
26678:  * - 32-bit arguments are placed in registers and 32-bit aligned
26678:  *   on the stack.
25562:  *
26678:  * Under legacy ABI:
26678:  * - doubles are placed in subsequent arg registers; if the next
26678:  *   available register is r3, the low order word goes into r3
26678:  *   and the high order goes on the stack.
26678:  * - 32-bit arguments are placed in the next available arg register,
26678:  * - both doubles and 32-bit arguments are placed on stack with 32-bit
26678:  *   alignment.
25562:  */
26678: void
26678: Assembler::asm_arg(ArgSize sz, LInsp arg, Register& r, int& stkd)
26678: {
32555:     // The stack pointer must always be at least aligned to 4 bytes.
32555:     NanoAssert((stkd & 3) == 0);
30460: 
26678:     if (sz == ARGSIZE_F) {
32555:         // This task is fairly complex and so is delegated to asm_arg_64.
32555:         asm_arg_64(arg, r, stkd);
32555:     } else if (sz & ARGSIZE_MASK_INT) {
32555:         // pre-assign registers R0-R3 for arguments (if they fit)
32555:         if (r < R4) {
32555:             asm_regarg(sz, arg, r);
32555:             r = nextreg(r);
32555:         } else {
32555:             asm_stkarg(arg, stkd);
32555:             stkd += 4;
32555:         }
32555:     } else {
32555:         NanoAssert(sz == ARGSIZE_Q);
32555:         // shouldn't have 64 bit int params on ARM
32555:         NanoAssert(false);
32555:     }
32555: }
32555: 
32555: // Encode a 64-bit floating-point argument using the appropriate ABI.
32555: // This function operates in the same way as asm_arg, except that it will only
32555: // handle arguments where (ArgSize)sz == ARGSIZE_F.
32555: void
32555: Assembler::asm_arg_64(LInsp arg, Register& r, int& stkd)
32555: {
32555:     // The stack pointer must always be at least aligned to 4 bytes.
32555:     NanoAssert((stkd & 3) == 0);
32555:     // The only use for this function when we are using soft floating-point
32555:     // is for LIR_qjoin.
33942:     NanoAssert(ARM_VFP || arg->isop(LIR_qjoin));
32555: 
32555:     Register    fp_reg = UnknownReg;
32555: 
33942:     if (ARM_VFP) {
32555:         fp_reg = findRegFor(arg, FpRegs);
35062:         NanoAssert(isKnownReg(fp_reg));
32555:     }
32555: 
26678: #ifdef NJ_ARM_EABI
32555:     // EABI requires that 64-bit arguments are aligned on even-numbered
32555:     // registers, as R0:R1 or R2:R3. If the register base is at an
32555:     // odd-numbered register, advance it. Note that this will push r past
32555:     // R3 if r is R3 to start with, and will force the argument to go on
32555:     // the stack.
32555:     if ((r == R1) || (r == R3)) {
32555:         r = nextreg(r);
32555:     }
32555: #endif
26678: 
32555:     if (r < R3) {
32555:         Register    ra = r;
32555:         Register    rb = nextreg(r);
32555:         r = nextreg(rb);
32555: 
32555: #ifdef NJ_ARM_EABI
32555:         // EABI requires that 64-bit arguments are aligned on even-numbered
32555:         // registers, as R0:R1 or R2:R3.
32555:         NanoAssert( ((ra == R0) && (rb == R1)) || ((ra == R2) && (rb == R3)) );
32555: #endif
32555: 
32555:         // Put the argument in ra and rb. If the argument is in a VFP register,
32555:         // use FMRRD to move it to ra and rb. Otherwise, let asm_regarg deal
32555:         // with the argument as if it were two 32-bit arguments.
33942:         if (ARM_VFP) {
32555:             FMRRD(ra, rb, fp_reg);
32555:         } else {
32555:             asm_regarg(ARGSIZE_LO, arg->oprnd1(), ra);
32555:             asm_regarg(ARGSIZE_LO, arg->oprnd2(), rb);
32555:         }
32555: 
32555: #ifndef NJ_ARM_EABI
32555:     } else if (r == R3) {
32555:         // We only have one register left, but the legacy ABI requires that we
32555:         // put 32 bits of the argument in the register (R3) and the remaining
32555:         // 32 bits on the stack.
32555:         Register    ra = r;
32555:         r = nextreg(r);
32555: 
32555:         // This really just checks that nextreg() works properly, as we know
32555:         // that r was previously R3.
32555:         NanoAssert(r == R4);
32555: 
32555:         // We're splitting the argument between registers and the stack.  This
32555:         // must be the first time that the stack is used, so stkd must be at 0.
32555:         NanoAssert(stkd == 0);
32555: 
33942:         if (ARM_VFP) {
32555:             // TODO: We could optimize the this to store directly from
32555:             // the VFP register to memory using "FMRRD ra, fp_reg[31:0]" and
32555:             // "STR fp_reg[63:32], [SP, #stkd]".
32555: 
32555:             // Load from the floating-point register as usual, but use IP
32555:             // as a swap register.
32555:             STR(IP, SP, 0);
32555:             stkd += 4;
32555:             FMRRD(ra, IP, fp_reg);
32555:         } else {
32555:             // Without VFP, we can simply use asm_regarg and asm_stkarg to
32555:             // encode the two 32-bit words as we don't need to load from a VFP
32555:             // register.
32555:             asm_regarg(ARGSIZE_LO, arg->oprnd1(), ra);
32555:             asm_stkarg(arg->oprnd2(), 0);
26678:             stkd += 4;
26678:         }
26678: #endif
26678:     } else {
32555:         // The argument won't fit in registers, so pass on to asm_stkarg.
32555: #ifdef NJ_ARM_EABI
32555:         // EABI requires that 64-bit arguments are 64-bit aligned.
32555:         if ((stkd & 7) != 0) {
32555:             // stkd will always be aligned to at least 4 bytes; this was
32555:             // asserted on entry to this function.
26678:             stkd += 4;
26678:         }
32555: #endif
32555:         asm_stkarg(arg, stkd);
32555:         stkd += 8;
26678:     }
32555: }
32555: 
32555: void
32555: Assembler::asm_regarg(ArgSize sz, LInsp p, Register r)
32555: {
35062:     NanoAssert(isKnownReg(r));
32555:     if (sz & ARGSIZE_MASK_INT)
32555:     {
32555:         // arg goes in specific register
32555:         if (p->isconst()) {
32555:             asm_ld_imm(r, p->imm32());
26678:         } else {
35062:             if (p->isUsed()) {
35062:                 if (!p->hasKnownReg()) {
26678:                     // load it into the arg reg
32555:                     int d = findMemFor(p);
32582:                     if (p->isop(LIR_alloc)) {
32555:                         asm_add_imm(r, FP, d, 0);
26678:                     } else {
26678:                         LDR(r, FP, d);
26678:                     }
26678:                 } else {
32555:                     // it must be in a saved reg
35062:                     MOV(r, p->getReg());
32555:                 }
32555:             }
32555:             else {
32555:                 // this is the last use, so fine to assign it
32555:                 // to the scratch reg, it's dead after this point.
32555:                 findSpecificRegFor(p, r);
32555:             }
32555:         }
32555:     }
32555:     else if (sz == ARGSIZE_Q) {
32555:         // 64 bit integer argument - should never happen on ARM
32555:         NanoAssert(false);
32555:     }
32555:     else
32555:     {
32555:         NanoAssert(sz == ARGSIZE_F);
32555:         // fpu argument in register - should never happen since FPU
32555:         // args are converted to two 32-bit ints on ARM
32555:         NanoAssert(false);
32555:     }
32555: }
32555: 
32555: void
32555: Assembler::asm_stkarg(LInsp arg, int stkd)
32555: {
32555:     bool isQuad = arg->isQuad();
32555: 
35062:     Register rr;
35062:     if (arg->isUsed() && (rr = arg->getReg(), isKnownReg(rr))) {
32555:         // The argument resides somewhere in registers, so we simply need to
32555:         // push it onto the stack.
33993:         if (!ARM_VFP || !isQuad) {
35062:             NanoAssert(IsGpReg(rr));
32555: 
35062:             STR(rr, SP, stkd);
32555:         } else {
32555:             // According to the comments in asm_arg_64, LIR_qjoin
32555:             // can have a 64-bit argument even if VFP is disabled. However,
32555:             // asm_arg_64 will split the argument and issue two 32-bit
32555:             // arguments to asm_stkarg so we can ignore that case here and
32555:             // assert that we will never get 64-bit arguments unless VFP is
32555:             // available.
33942:             NanoAssert(ARM_VFP);
35062:             NanoAssert(IsFpReg(rr));
32555: 
32555: #ifdef NJ_ARM_EABI
32555:             // EABI requires that 64-bit arguments are 64-bit aligned.
32555:             NanoAssert((stkd & 7) == 0);
32555: #endif
32555: 
35062:             FSTD(rr, SP, stkd);
26678:         }
26678:     } else {
32555:         // The argument does not reside in registers, so we need to get some
32555:         // memory for it and then copy it onto the stack.
26678:         int d = findMemFor(arg);
32555:         if (!isQuad) {
32555:             STR(IP, SP, stkd);
32582:             if (arg->isop(LIR_alloc)) {
28742:                 asm_add_imm(IP, FP, d);
28742:             } else {
26678:                 LDR(IP, FP, d);
28742:             }
32555:         } else {
32555: #ifdef NJ_ARM_EABI
32555:             // EABI requires that 64-bit arguments are 64-bit aligned.
32555:             NanoAssert((stkd & 7) == 0);
32555: #endif
32555: 
33518:             STR(IP, SP, stkd+4);
32555:             LDR(IP, FP, d+4);
33518:             STR(IP, SP, stkd);
32555:             LDR(IP, FP, d);
26678:         }
26678:     }
26678: }
26678: 
26678: void
18775: Assembler::asm_call(LInsp ins)
18774: {
36374:     if (ARM_VFP && ins->isop(LIR_fcall)) {
36374:         /* Because ARM actually returns the result in (R0,R1), and not in a
36374:          * floating point register, the code to move the result into a correct
36374:          * register is below.  We do nothing here.
36374:          *
36374:          * The reason being that if we did something here, the final code
36374:          * sequence we'd get would be something like:
36374:          *     MOV {R0-R3},params        [from below]
36374:          *     BL function               [from below]
36374:          *     MOV {R0-R3},spilled data  [from evictScratchRegs()]
36374:          *     MOV Dx,{R0,R1}            [from here]
36374:          * which is clearly broken.
36374:          *
36374:          * This is not a problem for non-floating point calls, because the
36374:          * restoring of spilled data into R0 is done via a call to
36374:          * prepResultReg(R0) in the other branch of this if-then-else,
36374:          * meaning that evictScratchRegs() will not modify R0. However,
36374:          * prepResultReg is not aware of the concept of using a register pair
36374:          * (R0,R1) for the result of a single operation, so it can only be
36374:          * used here with the ultimate VFP register, and not R0/R1, which
36374:          * potentially allows for R0/R1 to get corrupted as described.
36374:          */
36374:     } else {
36374:         prepResultReg(ins, rmask(retRegs[0]));
36374:     }
36374: 
36374:     // Do this after we've handled the call result, so we don't
36374:     // force the call result to be spilled unnecessarily.
36374: 
36374:     evictScratchRegs();
36374: 
33943:     const CallInfo* call = ins->callInfo();
26678:     ArgSize sizes[MAXARGS];
18774:     uint32_t argc = call->get_sizes(sizes);
33943:     bool indirect = call->isIndirect();
32555: 
32555:     // If we aren't using VFP, assert that the LIR operation is an integer
32555:     // function call.
33942:     NanoAssert(ARM_VFP || ins->isop(LIR_icall));
32555: 
32555:     // If we're using VFP, and the return type is a double, it'll come back in
32555:     // R0/R1. We need to either place it in the result fp reg, or store it.
36374:     // See comments above for more details as to why this is necessary here
36374:     // for floating point calls, but not for integer calls.
34345:     if (ARM_VFP && ins->isUsed()) {
32555:         // Determine the size (and type) of the instruction result.
32555:         ArgSize rsize = (ArgSize)(call->_argtypes & ARGSIZE_MASK_ANY);
32555: 
32555:         // If the result size is a floating-point value, treat the result
32555:         // specially, as described previously.
32555:         if (rsize == ARGSIZE_F) {
35062:             Register rr = ins->getReg();
32555: 
32555:             NanoAssert(ins->opcode() == LIR_fcall);
32555: 
35062:             if (!isKnownReg(rr)) {
35062:                 int d = disp(ins);
32555:                 NanoAssert(d != 0);
34291:                 freeRsrcOf(ins, false);
32555: 
32555:                 // The result doesn't have a register allocated, so store the
32555:                 // result (in R0,R1) directly to its stack slot.
32555:                 STR(R0, FP, d+0);
32555:                 STR(R1, FP, d+4);
32555:             } else {
32555:                 NanoAssert(IsFpReg(rr));
32555: 
32555:                 // Copy the result to the (VFP) result register.
34291:                 prepResultReg(ins, rmask(rr));
32555:                 FMDRR(rr, R0, R1);
32555:             }
32555:         }
32555:     }
32555: 
32555:     // Emit the branch.
33943:     if (!indirect) {
32555:         verbose_only(if (_logc->lcbits & LC_Assembly)
32555:             outputf("        %p:", _nIns);
32555:         )
32555: 
33943:         // Direct call: on v5 and above (where the calling sequence doesn't
33943:         // corrupt LR until the actual branch instruction), we can avoid an
33943:         // interlock in the "long" branch sequence by manually loading the
33943:         // target address into LR ourselves before setting up the parameters
33943:         // in other registers.
33943:         BranchWithLink((NIns*)call->_address);
32555:     } else {
33943:         // Indirect call: we assign the address arg to LR since it's not
33943:         // used for regular arguments, and is otherwise scratch since it's
33943:         // clobberred by the call. On v4/v4T, where we have to manually do
33943:         // the equivalent of a BLX, move LR into IP before corrupting LR
33943:         // with the return address.
33943:         if (blx_lr_bug) {
33943:             // workaround for msft device emulator bug (blx lr emulated as no-op)
33943:             underrunProtect(8);
33943:             BLX(IP);
33943:             MOV(IP,LR);
33943:         } else {
33943:             BLX(LR);
33943:         }
33943:         asm_regarg(ARGSIZE_LO, ins->arg(--argc), LR);
32555:     }
32555: 
32555:     // Encode the arguments, starting at R0 and with an empty argument stack.
26678:     Register    r = R0;
26678:     int         stkd = 0;
26678: 
32555:     // Iterate through the argument list and encode each argument according to
32555:     // the ABI.
32555:     // Note that we loop through the arguments backwards as LIR specifies them
32555:     // in reverse order.
32555:     uint32_t    i = argc;
32555:     while(i--) {
32555:         asm_arg(sizes[i], ins->arg(i), r, stkd);
32555:     }
26678: 
32555:     if (stkd > max_out_args) {
32555:         max_out_args = stkd;
18774:     }
18774: }
18774: 
18775: Register
32584: Assembler::nRegisterAllocFromSet(RegisterMask set)
18774: {
30460:     NanoAssert(set != 0);
30460: 
29861:     // The CountLeadingZeroes function will use the CLZ instruction where
29861:     // available. In other cases, it will fall back to a (slower) C
29861:     // implementation.
29861:     Register r = (Register)(31-CountLeadingZeroes(set));
18775:     _allocator.free &= ~rmask(r);
30460: 
30460:     NanoAssert(IsGpReg(r) || IsFpReg(r));
30460:     NanoAssert((rmask(r) & set) == rmask(r));
30460: 
18775:     return r;
18774: }
18774: 
18775: void
18775: Assembler::nRegisterResetAll(RegAlloc& a)
18774: {
18774:     // add scratch registers to our free list for the allocator
18774:     a.clear();
20925:     a.free =
20925:         rmask(R0) | rmask(R1) | rmask(R2) | rmask(R3) | rmask(R4) |
20925:         rmask(R5) | rmask(R6) | rmask(R7) | rmask(R8) | rmask(R9) |
34303:         rmask(R10) | rmask(LR);
33942:     if (ARM_VFP)
20925:         a.free |= FpRegs;
20925: 
18774:     debug_only(a.managed = a.free);
18774: }
18774: 
34576: static inline ConditionCode
34576: get_cc(NIns *ins)
34576: {
34576:     return ConditionCode((*ins >> 28) & 0xF);
34576: }
34576: 
34576: static inline bool
34576: branch_is_B(NIns* branch)
34576: {
34576:     return (*branch & 0x0E000000) == 0x0A000000;
34576: }
34576: 
34576: static inline bool
34576: branch_is_LDR_PC(NIns* branch)
34576: {
34576:     return (*branch & 0x0F7FF000) == 0x051FF000;
34576: }
34576: 
35352: // Is this an instruction of the form  ldr/str reg, [fp, #-imm] ?
35352: static inline bool
35352: is_ldstr_reg_fp_minus_imm(/*OUT*/uint32_t* isLoad, /*OUT*/uint32_t* rX,
35352:                           /*OUT*/uint32_t* immX, NIns i1)
35352: {
35352:     if ((i1 & 0xFFEF0000) != 0xE50B0000)
35352:         return false;
35352:     *isLoad = (i1 >> 20) & 1;
35352:     *rX     = (i1 >> 12) & 0xF;
35352:     *immX   = i1 & 0xFFF;
35352:     return true;
35352: }
35352: 
35352: // Is this an instruction of the form  ldmdb/stmdb fp, regset ?
35352: static inline bool
35352: is_ldstmdb_fp(/*OUT*/uint32_t* isLoad, /*OUT*/uint32_t* regSet, NIns i1)
35352: {
35352:     if ((i1 & 0xFFEF0000) != 0xE90B0000)
35352:         return false;
35352:     *isLoad = (i1 >> 20) & 1;
35352:     *regSet = i1 & 0xFFFF;
35352:     return true;
35352: }
35352: 
35352: // Make an instruction of the form ldmdb/stmdb fp, regset
35352: static inline NIns
35352: mk_ldstmdb_fp(uint32_t isLoad, uint32_t regSet)
35352: {
35352:     return 0xE90B0000 | (regSet & 0xFFFF) | ((isLoad & 1) << 20);
35352: }
35352: 
35352: // Compute the number of 1 bits in the lowest 16 bits of regSet
35352: static inline uint32_t
35352: size_of_regSet(uint32_t regSet)
35352: {
35352:    uint32_t x = regSet;
35352:    x = (x & 0x5555) + ((x >> 1) & 0x5555);
35352:    x = (x & 0x3333) + ((x >> 2) & 0x3333);
35352:    x = (x & 0x0F0F) + ((x >> 4) & 0x0F0F);
35352:    x = (x & 0x00FF) + ((x >> 8) & 0x00FF);
35352:    return x;
35352: }
35352: 
35352: // See if two ARM instructions, i1 and i2, can be combined into one
35352: static bool
35352: do_peep_2_1(/*OUT*/NIns* merged, NIns i1, NIns i2)
35352: {
35352:     uint32_t rX, rY, immX, immY, isLoadX, isLoadY, regSet;
35352:     /*   ld/str rX, [fp, #-8]
35352:          ld/str rY, [fp, #-4]
35352:          ==>
35352:          ld/stmdb fp, {rX, rY}
35352:          when
35352:          X < Y and X != fp and Y != fp and X != 15 and Y != 15
35352:     */
35352:     if (is_ldstr_reg_fp_minus_imm(&isLoadX, &rX, &immX, i1) &&
35352:         is_ldstr_reg_fp_minus_imm(&isLoadY, &rY, &immY, i2) &&
35352:         immX == 8 && immY == 4 && rX < rY &&
35352:         isLoadX == isLoadY &&
35352:         rX != FP && rY != FP &&
35352:          rX != 15 && rY != 15) {
35352:         *merged = mk_ldstmdb_fp(isLoadX, (1 << rX) | (1<<rY));
35352:         return true;
35352:     }
35352:     /*   ld/str   rX, [fp, #-N]
35352:          ld/stmdb fp, regset
35352:          ==>
35352:          ld/stmdb fp, union(regset,{rX})
35352:          when
35352:          regset is nonempty
35352:          X < all elements of regset
35352:          N == 4 * (1 + card(regset))
35352:          X != fp and X != 15
35352:     */
35352:     if (is_ldstr_reg_fp_minus_imm(&isLoadX, &rX, &immX, i1) &&
35352:         is_ldstmdb_fp(&isLoadY, &regSet, i2) &&
35352:         regSet != 0 &&
35352:         (regSet & ((1 << (rX + 1)) - 1)) == 0 &&
35352:         immX == 4 * (1 + size_of_regSet(regSet)) &&
35352:         isLoadX == isLoadY &&
35352:         rX != FP && rX != 15) {
35352:         *merged = mk_ldstmdb_fp(isLoadX, regSet | (1 << rX));
35352:         return true;
35352:     }
35352:     return false;
35352: }
35352: 
35352: // Determine whether or not it's safe to look at _nIns[1].
35352: // Necessary condition for safe peepholing with do_peep_2_1.
35352: static inline bool
35352: does_next_instruction_exist(NIns* _nIns, NIns* codeStart, NIns* codeEnd,
35352:                             NIns* exitStart, NIns* exitEnd)
35352: {
35352:     return (exitStart <= _nIns && _nIns+1 < exitEnd) ||
35352:            (codeStart <= _nIns && _nIns+1 < codeEnd);
35352: }
35352: 
32583: void
34303: Assembler::nPatchBranch(NIns* branch, NIns* target)
18774: {
34303:     // Patch the jump in a loop
18774: 
34576:     //
34576:     // There are two feasible cases here, the first of which has 2 sub-cases:
34576:     //
34576:     //   (1) We are patching a patchable unconditional jump emitted by
34576:     //       JMP_far.  All possible encodings we may be looking at with
34576:     //       involve 2 words, though we *may* have to change from 1 word to
34576:     //       2 or vice verse.
34576:     //
34576:     //          1a:  B ±32MB ; BKPT
34576:     //          1b:  LDR PC [PC, #-4] ; $imm
34576:     //
34576:     //   (2) We are patching a patchable conditional jump emitted by
34576:     //       B_cond_chk.  Short conditional jumps are non-patchable, so we
34576:     //       won't have one here; will only ever have an instruction of the
34576:     //       following form:
34576:     //
34576:     //          LDRcc PC [PC, #lit] ...
34576:     //
34576:     //       We don't actually know whether the lit-address is in the
34576:     //       constant pool or in-line of the instruction stream, following
34576:     //       the insn (with a jump over it) and we don't need to. For our
34576:     //       purposes here, cases 2, 3 and 4 all look the same.
34576:     //
34576:     // For purposes of handling our patching task, we group cases 1b and 2
34576:     // together, and handle case 1a on its own as it might require expanding
34576:     // from a short-jump to a long-jump.
34576:     //
34576:     // We do not handle contracting from a long-jump to a short-jump, though
34576:     // this is a possible future optimisation for case 1b. For now it seems
34576:     // not worth the trouble.
34576:     //
34576: 
34576:     if (branch_is_B(branch)) {
34576:         // Case 1a
34576:         // A short B branch, must be unconditional.
34576:         NanoAssert(get_cc(branch) == AL);
34576: 
34303:         int32_t offset = PC_OFFSET_FROM(target, branch);
34303:         if (isS24(offset>>2)) {
34576:             // We can preserve the existing form, just rewrite its offset.
34303:             NIns cond = *branch & 0xF0000000;
34303:             *branch = (NIns)( cond | (0xA<<24) | ((offset>>2) & 0xFFFFFF) );
21548:         } else {
34576:             // We need to expand the existing branch to a long jump.
34576:             // make sure the next instruction is a dummy BKPT
34576:             NanoAssert(*(branch+1) == BKPT_insn);
34576: 
34576:             // Set the branch instruction to   LDRcc pc, [pc, #-4]
34576:             NIns cond = *branch & 0xF0000000;
34576:             *branch++ = (NIns)( cond | (0x51<<20) | (PC<<16) | (PC<<12) | (4));
34576:             *branch++ = (NIns)target;
34576:         }
34576:     } else {
34576:         // Case 1b & 2
34576:         // Not a B branch, must be LDR, might be any kind of condition.
34576:         NanoAssert(branch_is_LDR_PC(branch));
34303: 
34303:         NIns *addr = branch+2;
34303:         int offset = (*branch & 0xFFF) / sizeof(NIns);
34303:         if (*branch & (1<<23)) {
34303:             addr += offset;
34303:         } else {
34303:             addr -= offset;
21548:         }
18774: 
34576:         // Just redirect the jump target, leave the insn alone.
34303:         *addr = (NIns) target;
34303:     }
18774: }
18774: 
18775: RegisterMask
18775: Assembler::hint(LIns* i, RegisterMask allow /* = ~0 */)
18774: {
18774:     uint32_t op = i->opcode();
18774:     int prefer = ~0;
32598:     if (op==LIR_icall)
18774:         prefer = rmask(R0);
18774:     else if (op == LIR_callh)
18774:         prefer = rmask(R1);
32598:     else if (op == LIR_param) {
32598:         if (i->paramArg() < 4)
32598:             prefer = rmask(argRegs[i->paramArg()]);
32598:     }
18774:     if (_allocator.free & allow & prefer)
18774:         allow &= prefer;
18774:     return allow;
18774: }
18774: 
18775: void
18775: Assembler::asm_qjoin(LIns *ins)
18774: {
18774:     int d = findMemFor(ins);
26677:     NanoAssert(d);
18774:     LIns* lo = ins->oprnd1();
18774:     LIns* hi = ins->oprnd2();
18774: 
18774:     Register r = findRegFor(hi, GpRegs);
18776:     STR(r, FP, d+4);
18774: 
18774:     // okay if r gets recycled.
18774:     r = findRegFor(lo, GpRegs);
18776:     STR(r, FP, d);
18774:     freeRsrcOf(ins, false); // if we had a reg in use, emit a ST to flush it to mem
18774: }
18774: 
18775: void
36372: Assembler::asm_store32(LOpcode op, LIns *value, int dr, LIns *base)
18774: {
36372:     switch (op) {
36372:         case LIR_sti:
36372:             // handled by mainline code below for now
36372:             break;
36372:         case LIR_stb:
36372:         case LIR_sts:
36372:             NanoAssertMsg(0, "NJ_EXPANDED_LOADSTORE_SUPPORTED not yet supported for this architecture");
36372:             return;
36372:         default:
36372:             NanoAssertMsg(0, "asm_store32 should never receive this LIR opcode");
36372:             return;
36372:     }
36372: 
26677:     Register ra, rb;
37705:     getBaseReg2(GpRegs, value, ra, GpRegs, base, rb, dr);
30406: 
36552:     if (isU12(-dr) || isU12(dr)) {
36552:         STR(ra, rb, dr);
36552:     } else {
30406:         STR(ra, IP, 0);
30406:         asm_add_imm(IP, rb, dr);
18774:     }
30406: }
18774: 
18775: void
35365: Assembler::asm_restore(LInsp i, Register r)
18774: {
32582:     if (i->isop(LIR_alloc)) {
35062:         asm_add_imm(r, FP, disp(i));
34334:     } else if (i->isconst()) {
35062:         if (!i->getArIndex()) {
35062:             i->markAsClear();
34334:         }
34334:         asm_ld_imm(r, i->imm32());
34334:     }
34334:     else {
28401:         // We can't easily load immediate values directly into FP registers, so
28401:         // ensure that memory is allocated for the constant and load it from
28401:         // memory.
18774:         int d = findMemFor(i);
34334:         if (ARM_VFP && IsFpReg(r)) {
18776:             if (isS8(d >> 2)) {
18776:                 FLDD(r, FP, d);
18776:             } else {
26536:                 FLDD(r, IP, 0);
30460:                 asm_add_imm(IP, FP, d);
18776:             }
18776:         } else {
35352:             NIns merged;
18776:             LDR(r, FP, d);
35352:             // See if we can merge this load into an immediately following
35352:             // one, by creating or extending an LDM instruction.
35352:             if (/* is it safe to poke _nIns[1] ? */
35352:                 does_next_instruction_exist(_nIns, codeStart, codeEnd,
35352:                                                    exitStart, exitEnd)
35352:                 && /* can we merge _nIns[0] into _nIns[1] ? */
35352:                    do_peep_2_1(&merged, _nIns[0], _nIns[1])) {
35352:                 _nIns[1] = merged;
35352:                 _nIns++;
35352:                 verbose_only( asm_output("merge next into LDMDB"); )
35352:             }
18776:         }
34334:     }
18774: }
18774: 
18775: void
20919: Assembler::asm_spill(Register rr, int d, bool pop, bool quad)
18774: {
18774:     (void) pop;
20919:     (void) quad;
20919:     if (d) {
33993:         if (ARM_VFP && IsFpReg(rr)) {
18776:             if (isS8(d >> 2)) {
18776:                 FSTD(rr, FP, d);
18776:             } else {
26536:                 FSTD(rr, IP, 0);
30460:                 asm_add_imm(IP, FP, d);
18776:             }
18776:         } else {
35352:             NIns merged;
18776:             STR(rr, FP, d);
35352:             // See if we can merge this store into an immediately following one,
35352:             // one, by creating or extending a STM instruction.
35352:             if (/* is it safe to poke _nIns[1] ? */
35352:                 does_next_instruction_exist(_nIns, codeStart, codeEnd,
35352:                                                    exitStart, exitEnd)
35352:                 && /* can we merge _nIns[0] into _nIns[1] ? */
35352:                    do_peep_2_1(&merged, _nIns[0], _nIns[1])) {
35352:                 _nIns[1] = merged;
35352:                 _nIns++;
35352:                 verbose_only( asm_output("merge next into STMDB"); )
35352:             }
18776:         }
18774:     }
18774: }
18774: 
18775: void
18775: Assembler::asm_load64(LInsp ins)
18774: {
37020:     NanoAssert(!ins->isop(LIR_ldq) && !ins->isop(LIR_ldqc));
37020: 
33993:     //asm_output("<<< load64");
29861: 
36372:     switch (ins->opcode()) {
37020:         case LIR_ldf:
37020:         case LIR_ldfc:
36372:             // handled by mainline code below for now
36372:             break;
37020: 
36372:         case LIR_ld32f:
36372:         case LIR_ldc32f:
36372:             NanoAssertMsg(0, "NJ_EXPANDED_LOADSTORE_SUPPORTED not yet supported for this architecture");
36372:             return;
36372:         default:
36372:             NanoAssertMsg(0, "asm_load64 should never receive this LIR opcode");
36372:             return;
36372:     }
36372: 
29861:     NanoAssert(ins->isQuad());
18776: 
18774:     LIns* base = ins->oprnd1();
30238:     int offset = ins->disp();
18776: 
35062:     Register rr = ins->getReg();
35062:     int d = disp(ins);
18774: 
28402:     Register rb = findRegFor(base, GpRegs);
28402:     NanoAssert(IsGpReg(rb));
29861:     freeRsrcOf(ins, false);
29861: 
35364:     //outputf("--- load64: Finished register allocation.");
18776: 
35062:     if (ARM_VFP && isKnownReg(rr)) {
28402:         // VFP is enabled and the result will go into a register.
28402:         NanoAssert(IsFpReg(rr));
18776: 
18776:         if (!isS8(offset >> 2) || (offset&3) != 0) {
26536:             FLDD(rr,IP,0);
30460:             asm_add_imm(IP, rb, offset);
18776:         } else {
18776:             FLDD(rr,rb,offset);
18776:         }
18776:     } else {
28402:         // Either VFP is not available or the result needs to go into memory;
28402:         // in either case, VFP instructions are not required. Note that the
28402:         // result will never be loaded into registers if VFP is not available.
35062:         NanoAssert(!isKnownReg(rr));
28402:         NanoAssert(d != 0);
18776: 
29861:         // Check that the offset is 8-byte (64-bit) aligned.
29861:         NanoAssert((d & 0x7) == 0);
29861: 
29861:         // *(uint64_t*)(FP+d) = *(uint64_t*)(rb+offset)
18776:         asm_mmq(FP, d, rb, offset);
26545:     }
18776: 
33993:     //asm_output(">>> load64");
18774: }
18774: 
18775: void
36372: Assembler::asm_store64(LOpcode op, LInsp value, int dr, LInsp base)
18774: {
37020:     NanoAssert(op != LIR_stqi);
37020: 
22649:     //asm_output("<<< store64 (dr: %d)", dr);
18776: 
36372:     switch (op) {
37020:         case LIR_stfi:
36372:             // handled by mainline code below for now
36372:             break;
37020: 
36372:         case LIR_st32f:
36372:             NanoAssertMsg(0, "NJ_EXPANDED_LOADSTORE_SUPPORTED not yet supported for this architecture");
36372:             return;
37020: 
36372:         default:
36372:             NanoAssertMsg(0, "asm_store64 should never receive this LIR opcode");
36372:             return;
36372:     }
36372: 
33942:     if (ARM_VFP) {
19061:         Register rb = findRegFor(base, GpRegs);
18776: 
19061:         if (value->isconstq()) {
26547:             underrunProtect(LD32_size*2 + 8);
26547: 
26544:             // XXX use another reg, get rid of dependency
26536:             STR(IP, rb, dr);
30460:             asm_ld_imm(IP, value->imm64_0(), false);
26536:             STR(IP, rb, dr+4);
30460:             asm_ld_imm(IP, value->imm64_1(), false);
19061: 
19061:             return;
19061:         }
19061: 
18776:         Register rv = findRegFor(value, FpRegs);
18776: 
35062:         NanoAssert(isKnownReg(rb));
35062:         NanoAssert(isKnownReg(rv));
18776: 
18776:         Register baseReg = rb;
18776:         intptr_t baseOffset = dr;
18776: 
18776:         if (!isS8(dr)) {
26536:             baseReg = IP;
18776:             baseOffset = 0;
18776:         }
18776: 
18776:         FSTD(rv, baseReg, baseOffset);
18776: 
18776:         if (!isS8(dr)) {
30460:             asm_add_imm(IP, rb, dr);
18776:         }
18776: 
18776:         // if it's a constant, make sure our baseReg/baseOffset location
18776:         // has the right value
18776:         if (value->isconstq()) {
26547:             underrunProtect(4*4);
28661:             asm_quad_nochk(rv, value->imm64_0(), value->imm64_1());
18776:         }
26545:     } else {
18774:         int da = findMemFor(value);
18774:         Register rb = findRegFor(base, GpRegs);
29861:         // *(uint64_t*)(rb+dr) = *(uint64_t*)(FP+da)
18774:         asm_mmq(rb, dr, FP, da);
26545:     }
26545: 
18776:     //asm_output(">>> store64");
18776: }
18776: 
18776: // stick a quad into register rr, where p points to the two
18776: // 32-bit parts of the quad, optinally also storing at FP+d
18776: void
28661: Assembler::asm_quad_nochk(Register rr, int32_t imm64_0, int32_t imm64_1)
18776: {
19060:     // We're not going to use a slot, because it might be too far
19060:     // away.  Instead, we're going to stick a branch in the stream to
19060:     // jump over the constants, and then load from a short PC relative
19060:     // offset.
18776: 
19060:     // stream should look like:
19060:     //    branch A
28661:     //    imm64_0
28661:     //    imm64_1
19060:     // A: FLDD PC-16
18776: 
19060:     FLDD(rr, PC, -16);
18776: 
28661:     *(--_nIns) = (NIns) imm64_1;
28661:     *(--_nIns) = (NIns) imm64_0;
18776: 
33993:     B_nochk(_nIns+2);
18774: }
18774: 
18775: void
18775: Assembler::asm_quad(LInsp ins)
18774: {
33993:     //asm_output(">>> asm_quad");
33993: 
35062:     int d = disp(ins);
35062:     Register rr = ins->getReg();
18776: 
18774:     freeRsrcOf(ins, false);
18775: 
35062:     if (ARM_VFP && isKnownReg(rr))
26547:     {
34303:         asm_spill(rr, d, false, true);
18776: 
26547:         underrunProtect(4*4);
28661:         asm_quad_nochk(rr, ins->imm64_0(), ins->imm64_1());
26545:     } else {
28403:         NanoAssert(d);
34303:         // asm_mmq might spill a reg, so don't call it;
34303:         // instead do the equivalent directly.
34303:         //asm_mmq(FP, d, PC, -16);
34303: 
26536:         STR(IP, FP, d+4);
28661:         asm_ld_imm(IP, ins->imm64_1());
26536:         STR(IP, FP, d);
28661:         asm_ld_imm(IP, ins->imm64_0());
26545:     }
33993: 
33993:     //asm_output("<<< asm_quad");
18774: }
18774: 
18775: void
18775: Assembler::asm_nongp_copy(Register r, Register s)
18774: {
33993:     if (ARM_VFP && IsFpReg(r) && IsFpReg(s)) {
18776:         // fp->fp
18776:         FCPYD(r, s);
18776:     } else {
28399:         // We can't move a double-precision FP register into a 32-bit GP
28399:         // register, so assert that no calling code is trying to do that.
18776:         NanoAssert(0);
18776:     }
18774: }
18774: 
18775: Register
20919: Assembler::asm_binop_rhs_reg(LInsp)
18774: {
18774:     return UnknownReg;
18774: }
18774: 
18774: /**
18774:  * copy 64 bits: (rd+dd) <- (rs+ds)
18774:  */
18775: void
18775: Assembler::asm_mmq(Register rd, int dd, Register rs, int ds)
18774: {
29861:     // The value is either a 64bit struct or maybe a float that isn't live in
29861:     // an FPU reg.  Either way, don't put it in an FPU reg just to load & store
29861:     // it.
29861:     // This operation becomes a simple 64-bit memcpy.
19062: 
29861:     // In order to make the operation optimal, we will require two GP
29861:     // registers. We can't allocate a register here because the caller may have
29861:     // called freeRsrcOf, and allocating a register here may cause something
29861:     // else to spill onto the stack which has just be conveniently freed by
29861:     // freeRsrcOf (resulting in stack corruption).
29861:     //
29861:     // Falling back to a single-register implementation of asm_mmq is better
29861:     // than adjusting the callers' behaviour (to allow us to allocate another
29861:     // register here) because spilling a register will end up being slower than
29861:     // just using the same register twice anyway.
29861:     //
29861:     // Thus, if there is a free register which we can borrow, we will emit the
29861:     // following code:
29861:     //  LDR rr, [rs, #ds]
29861:     //  LDR ip, [rs, #(ds+4)]
29861:     //  STR rr, [rd, #dd]
29861:     //  STR ip, [rd, #(dd+4)]
29861:     // (Where rr is the borrowed register.)
29861:     //
29861:     // If there is no free register, don't spill an existing allocation. Just
29861:     // do the following:
29861:     //  LDR ip, [rs, #ds]
29861:     //  STR ip, [rd, #dd]
29861:     //  LDR ip, [rs, #(ds+4)]
29861:     //  STR ip, [rd, #(dd+4)]
29861: 
29861:     // Ensure that the PC is not used as either base register. The instruction
29861:     // generation macros call underrunProtect, and a side effect of this is
29861:     // that we may be pushed onto another page, so the PC is not a reliable
29861:     // base register.
19065:     NanoAssert(rs != PC);
29861:     NanoAssert(rd != PC);
19065: 
29861:     // Find the list of free registers from the allocator's free list and the
29861:     // GpRegs mask. This excludes any floating-point registers that may be on
29861:     // the free list.
34303:     RegisterMask    free = _allocator.free & AllowableFlagRegs;
19062: 
29861:     if (free) {
29861:         // There is at least one register on the free list, so grab one for
29861:         // temporary use. There is no need to allocate it explicitly because
29861:         // we won't need it after this function returns.
29861: 
29861:         // The CountLeadingZeroes can be used to quickly find a set bit in the
29861:         // free mask.
29861:         Register    rr = (Register)(31-CountLeadingZeroes(free));
29861: 
29861:         // Note: Not every register in GpRegs is usable here. However, these
29861:         // registers will never appear on the free list.
29861:         NanoAssert((free & rmask(PC)) == 0);
29861:         NanoAssert((free & rmask(LR)) == 0);
29861:         NanoAssert((free & rmask(SP)) == 0);
29861:         NanoAssert((free & rmask(IP)) == 0);
29861:         NanoAssert((free & rmask(FP)) == 0);
29861: 
29861:         // Emit the actual instruction sequence.
29861: 
26536:         STR(IP, rd, dd+4);
29861:         STR(rr, rd, dd);
26536:         LDR(IP, rs, ds+4);
29861:         LDR(rr, rs, ds);
29861:     } else {
29861:         // There are no free registers, so fall back to using IP twice.
29861:         STR(IP, rd, dd+4);
29861:         LDR(IP, rs, ds+4);
29861:         STR(IP, rd, dd);
29861:         LDR(IP, rs, ds);
29861:     }
18774: }
18774: 
32784: // Increment the 32-bit profiling counter at pCtr, without
32784: // changing any registers.
32784: verbose_only(
35091: void Assembler::asm_inc_m32(uint32_t* pCtr)
32784: {
35091:     // We need to temporarily free up two registers to do this, so
35091:     // just push r0 and r1 on the stack.  This assumes that the area
35091:     // at r13 - 8 .. r13 - 1 isn't being used for anything else at
35091:     // this point.  This guaranteed us by the EABI; although the
35091:     // situation with the legacy ABI I'm not sure of.
35091:     //
35091:     // Plan: emit the following bit of code.  It's not efficient, but
35091:     // this is for profiling debug builds only, and is self contained,
35091:     // except for above comment re stack use.
35091:     //
35091:     // E92D0003                 push    {r0,r1}
35091:     // E59F0000                 ldr     r0, [r15]   ; pCtr
35091:     // EA000000                 b       .+8         ; jump over imm
35091:     // 12345678                 .word   0x12345678  ; pCtr
35091:     // E5901000                 ldr     r1, [r0]
35091:     // E2811001                 add     r1, r1, #1
35091:     // E5801000                 str     r1, [r0]
35091:     // E8BD0003                 pop     {r0,r1}
35091: 
35091:     // We need keep the 4 words beginning at "ldr r0, [r15]"
35091:     // together.  Simplest to underrunProtect the whole thing.
35091:     underrunProtect(8*4);
35091:     IMM32(0xE8BD0003);       //  pop     {r0,r1}
35091:     IMM32(0xE5801000);       //  str     r1, [r0]
35091:     IMM32(0xE2811001);       //  add     r1, r1, #1
35091:     IMM32(0xE5901000);       //  ldr     r1, [r0]
35091:     IMM32((uint32_t)pCtr);   //  .word   pCtr
35091:     IMM32(0xEA000000);       //  b       .+8
35091:     IMM32(0xE59F0000);       //  ldr     r0, [r15]
35091:     IMM32(0xE92D0003);       //  push    {r0,r1}
32784: }
32784: )
32784: 
18775: void
18775: Assembler::nativePageReset()
18774: {
18774:     _nSlot = 0;
18774:     _nExitSlot = 0;
18774: }
18774: 
18775: void
18775: Assembler::nativePageSetup()
18774: {
35356:     NanoAssert(!_inExit);
31475:     if (!_nIns)
32784:         codeAlloc(codeStart, codeEnd, _nIns verbose_only(, codeBytes));
18774: 
18774:     // constpool starts at top of page and goes down,
18774:     // code starts at bottom of page and moves up
31475:     if (!_nSlot)
31475:         _nSlot = codeStart;
18774: }
18774: 
28184: 
18775: void
18775: Assembler::underrunProtect(int bytes)
18774: {
22662:     NanoAssertMsg(bytes<=LARGEST_UNDERRUN_PROT, "constant LARGEST_UNDERRUN_PROT is too small");
31475:     NanoAssert(_nSlot != 0 && int(_nIns)-int(_nSlot) <= 4096);
31475:     uintptr_t top = uintptr_t(_nSlot);
31475:     uintptr_t pc = uintptr_t(_nIns);
31475:     if (pc - bytes < top)
18774:     {
31475:         verbose_only(verbose_outputf("        %p:", _nIns);)
18774:         NIns* target = _nIns;
35356:         // This may be in a normal code chunk or an exit code chunk.
32784:         codeAlloc(codeStart, codeEnd, _nIns verbose_only(, codeBytes));
18774: 
35356:         _nSlot = codeStart;
18774: 
31475:         // _nSlot points to the first empty position in the new code block
31475:         // _nIns points just past the last empty position.
31475:         // Assume B_nochk won't ever try to write to _nSlot. See B_cond_chk macro.
33993:         B_nochk(target);
18774:     }
18774: }
18774: 
18775: void
20937: Assembler::JMP_far(NIns* addr)
20937: {
29876:     // Even if a simple branch is all that is required, this function must emit
29876:     // two words so that the branch can be arbitrarily patched later on.
21548:     underrunProtect(8);
20937: 
21548:     intptr_t offs = PC_OFFSET_FROM(addr,_nIns-2);
20937: 
21548:     if (isS24(offs>>2)) {
29876:         // Emit a BKPT to ensure that we reserve enough space for a full 32-bit
29876:         // branch patch later on. The BKPT should never be executed.
21548:         BKPT_nochk();
29876: 
34576:         asm_output("bkpt");
34576: 
29876:         // B [PC+offs]
21548:         *(--_nIns) = (NIns)( COND_AL | (0xA<<24) | ((offs>>2) & 0xFFFFFF) );
21548: 
29883:         asm_output("b %p", (void*)addr);
21548:     } else {
29876:         // Insert the target address as a constant in the instruction stream.
20937:         *(--_nIns) = (NIns)((addr));
29878:         // ldr pc, [pc, #-4] // load the address into pc, reading it from [pc-4] (e.g.,
21548:         // the next instruction)
21548:         *(--_nIns) = (NIns)( COND_AL | (0x51<<20) | (PC<<16) | (PC<<12) | (4));
20937: 
29883:         asm_output("ldr pc, =%p", (void*)addr);
20937:     }
18774: }
18774: 
29878: // Perform a branch with link, and ARM/Thumb exchange if necessary. The actual
29878: // BLX instruction is only available from ARMv5 onwards, but as we don't
29878: // support anything older than that this function will not attempt to output
29878: // pre-ARMv5 sequences.
29878: //
29878: // Note: This function is not designed to be used with branches which will be
29878: // patched later, though it will work if the patcher knows how to patch the
29878: // generated instruction sequence.
29878: void
29878: Assembler::BranchWithLink(NIns* addr)
29878: {
29878:     // Most branches emitted by TM are loaded through a register, so always
29878:     // reserve enough space for the LDR sequence. This should give us a slight
29878:     // net gain over reserving the exact amount required for shorter branches.
29878:     // This _must_ be called before PC_OFFSET_FROM as it can move _nIns!
29878:     underrunProtect(4+LD32_size);
29878: 
29878:     // Calculate the offset from the instruction that is about to be
29878:     // written (at _nIns-1) to the target.
29878:     intptr_t offs = PC_OFFSET_FROM(addr,_nIns-1);
29878: 
29878:     // ARMv5 and above can use BLX <imm> for branches within ±32MB of the
29878:     // PC and BLX Rm for long branches.
29878:     if (isS24(offs>>2)) {
30379:         // the value we need to stick in the instruction; masked,
30379:         // because it will be sign-extended back to 32 bits.
30379:         intptr_t offs2 = (offs>>2) & 0xffffff;
29878: 
29878:         if (((intptr_t)addr & 1) == 0) {
29878:             // The target is ARM, so just emit a BL.
29878: 
32555:             // BL target
30379:             *(--_nIns) = (NIns)( (COND_AL) | (0xB<<24) | (offs2) );
29883:             asm_output("bl %p", (void*)addr);
29878:         } else {
29878:             // The target is Thumb, so emit a BLX.
29878: 
32555:             // We need to emit an ARMv5+ instruction, so assert that we have a
32555:             // suitable processor. Note that we don't support ARMv4(T), but
32555:             // this serves as a useful sanity check.
33942:             NanoAssert(ARM_ARCH >= 5);
32555: 
29878:             // The (pre-shifted) value of the "H" bit in the BLX encoding.
29878:             uint32_t    H = (offs & 0x2) << 23;
29878: 
29878:             // BLX addr
30379:             *(--_nIns) = (NIns)( (0xF << 28) | (0x5<<25) | (H) | (offs2) );
29883:             asm_output("blx %p", (void*)addr);
29878:         }
29878:     } else {
32555:         // Load the target address into IP and branch to that. We've already
32555:         // done underrunProtect, so we can skip that here.
32555:         BLX(IP, false);
29878: 
29878:         // LDR IP, =addr
30460:         asm_ld_imm(IP, (int32_t)addr, false);
28552:     }
28552: }
28552: 
32555: // This is identical to BranchWithLink(NIns*) but emits a branch to an address
32555: // held in a register rather than a literal address.
32555: inline void
32555: Assembler::BLX(Register addr, bool chk /* = true */)
32555: {
32555:     // We need to emit an ARMv5+ instruction, so assert that we have a suitable
32555:     // processor. Note that we don't support ARMv4(T), but this serves as a
32555:     // useful sanity check.
33942:     NanoAssert(ARM_ARCH >= 5);
32555: 
32555:     NanoAssert(IsGpReg(addr));
32555:     // There is a bug in the WinCE device emulator which stops "BLX LR" from
32555:     // working as expected. Assert that we never do that!
33993:     if (blx_lr_bug) { NanoAssert(addr != LR); }
32555: 
32555:     if (chk) {
32555:         underrunProtect(4);
32555:     }
32555: 
32555:     // BLX IP
32555:     *(--_nIns) = (NIns)( (COND_AL) | (0x12<<20) | (0xFFF<<8) | (0x3<<4) | (addr) );
32555:     asm_output("blx ip");
32555: }
32555: 
30460: // Emit the code required to load a memory address into a register as follows:
30460: // d = *(b+off)
30460: // underrunProtect calls from this function can be disabled by setting chk to
30460: // false. However, this function can use more than LD32_size bytes of space if
30460: // the offset is out of the range of a LDR instruction; the maximum space this
30460: // function requires for underrunProtect is 4+LD32_size.
26541: void
26541: Assembler::asm_ldr_chk(Register d, Register b, int32_t off, bool chk)
26541: {
33993:     if (ARM_VFP && IsFpReg(d)) {
26541:         FLDD_chk(d,b,off,chk);
26541:         return;
26541:     }
26541: 
30460:     NanoAssert(IsGpReg(d));
30460:     NanoAssert(IsGpReg(b));
30460: 
30460:     // We can't use underrunProtect if the base register is the PC because
30460:     // underrunProtect might move the PC if there isn't enough space on the
30460:     // current page.
30460:     NanoAssert((b != PC) || (!chk));
30460: 
28406:     if (isU12(off)) {
28406:         // LDR d, b, #+off
26541:         if (chk) underrunProtect(4);
28406:         *(--_nIns) = (NIns)( COND_AL | (0x59<<20) | (b<<16) | (d<<12) | off );
28406:     } else if (isU12(-off)) {
28406:         // LDR d, b, #-off
28406:         if (chk) underrunProtect(4);
28406:         *(--_nIns) = (NIns)( COND_AL | (0x51<<20) | (b<<16) | (d<<12) | -off );
26541:     } else {
28406:         // The offset is over 4096 (and outside the range of LDR), so we need
28406:         // to add a level of indirection to get the address into IP.
28543: 
28543:         // Because of that, we can't do a PC-relative load unless it fits within
28543:         // the single-instruction forms above.
28543: 
33993:         NanoAssert(b != PC);
28543:         NanoAssert(b != IP);
28543: 
26541:         if (chk) underrunProtect(4+LD32_size);
28543: 
26541:         *(--_nIns) = (NIns)( COND_AL | (0x79<<20) | (b<<16) | (d<<12) | IP );
30460:         asm_ld_imm(IP, off, false);
26541:     }
26541: 
26541:     asm_output("ldr %s, [%s, #%d]",gpn(d),gpn(b),(off));
26541: }
26541: 
30460: // Emit the code required to load an immediate value (imm) into general-purpose
30460: // register d. Optimal (MOV-based) mechanisms are used if the immediate can be
30460: // encoded using ARM's operand 2 encoding. Otherwise, a slot is used on the
30460: // literal pool and LDR is used to load the value.
30460: //
30460: // chk can be explicitly set to false in order to disable underrunProtect calls
30460: // from this function; this allows the caller to perform the check manually.
30460: // This function guarantees not to use more than LD32_size bytes of space.
26541: void
30460: Assembler::asm_ld_imm(Register d, int32_t imm, bool chk /* = true */)
26541: {
30460:     uint32_t    op2imm;
30460: 
28542:     NanoAssert(IsGpReg(d));
30460: 
30460:     // Attempt to encode the immediate using the second operand of MOV or MVN.
30460:     // This is the simplest solution and generates the shortest and fastest
30460:     // code, but can only encode a limited set of values.
30460: 
30460:     if (encOp2Imm(imm, &op2imm)) {
30460:         // Use MOV to encode the literal.
30460:         MOVis(d, op2imm, 0);
30460:         return;
30460:     }
30460: 
30460:     if (encOp2Imm(~imm, &op2imm)) {
30460:         // Use MVN to encode the inverted literal.
30460:         MVNis(d, op2imm, 0);
30460:         return;
30460:     }
30460: 
30460:     // Try to use simple MOV, MVN or MOV(W|T) instructions to load the
30460:     // immediate. If this isn't possible, load it from memory.
30460:     //  - We cannot use MOV(W|T) on cores older than the introduction of
30460:     //    Thumb-2 or if the target register is the PC.
33942:     if (ARM_THUMB2 && (d != PC)) {
30460:         // ARMv6T2 and above have MOVW and MOVT.
30460:         uint32_t    high_h = (uint32_t)imm >> 16;
30460:         uint32_t    low_h = imm & 0xffff;
30460: 
30460:         if (high_h != 0) {
30460:             // Load the high half-word (if necessary).
30460:             MOVTi_chk(d, high_h, chk);
30460:         }
30460:         // Load the low half-word. This also zeroes the high half-word, and
30460:         // thus must execute _before_ MOVT, and is necessary even if low_h is 0
30460:         // because MOVT will not change the existing low half-word.
30460:         MOVWi_chk(d, low_h, chk);
30460: 
30460:         return;
30460:     }
30460: 
30460:     // We couldn't encode the literal in the instruction stream, so load it
30460:     // from memory.
30460: 
30460:     // Because the literal pool is on the same page as the generated code, it
30460:     // will almost always be within the ±4096 range of a LDR. However, this may
30460:     // not be the case if _nSlot is at the start of the page and _nIns is at
30460:     // the end because the PC is 8 bytes ahead of _nIns. This is unlikely to
30460:     // happen, but if it does occur we can simply waste a word or two of
30460:     // literal space.
30460: 
30460:     // We must do the underrunProtect before PC_OFFSET_FROM as underrunProtect
30460:     // can move the PC if there isn't enough space on the current page!
30460:     if (chk) {
26541:         underrunProtect(LD32_size);
26541:     }
30460: 
31475:     int offset = PC_OFFSET_FROM(_nSlot, _nIns-1);
30460:     // If the offset is out of range, waste literal space until it is in range.
30460:     while (offset <= -4096) {
30460:         ++_nSlot;
30460:         offset += sizeof(_nSlot);
30460:     }
36552:     NanoAssert((isU12(-offset) || isU12(offset)) && (offset <= -8));
30460: 
30460:     // Write the literal.
31475:     *(_nSlot++) = imm;
31475:     asm_output("## imm= 0x%x", imm);
30460: 
30460:     // Load the literal.
30460:     LDR_nochk(d,PC,offset);
31475:     NanoAssert(uintptr_t(_nIns) + 8 + offset == uintptr_t(_nSlot-1));
31475:     NanoAssert(*((int32_t*)_nSlot-1) == imm);
26541: }
18776: 
18776: // Branch to target address _t with condition _c, doing underrun
18776: // checks (_chk == 1) or skipping them (_chk == 0).
18776: //
29877: // Set the target address (_t) to 0 if the target is not yet known and the
29877: // branch will be patched up later.
29877: //
29877: // If the jump is to a known address (with _t != 0) and it fits in a relative
29877: // jump (±32MB), emit that.
18776: // If the jump is unconditional, emit the dest address inline in
18776: // the instruction stream and load it into pc.
18776: // If the jump has a condition, but noone's mucked with _nIns and our _nSlot
18776: // pointer is valid, stick the constant in the slot and emit a conditional
18776: // load into pc.
18776: // Otherwise, emit the conditional load into pc from a nearby constant,
18776: // and emit a jump to jump over it it in case the condition fails.
18776: //
33993: // NB: B_nochk depends on this not calling samepage() when _c == AL
18776: void
18776: Assembler::B_cond_chk(ConditionCode _c, NIns* _t, bool _chk)
18776: {
20919:     int32_t offs = PC_OFFSET_FROM(_t,_nIns-1);
28810:     //nj_dprintf("B_cond_chk target: 0x%08x offset: %d @0x%08x\n", _t, offs, _nIns-1);
26547: 
26547:     // optimistically check if this will fit in 24 bits
29877:     if (_chk && isS24(offs>>2) && (_t != 0)) {
29877:         underrunProtect(4);
26547:         // recalculate the offset, because underrunProtect may have
26547:         // moved _nIns to a new page
18776:         offs = PC_OFFSET_FROM(_t,_nIns-1);
18776:     }
18776: 
29877:     // Emit one of the following patterns:
29877:     //
29877:     //  --- Short branch. This can never be emitted if the branch target is not
29877:     //      known.
29877:     //          B(cc)   ±32MB
29877:     //
29877:     //  --- Long unconditional branch.
29877:     //          LDR     PC, #lit
29877:     //  lit:    #target
29877:     //
29877:     //  --- Long conditional branch. Note that conditional branches will never
29877:     //      be patched, so the nPatchBranch function doesn't need to know where
29877:     //      the literal pool is located.
29877:     //          LDRcc   PC, #lit
32766:     //          ; #lit is in the literal pool at _nSlot
29877:     //
32766:     //  --- Long conditional branch (if the slot isn't on the same page as the instruction).
29877:     //          LDRcc   PC, #lit
29877:     //          B       skip        ; Jump over the literal data.
29877:     //  lit:    #target
29877:     //  skip:   [...]
29877: 
29877:     if (isS24(offs>>2) && (_t != 0)) {
29877:         // The underrunProtect for this was done above (if required by _chk).
18776:         *(--_nIns) = (NIns)( ((_c)<<28) | (0xA<<24) | (((offs)>>2) & 0xFFFFFF) );
34303:         asm_output("b%s %p", _c == AL ? "" : condNames[_c], (void*)(_t));
18776:     } else if (_c == AL) {
18776:         if(_chk) underrunProtect(8);
18776:         *(--_nIns) = (NIns)(_t);
18776:         *(--_nIns) = (NIns)( COND_AL | (0x51<<20) | (PC<<16) | (PC<<12) | 0x4 );
34303:         asm_output("b%s %p", _c == AL ? "" : condNames[_c], (void*)(_t));
34303:     } else if (PC_OFFSET_FROM(_nSlot, _nIns-1) > -0x1000) {
18776:         if(_chk) underrunProtect(8);
32766:         *(_nSlot++) = (NIns)(_t);
32766:         offs = PC_OFFSET_FROM(_nSlot-1,_nIns-1);
18776:         NanoAssert(offs < 0);
32766:         *(--_nIns) = (NIns)( ((_c)<<28) | (0x51<<20) | (PC<<16) | (PC<<12) | ((-offs) & 0xFFF) );
32766:         asm_output("ldr%s %s, [%s, #-%d]", condNames[_c], gpn(PC), gpn(PC), -offs);
32766:         NanoAssert(uintptr_t(_nIns)+8+offs == uintptr_t(_nSlot-1));
18776:     } else {
18776:         if(_chk) underrunProtect(12);
29877:         // Emit a pointer to the target as a literal in the instruction stream.
18776:         *(--_nIns) = (NIns)(_t);
29877:         // Emit a branch to skip over the literal. The PC value is 8 bytes
29877:         // ahead of the executing instruction, so to branch two instructions
29877:         // forward this must branch 8-8=0 bytes.
29877:         *(--_nIns) = (NIns)( COND_AL | (0xA<<24) | 0x0 );
29877:         // Emit the conditional branch.
18776:         *(--_nIns) = (NIns)( ((_c)<<28) | (0x51<<20) | (PC<<16) | (PC<<12) | 0x0 );
34303:         asm_output("b%s %p", _c == AL ? "" : condNames[_c], (void*)(_t));
18776:     }
18776: }
18776: 
18776: /*
18776:  * VFP
18776:  */
18776: 
18776: void
18776: Assembler::asm_i2f(LInsp ins)
18776: {
18776:     Register rr = prepResultReg(ins, FpRegs);
18776:     Register srcr = findRegFor(ins->oprnd1(), GpRegs);
18776: 
18776:     // todo: support int value in memory, as per x86
35062:     NanoAssert(isKnownReg(srcr));
18776: 
18776:     FSITOD(rr, FpSingleScratch);
18776:     FMSR(FpSingleScratch, srcr);
18776: }
18776: 
18776: void
18776: Assembler::asm_u2f(LInsp ins)
18776: {
18776:     Register rr = prepResultReg(ins, FpRegs);
18776:     Register sr = findRegFor(ins->oprnd1(), GpRegs);
18776: 
18776:     // todo: support int value in memory, as per x86
35062:     NanoAssert(isKnownReg(sr));
18776: 
18776:     FUITOD(rr, FpSingleScratch);
18776:     FMSR(FpSingleScratch, sr);
18776: }
18776: 
37700: void Assembler::asm_f2i(LInsp ins)
37700: {
37700:     // where our result goes
37700:     Register rr = prepResultReg(ins, GpRegs);
37700:     Register sr = findRegFor(ins->oprnd1(), FpRegs);
37700: 
37701:     FMRS(rr, FpSingleScratch);
37701:     FTOSID(FpSingleScratch, sr);
37700: }
37700: 
18776: void
18776: Assembler::asm_fneg(LInsp ins)
18776: {
18776:     LInsp lhs = ins->oprnd1();
18776:     Register rr = prepResultReg(ins, FpRegs);
18776: 
35062:     Register sr = ( lhs->isUnusedOrHasUnknownReg()
35062:                   ? findRegFor(lhs, FpRegs)
35062:                   : lhs->getReg() );
18776: 
18776:     FNEGD(rr, sr);
18776: }
18776: 
18776: void
18776: Assembler::asm_fop(LInsp ins)
18776: {
18776:     LInsp lhs = ins->oprnd1();
18776:     LInsp rhs = ins->oprnd2();
18776:     LOpcode op = ins->opcode();
18776: 
18776:     NanoAssert(op >= LIR_fadd && op <= LIR_fdiv);
18776: 
18776:     // rr = ra OP rb
18776: 
18776:     Register rr = prepResultReg(ins, FpRegs);
18776: 
18776:     Register ra = findRegFor(lhs, FpRegs);
28407:     Register rb = (rhs == lhs) ? ra : findRegFor(rhs, FpRegs & ~rmask(ra));
18776: 
18776:     // XXX special-case 1.0 and 0.0
18776: 
28407:     switch (op)
28407:     {
28407:         case LIR_fadd:      FADDD(rr,ra,rb);    break;
28407:         case LIR_fsub:      FSUBD(rr,ra,rb);    break;
28407:         case LIR_fmul:      FMULD(rr,ra,rb);    break;
28407:         case LIR_fdiv:      FDIVD(rr,ra,rb);    break;
28407:         default:            NanoAssert(0);      break;
28407:     }
18776: }
18776: 
18776: void
18776: Assembler::asm_fcmp(LInsp ins)
18776: {
18776:     LInsp lhs = ins->oprnd1();
18776:     LInsp rhs = ins->oprnd2();
18776:     LOpcode op = ins->opcode();
18776: 
18776:     NanoAssert(op >= LIR_feq && op <= LIR_fge);
18776: 
35062:     Register ra, rb;
37705:     findRegFor2(FpRegs, lhs, ra, FpRegs, rhs, rb);
34303: 
33945:     int e_bit = (op != LIR_feq);
18776: 
33945:     // do the comparison and get results loaded in ARM status register
18776:     FMSTAT();
33945:     FCMPD(ra, rb, e_bit);
18776: }
18776: 
33993: /* Call this with targ set to 0 if the target is not yet known and the branch
33993:  * will be patched up later.
33993:  */
20923: NIns*
30730: Assembler::asm_branch(bool branchOnFalse, LInsp cond, NIns* targ)
20923: {
20923:     LOpcode condop = cond->opcode();
20923:     NanoAssert(cond->isCond());
33993:     NanoAssert(ARM_VFP || ((condop < LIR_feq) || (condop > LIR_fge)));
20923: 
29862:     // The old "never" condition code has special meaning on newer ARM cores,
29862:     // so use "always" as a sensible default code.
29862:     ConditionCode cc = AL;
29862: 
29862:     // Detect whether or not this is a floating-point comparison.
29862:     bool    fp_cond;
29862: 
35063:     // Because MUL can't set the V flag, we use SMULL and CMP to set the Z flag
35063:     // to detect overflow on multiply. Thus, if cond points to a LIR_ov which
35063:     // in turn points to a LIR_mul, we must be conditional on !Z, not V.
35063:     if ((condop == LIR_ov) && (cond->oprnd1()->isop(LIR_mul))) {
35063:         condop = LIR_eq;
35063:         branchOnFalse = !branchOnFalse;
35063:     }
35063: 
29862:     // Select the appropriate ARM condition code to match the LIR instruction.
29862:     switch (condop)
20923:     {
29862:         // Floating-point conditions. Note that the VFP LT/LE conditions
29862:         // require use of the unsigned condition codes, even though
29862:         // float-point comparisons are always signed.
29862:         case LIR_feq:   cc = EQ;    fp_cond = true;     break;
29862:         case LIR_flt:   cc = LO;    fp_cond = true;     break;
29862:         case LIR_fle:   cc = LS;    fp_cond = true;     break;
29862:         case LIR_fge:   cc = GE;    fp_cond = true;     break;
29862:         case LIR_fgt:   cc = GT;    fp_cond = true;     break;
24285: 
29862:         // Standard signed and unsigned integer comparisons.
29862:         case LIR_eq:    cc = EQ;    fp_cond = false;    break;
29862:         case LIR_ov:    cc = VS;    fp_cond = false;    break;
29862:         case LIR_lt:    cc = LT;    fp_cond = false;    break;
29862:         case LIR_le:    cc = LE;    fp_cond = false;    break;
29862:         case LIR_gt:    cc = GT;    fp_cond = false;    break;
29862:         case LIR_ge:    cc = GE;    fp_cond = false;    break;
29862:         case LIR_ult:   cc = LO;    fp_cond = false;    break;
29862:         case LIR_ule:   cc = LS;    fp_cond = false;    break;
29862:         case LIR_ugt:   cc = HI;    fp_cond = false;    break;
29862:         case LIR_uge:   cc = HS;    fp_cond = false;    break;
29862: 
29862:         // Default case for invalid or unexpected LIR instructions.
29862:         default:        cc = AL;    fp_cond = false;    break;
24285:     }
24285: 
29862:     // Invert the condition if required.
29862:     if (branchOnFalse)
29862:         cc = OppositeCond(cc);
29862: 
29862:     // Ensure that we got a sensible condition code.
29862:     NanoAssert((cc != AL) && (cc != NV));
29862: 
29862:     // Ensure that we don't hit floating-point LIR codes if VFP is disabled.
33942:     NanoAssert(ARM_VFP || !fp_cond);
29862: 
29862:     // Emit a suitable branch instruction.
24285:     B_cond(cc, targ);
20923: 
29862:     // Store the address of the branch instruction so that we can return it.
29862:     // asm_[f]cmp will move _nIns so we must do this now.
20923:     NIns *at = _nIns;
29862: 
33993:     if (ARM_VFP && fp_cond)
20923:         asm_fcmp(cond);
29862:     else
29862:         asm_cmp(cond);
20923: 
20923:     return at;
20923: }
20923: 
20923: void
20923: Assembler::asm_cmp(LIns *cond)
20923: {
20923:     LOpcode condop = cond->opcode();
20923: 
30029:     // LIR_ov recycles the flags set by arithmetic ops
30029:     if ((condop == LIR_ov))
20923:         return;
20923: 
20923:     LInsp lhs = cond->oprnd1();
20923:     LInsp rhs = cond->oprnd2();
20923: 
20923:     // Not supported yet.
20923:     NanoAssert(!lhs->isQuad() && !rhs->isQuad());
20923: 
20923:     // ready to issue the compare
20923:     if (rhs->isconst()) {
28182:         int c = rhs->imm32();
36549:         Register r = findRegFor(lhs, GpRegs);
20923:         if (c == 0 && cond->isop(LIR_eq)) {
28542:             TST(r, r);
36549:         } else {
26538:             asm_cmpi(r, c);
20923:         }
20923:     } else {
35062:         Register ra, rb;
37705:         findRegFor2(GpRegs, lhs, ra, GpRegs, rhs, rb);
20923:         CMP(ra, rb);
20923:     }
20923: }
20923: 
20923: void
26538: Assembler::asm_cmpi(Register r, int32_t imm)
26538: {
26538:     if (imm < 0) {
26538:         if (imm > -256) {
26538:             ALUi(AL, cmn, 1, 0, r, -imm);
26538:         } else {
34303:             underrunProtect(4 + LD32_size);
26538:             CMP(r, IP);
26547:             asm_ld_imm(IP, imm);
26538:         }
26538:     } else {
26538:         if (imm < 256) {
26538:             ALUi(AL, cmp, 1, 0, r, imm);
26538:         } else {
34303:             underrunProtect(4 + LD32_size);
26538:             CMP(r, IP);
26547:             asm_ld_imm(IP, imm);
26538:         }
26538:     }
26538: }
26538: 
26538: void
20923: Assembler::asm_fcond(LInsp ins)
20923: {
20923:     // only want certain regs
20923:     Register r = prepResultReg(ins, AllowableFlagRegs);
20923: 
24285:     switch (ins->opcode()) {
33945:         case LIR_feq: SETEQ(r); break;
33945:         case LIR_flt: SETLO(r); break; // } note: VFP LT/LE operations require use of
33945:         case LIR_fle: SETLS(r); break; // } unsigned LO/LS condition codes!
33945:         case LIR_fge: SETGE(r); break;
33945:         case LIR_fgt: SETGT(r); break;
26537:         default: NanoAssert(0); break;
24285:     }
24285: 
20923:     asm_fcmp(ins);
20923: }
20923: 
20923: void
20923: Assembler::asm_cond(LInsp ins)
20923: {
20923:     Register r = prepResultReg(ins, AllowableFlagRegs);
35063:     LOpcode op = ins->opcode();
35063: 
35063:     switch(op)
29863:     {
33945:         case LIR_eq:  SETEQ(r); break;
33945:         case LIR_lt:  SETLT(r); break;
33945:         case LIR_le:  SETLE(r); break;
33945:         case LIR_gt:  SETGT(r); break;
33945:         case LIR_ge:  SETGE(r); break;
33945:         case LIR_ult: SETLO(r); break;
33945:         case LIR_ule: SETLS(r); break;
33945:         case LIR_ugt: SETHI(r); break;
33945:         case LIR_uge: SETHS(r); break;
35063:         case LIR_ov:
35063:             // Because MUL can't set the V flag, we use SMULL and CMP to set
35063:             // the Z flag to detect overflow on multiply. Thus, if ins points
35063:             // to a LIR_ov which in turn points to a LIR_mul, we must be
35063:             // conditional on !Z, not V.
35063:             if (!ins->oprnd1()->isop(LIR_mul)) {
35063:                 SETVS(r);
35063:             } else {
35063:                 SETNE(r);
35063:             }
35063:             break;
29863:         default:      NanoAssert(0);  break;
29863:     }
20923:     asm_cmp(ins);
20923: }
20923: 
20923: void
20923: Assembler::asm_arith(LInsp ins)
20923: {
20923:     LOpcode op = ins->opcode();
20923:     LInsp   lhs = ins->oprnd1();
20923:     LInsp   rhs = ins->oprnd2();
20923: 
30460:     RegisterMask    allow = GpRegs;
30460: 
30460:     // We always need the result register and the first operand register.
30460:     Register        rr = prepResultReg(ins, allow);
20923: 
30460:     // If this is the last use of lhs in reg, we can re-use the result reg.
35062:     // Else, lhs already has a register assigned.
35062:     Register        ra = ( lhs->isUnusedOrHasUnknownReg()
35062:                          ? findSpecificRegFor(lhs, rr)
35062:                          : lhs->getReg() );
30460: 
30460:     // Don't re-use the registers we've already allocated.
35062:     NanoAssert(isKnownReg(rr));
35062:     NanoAssert(isKnownReg(ra));
30460:     allow &= ~rmask(rr);
30460:     allow &= ~rmask(ra);
30460: 
30460:     // If the rhs is constant, we can use the instruction-specific code to
30460:     // determine if the value can be encoded in an ARM instruction. If the
30460:     // value cannot be encoded, it will be loaded into a register.
30460:     //
30460:     // Note that the MUL instruction can never take an immediate argument so
30460:     // even if the argument is constant, we must allocate a register for it.
30460:     //
30460:     // Note: It is possible to use a combination of the barrel shifter and the
30460:     // basic arithmetic instructions to generate constant multiplications.
30460:     // However, LIR_mul is never invoked with a constant during
30460:     // trace-tests.js so it is very unlikely to be worthwhile implementing it.
30460:     if (rhs->isconst() && op != LIR_mul)
30460:     {
33994:         if ((op == LIR_add || op == LIR_iaddp) && lhs->isop(LIR_ialloc)) {
30460:             // Add alloc+const. The result should be the address of the
30460:             // allocated space plus a constant.
30460:             Register    rs = prepResultReg(ins, allow);
30460:             int         d = findMemFor(lhs) + rhs->imm32();
30460: 
35062:             NanoAssert(isKnownReg(rs));
30460:             asm_add_imm(rs, FP, d);
20923:         }
20923: 
30460:         int32_t imm32 = rhs->imm32();
30460: 
30460:         switch (op)
30460:         {
30460:             case LIR_iaddp: asm_add_imm(rr, ra, imm32);     break;
30460:             case LIR_add:   asm_add_imm(rr, ra, imm32, 1);  break;
30460:             case LIR_sub:   asm_sub_imm(rr, ra, imm32, 1);  break;
30460:             case LIR_and:   asm_and_imm(rr, ra, imm32);     break;
30460:             case LIR_or:    asm_orr_imm(rr, ra, imm32);     break;
30460:             case LIR_xor:   asm_eor_imm(rr, ra, imm32);     break;
30460:             case LIR_lsh:   LSLi(rr, ra, imm32);            break;
30460:             case LIR_rsh:   ASRi(rr, ra, imm32);            break;
30460:             case LIR_ush:   LSRi(rr, ra, imm32);            break;
30460: 
30460:             default:
30460:                 NanoAssertMsg(0, "Unsupported");
30460:                 break;
20923:         }
20923: 
30460:         // We've already emitted an instruction, so return now.
30460:         return;
30460:     }
20923: 
30460:     // The rhs is either a register or cannot be encoded as a constant.
30460: 
35062:     Register rb;
30460:     if (lhs == rhs) {
20923:         rb = ra;
30460:     } else {
30460:         rb = asm_binop_rhs_reg(ins);
35062:         if (!isKnownReg(rb))
30460:             rb = findRegFor(rhs, allow);
30460:         allow &= ~rmask(rb);
30460:     }
35062:     NanoAssert(isKnownReg(rb));
20923: 
30460:     switch (op)
30460:     {
30460:         case LIR_iaddp: ADDs(rr, ra, rb, 0);    break;
30460:         case LIR_add:   ADDs(rr, ra, rb, 1);    break;
30460:         case LIR_sub:   SUBs(rr, ra, rb, 1);    break;
30460:         case LIR_and:   ANDs(rr, ra, rb, 0);    break;
30460:         case LIR_or:    ORRs(rr, ra, rb, 0);    break;
30460:         case LIR_xor:   EORs(rr, ra, rb, 0);    break;
30460: 
30460:         case LIR_mul:
30460:             // ARMv5 and earlier cores cannot do a MUL where the first operand
30460:             // is also the result, so we need a special case to handle that.
30460:             //
30460:             // We try to use rb as the first operand by default because it is
30460:             // common for (rr == ra) and is thus likely to be the most
35063:             // efficient method.
35063: 
33942:             if ((ARM_ARCH > 5) || (rr != rb)) {
35063:                 // IP is used to temporarily store the high word of the result from
35063:                 // SMULL, so we make use of this to perform an overflow check, as
35063:                 // ARM's MUL instruction can't set the overflow flag by itself.
35063:                 // We can check for overflow using the following:
35063:                 //   SMULL  rr, ip, ra, rb
35063:                 //   CMP    ip, rr, ASR #31
35063:                 // An explanation can be found in bug 521161. This sets Z if we did
35063:                 // _not_ overflow, and clears it if we did.
35063:                 ALUr_shi(AL, cmp, 1, IP, IP, rr, ASR_imm, 31);
35063:                 SMULL(rr, IP, rb, ra);
30460:             } else {
33942:                 // ARM_ARCH is ARMv5 (or below) and rr == rb, so we must
30460:                 // find a different way to encode the instruction.
30460: 
30460:                 // If possible, swap the arguments to avoid the restriction.
30460:                 if (rr != ra) {
30460:                     // We know that rr == rb, so this will be something like
30460:                     // rX = rY * rX.
35063:                     // Other than swapping ra and rb, this works in the same as
35063:                     // as the ARMv6+ case, above.
35063:                     ALUr_shi(AL, cmp, 1, IP, IP, rr, ASR_imm, 31);
35063:                     SMULL(rr, IP, ra, rb);
30460:                 } else {
35063:                     // We're trying to do rX = rX * rX, but we also need to
35063:                     // check for overflow so we would need two extra registers
35063:                     // on ARMv5 and below. We achieve this by observing the
35063:                     // following:
35063:                     //   - abs(rX)*abs(rX) = rX*rX, so we force the input to be
35063:                     //     positive to simplify the detection logic.
35063:                     //   - Any argument greater than 0xffff will _always_
35063:                     //     overflow, and we can easily check that the top 16
35063:                     //     bits are zero.
35063:                     //   - Any argument lower than (or equal to) 0xffff that
35063:                     //     also overflows is guaranteed to set output bit 31.
35063:                     //
35063:                     // Thus, we know we have _not_ overflowed if:
35063:                     //   abs(rX)&0xffff0000 == 0 AND result[31] == 0
35063:                     //
35063:                     // The following instruction sequence will be emitted:
35063:                     // MOVS     IP, rX      // Put abs(rX) into IP.
35063:                     // RSBMI    IP, IP, #0  // ...
35063:                     // MUL      rX, IP, IP  // Do the actual multiplication.
35063:                     // MOVS     IP, IP, LSR #16 // Check that abs(arg)<=0xffff
35063:                     // CMPEQ    IP, rX, ASR #31 // Check that result[31] == 0
30460: 
30460:                     NanoAssert(rr != IP);
30460: 
35063:                     ALUr_shi(AL, cmp, 1, IP, rr, rr, ASR_imm, 31);
35063:                     ALUr_shi(AL, mov, 1, IP, IP, IP, LSR_imm, 16);
35063:                     MUL(rr, IP, IP);
35063:                     ALUi(MI, rsb, 0, IP, IP, 0);
35063:                     ALUr(AL, mov, 1, IP, ra, ra);
30460:                 }
30460:             }
30460:             break;
30460: 
30460:         // The shift operations need a mask to match the JavaScript
30460:         // specification because the ARM architecture allows a greater shift
30460:         // range than JavaScript.
30460:         case LIR_lsh:
28542:             LSL(rr, ra, IP);
28238:             ANDi(IP, rb, 0x1f);
30460:             break;
30460:         case LIR_rsh:
28542:             ASR(rr, ra, IP);
28238:             ANDi(IP, rb, 0x1f);
30460:             break;
30460:         case LIR_ush:
28542:             LSR(rr, ra, IP);
28238:             ANDi(IP, rb, 0x1f);
30460:             break;
30460:         default:
20923:             NanoAssertMsg(0, "Unsupported");
30460:             break;
20923:     }
20923: }
20923: 
20923: void
20923: Assembler::asm_neg_not(LInsp ins)
20923: {
20923:     LOpcode op = ins->opcode();
20923:     Register rr = prepResultReg(ins, GpRegs);
20923: 
20923:     LIns* lhs = ins->oprnd1();
35062:     // If this is the last use of lhs in reg, we can re-use result reg.
35062:     // Else, lhs already has a register assigned.
35062:     Register ra = ( lhs->isUnusedOrHasUnknownReg()
35062:                   ? findSpecificRegFor(lhs, rr)
35062:                   : lhs->getReg() );
35062:     NanoAssert(isKnownReg(ra));
20923: 
20923:     if (op == LIR_not)
26538:         MVN(rr, ra);
20923:     else
26538:         RSBS(rr, ra);
20923: }
20923: 
20923: void
36372: Assembler::asm_load32(LInsp ins)
20923: {
20923:     LOpcode op = ins->opcode();
20923:     LIns* base = ins->oprnd1();
30238:     int d = ins->disp();
29861: 
20923:     Register rr = prepResultReg(ins, GpRegs);
36552:     Register ra = getBaseReg(base, d, GpRegs);
25562: 
36372:     switch (op) {
36372:         case LIR_ldzb:
36372:         case LIR_ldcb:
36552:             if (isU12(-d) || isU12(d)) {
36372:                 LDRB(rr, ra, d);
36552:             } else {
36552:                 LDRB(rr, IP, 0);
36552:                 asm_add_imm(IP, ra, d);
36552:             }
36372:             return;
36372:         case LIR_ldzs:
36372:         case LIR_ldcs:
36552:             // These are expected to be 2-byte aligned.  (Not all ARM machines
36552:             // can handle unaligned accesses.)
36552:             // Similar to the ldcb/ldzb case, but the max offset is smaller.
36552:             if (isU8(-d) || isU8(d)) {
36372:                 LDRH(rr, ra, d);
36552:             } else {
36552:                 LDRH(rr, IP, 0);
36552:                 asm_add_imm(IP, ra, d);
36552:             }
36372:             return;
36372:         case LIR_ld:
36372:         case LIR_ldc:
36552:             // These are expected to be 4-byte aligned.
36552:             if (isU12(-d) || isU12(d)) {
30460:                 LDR(rr, ra, d);
36552:             } else {
36552:                 LDR(rr, IP, 0);
36552:                 asm_add_imm(IP, ra, d);
36552:             }
25562:             return;
36372:         case LIR_ldsb:
36372:         case LIR_ldss:
36372:         case LIR_ldcsb:
36372:         case LIR_ldcss:
36372:             NanoAssertMsg(0, "NJ_EXPANDED_LOADSTORE_SUPPORTED not yet supported for this architecture");
36372:             return;
36372:         default:
36372:             NanoAssertMsg(0, "asm_load32 should never receive this LIR opcode");
25741:             return;
25562:     }
20923: }
20923: 
20923: void
20923: Assembler::asm_cmov(LInsp ins)
20923: {
26539:     NanoAssert(ins->opcode() == LIR_cmov);
20923:     LIns* condval = ins->oprnd1();
30648:     LIns* iftrue  = ins->oprnd2();
30648:     LIns* iffalse = ins->oprnd3();
30648: 
30485:     NanoAssert(condval->isCmp());
26539:     NanoAssert(!iftrue->isQuad() && !iffalse->isQuad());
20923: 
20923:     const Register rr = prepResultReg(ins, GpRegs);
20923: 
20923:     // this code assumes that neither LD nor MR nor MRcc set any of the condition flags.
20923:     // (This is true on Intel, is it true on all architectures?)
20923:     const Register iffalsereg = findRegFor(iffalse, GpRegs & ~rmask(rr));
20923:     switch (condval->opcode()) {
20923:         // note that these are all opposites...
26537:         case LIR_eq:    MOVNE(rr, iffalsereg);  break;
26537:         case LIR_lt:    MOVGE(rr, iffalsereg);  break;
26540:         case LIR_le:    MOVGT(rr, iffalsereg);  break;
26537:         case LIR_gt:    MOVLE(rr, iffalsereg);  break;
26540:         case LIR_ge:    MOVLT(rr, iffalsereg);  break;
33945:         case LIR_ult:   MOVHS(rr, iffalsereg);  break;
26540:         case LIR_ule:   MOVHI(rr, iffalsereg);  break;
26540:         case LIR_ugt:   MOVLS(rr, iffalsereg);  break;
33945:         case LIR_uge:   MOVLO(rr, iffalsereg);  break;
35063:         case LIR_ov:
35063:             // Because MUL can't set the V flag, we use SMULL and CMP to set
35063:             // the Z flag to detect overflow on multiply. Thus, if ins points
35063:             // to a LIR_ov which in turn points to a LIR_mul, we must be
35063:             // conditional on !Z, not V.
35063:             if (!condval->oprnd1()->isop(LIR_mul)) {
35063:                 MOVVC(rr, iffalsereg);
35063:             } else {
35063:                 MOVEQ(rr, iffalsereg);
35063:             }
35063:             break;
21546:         default: debug_only( NanoAssert(0) );   break;
20923:     }
20923:     /*const Register iftruereg =*/ findSpecificRegFor(iftrue, rr);
20923:     asm_cmp(condval);
20923: }
20923: 
20923: void
20923: Assembler::asm_qhi(LInsp ins)
20923: {
20923:     Register rr = prepResultReg(ins, GpRegs);
20923:     LIns *q = ins->oprnd1();
20923:     int d = findMemFor(q);
30460:     LDR(rr, FP, d+4);
20923: }
20923: 
20923: void
20923: Assembler::asm_qlo(LInsp ins)
20923: {
20923:     Register rr = prepResultReg(ins, GpRegs);
20923:     LIns *q = ins->oprnd1();
20923:     int d = findMemFor(q);
30460:     LDR(rr, FP, d);
20923: }
20923: 
20923: void
20923: Assembler::asm_param(LInsp ins)
20923: {
30026:     uint32_t a = ins->paramArg();
30026:     uint32_t kind = ins->paramKind();
20923:     if (kind == 0) {
20923:         // ordinary param
20923:         AbiKind abi = _thisfrag->lirbuf->abi;
33945:         uint32_t abi_regcount = abi == ABI_CDECL ? 4 : abi == ABI_FASTCALL ? 2 : abi == ABI_THISCALL ? 1 : 0;
20923:         if (a < abi_regcount) {
20923:             // incoming arg in register
20923:             prepResultReg(ins, rmask(argRegs[a]));
20923:         } else {
20923:             // incoming arg is on stack, and EBP points nearby (see genPrologue)
20923:             Register r = prepResultReg(ins, GpRegs);
20923:             int d = (a - abi_regcount) * sizeof(intptr_t) + 8;
30460:             LDR(r, FP, d);
20923:         }
20923:     } else {
20923:         // saved param
20923:         prepResultReg(ins, rmask(savedRegs[a]));
20923:     }
20923: }
20923: 
20923: void
20923: Assembler::asm_int(LInsp ins)
20923: {
20923:     Register rr = prepResultReg(ins, GpRegs);
30460:     asm_ld_imm(rr, ins->imm32());
20923: }
20923: 
32556: void
32556: Assembler::asm_ret(LIns *ins)
32556: {
32634:     genEpilogue();
32634: 
34576:     // NB: our contract with genEpilogue is actually that the return value
34576:     // we are intending for R0 is currently IP, not R0. This has to do with
34576:     // the strange dual-nature of the patchable jump in a side-exit. See
34576:     // nPatchBranch.
34576: 
34576:     MOV(IP, R0);
34576: 
32634:     // Pop the stack frame.
32634:     MOV(SP,FP);
32634: 
36672:     releaseRegisters();
32556:     assignSavedRegs();
32556:     LIns *value = ins->oprnd1();
32556:     if (ins->isop(LIR_ret)) {
32556:         findSpecificRegFor(value, R0);
32556:     }
32556:     else {
32556:         NanoAssert(ins->isop(LIR_fret));
33993:         if (ARM_VFP) {
32556:             Register reg = findRegFor(value, FpRegs);
32556:             FMRRD(R0, R1, reg);
33993:         } else {
32556:             NanoAssert(value->isop(LIR_qjoin));
32556:             findSpecificRegFor(value->oprnd1(), R0); // lo
32556:             findSpecificRegFor(value->oprnd2(), R1); // hi
33993:         }
32556:     }
32556: }
32556: 
32556: void
32556: Assembler::asm_promote(LIns *ins)
32556: {
32556:     /* The LIR opcodes that result in a call to asm_promote are only generated
32556:      * if NANOJIT_64BIT is #define'd, which it never is for ARM.
32556:      */
32556:     (void)ins;
32556:     NanoAssert(0);
32556: }
32556: 
35087: void
35087: Assembler::asm_jtbl(LIns* ins, NIns** table)
35087: {
35087:     Register indexreg = findRegFor(ins->oprnd1(), GpRegs);
35317:     Register tmp = registerAllocTmp(GpRegs & ~rmask(indexreg));
35087:     LDR_scaled(PC, tmp, indexreg, 2);      // LDR PC, [tmp + index*4]
35087:     asm_ld_imm(tmp, (int32_t)table);       // tmp = #table
35087: }
35087: 
35356: void Assembler::swapCodeChunks() {
37698:     if (!_nExitIns)
37698:         codeAlloc(exitStart, exitEnd, _nExitIns verbose_only(, exitBytes));
37698:     if (!_nExitSlot)
37698:         _nExitSlot = exitStart;
35356:     SWAP(NIns*, _nIns, _nExitIns);
35356:     SWAP(NIns*, _nSlot, _nExitSlot);        // this one is ARM-specific
35356:     SWAP(NIns*, codeStart, exitStart);
35356:     SWAP(NIns*, codeEnd, exitEnd);
35356:     verbose_only( SWAP(size_t, codeBytes, exitBytes); )
35356: }
35356: 
26545: }
18776: #endif /* FEATURE_NANOJIT */
