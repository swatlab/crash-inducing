30266: /* -*- Mode: C++; c-basic-offset: 4; indent-tabs-mode: nil; tab-width: 4 -*- */
30266: /* vi: set ts=4 sw=4 expandtab: (add to ~/.vimrc: set modeline modelines=5) */
17275: /* ***** BEGIN LICENSE BLOCK *****
17275:  * Version: MPL 1.1/GPL 2.0/LGPL 2.1
17275:  *
17275:  * The contents of this file are subject to the Mozilla Public License Version
17275:  * 1.1 (the "License"); you may not use this file except in compliance with
17275:  * the License. You may obtain a copy of the License at
17275:  * http://www.mozilla.org/MPL/
17275:  *
17275:  * Software distributed under the License is distributed on an "AS IS" basis,
17275:  * WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
17275:  * for the specific language governing rights and limitations under the
17275:  * License.
17275:  *
17275:  * The Original Code is [Open Source Virtual Machine].
17275:  *
17275:  * The Initial Developer of the Original Code is
17275:  * Adobe System Incorporated.
17275:  * Portions created by the Initial Developer are Copyright (C) 2004-2007
17275:  * the Initial Developer. All Rights Reserved.
17275:  *
17275:  * Contributor(s):
17275:  *   Adobe AS3 Team
18720:  *   Mozilla TraceMonkey Team
18720:  *   Asko Tontti <atontti@cc.hut.fi>
17275:  *
17275:  * Alternatively, the contents of this file may be used under the terms of
17275:  * either the GNU General Public License Version 2 or later (the "GPL"), or
17275:  * the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
17275:  * in which case the provisions of the GPL or the LGPL are applicable instead
17275:  * of those above. If you wish to allow use of your version of this file only
17275:  * under the terms of either the GPL or the LGPL, and not to allow others to
17275:  * use your version of this file under the terms of the MPL, indicate your
17275:  * decision by deleting the provisions above and replace them with the notice
17275:  * and other provisions required by the GPL or the LGPL. If you do not delete
17275:  * the provisions above, a recipient may use your version of this file under
17275:  * the terms of any one of the MPL, the GPL or the LGPL.
17275:  *
17275:  * ***** END LICENSE BLOCK ***** */
33125: #include "nanojit.h"
17275: 
33939: #ifdef _MSC_VER
33939:     // disable some specific warnings which are normally useful, but pervasive in the code-gen macros
33939:     #pragma warning(disable:4310) // cast truncates constant value
17485: #endif
17275: 
17275: namespace nanojit
17275: {
33125:     #if defined FEATURE_NANOJIT && defined NANOJIT_IA32
17275: 
17275:     #ifdef NJ_VERBOSE
17275:         const char *regNames[] = {
17275:             "eax", "ecx", "edx", "ebx", "esp", "ebp", "esi", "edi",
17275:             "xmm0","xmm1","xmm2","xmm3","xmm4","xmm5","xmm6","xmm7",
32639:             "f0"
17275:         };
17275:     #endif
17275: 
32556:     #define TODO(x) do{ verbose_only(outputf(#x);) NanoAssertMsgf(false, "%s", #x); } while(0)
32556: 
17275:     const Register Assembler::argRegs[] = { ECX, EDX };
17275:     const Register Assembler::retRegs[] = { EAX, EDX };
20893:     const Register Assembler::savedRegs[] = { EBX, ESI, EDI };
17275: 
20893:     const static uint8_t max_abi_regs[] = {
20893:         2, /* ABI_FASTCALL */
20893:         1, /* ABI_THISCALL */
20893:         0, /* ABI_STDCALL */
20893:         0  /* ABI_CDECL */
20893:     };
20893: 
36371:     static bool CheckForSSE2()
36371:     {
36371:         int features = 0;
36371:     #if defined _MSC_VER
36371:         __asm
36371:         {
36371:             pushad
36371:             mov eax, 1
36371:             cpuid
36371:             mov features, edx
36371:             popad
36371:         }
36371:     #elif defined __GNUC__
36371:         asm("xchg %%esi, %%ebx\n" /* we can't clobber ebx on gcc (PIC register) */
36371:             "mov $0x01, %%eax\n"
36371:             "cpuid\n"
36371:             "mov %%edx, %0\n"
36371:             "xchg %%esi, %%ebx\n"
36371:             : "=m" (features)
36371:             : /* We have no inputs */
36371:             : "%eax", "%esi", "%ecx", "%edx"
36371:            );
36371:     #elif defined __SUNPRO_C || defined __SUNPRO_CC
37665:         asm("push %%ebx\n"
36371:             "mov $0x01, %%eax\n"
36371:             "cpuid\n"
37665:             "pop %%ebx\n"
36371:             : "=d" (features)
36371:             : /* We have no inputs */
37665:             : "%eax", "%ecx"
36371:            );
36371:     #endif
36371:         return (features & (1<<26)) != 0;
36371:     }
20893: 
17275:     void Assembler::nInit(AvmCore* core)
17275:     {
22668:         (void) core;
36371:         config.sse2 = config.sse2 && CheckForSSE2();
17275:     }
17275: 
32634:     void Assembler::nBeginAssembly() {
33740:         max_stk_args = 0;
32634:     }
32634: 
20893:     NIns* Assembler::genPrologue()
17275:     {
36553:         // Prologue
36664:         uint32_t stackNeeded = max_stk_args + STACK_GRANULARITY * _activation.stackSlotsNeeded();
17275: 
20893:         uint32_t stackPushed =
20893:             STACK_GRANULARITY + // returnaddr
32600:             STACK_GRANULARITY; // ebp
20944: 
17275:         uint32_t aligned = alignUp(stackNeeded + stackPushed, NJ_ALIGN_STACK);
17275:         uint32_t amt = aligned - stackPushed;
17275: 
17378:         // Reserve stackNeeded bytes, padded
17378:         // to preserve NJ_ALIGN_STACK-byte alignment.
17275:         if (amt)
30658:         {
17275:             SUBi(SP, amt);
30658:         }
17275: 
36388:         verbose_only( asm_output("[frag entry]"); )
21490:         NIns *fragEntry = _nIns;
17378:         MR(FP, SP); // Establish our own FP.
17378:         PUSHr(FP); // Save caller's FP.
17275: 
21490:         return fragEntry;
17275:     }
17275: 
17516:     void Assembler::nFragExit(LInsp guard)
17275:     {
20931:         SideExit *exit = guard->record()->exit;
31475:         bool trees = config.tree_opt;
17275:         Fragment *frag = exit->target;
17275:         GuardRecord *lr = 0;
17275:         bool destKnown = (frag && frag->fragEntry);
32634: 
25099:         // Generate jump to epilog and initialize lr.
25099:         // If the guard is LIR_xtbl, use a jump table with epilog in every entry
25099:         if (guard->isop(LIR_xtbl)) {
25099:             lr = guard->record();
32600:             Register r = EDX;
25099:             SwitchInfo* si = guard->record()->exit->switchInfo;
32634:             if (!_epilogue)
32634:                 _epilogue = genEpilogue();
25099:             emitJumpTable(si, _epilogue);
25099:             JMP_indirect(r);
25099:             LEAmi4(r, si->table, r);
25099:         } else {
25099:             // If the guard already exists, use a simple jump.
25099:             if (destKnown && !trees) {
17275:                 JMP(frag->fragEntry);
17275:                 lr = 0;
32634:             } else {  // Target doesn't exist. Jump to an epilogue for now. This can be patched later.
32634:                 if (!_epilogue)
32634:                     _epilogue = genEpilogue();
20931:                 lr = guard->record();
17275:                 JMP_long(_epilogue);
22669:                 lr->jmp = _nIns;
17275:             }
25099:         }
30658: 
32784:         // profiling for the exit
32784:         verbose_only(
32784:            if (_logc->lcbits & LC_FragProfile) {
32784:               INCLi( &guard->record()->profCount );
32784:            }
32784:         )
32784: 
32634:         // Restore ESP from EBP, undoing SUBi(SP,amt) in the prologue
17275:         MR(SP,FP);
17275: 
17275:         // return value is GuardRecord*
36368:         asm_int(EAX, int(lr), /*canClobberCCs*/true);
17516:     }
17275: 
20893:     NIns *Assembler::genEpilogue()
17275:     {
17275:         RET();
32634:         POPr(FP); // Restore caller's FP.
32600: 
17275:         return  _nIns;
17275:     }
17275: 
17687:     void Assembler::asm_call(LInsp ins)
17275:     {
36374:         Register retReg = ( ins->isop(LIR_fcall) ? FST0 : retRegs[0] );
36374:         prepResultReg(ins, rmask(retReg));
36374: 
36374:         // Do this after we've handled the call result, so we don't
36374:         // force the call result to be spilled unnecessarily.
36374: 
36374:         evictScratchRegs();
36374: 
20408:         const CallInfo* call = ins->callInfo();
17687:         // must be signed, not unsigned
20893:         uint32_t iargs = call->count_iargs();
29385:         int32_t fargs = call->count_args() - iargs;
20893: 
31077:         bool indirect = call->isIndirect();
31077:         if (indirect) {
31077:             // target arg isn't pushed, its consumed in the call
31077:             iargs --;
31077:         }
31077: 
33740:         AbiKind abi = call->_abi;
33740:         uint32_t max_regs = max_abi_regs[abi];
20893:         if (max_regs > iargs)
20893:             max_regs = iargs;
20893: 
20893:         int32_t istack = iargs-max_regs;  // first 2 4B args are in registers
34340:         int32_t extra = 0;
34340:         const int32_t pushsize = 4*istack + 8*fargs; // actual stack space used
18723: 
34340: #if _MSC_VER
34340:         // msc only provides 4-byte alignment, anything more than 4 on windows
34340:         // x86-32 requires dynamic ESP alignment in prolog/epilog and static
34340:         // esp-alignment here.
34340:         uint32_t align = 4;//NJ_ALIGN_STACK;
34340: #else
34340:         uint32_t align = NJ_ALIGN_STACK;
34340: #endif
34340: 
34340:         if (pushsize) {
34340:             if (config.fixed_esp) {
33740:                 // In case of fastcall, stdcall and thiscall the callee cleans up the stack,
33740:                 // and since we reserve max_stk_args words in the prolog to call functions
33740:                 // and don't adjust the stack pointer individually for each call we have
33740:                 // to undo here any changes the callee just did to the stack.
34340:                 if (abi != ABI_CDECL)
33740:                     SUBi(SP, pushsize);
34340:             } else {
34340:                 // stack re-alignment
34340:                 // only pop our adjustment amount since callee pops args in FASTCALL mode
34340:                 extra = alignUp(pushsize, align) - pushsize;
34340:                 if (call->_abi == ABI_CDECL) {
34340:                     // with CDECL only, caller pops args
34340:                     ADDi(SP, extra+pushsize);
34340:                 } else if (extra > 0) {
34340:                     ADDi(SP, extra);
34340:                 }
34340:             }
34340:         }
17687: 
32598:         NanoAssert(ins->isop(LIR_pcall) || ins->isop(LIR_fcall));
31077:         if (!indirect) {
17687:             CALL(call);
31077:         }
31077:         else {
31077:             // indirect call.  x86 Calling conventions don't use EAX as an
31077:             // argument, and do use EAX as a return value.  We need a register
31077:             // for the address to call, so we use EAX since it will always be
31077:             // available
31077:             CALLr(call, EAX);
31077:         }
18723: 
17687:         // make sure fpu stack is empty before call (restoreCallerSaved)
17687:         NanoAssert(_allocator.isFree(FST0));
17687:         // note: this code requires that ref arguments (ARGSIZE_Q)
17687:         // be one of the first two arguments
20893:         // pre-assign registers to the first N 4B args based on the calling convention
20893:         uint32_t n = 0;
17687: 
31051:         ArgSize sizes[MAXARGS];
17687:         uint32_t argc = call->get_sizes(sizes);
33740:         int32_t stkd = 0;
33740: 
31077:         if (indirect) {
31077:             argc--;
33740:             asm_arg(ARGSIZE_P, ins->arg(argc), EAX, stkd);
34340:             if (!config.fixed_esp)
34340:                 stkd = 0;
31077:         }
17687: 
17687:         for(uint32_t i=0; i < argc; i++)
17687:         {
17687:             uint32_t j = argc-i-1;
17687:             ArgSize sz = sizes[j];
17687:             Register r = UnknownReg;
20893:             if (n < max_regs && sz != ARGSIZE_F) {
17687:                 r = argRegs[n++]; // tell asm_arg what reg to use
20893:             }
33740:             asm_arg(sz, ins->arg(j), r, stkd);
34340:             if (!config.fixed_esp)
34340:                 stkd = 0;
17687:         }
17687: 
34340:         if (config.fixed_esp) {
33740:             if (pushsize > max_stk_args)
33740:                 max_stk_args = pushsize;
34340:         } else if (extra > 0) {
34340:             SUBi(SP, extra);
34340:         }
17275:     }
17916: 
32584:     Register Assembler::nRegisterAllocFromSet(RegisterMask set)
17275:     {
17275:         Register r;
17275:         RegAlloc &regs = _allocator;
17275:     #ifdef WIN32
17275:         _asm
17275:         {
17275:             mov ecx, regs
17275:             bsf eax, set                    // i = first bit set
17275:             btr RegAlloc::free[ecx], eax    // free &= ~rmask(i)
17275:             mov r, eax
17275:         }
17275:     #else
17275:         asm(
17275:             "bsf    %1, %%eax\n\t"
17275:             "btr    %%eax, %2\n\t"
17275:             "movl   %%eax, %0\n\t"
17275:             : "=m"(r) : "m"(set), "m"(regs.free) : "%eax", "memory" );
17275:     #endif /* WIN32 */
17275:         return r;
17275:     }
17275: 
17275:     void Assembler::nRegisterResetAll(RegAlloc& a)
17275:     {
17275:         // add scratch registers to our free list for the allocator
17275:         a.clear();
17275:         a.free = SavedRegs | ScratchRegs;
22667:         if (!config.sse2)
17275:             a.free &= ~XmmRegs;
17275:         debug_only( a.managed = a.free; )
17275:     }
17275: 
32583:     void Assembler::nPatchBranch(NIns* branch, NIns* targ)
17275:     {
21489:         intptr_t offset = intptr_t(targ) - intptr_t(branch);
21489:         if (branch[0] == JMP32) {
21489:             *(int32_t*)&branch[1] = offset - 5;
21489:         } else if (branch[0] == JCC32) {
21489:             *(int32_t*)&branch[2] = offset - 6;
21489:         } else
21489:             NanoAssertMsg(0, "Unknown branch type in nPatchBranch");
17275:     }
17275: 
17275:     RegisterMask Assembler::hint(LIns* i, RegisterMask allow)
17275:     {
17275:         uint32_t op = i->opcode();
17275:         int prefer = allow;
32598:         if (op == LIR_icall) {
20893:             prefer &= rmask(retRegs[0]);
20893:         }
29385:         else if (op == LIR_fcall) {
20893:             prefer &= rmask(FST0);
20893:         }
32598:         else if (op == LIR_param) {
32655:             if (i->paramKind() == 0) {
20893:                 uint32_t max_regs = max_abi_regs[_thisfrag->lirbuf->abi];
30026:                 if (i->paramArg() < max_regs)
32655:                     prefer &= rmask(argRegs[i->paramArg()]);
32655:             } else {
32655:                 if (i->paramArg() < NumSavedRegs)
32655:                     prefer &= rmask(savedRegs[i->paramArg()]);
32655:             }
20893:         }
25469:         else if (op == LIR_callh || (op == LIR_rsh && i->oprnd1()->opcode()==LIR_callh)) {
20893:             prefer &= rmask(retRegs[1]);
20893:         }
20893:         else if (i->isCmp()) {
17275:             prefer &= AllowableFlagRegs;
20893:         }
20893:         else if (i->isconst()) {
17275:             prefer &= ScratchRegs;
20893:         }
17275:         return (_allocator.free & prefer) ? prefer : allow;
17275:     }
17275: 
17275:     void Assembler::asm_qjoin(LIns *ins)
17275:     {
17275:         int d = findMemFor(ins);
17275:         AvmAssert(d);
17275:         LIns* lo = ins->oprnd1();
17275:         LIns* hi = ins->oprnd2();
17275: 
32727:         Register rr = ins->getReg();
32727:         if (isKnownReg(rr) && (rmask(rr) & FpRegs))
36553:             evict(ins);
17275: 
17275:         if (hi->isconst())
17275:         {
28182:             STi(FP, d+4, hi->imm32());
17275:         }
17275:         else
17275:         {
17275:             Register r = findRegFor(hi, GpRegs);
17275:             ST(FP, d+4, r);
17275:         }
17275: 
17275:         if (lo->isconst())
17275:         {
28182:             STi(FP, d, lo->imm32());
17275:         }
17275:         else
17275:         {
17275:             // okay if r gets recycled.
17275:             Register r = findRegFor(lo, GpRegs);
17275:             ST(FP, d, r);
17275:         }
17275: 
17275:         freeRsrcOf(ins, false); // if we had a reg in use, emit a ST to flush it to mem
17275:     }
17275: 
36553:     // WARNING: the code generated by this function must not affect the
36553:     // condition codes.  See asm_cmp().
36553:     void Assembler::asm_restore(LInsp ins, Register r)
17275:     {
36553:         NanoAssert(ins->getReg() == r);
36553: 
36553:         uint32_t arg;
36553:         uint32_t abi_regcount;
36553:         if (ins->isop(LIR_alloc)) {
36553:             // The value of a LIR_alloc instruction is the address of the
36553:             // stack allocation.  We can rematerialize that from the record we
36553:             // have of where the allocation lies in the stack.
36553:             NanoAssert(ins->getArIndex());  // must have stack slots allocated
36553:             LEA(r, disp(ins), FP);
36553: 
36553:         } else if (ins->isconst()) {
36553:             asm_int(r, ins->imm32(), /*canClobberCCs*/false);
36553:             if (!ins->getArIndex()) {
36553:                 ins->markAsClear();
36553:             }
36553: 
37665:         } else if (ins->isconstq()) {
37665:             asm_quad(r, ins->imm64(), ins->imm64f(), /*canClobberCCs*/false);
37665:             if (!ins->getArIndex()) {
37665:                 ins->markAsClear();
37665:             }
37665: 
36553:         } else if (ins->isop(LIR_param) && ins->paramKind() == 0 &&
36553:             (arg = ins->paramArg()) >= (abi_regcount = max_abi_regs[_thisfrag->lirbuf->abi])) {
36553:             // Incoming arg is on stack, can restore it from there instead of spilling.
36553: 
36553:             // Compute position of argument relative to ebp.  Higher argument
36553:             // numbers are at higher positive offsets.  The first abi_regcount
36553:             // arguments are in registers, rest on stack.  +8 accomodates the
36553:             // return address and saved ebp value.  Assuming abi_regcount == 0:
36553:             //
36553:             //    low-addr  ebp
36553:             //    [frame...][saved-ebp][return-addr][arg0][arg1]...
36553:             //
36553:             int d = (arg - abi_regcount) * sizeof(intptr_t) + 8;
36553:             LD(r, d, FP);
36553:             if (!ins->getArIndex()) {
36553:                 ins->markAsClear();
36553:             }
36553: 
36553:         } else {
36553:             int d = findMemFor(ins);
36553:             if (rmask(r) & GpRegs) {
36553:                 LD(r, d, FP);
36553:             } else if (rmask(r) & XmmRegs) {
17916:                 SSE_LDQ(r, d, FP);
17275:             } else {
36553:                 NanoAssert(rmask(r) & x87Regs);
17275:                 FLDQ(d, FP);
17275:             }
17275:         }
17275:     }
17275: 
36372:     void Assembler::asm_store32(LOpcode op, LIns* value, int dr, LIns* base)
17275:     {
36553:         if (value->isconst()) {
36552:             Register rb = getBaseReg(base, dr, GpRegs);
28182:             int c = value->imm32();
36372:             switch (op) {
36372:                 case LIR_stb:
36372:                     ST8i(rb, dr, c);
36372:                     break;
36372:                 case LIR_sts:
36372:                     ST16i(rb, dr, c);
36372:                     break;
36372:                 case LIR_sti:
17275:                     STi(rb, dr, c);
36372:                     break;
36372:                 default:
36372:                     NanoAssertMsg(0, "asm_store32 should never receive this LIR opcode");
36372:                     break;
36372:             }
17275:         }
17275:         else
17275:         {
36553:             // Quirk of x86-32: reg must be a/b/c/d for single-byte stores.
36372:             const RegisterMask SrcRegs = (op == LIR_stb) ?
36372:                             (1<<EAX | 1<<ECX | 1<<EDX | 1<<EBX) :
36372:                             GpRegs;
36372: 
20893:             Register ra, rb;
37705:             if (base->isconst()) {
20893:                 // absolute address
36553:                 rb = UnknownReg;
28182:                 dr += base->imm32();
36372:                 ra = findRegFor(value, SrcRegs);
20893:             } else {
37705:                 getBaseReg2(SrcRegs, value, ra, GpRegs, base, rb, dr);
20893:             }
36372:             switch (op) {
36372:                 case LIR_stb:
36372:                     ST8(rb, dr, ra);
36372:                     break;
36372:                 case LIR_sts:
36372:                     ST16(rb, dr, ra);
36372:                     break;
36372:                 case LIR_sti:
17275:                     ST(rb, dr, ra);
36372:                     break;
36372:                 default:
36372:                     NanoAssertMsg(0, "asm_store32 should never receive this LIR opcode");
36372:                     break;
36372:             }
17275:         }
17275:     }
17275: 
20893:     void Assembler::asm_spill(Register rr, int d, bool pop, bool quad)
17275:     {
20893:         (void)quad;
17275:         if (d)
17275:         {
17275:             // save to spill location
17275:             if (rmask(rr) & FpRegs)
17275:             {
17275:                 if (rmask(rr) & XmmRegs) {
17916:                     SSE_STQ(d, FP, rr);
17275:                 } else {
17275:                     FSTQ((pop?1:0), d, FP);
17275:                 }
17275:             }
17275:             else
17275:             {
17275:                 ST(FP, d, rr);
17275:             }
17275:         }
17275:         else if (pop && (rmask(rr) & x87Regs))
17275:         {
17275:             // pop the fpu result since it isn't used
17275:             FSTP(FST0);
17275:         }
17275:     }
17275: 
17275:     void Assembler::asm_load64(LInsp ins)
17275:     {
37020:         NanoAssert(!ins->isop(LIR_ldq) && !ins->isop(LIR_ldqc));
37020: 
17275:         LIns* base = ins->oprnd1();
30238:         int db = ins->disp();
32727:         Register rr = ins->getReg();
17275: 
32727:         if (isKnownReg(rr) && rmask(rr) & XmmRegs)
17275:         {
17275:             freeRsrcOf(ins, false);
36552:             Register rb = getBaseReg(base, db, GpRegs);
36372:             switch (ins->opcode()) {
37020:                 case LIR_ldf:
37020:                 case LIR_ldfc:
17916:                     SSE_LDQ(rr, db, rb);
36372:                     break;
36372:                 case LIR_ld32f:
36372:                 case LIR_ldc32f:
36372:                     SSE_CVTSS2SD(rr, rr);
36372:                     SSE_LDSS(rr, db, rb);
36372:                     SSE_XORPDr(rr,rr);
36372:                     break;
36372:                 default:
36372:                     NanoAssertMsg(0, "asm_load64 should never receive this LIR opcode");
36372:                     break;
36372:             }
17916:         }
17916:         else
17916:         {
32727:             int dr = disp(ins);
20893:             Register rb;
32582:             if (base->isop(LIR_alloc)) {
20893:                 rb = FP;
20893:                 db += findMemFor(base);
20893:             } else {
20893:                 rb = findRegFor(base, GpRegs);
20893:             }
32727:             ins->setReg(UnknownReg);
17275: 
36372:             switch (ins->opcode()) {
37020:                 case LIR_ldf:
37020:                 case LIR_ldfc:
17275:                     // don't use an fpu reg to simply load & store the value.
17275:                     if (dr)
17275:                         asm_mmq(FP, dr, rb, db);
17275:                     freeRsrcOf(ins, false);
32727:                     if (isKnownReg(rr))
17275:                     {
36372:                         NanoAssert(rmask(rr)&x87Regs);
17275:                         _allocator.retire(rr);
17275:                         FLDQ(db, rb);
17275:                     }
36372:                     break;
36372:                 case LIR_ld32f:
36372:                 case LIR_ldc32f:
36372:                     freeRsrcOf(ins, false);
36372:                     if (isKnownReg(rr))
36372:                     {
36372:                         NanoAssert(rmask(rr)&x87Regs);
36372:                         _allocator.retire(rr);
36545:                         // Be sure to shadow the value onto our local area if there's space for it,
36545:                         // but don't pop the FP stack, we expect the register to stay valid.
36545:                         if (dr)
36545:                             FSTQ(0,dr, FP);
36372:                         FLD32(db, rb);
36372:                     }
36372:                     else
36372:                     {
36545:                         // We need to use fpu to expand 32->64, can't use asm_mmq...
36545:                         // just load-and-store-with-pop.
36372:                         NanoAssert(dr != 0);
36372:                         FSTPQ(dr, FP);
36372:                         FLD32(db, rb);
36372:                     }
36372:                     break;
36372:                 default:
36372:                     NanoAssertMsg(0, "asm_load64 should never receive this LIR opcode");
36372:                     break;
36372:             }
17275:         }
17275:     }
17275: 
36372:     void Assembler::asm_store64(LOpcode op, LInsp value, int dr, LInsp base)
17275:     {
37020:         NanoAssert(op != LIR_stqi);
37020: 
36553:         Register rb = getBaseReg(base, dr, GpRegs);
36372: 
36553:         if (op == LIR_st32f) {
32727:             bool pop = value->isUnusedOrHasUnknownReg();
32754:             Register rv = ( pop
32754:                           ? findRegFor(value, config.sse2 ? XmmRegs : FpRegs)
32754:                           : value->getReg() );
17275: 
36553:             if (rmask(rv) & XmmRegs) {
36553:                 // need a scratch reg
36553:                 Register rt = registerAllocTmp(XmmRegs);
36553: 
36553:                 // cvt to single-precision and store
36553:                 SSE_STSS(dr, rb, rt);
36553:                 SSE_CVTSD2SS(rt, rv);
36553:                 SSE_XORPDr(rt, rt);     // zero dest to ensure no dependency stalls
36553: 
36553:             } else {
36553:                 FST32(pop?1:0, dr, rb);
36553:             }
36553: 
36553:         } else if (value->isconstq()) {
36553:             STi(rb, dr+4, value->imm64_1());
36553:             STi(rb, dr,   value->imm64_0());
36553: 
37020:         } else if (value->isop(LIR_ldf) || value->isop(LIR_ldfc) || value->isop(LIR_qjoin)) {
36553:             // value is 64bit struct or int64_t, or maybe a double.
36553:             // It may be live in an FPU reg.  Either way, don't put it in an
36553:             // FPU reg just to load & store it.
36553: 
36553:             // a) If we know it's not a double, this is right.
36553:             // b) If we guarded that it's a double, this store could be on the
36553:             //    side exit, copying a non-double.
36553:             // c) Maybe it's a double just being stored.  Oh well.
36553: 
36553:             if (config.sse2) {
36553:                 Register rv = findRegFor(value, XmmRegs);
17916:                 SSE_STQ(dr, rb, rv);
36553:             } else {
36553:                 int da = findMemFor(value);
36553:                 asm_mmq(rb, dr, FP, da);
36372:             }
36553: 
36553:         } else {
37020:             NanoAssert(!value->isop(LIR_ldq) && !value->isop(LIR_ldqc));
36553:             bool pop = value->isUnusedOrHasUnknownReg();
36553:             Register rv = ( pop
36553:                           ? findRegFor(value, config.sse2 ? XmmRegs : FpRegs)
36553:                           : value->getReg() );
36553: 
36553:             if (rmask(rv) & XmmRegs) {
36553:                 SSE_STQ(dr, rb, rv);
36553:             } else {
33143:                 FSTQ(pop?1:0, dr, rb);
17275:             }
17275:         }
36553:     }
17275: 
36553:     // Copy 64 bits: (rd+dd) <- (rs+ds).
36553:     //
17275:     void Assembler::asm_mmq(Register rd, int dd, Register rs, int ds)
17275:     {
36553:         // Value is either a 64-bit struct or maybe a float that isn't live in
36553:         // an FPU reg.  Either way, avoid allocating an FPU reg just to load
36553:         // and store it.
36553:         if (config.sse2) {
35316:             Register t = registerAllocTmp(XmmRegs);
17916:             SSE_STQ(dd, rd, t);
17916:             SSE_LDQ(t, ds, rs);
36553:         } else {
36553:             // We avoid copying via the FP stack because it's slow and likely
36553:             // to cause spills.
35316:             Register t = registerAllocTmp(GpRegs & ~(rmask(rd)|rmask(rs)));
17275:             ST(rd, dd+4, t);
17275:             LD(t, ds+4, rs);
17275:             ST(rd, dd, t);
17275:             LD(t, ds, rs);
17275:         }
17275:     }
17275: 
30730:     NIns* Assembler::asm_branch(bool branchOnFalse, LInsp cond, NIns* targ)
20921:     {
20921:         LOpcode condop = cond->opcode();
20921:         NanoAssert(cond->isCond());
35314: 
35314:         // Handle float conditions separately.
35314:         if (condop >= LIR_feq && condop <= LIR_fge) {
35314:             return asm_fbranch(branchOnFalse, cond, targ);
20921:         }
35314: 
35314:         if (branchOnFalse) {
35314:             // op == LIR_xf
35314:             switch (condop) {
35314:             case LIR_ov:    JNO(targ);      break;
35314:             case LIR_eq:    JNE(targ);      break;
35314:             case LIR_lt:    JNL(targ);      break;
35314:             case LIR_le:    JNLE(targ);     break;
35314:             case LIR_gt:    JNG(targ);      break;
35314:             case LIR_ge:    JNGE(targ);     break;
35314:             case LIR_ult:   JNB(targ);      break;
35314:             case LIR_ule:   JNBE(targ);     break;
35314:             case LIR_ugt:   JNA(targ);      break;
35314:             case LIR_uge:   JNAE(targ);     break;
35314:             default:        NanoAssert(0);  break;
20921:             }
35314:         } else {
35314:             // op == LIR_xt
35314:             switch (condop) {
35314:             case LIR_ov:    JO(targ);       break;
35314:             case LIR_eq:    JE(targ);       break;
35314:             case LIR_lt:    JL(targ);       break;
35314:             case LIR_le:    JLE(targ);      break;
35314:             case LIR_gt:    JG(targ);       break;
35314:             case LIR_ge:    JGE(targ);      break;
35314:             case LIR_ult:   JB(targ);       break;
35314:             case LIR_ule:   JBE(targ);      break;
35314:             case LIR_ugt:   JA(targ);       break;
35314:             case LIR_uge:   JAE(targ);      break;
35314:             default:        NanoAssert(0);  break;
20921:             }
35314:         }
35314:         NIns* at = _nIns;
20921:         asm_cmp(cond);
20921:         return at;
20921:     }
20921: 
25099:     void Assembler::asm_switch(LIns* ins, NIns* exit)
25099:     {
25099:         LIns* diff = ins->oprnd1();
32600:         findSpecificRegFor(diff, EDX);
25099:         JMP(exit);
25099:     }
25099: 
35087:     void Assembler::asm_jtbl(LIns* ins, NIns** table)
35087:     {
35087:         Register indexreg = findRegFor(ins->oprnd1(), GpRegs);
35087:         JMP_indexed(indexreg, 2, table);
35087:     }
35087: 
33091:     // This generates a 'test' or 'cmp' instruction for a condition, which
33091:     // causes the condition codes to be set appropriately.  It's used with
33091:     // conditional branches, conditional moves, and when generating
33091:     // conditional values.  For example:
33091:     //
33091:     //   LIR:   eq1 = eq a, 0
33091:     //   LIR:   xf1: xf eq1 -> ...
33091:     //   asm:       test edx, edx       # generated by this function
33091:     //   asm:       je ...
33091:     //
33091:     // If this is the only use of eq1, then on entry 'cond' is *not* marked as
33091:     // used, and we do not allocate a register for it.  That's because its
33091:     // result ends up in the condition codes rather than a normal register.
33091:     // This doesn't get recorded in the regstate and so the asm code that
33091:     // consumes the result (eg. a conditional branch like 'je') must follow
33091:     // shortly after.
33091:     //
33091:     // If eq1 is instead used again later, we will also generate code
33091:     // (eg. in asm_cond()) to compute it into a normal register, something
33091:     // like this:
33091:     //
33091:     //   LIR:   eq1 = eq a, 0
33091:     //   LIR:       test edx, edx
33091:     //   asm:       sete ebx
33091:     //   asm:       movzx ebx, ebx
33091:     //
33091:     // In this case we end up computing the condition twice, but that's ok, as
33091:     // it's just as short as testing eq1's value in the code generated for the
33091:     // guard.
33091:     //
35314:     // WARNING: Because the condition code update is not recorded in the
35314:     // regstate, this function cannot generate any code that will affect the
35314:     // condition codes prior to the generation of the test/cmp, because any
35314:     // such code will be run after the test/cmp but before the instruction
35314:     // that consumes the condition code.  And because this function calls
35314:     // findRegFor() before the test/cmp is generated, and findRegFor() calls
35314:     // asm_restore(), that means that asm_restore() cannot generate code which
35314:     // affects the condition codes.
35314:     //
20921:     void Assembler::asm_cmp(LIns *cond)
20921:     {
20921:         LOpcode condop = cond->opcode();
20921: 
30029:         // LIR_ov recycles the flags set by arithmetic ops
30658:         if (condop == LIR_ov)
20921:             return;
20921: 
20921:         LInsp lhs = cond->oprnd1();
20921:         LInsp rhs = cond->oprnd2();
20921: 
20921:         NanoAssert((!lhs->isQuad() && !rhs->isQuad()) || (lhs->isQuad() && rhs->isQuad()));
20921: 
20921:         // Not supported yet.
20921:         NanoAssert(!lhs->isQuad() && !rhs->isQuad());
20921: 
36553:         // Ready to issue the compare.
36553:         if (rhs->isconst()) {
28182:             int c = rhs->imm32();
36553:             // findRegFor() can call asm_restore() -- asm_restore() better not
36553:             // disturb the CCs!
36549:             Register r = findRegFor(lhs, GpRegs);
20921:             if (c == 0 && cond->isop(LIR_eq)) {
20921:                 TEST(r, r);
36549:             } else {
20921:                 CMPi(r, c);
20921:             }
36553: 
36553:         } else {
32727:             Register ra, rb;
37705:             findRegFor2(GpRegs, lhs, ra, GpRegs, rhs, rb);
20921:             CMP(ra, rb);
20921:         }
20921:     }
20921: 
20921:     void Assembler::asm_fcond(LInsp ins)
20921:     {
35314:         LOpcode opcode = ins->opcode();
36553:         Register r = prepareResultReg(ins, AllowableFlagRegs);
35314: 
35314:         // SETcc only sets low 8 bits, so extend
35314:         MOVZX8(r,r);
35314: 
35314:         if (config.sse2) {
35314:             // LIR_flt and LIR_fgt are handled by the same case because
35314:             // asm_fcmp() converts LIR_flt(a,b) to LIR_fgt(b,a).  Likewise
35314:             // for LIR_fle/LIR_fge.
35314:             switch (opcode) {
35314:             case LIR_feq:   SETNP(r);       break;
35314:             case LIR_flt:
35314:             case LIR_fgt:   SETA(r);        break;
35314:             case LIR_fle:
35314:             case LIR_fge:   SETAE(r);       break;
35314:             default:        NanoAssert(0);  break;
35314:             }
35314:         } else {
35314:             SETNP(r);
35314:         }
36553: 
36553:         freeResourcesOf(ins);
36553: 
35314:         asm_fcmp(ins);
20921:     }
20921: 
20921:     void Assembler::asm_cond(LInsp ins)
20921:     {
20921:         LOpcode op = ins->opcode();
36553: 
36553:         Register r = prepareResultReg(ins, AllowableFlagRegs);
36553: 
20921:         // SETcc only sets low 8 bits, so extend
20921:         MOVZX8(r,r);
35314:         switch (op) {
35314:         case LIR_ov:    SETO(r);        break;
35314:         case LIR_eq:    SETE(r);        break;
35314:         case LIR_lt:    SETL(r);        break;
35314:         case LIR_le:    SETLE(r);       break;
35314:         case LIR_gt:    SETG(r);        break;
35314:         case LIR_ge:    SETGE(r);       break;
35314:         case LIR_ult:   SETB(r);        break;
35314:         case LIR_ule:   SETBE(r);       break;
35314:         case LIR_ugt:   SETA(r);        break;
35314:         case LIR_uge:   SETAE(r);       break;
35314:         default:        NanoAssert(0);  break;
35314:         }
36553: 
37020:         freeResourcesOf(ins);
36553: 
20921:         asm_cmp(ins);
20921:     }
20921: 
36553:     // Two example cases for "ins = add lhs, rhs".  '*' lines are those
36553:     // generated in this function.
36553:     //
36553:     //   asm:   define lhs into rr
36553:     //   asm:   define rhs into rb
36553:     //          ...
36553:     // * asm:   add rr, rb
36553:     // * asm:   spill rr if necessary
36553:     //          ... no more uses of lhs in rr...
36553:     //
36553:     //   asm:   define lhs into ra
36553:     //   asm:   define rhs into rb
36553:     //          ...
36553:     // * asm:   mov rr, ra
36553:     // * asm:   add rr, rb
36553:     // * asm:   spill rr if necessary
36553:     //          ... some uses of lhs in ra...
36553:     //
20921:     void Assembler::asm_arith(LInsp ins)
20921:     {
20921:         LOpcode op = ins->opcode();
29354: 
36553:         // First special case.
29354:         if (op == LIR_mod) {
33092:             asm_div_mod(ins);
29354:             return;
29354:         }
29354: 
36553:         LInsp lhs = ins->oprnd1();
20921:         LInsp rhs = ins->oprnd2();
20921: 
36553:         // Second special case.
36553:         if ((op == LIR_add || op == LIR_iaddp) && lhs->isop(LIR_alloc) && rhs->isconst()) {
36553:             // LIR_add(LIR_alloc, LIR_int) or LIR_addp(LIR_alloc, LIR_int) -- use lea.
36553:             Register rr = prepareResultReg(ins, GpRegs);
36553:             int d = findMemFor(lhs) + rhs->imm32();
36553: 
36553:             LEA(rr, d, FP);
36553: 
36553:             freeResourcesOf(ins);
36553: 
36553:             return;
36553:         }
36553: 
36553:         bool isConstRhs;
29354:         RegisterMask allow = GpRegs;
20921:         Register rb = UnknownReg;
20921: 
29354:         switch (op) {
29354:         case LIR_div:
33092:             // Nb: if the div feeds into a mod it will be handled by
33092:             // asm_div_mod() rather than here.
36553:             isConstRhs = false;
36553:             rb = findRegFor(rhs, (GpRegs & ~(rmask(EAX)|rmask(EDX))));
32627:             allow = rmask(EAX);
32621:             evictIfActive(EDX);
29354:             break;
29354:         case LIR_mul:
36553:             isConstRhs = false;
36553:             if (lhs != rhs) {
36553:                 rb = findRegFor(rhs, allow);
36553:                 allow &= ~rmask(rb);
36553:             }
29354:             break;
29354:         case LIR_lsh:
29354:         case LIR_rsh:
29354:         case LIR_ush:
36553:             isConstRhs = rhs->isconst();
36553:             if (!isConstRhs) {
29354:                 rb = findSpecificRegFor(rhs, ECX);
20921:                 allow &= ~rmask(rb);
20921:             }
29354:             break;
36553:         default:
36553:             isConstRhs = rhs->isconst();
36553:             if (!isConstRhs && lhs != rhs) {
36553:                 rb = findRegFor(rhs, allow);
36553:                 allow &= ~rmask(rb);
20921:             }
29354:             break;
29354:         }
29354: 
36553:         // Somewhere for the result of 'ins'.
36553:         Register rr = prepareResultReg(ins, allow);
20921: 
36553:         // If 'lhs' isn't in a register, it can be clobbered by 'ins'.
36553:         Register ra = lhs->isUnusedOrHasUnknownReg() ? rr : lhs->getReg();
20921: 
36553:         if (!isConstRhs) {
20921:             if (lhs == rhs)
20921:                 rb = ra;
20921: 
27294:             switch (op) {
27294:             case LIR_add:
36553:             case LIR_addp:  ADD(rr, rb); break;
36553:             case LIR_sub:   SUB(rr, rb); break;
36553:             case LIR_mul:   MUL(rr, rb); break;
36553:             case LIR_and:   AND(rr, rb); break;
36553:             case LIR_or:    OR( rr, rb); break;
36553:             case LIR_xor:   XOR(rr, rb); break;
36553:             case LIR_lsh:   SHL(rr, rb); break;
36553:             case LIR_rsh:   SAR(rr, rb); break;
36553:             case LIR_ush:   SHR(rr, rb); break;
29354:             case LIR_div:
29354:                 DIV(rb);
36553:                 CDQ(); // sign-extend EAX into EDX:EAX
29354:                 break;
36553:             default:        NanoAssert(0);  break;
20921:             }
36553: 
36553:         } else {
28182:             int c = rhs->imm32();
27294:             switch (op) {
32598:             case LIR_addp:
20921:                 // this doesn't set cc's, only use it when cc's not required.
20921:                 LEA(rr, c, ra);
20921:                 ra = rr; // suppress mov
27294:                 break;
36553:             case LIR_add:   ADDi(rr, c);    break;
36553:             case LIR_sub:   SUBi(rr, c);    break;
36553:             case LIR_and:   ANDi(rr, c);    break;
36553:             case LIR_or:    ORi( rr, c);    break;
36553:             case LIR_xor:   XORi(rr, c);    break;
36553:             case LIR_lsh:   SHLi(rr, c);    break;
36553:             case LIR_rsh:   SARi(rr, c);    break;
36553:             case LIR_ush:   SHRi(rr, c);    break;
36553:             default:        NanoAssert(0);  break;
20921:             }
20921:         }
20921: 
20921:         if (rr != ra)
20921:             MR(rr, ra);
36553: 
36553:         freeResourcesOf(ins);
36553:         if (lhs->isUnusedOrHasUnknownReg()) {
36553:             NanoAssert(ra == rr);
36553:             findSpecificRegForUnallocated(lhs, ra);
36553:         }
20921:     }
20921: 
36553:     // This is called when we have a mod(div(divL, divR)) sequence.
33092:     void Assembler::asm_div_mod(LInsp mod)
33092:     {
33092:         LInsp div = mod->oprnd1();
33092: 
36553:         // LIR_mod expects the LIR_div to be near (no interference from the register allocator).
33092:         NanoAssert(mod->isop(LIR_mod));
33092:         NanoAssert(div->isop(LIR_div));
33092: 
36553:         LInsp divL = div->oprnd1();
36553:         LInsp divR = div->oprnd2();
33092: 
36553:         prepareResultReg(mod, rmask(EDX));
36553:         prepareResultReg(div, rmask(EAX));
33092: 
36553:         Register rDivR = findRegFor(divR, (GpRegs & ~(rmask(EAX)|rmask(EDX))));
33092: 
36553:         Register rDivL = divL->isUnusedOrHasUnknownReg() ? EAX : divL->getReg();
33092: 
36553:         DIV(rDivR);
33092:         CDQ();     // sign-extend EAX into EDX:EAX
33092: 
36553:         if (EAX != rDivL)
36553:             MR(EAX, rDivL);
36553: 
36553:         freeResourcesOf(mod);
36553:         freeResourcesOf(div);
36553:         if (divL->isUnusedOrHasUnknownReg()) {
36553:             NanoAssert(rDivL == EAX);
36553:             findSpecificRegForUnallocated(divL, EAX);
36553:         }
33092:     }
33092: 
36553:     // Two example cases for "ins = neg lhs".  Lines marked with '*' are
36553:     // generated in this function.
36553:     //
36553:     //   asm:   define lhs into rr
36553:     //          ...
36553:     // * asm:   neg rr
36553:     // * asm:   spill rr if necessary
36553:     //          ... no more uses of lhs in rr...
36553:     //
36553:     //
36553:     //   asm:   define lhs into ra
36553:     //          ...
36553:     // * asm:   mov rr, ra
36553:     // * asm:   neg rr
36553:     // * asm:   spill rr if necessary
36553:     //          ... more uses of lhs in ra...
36553:     //
20921:     void Assembler::asm_neg_not(LInsp ins)
20921:     {
20921:         LOpcode op = ins->opcode();
36553:         LIns* lhs = ins->oprnd1();
20921: 
36553:         Register rr = prepareResultReg(ins, GpRegs);
36553: 
36553:         // If 'lhs' isn't in a register, it can be clobbered by 'ins'.
36553:         Register ra = lhs->isUnusedOrHasUnknownReg() ? rr : lhs->getReg();
20921: 
20921:         if (op == LIR_not)
20921:             NOT(rr);
20921:         else
20921:             NEG(rr);
20921: 
20921:         if (rr != ra)
20921:             MR(rr, ra);
36553: 
36553:         freeResourcesOf(ins);
36553:         if (lhs->isUnusedOrHasUnknownReg()) {
36553:             NanoAssert(ra == rr);
36553:             findSpecificRegForUnallocated(lhs, ra);
36553:         }
20921:     }
20921: 
36372:     void Assembler::asm_load32(LInsp ins)
20921:     {
20921:         LOpcode op = ins->opcode();
20921:         LIns* base = ins->oprnd1();
30238:         int32_t d = ins->disp();
20921:         Register rr = prepResultReg(ins, GpRegs);
21500: 
21500:         if (base->isconst()) {
28182:             intptr_t addr = base->imm32();
21500:             addr += d;
36372:             switch(op) {
36372:                 case LIR_ldzb:
36372:                 case LIR_ldcb:
21500:                     LD8Zdm(rr, addr);
36372:                     return;
36372:                 case LIR_ldsb:
36372:                 case LIR_ldcsb:
36372:                     LD8Sdm(rr, addr);
36372:                     return;
36372:                 case LIR_ldzs:
36372:                 case LIR_ldcs:
21500:                     LD16Zdm(rr, addr);
36372:                     return;
36372:                 case LIR_ldss:
36372:                 case LIR_ldcss:
36372:                     LD16Sdm(rr, addr);
36372:                     return;
36372:                 case LIR_ld:
36372:                 case LIR_ldc:
21500:                     LDdm(rr, addr);
21500:                     return;
36372:                 default:
36372:                     NanoAssertMsg(0, "asm_load32 should never receive this LIR opcode");
36372:                     return;
36372:             }
21500:         }
21500: 
21500:         /* Search for add(X,Y) */
21500:         if (base->opcode() == LIR_piadd) {
21500:             int scale = 0;
21500:             LIns *lhs = base->oprnd1();
21500:             LIns *rhs = base->oprnd2();
21500: 
21500:             /* See if we can bypass any SHLs, by searching for
21500:              * add(X, shl(Y,Z)) -> mov r, [X+Y*Z]
21500:              */
21500:             if (rhs->opcode() == LIR_pilsh && rhs->oprnd2()->isconst()) {
28182:                 scale = rhs->oprnd2()->imm32();
21500:                 if (scale >= 1 && scale <= 3)
21500:                     rhs = rhs->oprnd1();
21500:                 else
21500:                     scale = 0;
21500:             }
21500: 
21500:             /* Does LHS have a register yet? If not, re-use the result reg.
27294:              * @todo -- If LHS is const, we could eliminate a register use.
21500:              */
32727:             Register rleft = ( lhs->isUnusedOrHasUnknownReg()
34577:                              ? findSpecificRegForUnallocated(lhs, rr)
32727:                              : lhs->getReg() );
21500: 
21500:             /* Does RHS have a register yet? If not, try to re-use the result reg. */
32727:             Register rright = ( rr != rleft && rhs->isUnusedOrHasUnknownReg()
34577:                               ? findSpecificRegForUnallocated(rhs, rr)
32727:                               : findRegFor(rhs, GpRegs & ~(rmask(rleft))) );
21500: 
36372:             switch(op) {
36372:                 case LIR_ldzb:
36372:                 case LIR_ldcb:
21500:                     LD8Zsib(rr, d, rleft, rright, scale);
36372:                     return;
36372:                 case LIR_ldsb:
36372:                 case LIR_ldcsb:
36372:                     LD8Ssib(rr, d, rleft, rright, scale);
36372:                     return;
36372:                 case LIR_ldzs:
36372:                 case LIR_ldcs:
21500:                     LD16Zsib(rr, d, rleft, rright, scale);
36372:                     return;
36372:                 case LIR_ldss:
36372:                 case LIR_ldcss:
36372:                     LD16Ssib(rr, d, rleft, rright, scale);
36372:                     return;
36372:                 case LIR_ld:
36372:                 case LIR_ldc:
21500:                     LDsib(rr, d, rleft, rright, scale);
21500:                     return;
36372:                 default:
36372:                     NanoAssertMsg(0, "asm_load32 should never receive this LIR opcode");
36372:                     return;
36372:             }
21500:         }
21500: 
36552:         Register ra = getBaseReg(base, d, GpRegs);
36372:         switch(op) {
36372:             case LIR_ldzb:
36372:             case LIR_ldcb:
20921:                 LD8Z(rr, d, ra);
36372:                 return;
36372:             case LIR_ldsb:
36372:             case LIR_ldcsb:
36372:                 LD8S(rr, d, ra);
36372:                 return;
36372:             case LIR_ldzs:
36372:             case LIR_ldcs:
21469:                 LD16Z(rr, d, ra);
36372:                 return;
36372:             case LIR_ldss:
36372:             case LIR_ldcss:
36372:                 LD16S(rr, d, ra);
36372:                 return;
36372:             case LIR_ld:
36372:             case LIR_ldc:
20921:                 LD(rr, d, ra);
36372:                 return;
36372:             default:
36372:                 NanoAssertMsg(0, "asm_load32 should never receive this LIR opcode");
36372:                 return;
36372:         }
20921:     }
20921: 
20921:     void Assembler::asm_cmov(LInsp ins)
20921:     {
20921:         LOpcode op = ins->opcode();
20921:         LIns* condval = ins->oprnd1();
30648:         LIns* iftrue  = ins->oprnd2();
30648:         LIns* iffalse = ins->oprnd3();
30648: 
30485:         NanoAssert(condval->isCmp());
20921:         NanoAssert(op == LIR_qcmov || (!iftrue->isQuad() && !iffalse->isQuad()));
20921: 
20921:         const Register rr = prepResultReg(ins, GpRegs);
20921: 
20921:         // this code assumes that neither LD nor MR nor MRcc set any of the condition flags.
20921:         // (This is true on Intel, is it true on all architectures?)
20921:         const Register iffalsereg = findRegFor(iffalse, GpRegs & ~rmask(rr));
20921:         if (op == LIR_cmov) {
20921:             switch (condval->opcode())
20921:             {
20921:                 // note that these are all opposites...
20921:                 case LIR_eq:    MRNE(rr, iffalsereg);   break;
20921:                 case LIR_ov:    MRNO(rr, iffalsereg);   break;
20921:                 case LIR_lt:    MRGE(rr, iffalsereg);   break;
20921:                 case LIR_le:    MRG(rr, iffalsereg);    break;
20921:                 case LIR_gt:    MRLE(rr, iffalsereg);   break;
20921:                 case LIR_ge:    MRL(rr, iffalsereg);    break;
20921:                 case LIR_ult:   MRAE(rr, iffalsereg);   break;
20921:                 case LIR_ule:   MRA(rr, iffalsereg);    break;
20921:                 case LIR_ugt:   MRBE(rr, iffalsereg);   break;
20921:                 case LIR_uge:   MRB(rr, iffalsereg);    break;
30741:                 default: NanoAssert(0); break;
20921:             }
20921:         } else if (op == LIR_qcmov) {
20921:             NanoAssert(0);
20921:         }
20921:         /*const Register iftruereg =*/ findSpecificRegFor(iftrue, rr);
20921:         asm_cmp(condval);
20921:     }
20921: 
20921:     void Assembler::asm_qhi(LInsp ins)
20921:     {
20921:         Register rr = prepResultReg(ins, GpRegs);
20921:         LIns *q = ins->oprnd1();
37665:         if (q->isconstq())
37665:         {
37665:             // This should only be possible if ExprFilter isn't in use,
37665:             // as it will fold qhi(qconst()) properly... still, if it's
37665:             // disabled, we need this for proper behavior
37665:             LDi(rr, q->imm64_1());
37665:         }
37665:         else
37665:         {
20921:             int d = findMemFor(q);
20921:             LD(rr, d+4, FP);
20921:         }
37665:     }
20921: 
20921:     void Assembler::asm_param(LInsp ins)
20921:     {
30026:         uint32_t a = ins->paramArg();
30026:         uint32_t kind = ins->paramKind();
20921:         if (kind == 0) {
20921:             // ordinary param
20921:             AbiKind abi = _thisfrag->lirbuf->abi;
20944:             uint32_t abi_regcount = max_abi_regs[abi];
20921:             if (a < abi_regcount) {
36553:                 // Incoming arg in register.
20921:                 prepResultReg(ins, rmask(argRegs[a]));
20921:             } else {
36553:                 // Incoming arg is on stack, and EBP points nearby (see genPrologue()).
20921:                 Register r = prepResultReg(ins, GpRegs);
20921:                 int d = (a - abi_regcount) * sizeof(intptr_t) + 8;
20921:                 LD(r, d, FP);
20921:             }
20921:         }
20921:         else {
20921:             // saved param
20921:             prepResultReg(ins, rmask(savedRegs[a]));
20921:         }
20921:     }
20921: 
20921:     void Assembler::asm_int(LInsp ins)
20921:     {
36553:         Register rr = prepareResultReg(ins, GpRegs);
36553: 
36368:         asm_int(rr, ins->imm32(), /*canClobberCCs*/true);
36553: 
36553:         freeResourcesOf(ins);
36368:     }
36368: 
36368:     void Assembler::asm_int(Register r, int32_t val, bool canClobberCCs)
36368:     {
36368:         if (val == 0 && canClobberCCs)
36368:             XOR(r, r);
20921:         else
36368:             LDi(r, val);
20921:     }
20921: 
37665:     void Assembler::asm_quad(Register r, uint64_t q, double d, bool canClobberCCs)
17378:     {
37665:         // Quads require non-standard handling. There is no load-64-bit-immediate
37665:         // instruction on i386, so in the general case, we must load it from memory.
37665:         // This is unlike most other LIR operations which can be computed directly
37665:         // in a register. We can special-case 0.0 and various other small ints
37665:         // (1.0 on x87, any int32_t value on SSE2), but for all other values, we
37665:         // allocate an 8-byte chunk via dataAlloc and load from there. Note that
37665:         // this implies that quads never require spill area, since they will always
37665:         // be rematerialized from const data (or inline instructions in the special cases).
36553: 
37665:         if (rmask(r) & XmmRegs) {
37665:             if (q == 0) {
20893:                 // test (int64)0 since -0.0 == 0.0
37665:                 SSE_XORPDr(r, r);
37665:             } else if (d && d == (int)d && canClobberCCs) {
37665:                 // can fit in 32bits? then use cvt which is faster
37665:                 Register tr = registerAllocTmp(GpRegs);
37665:                 SSE_CVTSI2SD(r, tr);
37665:                 SSE_XORPDr(r, r);   // zero r to ensure no dependency stalls
37665:                 asm_int(tr, (int)d, canClobberCCs);
17378:             } else {
37665:                 const uint64_t* p = findQuadConstant(q);
37665:                 LDSDm(r, (const double*)p);
17378:             }
17378:         } else {
37665:             NanoAssert(r == FST0);
37665:             if (q == 0) {
20893:                 // test (int64)0 since -0.0 == 0.0
17378:                 FLDZ();
17378:             } else if (d == 1.0) {
17378:                 FLD1();
17378:             } else {
37665:                 const uint64_t* p = findQuadConstant(q);
37665:                 FLDQdm((const double*)p);
17378:             }
17378:         }
17378:     }
17378: 
37665:     void Assembler::asm_quad(LInsp ins)
17378:     {
37665:         Register rr = ins->getReg();
37665: 
37665:         if (isKnownReg(rr)) {
37665:             NanoAssert((rmask(rr) & FpRegs) != 0);
37665:             asm_quad(rr, ins->imm64(), ins->imm64f(), /*canClobberCCs*/true);
17378:         }
36553: 
36553:         freeResourcesOf(ins);
17378:     }
17378: 
20921:     void Assembler::asm_qlo(LInsp ins)
17378:     {
20921:         LIns *q = ins->oprnd1();
20921: 
22667:         if (!config.sse2)
17378:         {
20921:             Register rr = prepResultReg(ins, GpRegs);
37665:             if (q->isconstq())
37665:             {
37665:                 // This should only be possible if ExprFilter isn't in use,
37665:                 // as it will fold qlo(qconst()) properly... still, if it's
37665:                 // disabled, we need this for proper behavior
37665:                 LDi(rr, q->imm64_0());
37665:             }
37665:             else
37665:             {
20921:                 int d = findMemFor(q);
20921:                 LD(rr, d, FP);
17378:             }
37665:         }
20921:         else
20921:         {
32727:             Register rr = ins->getReg();
32727:             if (!isKnownReg(rr)) {
17378:                 // store quad in spill loc
32727:                 int d = disp(ins);
17378:                 freeRsrcOf(ins, false);
17378:                 Register qr = findRegFor(q, XmmRegs);
17916:                 SSE_MOVDm(d, FP, qr);
17378:             } else {
17378:                 freeRsrcOf(ins, false);
17378:                 Register qr = findRegFor(q, XmmRegs);
17916:                 SSE_MOVD(rr,qr);
17378:             }
20921:         }
17378:     }
17378: 
33558:     // negateMask is used by asm_fneg.
33558: #if defined __SUNPRO_CC
33558:     // From Sun Studio C++ Readme: #pragma align inside namespace requires mangled names.
33558:     // Initialize here to avoid multithreading contention issues during initialization.
33558:     static uint32_t negateMask_temp[] = {0, 0, 0, 0, 0, 0, 0};
33558: 
33558:     static uint32_t* negateMaskInit()
33558:     {
33558:         uint32_t* negateMask = (uint32_t*)alignUp(negateMask_temp, 16);
33558:         negateMask[1] = 0x80000000;
33558:         return negateMask;
33558:     }
33558: 
33558:     static uint32_t *negateMask = negateMaskInit();
33558: #else
33558:     static const AVMPLUS_ALIGN16(uint32_t) negateMask[] = {0,0x80000000,0,0};
33558: #endif
33558: 
17378:     void Assembler::asm_fneg(LInsp ins)
17378:     {
17378:         LIns *lhs = ins->oprnd1();
17378: 
36553:         if (config.sse2) {
36553:             Register rr = prepareResultReg(ins, XmmRegs);
36553: 
36553:             // If 'lhs' isn't in a register, it can be clobbered by 'ins'.
17378:             Register ra;
32727:             if (lhs->isUnusedOrHasUnknownReg()) {
36553:                 ra = rr;
36553:             } else if (!(rmask(lhs->getReg()) & XmmRegs)) {
36553:                 // We need to evict lhs from x87Regs, which then puts us in
36553:                 // the same situation as the isUnusedOrHasUnknownReg() case.
36553:                 evict(lhs);
36553:                 ra = rr;
32727:             } else {
32727:                 ra = lhs->getReg();
32727:             }
17378: 
17916:             SSE_XORPD(rr, negateMask);
17378: 
17378:             if (rr != ra)
17916:                 SSE_MOVSD(rr, ra);
36553: 
36553:             freeResourcesOf(ins);
36553:             if (lhs->isUnusedOrHasUnknownReg()) {
36553:                 NanoAssert(ra == rr);
36553:                 findSpecificRegForUnallocated(lhs, ra);
17378:             }
17378: 
36553:         } else {
36553:             verbose_only( Register rr = ) prepareResultReg(ins, x87Regs);
36553:             NanoAssert(FST0 == rr);
17378: 
36553:             NanoAssert(lhs->isUnusedOrHasUnknownReg() || FST0 == lhs->getReg());
32727: 
17378:             FCHS();
17378: 
36553:             freeResourcesOf(ins);
36553:             if (lhs->isUnusedOrHasUnknownReg())
36553:                 findSpecificRegForUnallocated(lhs, FST0);
17378:         }
17378:     }
17378: 
36553:     void Assembler::asm_arg(ArgSize sz, LInsp ins, Register r, int32_t& stkd)
20893:     {
36553:         // If 'r' is known, then that's the register we have to put 'ins'
36553:         // into.
36553: 
20893:         if (sz == ARGSIZE_Q)
20893:         {
20893:             // ref arg - use lea
32727:             if (isKnownReg(r))
20893:             {
36553:                 NanoAssert(rmask(r) & FpRegs);
36553: 
20893:                 // arg in specific reg
37665:                 if (ins->isconstq())
37665:                 {
37665:                     const uint64_t* p = findQuadConstant(ins->imm64());
37665:                     LDi(r, uint32_t(p));
37665:                 }
37665:                 else
37665:                 {
36553:                     int da = findMemFor(ins);
36553: 
20893:                     LEA(r, da, FP);
20893:                 }
37665:             }
20893:             else
20893:             {
20893:                 NanoAssert(0); // not supported
20893:             }
20893:         }
31051:         else if (sz == ARGSIZE_I || sz == ARGSIZE_U)
20893:         {
32727:             if (isKnownReg(r)) {
36553:                 if (ins->isconst()) {
36553:                     // Rematerialize the constant.
36553:                     asm_int(r, ins->imm32(), /*canClobberCCs*/true);
36553:                 } else if (ins->isUsed()) {
36553:                     if (!ins->hasKnownReg()) {
36553:                         int d = disp(ins);
36553:                         NanoAssert(d != 0);
36553:                         if (ins->isop(LIR_alloc)) {
20893:                             LEA(r, d, FP);
20893:                         } else {
20893:                             LD(r, d, FP);
20893:                         }
20893:                     } else {
36553:                         if (r != ins->getReg())
36553:                             MR(r, ins->getReg());
20893:                     }
36553: 
36553:                 } else {
36553:                     // This is the last use, so fine to assign it
20893:                     // to the scratch reg, it's dead after this point.
36553:                     findSpecificRegForUnallocated(ins, r);
20893:                 }
20893:             }
20893:             else {
34340:                 if (config.fixed_esp)
36553:                     asm_stkarg(ins, stkd);
34340:                 else
36553:                     asm_pusharg(ins);
20893:             }
20893:         }
20893:         else
20893:         {
20893:             NanoAssert(sz == ARGSIZE_F);
36553:             asm_farg(ins, stkd);
20893:         }
20893:     }
20893: 
36553:     void Assembler::asm_pusharg(LInsp ins)
34340:     {
34340:         // arg goes on stack
36553:         if (!ins->isUsed() && ins->isconst())
34340:         {
36553:             PUSHi(ins->imm32());    // small const we push directly
34340:         }
36553:         else if (!ins->isUsed() || ins->isop(LIR_alloc))
34340:         {
36553:             Register ra = findRegFor(ins, GpRegs);
34340:             PUSHr(ra);
34340:         }
36553:         else if (!ins->hasKnownReg())
34340:         {
36553:             PUSHm(disp(ins), FP);
34340:         }
34340:         else
34340:         {
36553:             PUSHr(ins->getReg());
34340:         }
34340:     }
34340: 
36553:     void Assembler::asm_stkarg(LInsp ins, int32_t& stkd)
17275:     {
17275:         // arg goes on stack
36553:         if (!ins->isUsed() && ins->isconst())
17275:         {
17275:             // small const we push directly
36553:             STi(SP, stkd, ins->imm32());
17275:         }
33740:         else {
33740:             Register ra;
36553:             if (ins->isUnusedOrHasUnknownReg() || ins->isop(LIR_alloc))
36553:                 ra = findRegFor(ins, GpRegs & (~SavedRegs));
17275:             else
36553:                 ra = ins->getReg();
33740:             ST(SP, stkd, ra);
17275:         }
17275: 
33740:         stkd += sizeof(int32_t);
33740:     }
33740: 
36553:     void Assembler::asm_farg(LInsp ins, int32_t& stkd)
17378:     {
36553:         NanoAssert(ins->isQuad());
36553:         Register r = findRegFor(ins, FpRegs);
17378:         if (rmask(r) & XmmRegs) {
33740:             SSE_STQ(stkd, SP, r);
17378:         } else {
33740:             FSTPQ(stkd, SP);
34384: 
34384:             //
34384:             // 22Jul09 rickr - Enabling the evict causes a 10% slowdown on primes
34384:             //
34384:             // evict() triggers a very expensive fstpq/fldq pair around the store.
34384:             // We need to resolve the bug some other way.
34384:             //
34384:             // see https://bugzilla.mozilla.org/show_bug.cgi?id=491084
34384: 
22630:             /* It's possible that the same LIns* with r=FST0 will appear in the argument list more
22630:              * than once.  In this case FST0 will not have been evicted and the multiple pop
22630:              * actions will unbalance the FPU stack.  A quick fix is to always evict FST0 manually.
22630:              */
32621:             evictIfActive(FST0);
17378:         }
34340:         if (!config.fixed_esp)
34340:             SUBi(ESP, 8);
33740: 
33740:         stkd += sizeof(double);
17378:     }
17378: 
17378:     void Assembler::asm_fop(LInsp ins)
17378:     {
17378:         LOpcode op = ins->opcode();
22667:         if (config.sse2)
17378:         {
17378:             LIns *lhs = ins->oprnd1();
17378:             LIns *rhs = ins->oprnd2();
17378: 
17378:             RegisterMask allow = XmmRegs;
17378:             Register rb = UnknownReg;
17378:             if (lhs != rhs) {
17378:                 rb = findRegFor(rhs,allow);
17378:                 allow &= ~rmask(rb);
17378:             }
17378: 
17378:             Register rr = prepResultReg(ins, allow);
17378:             Register ra;
17378: 
17378:             // if this is last use of lhs in reg, we can re-use result reg
32727:             if (lhs->isUnusedOrHasUnknownReg()) {
34577:                 ra = findSpecificRegForUnallocated(lhs, rr);
32727:             } else if ((rmask(lhs->getReg()) & XmmRegs) == 0) {
33125:                 // We need this case on AMD64, because it's possible that
33125:                 // an earlier instruction has done a quadword load and reserved a
33125:                 // GPR.  If so, ask for a new register.
19041:                 ra = findRegFor(lhs, XmmRegs);
32727:             } else {
32727:                 // lhs already has a register assigned but maybe not from the allow set
20893:                 ra = findRegFor(lhs, allow);
20893:             }
17378: 
17378:             if (lhs == rhs)
17378:                 rb = ra;
17378: 
17378:             if (op == LIR_fadd)
17916:                 SSE_ADDSD(rr, rb);
17378:             else if (op == LIR_fsub)
17916:                 SSE_SUBSD(rr, rb);
17378:             else if (op == LIR_fmul)
17916:                 SSE_MULSD(rr, rb);
17378:             else //if (op == LIR_fdiv)
17916:                 SSE_DIVSD(rr, rb);
17378: 
17378:             if (rr != ra)
17916:                 SSE_MOVSD(rr, ra);
17378:         }
17378:         else
17378:         {
17378:             // we swap lhs/rhs on purpose here, works out better
17378:             // if you only have one fpu reg.  use divr/subr.
17378:             LIns* rhs = ins->oprnd1();
17378:             LIns* lhs = ins->oprnd2();
17378:             Register rr = prepResultReg(ins, rmask(FST0));
17378: 
37665:             if (rhs->isconstq())
37665:             {
37665:                 const uint64_t* p = findQuadConstant(rhs->imm64());
37665: 
37665:                 // lhs into reg, prefer same reg as result
37665: 
37665:                 // last use of lhs in reg, can reuse rr
37665:                 // else, lhs already has a different reg assigned
37665:                 if (lhs->isUnusedOrHasUnknownReg())
37665:                     findSpecificRegForUnallocated(lhs, rr);
37665: 
37665:                 NanoAssert(lhs->getReg()==FST0);
37665:                 // assume that the lhs is in ST(0) and rhs is on stack
37665:                 if (op == LIR_fadd)
37665:                     { FADDdm((const double*)p); }
37665:                 else if (op == LIR_fsub)
37665:                     { FSUBRdm((const double*)p); }
37665:                 else if (op == LIR_fmul)
37665:                     { FMULdm((const double*)p); }
37665:                 else if (op == LIR_fdiv)
37665:                     { FDIVRdm((const double*)p); }
37665:             }
37665:             else
37665:             {
17378:                 // make sure rhs is in memory
17378:                 int db = findMemFor(rhs);
17378: 
17378:                 // lhs into reg, prefer same reg as result
32727: 
17378:                 // last use of lhs in reg, can reuse rr
32727:                 // else, lhs already has a different reg assigned
32727:                 if (lhs->isUnusedOrHasUnknownReg())
34577:                     findSpecificRegForUnallocated(lhs, rr);
17378: 
32727:                 NanoAssert(lhs->getReg()==FST0);
17378:                 // assume that the lhs is in ST(0) and rhs is on stack
17378:                 if (op == LIR_fadd)
17378:                     { FADD(db, FP); }
17378:                 else if (op == LIR_fsub)
17378:                     { FSUBR(db, FP); }
17378:                 else if (op == LIR_fmul)
17378:                     { FMUL(db, FP); }
17378:                 else if (op == LIR_fdiv)
17378:                     { FDIVR(db, FP); }
17378:             }
17378:         }
37665:     }
17378: 
17378:     void Assembler::asm_i2f(LInsp ins)
17378:     {
36553:         LIns* lhs = ins->oprnd1();
36553: 
36553:         Register rr = prepareResultReg(ins, FpRegs);
36553:         if (rmask(rr) & XmmRegs) {
17378:             // todo support int value in memory
36553:             Register ra = findRegFor(lhs, GpRegs);
36553:             SSE_CVTSI2SD(rr, ra);
33143:             SSE_XORPDr(rr, rr);     // zero rr to ensure no dependency stalls
36553:         } else {
36553:             int d = findMemFor(lhs);
17378:             FILD(d, FP);
17378:         }
36553: 
36553:         freeResourcesOf(ins);
17378:     }
17378: 
17378:     void Assembler::asm_u2f(LInsp ins)
17378:     {
36553:         LIns* lhs = ins->oprnd1();
17378: 
36553:         Register rr = prepareResultReg(ins, FpRegs);
36553:         if (rmask(rr) & XmmRegs) {
36553:             Register rt = registerAllocTmp(GpRegs);
36553: 
36553:             // Technique inspired by gcc disassembly.  Edwin explains it:
17378:             //
36553:             // rt is 0..2^32-1
17378:             //
36553:             //     sub rt,0x80000000
17378:             //
36553:             // Now rt is -2^31..2^31-1, i.e. the range of int, but not the same value
36553:             // as before.
17378:             //
36553:             //     cvtsi2sd rr,rt
17378:             //
36553:             // rr is now a double with the int value range.
17378:             //
17378:             //     addsd rr, 2147483648.0
17378:             //
36553:             // Adding back double(0x80000000) makes the range 0..2^32-1.
17378: 
17378:             static const double k_NEGONE = 2147483648.0;
17916:             SSE_ADDSDm(rr, &k_NEGONE);
17916: 
36553:             SSE_CVTSI2SD(rr, rt);
33143:             SSE_XORPDr(rr,rr);  // zero rr to ensure no dependency stalls
17378: 
36553:             Register ra;
36553:             if (lhs->isUsed() && (ra = lhs->getReg(), isKnownReg(ra)) && (rmask(ra) & GpRegs)) {
36553:                 LEA(rt, 0x80000000, ra);
36553: 
36553:             } else {
36553:                 const int d = findMemFor(lhs);
36553:                 SUBi(rt, 0x80000000);
36553:                 LD(rt, d, FP);
17378:             }
36553: 
36553:         } else {
17916:             const int disp = -8;
17916:             const Register base = SP;
36553:             Register ra = findRegFor(lhs, GpRegs);
17378:             NanoAssert(rr == FST0);
17378:             FILDQ(disp, base);
17378:             STi(base, disp+4, 0);   // high 32 bits = 0
36553:             ST(base, disp, ra);     // low 32 bits = unsigned value
17378:         }
17378: 
36553:         freeResourcesOf(ins);
36553:     }
36553: 
37700:     void Assembler::asm_f2i(LInsp ins)
37700:     {
37700:         LIns *lhs = ins->oprnd1();
37700: 
37700:         if (config.sse2) {
37700:             Register rr = prepareResultReg(ins, GpRegs);
37700:             Register ra = findRegFor(lhs, XmmRegs);
37700:             SSE_CVTSD2SI(rr, ra);
37700:         } else {
37700:             int pop = lhs->isUnusedOrHasUnknownReg();
37700:             findSpecificRegFor(lhs, FST0);
37700:             if (ins->hasKnownReg())
37700:                 evict(ins);
37700:             int d = findMemFor(ins);
37700:             FIST((pop?1:0), d, FP);
37700:         }
37700: 
37700:         freeResourcesOf(ins);
37700:     }
37700: 
36553:     void Assembler::asm_nongp_copy(Register rd, Register rs)
17378:     {
36553:         if ((rmask(rd) & XmmRegs) && (rmask(rs) & XmmRegs)) {
36553:             // xmm -> xmm
36553:             SSE_MOVSD(rd, rs);
36553:         } else if ((rmask(rd) & GpRegs) && (rmask(rs) & XmmRegs)) {
36553:             // xmm -> gp
36553:             SSE_MOVD(rd, rs);
17378:         } else {
36553:             NanoAssertMsgf(false, "bad asm_nongp_copy(%s, %s)", gpn(rd), gpn(rs));
17378:         }
17378:     }
17378: 
35314:     NIns* Assembler::asm_fbranch(bool branchOnFalse, LIns *cond, NIns *targ)
20893:     {
35314:         NIns* at;
35314:         LOpcode opcode = cond->opcode();
35314: 
35314:         if (config.sse2) {
35314:             // LIR_flt and LIR_fgt are handled by the same case because
35314:             // asm_fcmp() converts LIR_flt(a,b) to LIR_fgt(b,a).  Likewise
35314:             // for LIR_fle/LIR_fge.
35314:             if (branchOnFalse) {
35314:                 // op == LIR_xf
35314:                 switch (opcode) {
35314:                 case LIR_feq:   JP(targ);       break;
35314:                 case LIR_flt:
35314:                 case LIR_fgt:   JNA(targ);      break;
35314:                 case LIR_fle:
35314:                 case LIR_fge:   JNAE(targ);     break;
35314:                 default:        NanoAssert(0);  break;
20893:                 }
35314:             } else {
35314:                 // op == LIR_xt
35314:                 switch (opcode) {
35314:                 case LIR_feq:   JNP(targ);      break;
35314:                 case LIR_flt:
35314:                 case LIR_fgt:   JA(targ);       break;
35314:                 case LIR_fle:
35314:                 case LIR_fge:   JAE(targ);      break;
35314:                 default:        NanoAssert(0);  break;
20893:                 }
20893:             }
35314:         } else {
20893:             if (branchOnFalse)
30730:                 JP(targ);
20893:             else
30730:                 JNP(targ);
35314:         }
35314: 
35314:         at = _nIns;
20893:         asm_fcmp(cond);
35314: 
20893:         return at;
20893:     }
20893: 
35314:     // WARNING: This function cannot generate any code that will affect the
35314:     // condition codes prior to the generation of the
35314:     // ucomisd/fcompp/fcmop/fcom.  See asm_cmp() for more details.
17378:     void Assembler::asm_fcmp(LIns *cond)
17378:     {
17378:         LOpcode condop = cond->opcode();
17378:         NanoAssert(condop >= LIR_feq && condop <= LIR_fge);
17378:         LIns* lhs = cond->oprnd1();
17378:         LIns* rhs = cond->oprnd2();
35314:         NanoAssert(lhs->isQuad() && rhs->isQuad());
17378: 
35314:         if (config.sse2) {
35314:             // First, we convert (a < b) into (b > a), and (a <= b) into (b >= a).
35314:             if (condop == LIR_flt) {
35314:                 condop = LIR_fgt;
17378:                 LIns* t = lhs; lhs = rhs; rhs = t;
35314:             } else if (condop == LIR_fle) {
35314:                 condop = LIR_fge;
17378:                 LIns* t = lhs; lhs = rhs; rhs = t;
17378:             }
17378: 
35314:             if (condop == LIR_feq) {
35314:                 if (lhs == rhs) {
35314:                     // We can generate better code for LIR_feq when lhs==rhs (NaN test).
17378: 
35314:                     // ucomisd    ZPC  outcome (SETNP/JNP succeeds if P==0)
35314:                     // -------    ---  -------
35314:                     // UNORDERED  111  SETNP/JNP fails
35314:                     // EQUAL      100  SETNP/JNP succeeds
35314: 
17378:                     Register r = findRegFor(lhs, XmmRegs);
17916:                     SSE_UCOMISD(r, r);
35314:                 } else {
35314:                     // LAHF puts the flags into AH like so:  SF:ZF:0:AF:0:PF:1:CF (aka. SZ0A_0P1C).
35314:                     // We then mask out the bits as follows.
35314:                     // - LIR_feq: mask == 0x44 == 0100_0100b, which extracts 0Z00_0P00 from AH.
35314:                     int mask = 0x44;
35314: 
35314:                     // ucomisd       ZPC   lahf/test(0x44) SZP   outcome
35314:                     // -------       ---   ---------       ---   -------
35314:                     // UNORDERED     111   0100_0100       001   SETNP/JNP fails
35314:                     // EQUAL         100   0100_0000       000   SETNP/JNP succeeds
35314:                     // GREATER_THAN  000   0000_0000       011   SETNP/JNP fails
35314:                     // LESS_THAN     001   0000_0000       011   SETNP/JNP fails
35314: 
32621:                     evictIfActive(EAX);
35314:                     Register ra, rb;
37705:                     findRegFor2(XmmRegs, lhs, ra, XmmRegs, rhs, rb);
35314: 
17378:                     TEST_AH(mask);
17378:                     LAHF();
35314:                     SSE_UCOMISD(ra, rb);
35314:                 }
35314:             } else {
35314:                 // LIR_fgt:
35314:                 //   ucomisd       ZPC   outcome (SETA/JA succeeds if CZ==00)
35314:                 //   -------       ---   -------
35314:                 //   UNORDERED     111   SETA/JA fails
35314:                 //   EQUAL         100   SETA/JA fails
35314:                 //   GREATER_THAN  000   SETA/JA succeeds
35314:                 //   LESS_THAN     001   SETA/JA fails
35314:                 //
35314:                 // LIR_fge:
35314:                 //   ucomisd       ZPC   outcome (SETAE/JAE succeeds if C==0)
35314:                 //   -------       ---   -------
35314:                 //   UNORDERED     111   SETAE/JAE fails
35314:                 //   EQUAL         100   SETAE/JAE succeeds
35314:                 //   GREATER_THAN  000   SETAE/JAE succeeds
35314:                 //   LESS_THAN     001   SETAE/JAE fails
35314: 
32727:                 Register ra, rb;
37705:                 findRegFor2(XmmRegs, lhs, ra, XmmRegs, rhs, rb);
32727:                 SSE_UCOMISD(ra, rb);
17378:             }
35314: 
35314:         } else {
35314:             // First, we convert (a > b) into (b < a), and (a >= b) into (b <= a).
35314:             // Note that this is the opposite of the sse2 conversion above.
35314:             if (condop == LIR_fgt) {
35314:                 condop = LIR_flt;
35314:                 LIns* t = lhs; lhs = rhs; rhs = t;
35314:             } else if (condop == LIR_fge) {
35314:                 condop = LIR_fle;
35314:                 LIns* t = lhs; lhs = rhs; rhs = t;
17378:             }
35314: 
35314:             // FNSTSW_AX puts the flags into AH like so:  B:C3:TOP3:TOP2:TOP1:C2:C1:C0.
35314:             // Furthermore, fcom/fcomp/fcompp sets C3:C2:C0 the same values
35314:             // that Z:P:C are set by ucomisd, and the relative positions in AH
35314:             // line up.  (Someone at Intel has a sense of humour.)  Therefore
35314:             // we can use the same lahf/test(mask) technique as used in the
35314:             // sse2 case above.  We could use fcomi/fcomip/fcomipp which set
35314:             // ZPC directly and then use LAHF instead of FNSTSW_AX and make
35314:             // this code generally more like the sse2 code, but we don't
35314:             // because fcomi/fcomip/fcomipp/lahf aren't available on earlier
35314:             // x86 machines.
35314:             //
35314:             // The masks are as follows:
35314:             // - LIR_feq: mask == 0x44 == 0100_0100b, which extracts 0Z00_0P00 from AH.
35314:             // - LIR_flt: mask == 0x05 == 0000_0101b, which extracts 0000_0P0C from AH.
35314:             // - LIR_fle: mask == 0x41 == 0100_0001b, which extracts 0Z00_000C from AH.
35314:             //
35314:             // LIR_feq (very similar to the sse2 case above):
35314:             //   ucomisd  C3:C2:C0   lahf/test(0x44) SZP   outcome
35314:             //   -------  --------   ---------       ---   -------
35314:             //   UNORDERED     111   0100_0100       001   SETNP fails
35314:             //   EQUAL         100   0100_0000       000   SETNP succeeds
35314:             //   GREATER_THAN  000   0000_0000       011   SETNP fails
35314:             //   LESS_THAN     001   0000_0000       011   SETNP fails
35314:             //
35314:             // LIR_flt:
35314:             //   fcom     C3:C2:C0   lahf/test(0x05) SZP   outcome
35314:             //   -------  --------   ---------       ---   -------
35314:             //   UNORDERED     111   0000_0101       001   SETNP fails
35314:             //   EQUAL         100   0000_0000       011   SETNP fails
35314:             //   GREATER_THAN  000   0000_0000       011   SETNP fails
35314:             //   LESS_THAN     001   0000_0001       000   SETNP succeeds
35314:             //
35314:             // LIR_fle:
35314:             //   fcom     C3:C2:C0   lahf/test(0x41) SZP   outcome
35314:             //   -------       ---   ---------       ---   -------
35314:             //   UNORDERED     111   0100_0001       001   SETNP fails
35314:             //   EQUAL         100   0100_0000       000   SETNP succeeds
35314:             //   GREATER_THAN  000   0000_0000       011   SETNP fails
35314:             //   LESS_THAN     001   0000_0001       010   SETNP succeeds
35314: 
35315:             int mask = 0;   // init to avoid MSVC compile warnings
35314:             switch (condop) {
35314:             case LIR_feq:   mask = 0x44;    break;
35314:             case LIR_flt:   mask = 0x05;    break;
35314:             case LIR_fle:   mask = 0x41;    break;
35314:             default:        NanoAssert(0);  break;
35314:             }
35314: 
32621:             evictIfActive(EAX);
35314:             int pop = lhs->isUnusedOrHasUnknownReg();
35314:             findSpecificRegFor(lhs, FST0);
35314: 
35314:             if (lhs == rhs) {
35314:                 // NaN test.
17378:                 TEST_AH(mask);
36553:                 FNSTSW_AX();        // requires EAX to be free
17378:                 if (pop)
17378:                     FCOMPP();
17378:                 else
17378:                     FCOMP();
17378:                 FLDr(FST0); // DUP
35314:             } else {
35314:                 TEST_AH(mask);
36553:                 FNSTSW_AX();        // requires EAX to be free
37665:                 if (rhs->isconstq())
37665:                 {
37665:                     const uint64_t* p = findQuadConstant(rhs->imm64());
37665:                     FCOMdm((pop?1:0), (const double*)p);
37665:                 }
37665:                 else
37665:                 {
37665:                     int d = findMemFor(rhs);
35314:                     FCOM((pop?1:0), d, FP);
17378:                 }
17378:             }
17378:         }
37665:     }
17378: 
32784:     // Increment the 32-bit profiling counter at pCtr, without
32784:     // changing any registers.
32784:     verbose_only(
32784:     void Assembler::asm_inc_m32(uint32_t* pCtr)
32784:     {
32784:        INCLi(pCtr);
32784:     }
32784:     )
32784: 
17916:     void Assembler::nativePageReset()
33125:     {}
17275: 
17275:     void Assembler::nativePageSetup()
17275:     {
35356:         NanoAssert(!_inExit);
32784:         if (!_nIns)
32784:             codeAlloc(codeStart, codeEnd, _nIns verbose_only(, codeBytes));
17275:     }
20893: 
20893:     // enough room for n bytes
20893:     void Assembler::underrunProtect(int n)
20893:     {
31475:         NIns *eip = _nIns;
22662:         NanoAssertMsg(n<=LARGEST_UNDERRUN_PROT, "constant LARGEST_UNDERRUN_PROT is too small");
35356:         // This may be in a normal code chunk or an exit code chunk.
35356:         if (eip - n < codeStart) {
32784:             codeAlloc(codeStart, codeEnd, _nIns verbose_only(, codeBytes));
20893:             JMP(eip);
20893:         }
20893:     }
20893: 
32556:     void Assembler::asm_ret(LInsp ins)
32556:     {
32634:         genEpilogue();
32634: 
32634:         // Restore ESP from EBP, undoing SUBi(SP,amt) in the prologue
32634:         MR(SP,FP);
32634: 
36672:         releaseRegisters();
32556:         assignSavedRegs();
36553: 
32556:         LIns *val = ins->oprnd1();
32556:         if (ins->isop(LIR_ret)) {
32556:             findSpecificRegFor(val, retRegs[0]);
32556:         } else {
32556:             findSpecificRegFor(val, FST0);
32556:             fpu_pop();
32556:         }
32556:     }
32556: 
32556:     void Assembler::asm_promote(LIns *) {
32556:         // i2q or u2q
32556:         TODO(asm_promote);
32556:     }
32556: 
35356:     void Assembler::swapCodeChunks() {
37698:         if (!_nExitIns)
37698:             codeAlloc(exitStart, exitEnd, _nExitIns verbose_only(, exitBytes));
35356:         SWAP(NIns*, _nIns, _nExitIns);
35356:         SWAP(NIns*, codeStart, exitStart);
35356:         SWAP(NIns*, codeEnd, exitEnd);
35356:         verbose_only( SWAP(size_t, codeBytes, exitBytes); )
35356:     }
35356: 
17275:     #endif /* FEATURE_NANOJIT */
17275: }
