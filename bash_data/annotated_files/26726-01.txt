22705: /* -*- Mode: C++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*-
18056:  * vim: set ts=4 sw=4 et tw=99:
17181:  *
17181:  * ***** BEGIN LICENSE BLOCK *****
17181:  * Version: MPL 1.1/GPL 2.0/LGPL 2.1
17181:  *
17181:  * The contents of this file are subject to the Mozilla Public License Version
17181:  * 1.1 (the "License"); you may not use this file except in compliance with
17181:  * the License. You may obtain a copy of the License at
17181:  * http://www.mozilla.org/MPL/
17181:  *
17181:  * Software distributed under the License is distributed on an "AS IS" basis,
17181:  * WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
17181:  * for the specific language governing rights and limitations under the
17181:  * License.
17181:  *
17181:  * The Original Code is Mozilla SpiderMonkey JavaScript 1.9 code, released
17181:  * May 28, 2008.
17181:  *
17181:  * The Initial Developer of the Original Code is
17339:  *   Brendan Eich <brendan@mozilla.org>
17181:  *
17181:  * Contributor(s):
17339:  *   Andreas Gal <gal@mozilla.com>
17671:  *   Mike Shaver <shaver@mozilla.org>
17671:  *   David Anderson <danderson@mozilla.com>
17181:  *
17181:  * Alternatively, the contents of this file may be used under the terms of
17181:  * either of the GNU General Public License Version 2 or later (the "GPL"),
17181:  * or the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
17181:  * in which case the provisions of the GPL or the LGPL are applicable instead
17181:  * of those above. If you wish to allow use of your version of this file only
17181:  * under the terms of either the GPL or the LGPL, and not to allow others to
17181:  * use your version of this file under the terms of the MPL, indicate your
17181:  * decision by deleting the provisions above and replace them with the notice
17181:  * and other provisions required by the GPL or the LGPL. If you do not delete
17181:  * the provisions above, a recipient may use your version of this file under
17181:  * the terms of any one of the MPL, the GPL or the LGPL.
17181:  *
17181:  * ***** END LICENSE BLOCK ***** */
17181: 
26316: #include "jsstdint.h"
17923: #include "jsbit.h"              // low-level (NSPR-based) headers next
17923: #include "jsprf.h"
17666: #include <math.h>               // standard headers next
26053: 
26053: #if defined(_MSC_VER) || defined(__MINGW32__)
26053: #include <malloc.h>
17666: #ifdef _MSC_VER
17666: #define alloca _alloca
17666: #endif
26053: #endif
19058: #ifdef SOLARIS
19058: #include <alloca.h>
19058: #endif
25627: #include <limits.h>
17181: 
21062: #include "nanojit/nanojit.h"
25718: #include "jsapi.h"              // higher-level library and API headers
25718: #include "jsarray.h"
17421: #include "jsbool.h"
17630: #include "jscntxt.h"
17949: #include "jsdbgapi.h"
17949: #include "jsemit.h"
17630: #include "jsfun.h"
17630: #include "jsinterp.h"
17899: #include "jsiter.h"
17666: #include "jsobj.h"
17863: #include "jsopcode.h"
18115: #include "jsregexp.h"
17949: #include "jsscope.h"
17630: #include "jsscript.h"
19990: #include "jsdate.h"
20969: #include "jsstaticcheck.h"
17407: #include "jstracer.h"
17293: 
17666: #include "jsautooplen.h"        // generated headers last
23075: #include "imacros.c.out"
17597: 
19995: /* Never use JSVAL_IS_BOOLEAN because it restricts the value (true, false) and
19995:    the type. What you want to use is JSVAL_TAG(x) == JSVAL_BOOLEAN and then
19995:    handle the undefined case properly (bug 457363). */
19995: #undef JSVAL_IS_BOOLEAN
19995: #define JSVAL_IS_BOOLEAN(x) JS_STATIC_ASSERT(0)
19995: 
20399: /* Use a fake tag to represent boxed values, borrowing from the integer tag
20399:    range since we only use JSVAL_INT to indicate integers. */
20399: #define JSVAL_BOXED 3
20399: 
23075: /* Another fake jsval tag, used to distinguish null from object values. */
23075: #define JSVAL_TNULL 5
23075: 
20399: /* Map to translate a type tag into a printable representation. */
23075: static const char typeChar[] = "OIDVSNB?";
23075: static const char tagChar[]  = "OIDISIBI";
20399: 
25627: /* Blacklist parameters. */
25627: 
19591: /* Number of iterations of a loop where we start tracing.  That is, we don't
19592:    start tracing until the beginning of the HOTLOOP-th iteration. */
17821: #define HOTLOOP 2
17821: 
25627: /* Attempt recording this many times before blacklisting permanently. */
25937: #define BL_ATTEMPTS 2
25627: 
25627: /* Skip this many future hits before allowing recording again after blacklisting. */
25627: #define BL_BACKOFF 32
25627: 
17821: /* Number of times we wait to exit on a side exit before we try to extend the tree. */
18290: #define HOTEXIT 1
17821: 
25627: /* Number of times we try to extend the tree along a side exit. */
25627: #define MAXEXIT 3
25627: 
25627: /* Maximum number of peer trees allowed. */
25627: #define MAXPEERS 9
25627: 
18051: /* Max call depths for inlining. */
19070: #define MAX_CALLDEPTH 10
17852: 
18118: /* Max native stack size. */
18118: #define MAX_NATIVE_STACK_SLOTS 1024
18118: 
18133: /* Max call stack size. */
18133: #define MAX_CALL_STACK_ENTRIES 64
18133: 
23918: /* Max memory you can allocate in a LIR buffer via a single skip() call. */
23918: #define MAX_SKIP_BYTES (NJ_PAGE_SIZE - LIR_FAR_SLOTS)
23918: 
23447: /* Max memory needed to rebuild the interpreter stack when falling off trace. */
23447: #define MAX_INTERP_STACK_BYTES                                                \
23447:     (MAX_NATIVE_STACK_SLOTS * sizeof(jsval) +                                 \
23447:      MAX_CALL_STACK_ENTRIES * sizeof(JSInlineFrame))
23447: 
18781: /* Max number of branches per tree. */
26534: #define MAX_BRANCHES 32
18781: 
21459: #ifdef JS_JIT_SPEW
26398: #define debug_only_a(x) if (js_verboseAbort || js_verboseDebug ) { x; }
26398: #define ABORT_TRACE(msg)   do { debug_only_a(fprintf(stdout, "abort: %d: %s\n", __LINE__, msg);)  return false; } while (0)
17630: #else
26398: #define debug_only_a(x)
17630: #define ABORT_TRACE(msg)   return false
17630: #endif
17630: 
21459: #ifdef JS_JIT_SPEW
19592: struct __jitstats {
19592: #define JITSTAT(x) uint64 x;
19592: #include "jitstats.tbl"
19592: #undef JITSTAT
19623: } jitstats = { 0LL, };
19623: 
19623: JS_STATIC_ASSERT(sizeof(jitstats) % sizeof(uint64) == 0);
19592: 
19592: enum jitstat_ids {
19592: #define JITSTAT(x) STAT ## x ## ID,
19592: #include "jitstats.tbl"
19592: #undef JITSTAT
19598:     STAT_IDS_TOTAL
19592: };
19592: 
19592: static JSPropertySpec jitstats_props[] = {
19592: #define JITSTAT(x) { #x, STAT ## x ## ID, JSPROP_ENUMERATE | JSPROP_READONLY | JSPROP_PERMANENT },
19592: #include "jitstats.tbl"
19592: #undef JITSTAT
19592:     { 0 }
19592: };
19592: 
19592: static JSBool
19592: jitstats_getProperty(JSContext *cx, JSObject *obj, jsid id, jsval *vp)
19592: {
19592:     int index = -1;
19592: 
19592:     if (JSVAL_IS_STRING(id)) {
19592:         JSString* str = JSVAL_TO_STRING(id);
19592:         if (strcmp(JS_GetStringBytes(str), "HOTLOOP") == 0) {
19592:             *vp = INT_TO_JSVAL(HOTLOOP);
19592:             return JS_TRUE;
19592:         }
19592:     }
19592: 
19592:     if (JSVAL_IS_INT(id))
19592:         index = JSVAL_TO_INT(id);
19592: 
19592:     uint64 result = 0;
19592:     switch (index) {
19623: #define JITSTAT(x) case STAT ## x ## ID: result = jitstats.x; break;
19592: #include "jitstats.tbl"
19592: #undef JITSTAT
19592:       default:
19592:         *vp = JSVAL_VOID;
19592:         return JS_TRUE;
19592:     }
19592: 
19592:     if (result < JSVAL_INT_MAX) {
19592:         *vp = INT_TO_JSVAL(result);
19592:         return JS_TRUE;
19592:     }
19592:     char retstr[64];
19606:     JS_snprintf(retstr, sizeof retstr, "%llu", result);
19592:     *vp = STRING_TO_JSVAL(JS_NewStringCopyZ(cx, retstr));
19592:     return JS_TRUE;
19592: }
19592: 
19592: JSClass jitstats_class = {
19592:     "jitstats",
19592:     JSCLASS_HAS_PRIVATE,
19592:     JS_PropertyStub,       JS_PropertyStub,
19592:     jitstats_getProperty,  JS_PropertyStub,
19592:     JS_EnumerateStub,      JS_ResolveStub,
19592:     JS_ConvertStub,        JS_FinalizeStub,
19592:     JSCLASS_NO_OPTIONAL_MEMBERS
19592: };
19592: 
19592: void
19592: js_InitJITStatsClass(JSContext *cx, JSObject *glob)
19592: {
19592:     JS_InitClass(cx, glob, NULL, &jitstats_class, NULL, 0, jitstats_props, NULL, NULL, NULL);
19592: }
19592: 
19623: #define AUDIT(x) (jitstats.x++)
17726: #else
17726: #define AUDIT(x) ((void)0)
21459: #endif /* JS_JIT_SPEW */
17726: 
17870: #define INS_CONST(c)        addName(lir->insImm(c), #c)
26397: #define INS_CONSTPTR(p)     addName(lir->insImmPtr(p), #p)
25218: #define INS_CONSTFUNPTR(p)  addName(lir->insImmPtr(JS_FUNC_TO_DATA_PTR(void*, p)), #p)
17870: 
17409: using namespace avmplus;
17293: using namespace nanojit;
17293: 
17409: static GC gc = GC();
18056: static avmplus::AvmCore s_core = avmplus::AvmCore();
18056: static avmplus::AvmCore* core = &s_core;
17185: 
21459: #ifdef JS_JIT_SPEW
21433: void
24491: js_DumpPeerStability(JSTraceMonitor* tm, const void* ip, uint32 globalShape);
21433: #endif
21433: 
17997: /* We really need a better way to configure the JIT. Shaver, where is my fancy JIT object? */
26545: static bool did_we_check_processor_features = false;
17997: 
21459: #ifdef JS_JIT_SPEW
23450: bool js_verboseDebug = getenv("TRACEMONKEY") && strstr(getenv("TRACEMONKEY"), "verbose");
25882: bool js_verboseStats = getenv("TRACEMONKEY") && strstr(getenv("TRACEMONKEY"), "stats");
26398: bool js_verboseAbort = getenv("TRACEMONKEY") && strstr(getenv("TRACEMONKEY"), "abort");
18260: #endif
18260: 
17981: /* The entire VM shares one oracle. Collisions and concurrent updates are tolerated and worst
17981:    case cause performance regressions. */
19535: static Oracle oracle;
17981: 
17596: Tracker::Tracker()
17213: {
17293:     pagelist = 0;
17293: }
17239: 
17596: Tracker::~Tracker()
17293: {
17293:     clear();
17293: }
17293: 
17596: jsuword
17596: Tracker::getPageBase(const void* v) const
17293: {
17424:     return jsuword(v) & ~jsuword(NJ_PAGE_SIZE-1);
17293: }
17293: 
17596: struct Tracker::Page*
17596: Tracker::findPage(const void* v) const
17293: {
17424:     jsuword base = getPageBase(v);
17596:     struct Tracker::Page* p = pagelist;
17293:     while (p) {
17356:         if (p->base == base) {
17293:             return p;
17356:         }
17293:         p = p->next;
17293:     }
17293:     return 0;
17293: }
17293: 
17596: struct Tracker::Page*
17596: Tracker::addPage(const void* v) {
17424:     jsuword base = getPageBase(v);
17314:     struct Tracker::Page* p = (struct Tracker::Page*)
17596:         GC::Alloc(sizeof(*p) - sizeof(p->map) + (NJ_PAGE_SIZE >> 2) * sizeof(LIns*));
17293:     p->base = base;
17293:     p->next = pagelist;
17293:     pagelist = p;
17293:     return p;
17213: }
17213: 
17596: void
17596: Tracker::clear()
17213: {
17293:     while (pagelist) {
17293:         Page* p = pagelist;
17293:         pagelist = pagelist->next;
17293:         GC::Free(p);
17259:     }
17293: }
17293: 
17773: bool
17773: Tracker::has(const void *v) const
17773: {
17811:     return get(v) != NULL;
17811: }
17811: 
18230: #if defined NANOJIT_64BIT
18230: #define PAGEMASK 0x7ff
18230: #else
18230: #define PAGEMASK 0xfff
18230: #endif
18230: 
17811: LIns*
17811: Tracker::get(const void* v) const
17811: {
17773:     struct Tracker::Page* p = findPage(v);
17773:     if (!p)
17811:         return NULL;
18230:     return p->map[(jsuword(v) & PAGEMASK) >> 2];
17219: }
17247: 
17596: void
17596: Tracker::set(const void* v, LIns* i)
17247: {
17596:     struct Tracker::Page* p = findPage(v);
17293:     if (!p)
17293:         p = addPage(v);
18230:     p->map[(jsuword(v) & PAGEMASK) >> 2] = i;
17293: }
17293: 
17464: static inline bool isNumber(jsval v)
17464: {
17464:     return JSVAL_IS_INT(v) || JSVAL_IS_DOUBLE(v);
17464: }
17464: 
17464: static inline jsdouble asNumber(jsval v)
17464: {
17464:     JS_ASSERT(isNumber(v));
17464:     if (JSVAL_IS_DOUBLE(v))
17464:         return *JSVAL_TO_DOUBLE(v);
17464:     return (jsdouble)JSVAL_TO_INT(v);
17464: }
17464: 
17479: static inline bool isInt32(jsval v)
17479: {
17479:     if (!isNumber(v))
17479:         return false;
17479:     jsdouble d = asNumber(v);
17759:     jsint i;
17759:     return JSDOUBLE_IS_INT(d, i);
17479: }
17479: 
26274: static inline jsint asInt32(jsval v)
26274: {
26274:     JS_ASSERT(isNumber(v));
26274:     if (JSVAL_IS_INT(v))
26274:         return JSVAL_TO_INT(v);
26274: #ifdef DEBUG
26274:     jsint i;
26274:     JS_ASSERT(JSDOUBLE_IS_INT(*JSVAL_TO_DOUBLE(v), i));
26274: #endif
26274:     return jsint(*JSVAL_TO_DOUBLE(v));
25099: }
25099: 
19576: /* Return JSVAL_DOUBLE for all numbers (int and double) and the tag otherwise. */
19576: static inline uint8 getPromotedType(jsval v)
19576: {
23075:     return JSVAL_IS_INT(v) ? JSVAL_DOUBLE : JSVAL_IS_NULL(v) ? JSVAL_TNULL : uint8(JSVAL_TAG(v));
19576: }
19576: 
19576: /* Return JSVAL_INT for all whole numbers that fit into signed 32-bit and the tag otherwise. */
17891: static inline uint8 getCoercedType(jsval v)
17891: {
23075:     return isInt32(v) ? JSVAL_INT : JSVAL_IS_NULL(v) ? JSVAL_TNULL : uint8(JSVAL_TAG(v));
17891: }
17891: 
22613: /*
22613:  * Constant seed and accumulate step borrowed from the DJB hash.
22613:  */
22613: 
22613: #define ORACLE_MASK (ORACLE_SIZE - 1)
24307: #define FRAGMENT_TABLE_MASK (FRAGMENT_TABLE_SIZE - 1)
24307: #define HASH_SEED 5381
22613: 
22613: static inline void
24307: hash_accum(uintptr_t& h, uintptr_t i, uintptr_t mask)
24307: {
24307:     h = ((h << 5) + h + (mask & i)) & mask;
22613: }
22613: 
23456: JS_REQUIRES_STACK static inline int
22613: stackSlotHash(JSContext* cx, unsigned slot)
22613: {
24307:     uintptr_t h = HASH_SEED;
24307:     hash_accum(h, uintptr_t(cx->fp->script), ORACLE_MASK);
24307:     hash_accum(h, uintptr_t(cx->fp->regs->pc), ORACLE_MASK);
24307:     hash_accum(h, uintptr_t(slot), ORACLE_MASK);
22614:     return int(h);
22614: }
22614: 
23456: JS_REQUIRES_STACK static inline int
22613: globalSlotHash(JSContext* cx, unsigned slot)
22613: {
24307:     uintptr_t h = HASH_SEED;
22613:     JSStackFrame* fp = cx->fp;
22613: 
22613:     while (fp->down)
22613:         fp = fp->down;
22613: 
24307:     hash_accum(h, uintptr_t(fp->script), ORACLE_MASK);
24307:     hash_accum(h, uintptr_t(OBJ_SHAPE(JS_GetGlobalForObject(cx, fp->scopeChain))),
24307:                ORACLE_MASK);
24307:     hash_accum(h, uintptr_t(slot), ORACLE_MASK);
22614:     return int(h);
22613: }
22613: 
24290: Oracle::Oracle()
24290: {
24290:     clear();
24290: }
24290: 
17981: /* Tell the oracle that a certain global variable should not be demoted. */
23456: JS_REQUIRES_STACK void
22613: Oracle::markGlobalSlotUndemotable(JSContext* cx, unsigned slot)
22613: {
22613:     _globalDontDemote.set(&gc, globalSlotHash(cx, slot));
17981: }
17981: 
17981: /* Consult with the oracle whether we shouldn't demote a certain global variable. */
23456: JS_REQUIRES_STACK bool
22613: Oracle::isGlobalSlotUndemotable(JSContext* cx, unsigned slot) const
22613: {
22613:     return _globalDontDemote.get(globalSlotHash(cx, slot));
17981: }
17981: 
17981: /* Tell the oracle that a certain slot at a certain bytecode location should not be demoted. */
23456: JS_REQUIRES_STACK void
22613: Oracle::markStackSlotUndemotable(JSContext* cx, unsigned slot)
22613: {
22613:     _stackDontDemote.set(&gc, stackSlotHash(cx, slot));
17981: }
17981: 
17981: /* Consult with the oracle whether we shouldn't demote a certain slot. */
23456: JS_REQUIRES_STACK bool
22613: Oracle::isStackSlotUndemotable(JSContext* cx, unsigned slot) const
22613: {
22613:     return _stackDontDemote.get(stackSlotHash(cx, slot));
17981: }
17981: 
24290: void
24290: Oracle::clearDemotability()
18273: {
22613:     _stackDontDemote.reset();
22613:     _globalDontDemote.reset();
22613: }
22613: 
25627: static void
25627: js_Blacklist(Fragment* tree)
25627: {
25627:     JS_ASSERT(tree->root == tree);
25627:     jsbytecode* pc = (jsbytecode*)tree->ip;
25627:     JS_ASSERT(*pc == JSOP_LOOP || *pc == JSOP_NOP);
25627:     *pc = JSOP_NOP;
25627: }
25627: 
25627: static void
25627: js_Backoff(Fragment* tree, const jsbytecode* where)
25627: {
25627:     JS_ASSERT(tree->root == tree);
25627:     if (++tree->recordAttempts > BL_ATTEMPTS) {
25627:         js_Blacklist(tree);
25627:         return;
25627:     }
25627:     tree->hits() -= BL_BACKOFF;
25627: }
25627: 
24307: static inline size_t
24307: fragmentHash(const void *ip, uint32 globalShape)
24307: {
24307:     uintptr_t h = HASH_SEED;
24307:     hash_accum(h, uintptr_t(ip), FRAGMENT_TABLE_MASK);
24307:     hash_accum(h, uintptr_t(globalShape), FRAGMENT_TABLE_MASK);
24307:     return size_t(h);
24307: }
24307: 
24307: struct VMFragment : public Fragment
24307: {
24307:     VMFragment(const void* _ip, uint32 _globalShape) :
24307:         Fragment(_ip),
24307:         next(NULL),
24307:         globalShape(_globalShape)
24307:     {}
24307:     VMFragment* next;
24307:     uint32 globalShape;
24307: };
24307: 
24307: static VMFragment*
24307: getVMFragment(JSTraceMonitor* tm, const void *ip, uint32 globalShape)
24307: {
24307:     size_t h = fragmentHash(ip, globalShape);
24307:     VMFragment* vf = tm->vmfragments[h];
24307:     while (vf &&
24307:            ! (vf->globalShape == globalShape &&
24307:               vf->ip == ip)) {
24307:         vf = vf->next;
24307:     }
24307:     return vf;
24307: }
24307: 
24307: static Fragment*
24491: getLoop(JSTraceMonitor* tm, const void *ip, uint32 globalShape)
24307: {
24307:     return getVMFragment(tm, ip, globalShape);
24307: }
24307: 
24307: static Fragment*
24491: getAnchor(JSTraceMonitor* tm, const void *ip, uint32 globalShape)
24307: {
25102:     VMFragment *f = new (&gc) VMFragment(ip, globalShape);
24307:     JS_ASSERT(f);
24307: 
24307:     Fragment *p = getVMFragment(tm, ip, globalShape);
24307: 
24307:     if (p) {
24307:         f->first = p;
24307:         /* append at the end of the peer list */
24307:         Fragment* next;
24307:         while ((next = p->peer) != NULL)
24307:             p = next;
24307:         p->peer = f;
24307:     } else {
24307:         /* this is the first fragment */
24307:         f->first = f;
24307:         size_t h = fragmentHash(ip, globalShape);
24307:         f->next = tm->vmfragments[h];
24307:         tm->vmfragments[h] = f;
24307:     }
24307:     f->anchor = f;
24307:     f->root = f;
24307:     f->kind = LoopTrace;
24307:     return f;
24307: }
24307: 
25937: static void
25937: js_AttemptCompilation(JSTraceMonitor* tm, JSObject* globalObj, jsbytecode* pc)
25937: {
26726:     /*
26726:      * If we already permanently blacklisted the location, undo that.
26726:      */
26726:     JS_ASSERT(*(jsbytecode*)pc == JSOP_NOP || *(jsbytecode*)pc == JSOP_LOOP);
26726:     *(jsbytecode*)pc = JSOP_LOOP;
26726: 
26726:     /*
26726:      * Breath new live into all peer fragments at the designated loop header.
26726:      */
26726:     Fragment* f = (VMFragment*)getLoop(tm, pc, OBJ_SHAPE(globalObj));
26726:     if (!f) {
26726:         /*
26726:          * If the global object's shape changed, we can't easily find the
26726:          * corresponding loop header via a hash table lookup. In this
26726:          * we simply bail here and hope that the fragment has another
26726:          * outstanding compilation attempt. This case is extremely rare.
26726:          */
26726:         return;
26726:     }
25937:     JS_ASSERT(f->root == f);
25937:     f = f->first;
25937:     while (f) {
25937:         JS_ASSERT(f->root == f);
25937:         --f->recordAttempts;
25937:         f->hits() = HOTLOOP;
25937:         f = f->peer;
25937:     }
25937: }
18273: 
20923: JS_DEFINE_CALLINFO_1(static, DOUBLE,    i2f, INT32,                 1, 1)
20923: JS_DEFINE_CALLINFO_1(static, DOUBLE,    u2f, UINT32,                1, 1)
20424: 
18773: static bool isi2f(LInsp i)
18773: {
18773:     if (i->isop(LIR_i2f))
18773:         return true;
18773: 
26545:     if (nanojit::AvmCore::config.soft_float &&
26545:         i->isop(LIR_qjoin) &&
18773:         i->oprnd1()->isop(LIR_call) &&
18773:         i->oprnd2()->isop(LIR_callh))
18773:     {
20915:         if (i->oprnd1()->callInfo() == &i2f_ci)
18773:             return true;
18773:     }
18773: 
18773:     return false;
18773: }
18773: 
18773: static bool isu2f(LInsp i)
18773: {
18773:     if (i->isop(LIR_u2f))
18773:         return true;
18773: 
26545:     if (nanojit::AvmCore::config.soft_float &&
26545:         i->isop(LIR_qjoin) &&
18773:         i->oprnd1()->isop(LIR_call) &&
18773:         i->oprnd2()->isop(LIR_callh))
18773:     {
20915:         if (i->oprnd1()->callInfo() == &u2f_ci)
18773:             return true;
18773:     }
18773: 
18773:     return false;
18773: }
18773: 
18773: static LInsp iu2fArg(LInsp i)
18773: {
26545:     if (nanojit::AvmCore::config.soft_float &&
26545:         i->isop(LIR_qjoin))
26545:     {
18773:         return i->oprnd1()->arg(0);
26545:     }
18773: 
18773:     return i->oprnd1();
18773: }
18773: 
18773: 
17451: static LIns* demote(LirWriter *out, LInsp i)
17451: {
17451:     if (i->isCall())
17451:         return callArgN(i, 0);
18773:     if (isi2f(i) || isu2f(i))
18773:         return iu2fArg(i);
17997:     if (i->isconst())
17997:         return i;
17451:     AvmAssert(i->isconstq());
17451:     double cf = i->constvalf();
17451:     int32_t ci = cf > 0x7fffffff ? uint32_t(cf) : int32_t(cf);
17451:     return out->insImm(ci);
17451: }
17451: 
17451: static bool isPromoteInt(LIns* i)
17451: {
26393:     if (isi2f(i) || i->isconst())
26393:         return true;
26393:     if (!i->isconstq())
26393:         return false;
26393:     jsdouble d = i->constvalf();
26393:     return d == jsdouble(jsint(d)) && !JSDOUBLE_IS_NEGZERO(d);
17451: }
17451: 
17451: static bool isPromoteUint(LIns* i)
17451: {
26393:     if (isu2f(i) || i->isconst())
26393:         return true;
26393:     if (!i->isconstq())
26393:         return false;
26393:     jsdouble d = i->constvalf();
26393:     return d == jsdouble(jsuint(d)) && !JSDOUBLE_IS_NEGZERO(d);
17451: }
17451: 
17451: static bool isPromote(LIns* i)
17451: {
17758:     return isPromoteInt(i) || isPromoteUint(i);
17451: }
17451: 
17800: static bool isconst(LIns* i, int32_t c)
17800: {
17800:     return i->isconst() && i->constval() == c;
17800: }
17800: 
17796: static bool overflowSafe(LIns* i)
17796: {
17796:     LIns* c;
17796:     return (i->isop(LIR_and) && ((c = i->oprnd2())->isconst()) &&
17796:             ((c->constval() & 0xc0000000) == 0)) ||
17796:            (i->isop(LIR_rsh) && ((c = i->oprnd2())->isconst()) &&
17796:             ((c->constval() > 0)));
17796: }
17796: 
26545: /* soft float support */
20424: 
20923: JS_DEFINE_CALLINFO_1(static, DOUBLE,    fneg, DOUBLE,               1, 1)
20923: JS_DEFINE_CALLINFO_2(static, INT32,     fcmpeq, DOUBLE, DOUBLE,     1, 1)
20923: JS_DEFINE_CALLINFO_2(static, INT32,     fcmplt, DOUBLE, DOUBLE,     1, 1)
20923: JS_DEFINE_CALLINFO_2(static, INT32,     fcmple, DOUBLE, DOUBLE,     1, 1)
20923: JS_DEFINE_CALLINFO_2(static, INT32,     fcmpgt, DOUBLE, DOUBLE,     1, 1)
20923: JS_DEFINE_CALLINFO_2(static, INT32,     fcmpge, DOUBLE, DOUBLE,     1, 1)
20923: JS_DEFINE_CALLINFO_2(static, DOUBLE,    fmul, DOUBLE, DOUBLE,       1, 1)
20923: JS_DEFINE_CALLINFO_2(static, DOUBLE,    fadd, DOUBLE, DOUBLE,       1, 1)
20923: JS_DEFINE_CALLINFO_2(static, DOUBLE,    fdiv, DOUBLE, DOUBLE,       1, 1)
20923: JS_DEFINE_CALLINFO_2(static, DOUBLE,    fsub, DOUBLE, DOUBLE,       1, 1)
20923: 
20424: jsdouble FASTCALL
20923: fneg(jsdouble x)
20424: {
20424:     return -x;
20424: }
20424: 
20424: jsdouble FASTCALL
20923: i2f(int32 i)
20424: {
20424:     return i;
20424: }
20424: 
20424: jsdouble FASTCALL
20923: u2f(jsuint u)
20424: {
20424:     return u;
20424: }
20424: 
20424: int32 FASTCALL
20923: fcmpeq(jsdouble x, jsdouble y)
20424: {
20424:     return x==y;
20424: }
20424: 
20424: int32 FASTCALL
20923: fcmplt(jsdouble x, jsdouble y)
20424: {
20424:     return x < y;
20424: }
20424: 
20424: int32 FASTCALL
20923: fcmple(jsdouble x, jsdouble y)
20424: {
20424:     return x <= y;
20424: }
20424: 
20424: int32 FASTCALL
20923: fcmpgt(jsdouble x, jsdouble y)
20424: {
20424:     return x > y;
20424: }
20424: 
20424: int32 FASTCALL
20923: fcmpge(jsdouble x, jsdouble y)
20424: {
20424:     return x >= y;
20424: }
20424: 
20424: jsdouble FASTCALL
20923: fmul(jsdouble x, jsdouble y)
20424: {
20424:     return x * y;
20424: }
20424: 
20424: jsdouble FASTCALL
20923: fadd(jsdouble x, jsdouble y)
20424: {
20424:     return x + y;
20424: }
20424: 
20424: jsdouble FASTCALL
20923: fdiv(jsdouble x, jsdouble y)
20424: {
20424:     return x / y;
20424: }
20424: 
20424: jsdouble FASTCALL
20923: fsub(jsdouble x, jsdouble y)
20424: {
20424:     return x - y;
20424: }
20424: 
18773: class SoftFloatFilter: public LirWriter
18773: {
18773: public:
18773:     SoftFloatFilter(LirWriter* out):
18773:         LirWriter(out)
18773:     {
18773:     }
18773: 
20408:     LInsp quadCall(const CallInfo *ci, LInsp args[]) {
18773:         LInsp qlo, qhi;
18773: 
20408:         qlo = out->insCall(ci, args);
18773:         qhi = out->ins1(LIR_callh, qlo);
18773:         return out->qjoin(qlo, qhi);
18773:     }
18773: 
18773:     LInsp ins1(LOpcode v, LInsp s0)
18773:     {
18773:         if (v == LIR_fneg)
20915:             return quadCall(&fneg_ci, &s0);
18773: 
18773:         if (v == LIR_i2f)
20915:             return quadCall(&i2f_ci, &s0);
18773: 
18773:         if (v == LIR_u2f)
20915:             return quadCall(&u2f_ci, &s0);
18773: 
18773:         return out->ins1(v, s0);
18773:     }
18773: 
18773:     LInsp ins2(LOpcode v, LInsp s0, LInsp s1)
18773:     {
18773:         LInsp args[2];
18773:         LInsp bv;
18773: 
18773:         // change the numeric value and order of these LIR opcodes and die
18773:         if (LIR_fadd <= v && v <= LIR_fdiv) {
20915:             static const CallInfo *fmap[] = { &fadd_ci, &fsub_ci, &fmul_ci, &fdiv_ci };
18773: 
18773:             args[0] = s1;
18773:             args[1] = s0;
18773: 
18773:             return quadCall(fmap[v - LIR_fadd], args);
18773:         }
18773: 
18773:         if (LIR_feq <= v && v <= LIR_fge) {
20915:             static const CallInfo *fmap[] = { &fcmpeq_ci, &fcmplt_ci, &fcmpgt_ci, &fcmple_ci, &fcmpge_ci };
18773: 
18773:             args[0] = s1;
18773:             args[1] = s0;
18773: 
18773:             bv = out->insCall(fmap[v - LIR_feq], args);
18773:             return out->ins2(LIR_eq, bv, out->insImm(1));
18773:         }
18773: 
18773:         return out->ins2(v, s0, s1);
18773:     }
18773: 
20408:     LInsp insCall(const CallInfo *ci, LInsp args[])
18773:     {
18773:         // if the return type is ARGSIZE_F, we have
18773:         // to do a quadCall ( qjoin(call,callh) )
20408:         if ((ci->_argtypes & 3) == ARGSIZE_F)
20408:             return quadCall(ci, args);
20408: 
20408:         return out->insCall(ci, args);
18773:     }
18773: };
18773: 
17451: class FuncFilter: public LirWriter
17451: {
17451: public:
21799:     FuncFilter(LirWriter* out):
21799:         LirWriter(out)
17451:     {
17451:     }
17451: 
25213:     LInsp ins2(LOpcode v, LInsp s0, LInsp s1)
17451:     {
17451:         if (s0 == s1 && v == LIR_feq) {
17451:             if (isPromote(s0)) {
17451:                 // double(int) and double(uint) cannot be nan
17451:                 return insImm(1);
17451:             }
17451:             if (s0->isop(LIR_fmul) || s0->isop(LIR_fsub) || s0->isop(LIR_fadd)) {
17451:                 LInsp lhs = s0->oprnd1();
17451:                 LInsp rhs = s0->oprnd2();
17451:                 if (isPromote(lhs) && isPromote(rhs)) {
17451:                     // add/sub/mul promoted ints can't be nan
17451:                     return insImm(1);
17451:                 }
17451:             }
17538:         } else if (LIR_feq <= v && v <= LIR_fge) {
17451:             if (isPromoteInt(s0) && isPromoteInt(s1)) {
17451:                 // demote fcmp to cmp
17451:                 v = LOpcode(v + (LIR_eq - LIR_feq));
17797:                 return out->ins2(v, demote(out, s0), demote(out, s1));
17451:             } else if (isPromoteUint(s0) && isPromoteUint(s1)) {
17451:                 // uint compare
17451:                 v = LOpcode(v + (LIR_eq - LIR_feq));
17451:                 if (v != LIR_eq)
17451:                     v = LOpcode(v + (LIR_ult - LIR_lt)); // cmp -> ucmp
17797:                 return out->ins2(v, demote(out, s0), demote(out, s1));
17451:             }
17800:         } else if (v == LIR_or &&
17800:                    s0->isop(LIR_lsh) && isconst(s0->oprnd2(), 16) &&
17800:                    s1->isop(LIR_and) && isconst(s1->oprnd2(), 0xffff)) {
17800:             LIns* msw = s0->oprnd1();
17800:             LIns* lsw = s1->oprnd1();
17800:             LIns* x;
17800:             LIns* y;
17800:             if (lsw->isop(LIR_add) &&
17800:                 lsw->oprnd1()->isop(LIR_and) &&
17800:                 lsw->oprnd2()->isop(LIR_and) &&
17800:                 isconst(lsw->oprnd1()->oprnd2(), 0xffff) &&
17800:                 isconst(lsw->oprnd2()->oprnd2(), 0xffff) &&
17800:                 msw->isop(LIR_add) &&
17800:                 msw->oprnd1()->isop(LIR_add) &&
17800:                 msw->oprnd2()->isop(LIR_rsh) &&
17800:                 msw->oprnd1()->oprnd1()->isop(LIR_rsh) &&
17800:                 msw->oprnd1()->oprnd2()->isop(LIR_rsh) &&
17800:                 isconst(msw->oprnd2()->oprnd2(), 16) &&
17800:                 isconst(msw->oprnd1()->oprnd1()->oprnd2(), 16) &&
17800:                 isconst(msw->oprnd1()->oprnd2()->oprnd2(), 16) &&
17800:                 (x = lsw->oprnd1()->oprnd1()) == msw->oprnd1()->oprnd1()->oprnd1() &&
17800:                 (y = lsw->oprnd2()->oprnd1()) == msw->oprnd1()->oprnd2()->oprnd1() &&
17800:                 lsw == msw->oprnd2()->oprnd1()) {
17800:                 return out->ins2(LIR_add, x, y);
17800:             }
17451:         }
18776: #ifdef NANOJIT_ARM
18776:         else if (v == LIR_lsh ||
18776:                  v == LIR_rsh ||
18776:                  v == LIR_ush)
18776:         {
18776:             // needed on ARM -- arm doesn't mask shifts to 31 like x86 does
18776:             if (s1->isconst())
18776:                 s1->setimm16(s1->constval() & 31);
18776:             else
18776:                 s1 = out->ins2(LIR_and, s1, out->insImm(31));
18776:             return out->ins2(v, s0, s1);
18776:         }
18776: #endif
18776: 
17797:         return out->ins2(v, s0, s1);
17451:     }
17451: 
20408:     LInsp insCall(const CallInfo *ci, LInsp args[])
17451:     {
17478:         LInsp s0 = args[0];
20915:         if (ci == &js_DoubleToUint32_ci) {
17783:             if (s0->isconstq())
17783:                 return out->insImm(js_DoubleToECMAUint32(s0->constvalf()));
18773:             if (isi2f(s0) || isu2f(s0))
18773:                 return iu2fArg(s0);
20915:         } else if (ci == &js_DoubleToInt32_ci) {
17587:             if (s0->isconstq())
17587:                 return out->insImm(js_DoubleToECMAInt32(s0->constvalf()));
21792:             if (s0->isop(LIR_fadd) || s0->isop(LIR_fsub)) {
17451:                 LInsp lhs = s0->oprnd1();
17451:                 LInsp rhs = s0->oprnd2();
17451:                 if (isPromote(lhs) && isPromote(rhs)) {
17451:                     LOpcode op = LOpcode(s0->opcode() & ~LIR64);
17451:                     return out->ins2(op, demote(out, lhs), demote(out, rhs));
17451:                 }
17451:             }
18773:             if (isi2f(s0) || isu2f(s0))
18773:                 return iu2fArg(s0);
20408:             // XXX ARM -- check for qjoin(call(UnboxDouble),call(UnboxDouble))
25707:             if (s0->isCall()) {
25707:                 const CallInfo* ci2 = s0->callInfo();
25707:                 if (ci2 == &js_UnboxDouble_ci) {
17847:                     LIns* args2[] = { callArgN(s0, 0) };
20915:                     return out->insCall(&js_UnboxInt32_ci, args2);
25707:                 } else if (ci2 == &js_StringToNumber_ci) {
25707:                     // callArgN's ordering is that as seen by the builtin, not as stored in
25707:                     // args here. True story!
17912:                     LIns* args2[] = { callArgN(s0, 1), callArgN(s0, 0) };
20915:                     return out->insCall(&js_StringToInt32_ci, args2);
25707:                 } else if (ci2 == &js_String_p_charCodeAt0_ci) {
25707:                     // Use a fast path builtin for a charCodeAt that converts to an int right away.
25707:                     LIns* args2[] = { callArgN(s0, 0) };
25707:                     return out->insCall(&js_String_p_charCodeAt0_int_ci, args2);
25707:                 } else if (ci2 == &js_String_p_charCodeAt_ci) {
25707:                     LIns* idx = callArgN(s0, 1);
25707:                     // If the index is not already an integer, force it to be an integer.
25707:                     idx = isPromote(idx)
25707:                         ? demote(out, idx)
25707:                         : out->insCall(&js_DoubleToInt32_ci, &idx);
25707:                     LIns* args2[] = { idx, callArgN(s0, 0) };
25707:                     return out->insCall(&js_String_p_charCodeAt_int_ci, args2);
25707:                 }
20915:             }
20915:         } else if (ci == &js_BoxDouble_ci) {
17478:             JS_ASSERT(s0->isQuad());
25707:             if (isi2f(s0)) {
26546:                 LIns* args2[] = { iu2fArg(s0), args[1] };
20915:                 return out->insCall(&js_BoxInt32_ci, args2);
20915:             }
20915:             if (s0->isCall() && s0->callInfo() == &js_UnboxDouble_ci)
18289:                 return callArgN(s0, 0);
20408:         }
20408:         return out->insCall(ci, args);
17451:     }
17451: };
17451: 
17512: /* In debug mode vpname contains a textual description of the type of the
24248:    slot during the forall iteration over all slots. If JS_JIT_SPEW is not
24248:    defined, vpnum is set to a very large integer to catch invalid uses of
24248:    it. Non-debug code should never use vpnum. */
21459: #ifdef JS_JIT_SPEW
17676: #define DEF_VPNAME          const char* vpname; unsigned vpnum
17512: #define SET_VPNAME(name)    do { vpname = name; vpnum = 0; } while(0)
17512: #define INC_VPNUM()         do { ++vpnum; } while(0)
17512: #else
17531: #define DEF_VPNAME          do {} while (0)
17531: #define vpname ""
24248: #define vpnum 0x40000000
17512: #define SET_VPNAME(name)    ((void)0)
17512: #define INC_VPNUM()         ((void)0)
17512: #endif
17512: 
17936: /* Iterate over all interned global variables. */
17815: #define FORALL_GLOBAL_SLOTS(cx, ngslots, gslots, code)                        \
17512:     JS_BEGIN_MACRO                                                            \
17531:         DEF_VPNAME;                                                           \
17657:         JSObject* globalObj = JS_GetGlobalForObject(cx, cx->fp->scopeChain);  \
17512:         unsigned n;                                                           \
17512:         jsval* vp;                                                            \
17647:         SET_VPNAME("global");                                                 \
17663:         for (n = 0; n < ngslots; ++n) {                                       \
17663:             vp = &STOBJ_GET_SLOT(globalObj, gslots[n]);                       \
17512:             { code; }                                                         \
17512:             INC_VPNUM();                                                      \
17512:         }                                                                     \
17815:     JS_END_MACRO
17815: 
17936: /* Iterate over all slots in the frame, consisting of args, vars, and stack
17936:    (except for the top-level frame which does not have args or vars. */
17949: #define FORALL_FRAME_SLOTS(fp, depth, code)                                   \
17815:     JS_BEGIN_MACRO                                                            \
17815:         jsval* vp;                                                            \
17512:         jsval* vpstop;                                                        \
17949:         if (fp->callee) {                                                     \
17936:             if (depth == 0) {                                                 \
18187:                 SET_VPNAME("callee");                                         \
18187:                 vp = &fp->argv[-2];                                           \
18187:                 { code; }                                                     \
17688:                 SET_VPNAME("this");                                           \
17949:                 vp = &fp->argv[-1];                                           \
18187:                 { code; }                                                     \
17512:                 SET_VPNAME("argv");                                           \
18425:                 vp = &fp->argv[0]; vpstop = &fp->argv[fp->fun->nargs];        \
17512:                 while (vp < vpstop) { code; ++vp; INC_VPNUM(); }              \
17923:             }                                                                 \
17512:             SET_VPNAME("vars");                                               \
17949:             vp = fp->slots; vpstop = &fp->slots[fp->script->nfixed];          \
17512:             while (vp < vpstop) { code; ++vp; INC_VPNUM(); }                  \
17512:         }                                                                     \
17512:         SET_VPNAME("stack");                                                  \
17949:         vp = StackBase(fp); vpstop = fp->regs->sp;                            \
17512:         while (vp < vpstop) { code; ++vp; INC_VPNUM(); }                      \
18119:         if (fsp < fspstop - 1) {                                              \
18119:             JSStackFrame* fp2 = fsp[1];                                       \
18119:             int missing = fp2->fun->nargs - fp2->argc;                        \
18119:             if (missing > 0) {                                                \
18110:                 SET_VPNAME("missing");                                        \
18119:                 vp = fp->regs->sp;                                            \
18119:                 vpstop = vp + missing;                                        \
18110:                 while (vp < vpstop) { code; ++vp; INC_VPNUM(); }              \
18110:             }                                                                 \
18119:         }                                                                     \
17936:     JS_END_MACRO
17936: 
17936: /* Iterate over all slots in each pending frame. */
18169: #define FORALL_SLOTS_IN_PENDING_FRAMES(cx, callDepth, code)                   \
17936:     JS_BEGIN_MACRO                                                            \
17936:         DEF_VPNAME;                                                           \
17936:         unsigned n;                                                           \
18169:         JSStackFrame* currentFrame = cx->fp;                                  \
17936:         JSStackFrame* entryFrame;                                             \
17936:         JSStackFrame* fp = currentFrame;                                      \
17936:         for (n = 0; n < callDepth; ++n) { fp = fp->down; }                    \
17936:         entryFrame = fp;                                                      \
17936:         unsigned frames = callDepth+1;                                        \
17949:         JSStackFrame** fstack =                                               \
17949:             (JSStackFrame**) alloca(frames * sizeof (JSStackFrame*));         \
17936:         JSStackFrame** fspstop = &fstack[frames];                             \
17936:         JSStackFrame** fsp = fspstop-1;                                       \
17936:         fp = currentFrame;                                                    \
17936:         for (;; fp = fp->down) { *fsp-- = fp; if (fp == entryFrame) break; }  \
17936:         unsigned depth;                                                       \
17936:         for (depth = 0, fsp = fstack; fsp < fspstop; ++fsp, ++depth) {        \
18119:             fp = *fsp;                                                        \
17949:             FORALL_FRAME_SLOTS(fp, depth, code);                              \
17512:         }                                                                     \
17544:     JS_END_MACRO
17512: 
18169: #define FORALL_SLOTS(cx, ngslots, gslots, callDepth, code)                    \
17820:     JS_BEGIN_MACRO                                                            \
24246:         FORALL_SLOTS_IN_PENDING_FRAMES(cx, callDepth, code);                  \
17820:         FORALL_GLOBAL_SLOTS(cx, ngslots, gslots, code);                       \
17820:     JS_END_MACRO
17820: 
18144: /* Calculate the total number of native frame slots we need from this frame
18144:    all the way back to the entry frame, including the current stack usage. */
22652: JS_REQUIRES_STACK unsigned
18425: js_NativeStackSlots(JSContext *cx, unsigned callDepth)
18193: {
18193:     JSStackFrame* fp = cx->fp;
18144:     unsigned slots = 0;
18144: #if defined _DEBUG
18144:     unsigned int origCallDepth = callDepth;
18144: #endif
18144:     for (;;) {
18144:         unsigned operands = fp->regs->sp - StackBase(fp);
18144:         slots += operands;
18144:         if (fp->callee)
18144:             slots += fp->script->nfixed;
18144:         if (callDepth-- == 0) {
18617:             if (fp->callee)
18425:                 slots += 2/*callee,this*/ + fp->fun->nargs;
18144: #if defined _DEBUG
18144:             unsigned int m = 0;
18169:             FORALL_SLOTS_IN_PENDING_FRAMES(cx, origCallDepth, m++);
18144:             JS_ASSERT(m == slots);
18144: #endif
18144:             return slots;
18144:         }
18144:         JSStackFrame* fp2 = fp;
18144:         fp = fp->down;
18144:         int missing = fp2->fun->nargs - fp2->argc;
18144:         if (missing > 0)
18144:             slots += missing;
18144:     }
18425:     JS_NOT_REACHED("js_NativeStackSlots");
18144: }
18144: 
24246: /*
24246:  * Capture the type map for the selected slots of the global object and currently pending
24246:  * stack frames.
24246:  */
22652: JS_REQUIRES_STACK void
24246: TypeMap::captureTypes(JSContext* cx, SlotList& slots, unsigned callDepth)
17991: {
17991:     unsigned ngslots = slots.length();
17991:     uint16* gslots = slots.data();
24246:     setLength(js_NativeStackSlots(cx, callDepth) + ngslots);
17991:     uint8* map = data();
17991:     uint8* m = map;
24246:     FORALL_SLOTS_IN_PENDING_FRAMES(cx, callDepth,
24246:         uint8 type = getCoercedType(*vp);
24246:         if ((type == JSVAL_INT) && oracle.isStackSlotUndemotable(cx, unsigned(m - map)))
24246:             type = JSVAL_DOUBLE;
24246:         JS_ASSERT(type != JSVAL_BOXED);
24246:         debug_only_v(printf("capture stack type %s%d: %d=%c\n", vpname, vpnum, type, typeChar[type]);)
24246:         JS_ASSERT(uintptr_t(m - map) < length());
24246:         *m++ = type;
24246:     );
17991:     FORALL_GLOBAL_SLOTS(cx, ngslots, gslots,
17991:         uint8 type = getCoercedType(*vp);
22613:         if ((type == JSVAL_INT) && oracle.isGlobalSlotUndemotable(cx, gslots[n]))
17991:             type = JSVAL_DOUBLE;
20399:         JS_ASSERT(type != JSVAL_BOXED);
22613:         debug_only_v(printf("capture global type %s%d: %d=%c\n", vpname, vpnum, type, typeChar[type]);)
24246:         JS_ASSERT(uintptr_t(m - map) < length());
17991:         *m++ = type;
17991:     );
24246:     JS_ASSERT(uintptr_t(m - map) == length());
24246: }
24246: 
22652: JS_REQUIRES_STACK void
24246: TypeMap::captureMissingGlobalTypes(JSContext* cx, SlotList& slots, unsigned stackSlots)
24246: {
24246:     unsigned oldSlots = length() - stackSlots;
24246:     int diff = slots.length() - oldSlots;
24246:     JS_ASSERT(diff >= 0);
24246:     unsigned ngslots = slots.length();
24246:     uint16* gslots = slots.data();
24246:     setLength(length() + diff);
24246:     uint8* map = data() + stackSlots;
17985:     uint8* m = map;
24246:     FORALL_GLOBAL_SLOTS(cx, ngslots, gslots,
24246:         if (n >= oldSlots) {
17985:             uint8 type = getCoercedType(*vp);
24246:             if ((type == JSVAL_INT) && oracle.isGlobalSlotUndemotable(cx, gslots[n]))
17985:                 type = JSVAL_DOUBLE;
24246:             JS_ASSERT(type != JSVAL_BOXED);
24246:             debug_only_v(printf("capture global type %s%d: %d=%c\n", vpname, vpnum, type, typeChar[type]);)
24246:             *m = type;
24246:             JS_ASSERT((m > map + oldSlots) || (*m == type));
24246:         }
24246:         m++;
17985:     );
17986: }
17986: 
17986: /* Compare this type map to another one and see whether they match. */
17986: bool
18239: TypeMap::matches(TypeMap& other) const
18239: {
18239:     if (length() != other.length())
18239:         return false;
17987:     return !memcmp(data(), other.data(), length());
17985: }
17985: 
18198: /* Use the provided storage area to create a new type map that contains the partial type map
18198:    with the rest of it filled up from the complete type map. */
18198: static void
18198: mergeTypeMaps(uint8** partial, unsigned* plength, uint8* complete, unsigned clength, uint8* mem)
18198: {
18198:     unsigned l = *plength;
18198:     JS_ASSERT(l < clength);
18198:     memcpy(mem, *partial, l * sizeof(uint8));
18198:     memcpy(mem + l, complete + l, (clength - l) * sizeof(uint8));
18198:     *partial = mem;
18198:     *plength = clength;
18198: }
18198: 
25491: /* Specializes a tree to any missing globals, including any dependent trees. */
25509: static JS_REQUIRES_STACK void
25491: specializeTreesToMissingGlobals(JSContext* cx, TreeInfo* root)
25491: {
25491:     TreeInfo* ti = root;
25491: 
25491:     ti->typeMap.captureMissingGlobalTypes(cx, *ti->globalSlots, ti->nStackTypes);
25491:     JS_ASSERT(ti->globalSlots->length() == ti->typeMap.length() - ti->nStackTypes);
25491: 
25491:     for (unsigned i = 0; i < root->dependentTrees.length(); i++) {
25491:         ti = (TreeInfo*)root->dependentTrees.data()[i]->vmprivate;
25491:         /* ti can be NULL if we hit the recording tree in emitTreeCall; this is harmless. */
25491:         if (ti && ti->nGlobalTypes() < ti->globalSlots->length())
25491:             specializeTreesToMissingGlobals(cx, ti);
25491:     }
25491:     for (unsigned i = 0; i < root->linkedTrees.length(); i++) {
25491:         ti = (TreeInfo*)root->linkedTrees.data()[i]->vmprivate;
25491:         if (ti && ti->nGlobalTypes() < ti->globalSlots->length())
25491:             specializeTreesToMissingGlobals(cx, ti);
25491:     }
25491: }
25491: 
18650: static void
18650: js_TrashTree(JSContext* cx, Fragment* f);
18650: 
22652: JS_REQUIRES_STACK
21521: TraceRecorder::TraceRecorder(JSContext* cx, VMSideExit* _anchor, Fragment* _fragment,
24246:         TreeInfo* ti, unsigned stackSlots, unsigned ngslots, uint8* typeMap,
25937:         VMSideExit* innermostNestedGuard, jsbytecode* outer)
18211: {
26286:     JS_ASSERT(!_fragment->vmprivate && ti);
18211: 
26557:     /* Reset the fragment state we care about in case we got a recycled fragment. */
26557:     _fragment->lastIns = NULL;
26557: 
17351:     this->cx = cx;
18239:     this->traceMonitor = &JS_TRACE_MONITOR(cx);
17657:     this->globalObj = JS_GetGlobalForObject(cx, cx->fp->scopeChain);
26238:     this->lexicalBlock = cx->fp->blockChain;
17738:     this->anchor = _anchor;
17397:     this->fragment = _fragment;
17701:     this->lirbuf = _fragment->lirbuf;
18211:     this->treeInfo = ti;
21521:     this->callDepth = _anchor ? _anchor->calldepth : 0;
24293:     this->atoms = FrameAtomBase(cx, cx->fp);
19068:     this->deepAborted = false;
22609:     this->trashSelf = false;
19653:     this->global_dslots = this->globalObj->dslots;
26557:     this->loop = true; /* default assumption is we are compiling a loop */
21723:     this->wasRootFragment = _fragment == _fragment->root;
25627:     this->outer = outer;
26552:     this->generatedTraceableNative = new JSTraceableNative();
26552:     JS_ASSERT(generatedTraceableNative);
18260: 
21685:     debug_only_v(printf("recording starting from %s:%u@%u\n",
25627:                         ti->treeFileName, ti->treeLineNumber, ti->treePCOffset);)
25469:     debug_only_v(printf("globalObj=%p, shape=%d\n", (void*)this->globalObj, OBJ_SHAPE(this->globalObj));)
17414: 
17370:     lir = lir_buf_writer = new (&gc) LirBufWriter(lirbuf);
23450:     debug_only_v(lir = verbose_filter = new (&gc) VerboseWriter(&gc, lir, lirbuf->names);)
26545:     if (nanojit::AvmCore::config.soft_float)
18773:         lir = float_filter = new (&gc) SoftFloatFilter(lir);
26545:     else
26545:         float_filter = 0;
17370:     lir = cse_filter = new (&gc) CseFilter(lir, &gc);
17370:     lir = expr_filter = new (&gc) ExprFilter(lir);
21799:     lir = func_filter = new (&gc) FuncFilter(lir);
20893:     lir->ins0(LIR_start);
17663: 
20946:     if (!nanojit::AvmCore::config.tree_opt || fragment->root == fragment)
20893:         lirbuf->state = addName(lir->insParam(0, 0), "state");
20893: 
18230:     lirbuf->sp = addName(lir->insLoad(LIR_ldp, lirbuf->state, (int)offsetof(InterpState, sp)), "sp");
18230:     lirbuf->rp = addName(lir->insLoad(LIR_ldp, lirbuf->state, offsetof(InterpState, rp)), "rp");
18230:     cx_ins = addName(lir->insLoad(LIR_ldp, lirbuf->state, offsetof(InterpState, cx)), "cx");
18230:     eos_ins = addName(lir->insLoad(LIR_ldp, lirbuf->state, offsetof(InterpState, eos)), "eos");
18230:     eor_ins = addName(lir->insLoad(LIR_ldp, lirbuf->state, offsetof(InterpState, eor)), "eor");
24282:     globalObj_ins = addName(lir->insLoad(LIR_ldp, lirbuf->state, offsetof(InterpState, globalObj)), "globalObj");
23918: 
24246:     /* If we came from exit, we might not have enough global types. */
25491:     if (ti->globalSlots->length() > ti->nGlobalTypes())
25491:         specializeTreesToMissingGlobals(cx, ti);
17334: 
17997:     /* read into registers all values on the stack and all globals we know so far */
24246:     import(treeInfo, lirbuf->sp, stackSlots, ngslots, callDepth, typeMap);
18284: 
22615:     if (fragment == fragment->root) {
25087:         /*
25087:          * We poll the operation callback request flag. It is updated asynchronously whenever 
25087:          * the callback is to be invoked.
25087:          */
25087:         LIns* x = lir->insLoadi(cx_ins, offsetof(JSContext, operationCallbackFlag));
25087:         guard(true, lir->ins_eq0(x), snapshot(TIMEOUT_EXIT));
23455:     }
22615: 
18284:     /* If we are attached to a tree call guard, make sure the guard the inner tree exited from
18284:        is what we expect it to be. */
20931:     if (_anchor && _anchor->exitType == NESTED_EXIT) {
18284:         LIns* nested_ins = addName(lir->insLoad(LIR_ldp, lirbuf->state,
19590:                                                 offsetof(InterpState, lastTreeExitGuard)),
19590:                                                 "lastTreeExitGuard");
18712:         guard(true, lir->ins2(LIR_eq, nested_ins, INS_CONSTPTR(innermostNestedGuard)), NESTED_EXIT);
18284:     }
17334: }
17334: 
25102: TreeInfo::~TreeInfo()
25102: {
25102:     UnstableExit* temp;
25102:     
25102:     while (unstableExits) {
25102:         temp = unstableExits->next;
25102:         delete unstableExits;
25102:         unstableExits = temp;
25102:     }
25102: }
21433: 
17334: TraceRecorder::~TraceRecorder()
17334: {
21723:     JS_ASSERT(nextRecorderToAbort == NULL);
21723:     JS_ASSERT(treeInfo && (fragment || wasDeepAborted()));
21767: #ifdef DEBUG
21767:     TraceRecorder* tr = JS_TRACE_MONITOR(cx).abortStack;
21767:     while (tr != NULL)
21767:     {
21767:         JS_ASSERT(this != tr);
21767:         tr = tr->nextRecorderToAbort;
21767:     }
21767: #endif
21723:     if (fragment) {
21723:         if (wasRootFragment && !fragment->root->code()) {
18211:             JS_ASSERT(!fragment->root->vmprivate);
25102:             delete treeInfo;
18211:         }
22609: 
22609:         if (trashSelf)
22612:             js_TrashTree(cx, fragment->root);
22609: 
22609:         for (unsigned int i = 0; i < whichTreesToTrash.length(); i++)
22609:             js_TrashTree(cx, whichTreesToTrash.get(i));
25102:     } else if (wasRootFragment) {
25102:         delete treeInfo;
21723:     }
17334: #ifdef DEBUG
17370:     delete verbose_filter;
17334: #endif
17370:     delete cse_filter;
17370:     delete expr_filter;
17453:     delete func_filter;
18773:     delete float_filter;
17370:     delete lir_buf_writer;
26552:     delete generatedTraceableNative;
17319: }
17319: 
21723: void TraceRecorder::removeFragmentoReferences()
21723: {
21723:     fragment = NULL;
21723: }
21723: 
17722: /* Add debug information to a LIR instruction as we emit it. */
17722: inline LIns*
17722: TraceRecorder::addName(LIns* ins, const char* name)
17722: {
24305: #ifdef JS_JIT_SPEW
24305:     if (js_verboseDebug)
17722:         lirbuf->names->addName(ins, name);
17722: #endif
17722:     return ins;
17722: }
17722: 
17346: /* Determine the current call depth (starting with the entry frame.) */
17346: unsigned
17442: TraceRecorder::getCallDepth() const
17346: {
17789:     return callDepth;
17346: }
17346: 
17815: /* Determine the offset in the native global frame for a jsval we track */
17811: ptrdiff_t
17815: TraceRecorder::nativeGlobalOffset(jsval* p) const
17815: {
17900:     JS_ASSERT(isGlobal(p));
17900:     if (size_t(p - globalObj->fslots) < JS_INITIAL_NSLOTS)
26282:         return sizeof(InterpState) + size_t(p - globalObj->fslots) * sizeof(double);
26282:     return sizeof(InterpState) + ((p - globalObj->dslots) + JS_INITIAL_NSLOTS) * sizeof(double);
17815: }
17815: 
17894: /* Determine whether a value is a global stack slot */
17893: bool
17893: TraceRecorder::isGlobal(jsval* p) const
17893: {
17893:     return ((size_t(p - globalObj->fslots) < JS_INITIAL_NSLOTS) ||
17893:             (size_t(p - globalObj->dslots) < (STOBJ_NSLOTS(globalObj) - JS_INITIAL_NSLOTS)));
17893: }
17893: 
17815: /* Determine the offset in the native stack for a jsval we track */
22652: JS_REQUIRES_STACK ptrdiff_t
17815: TraceRecorder::nativeStackOffset(jsval* p) const
17346: {
17814: #ifdef DEBUG
17814:     size_t slow_offset = 0;
18169:     FORALL_SLOTS_IN_PENDING_FRAMES(cx, callDepth,
17814:         if (vp == p) goto done;
17814:         slow_offset += sizeof(double)
17512:     );
17923: 
17923:     /*
17923:      * If it's not in a pending frame, it must be on the stack of the current frame above
17923:      * sp but below fp->slots + script->nslots.
17923:      */
17923:     JS_ASSERT(size_t(p - cx->fp->slots) < cx->fp->script->nslots);
17814:     slow_offset += size_t(p - cx->fp->regs->sp) * sizeof(double);
17923: 
17814: done:
17814: #define RETURN(offset) { JS_ASSERT((offset) == slow_offset); return offset; }
17814: #else
17814: #define RETURN(offset) { return offset; }
17814: #endif
17814:     size_t offset = 0;
17814:     JSStackFrame* currentFrame = cx->fp;
17814:     JSStackFrame* entryFrame;
17814:     JSStackFrame* fp = currentFrame;
17814:     for (unsigned n = 0; n < callDepth; ++n) { fp = fp->down; }
17814:     entryFrame = fp;
17814:     unsigned frames = callDepth+1;
17814:     JSStackFrame** fstack = (JSStackFrame **)alloca(frames * sizeof (JSStackFrame *));
17814:     JSStackFrame** fspstop = &fstack[frames];
17814:     JSStackFrame** fsp = fspstop-1;
17814:     fp = currentFrame;
17814:     for (;; fp = fp->down) { *fsp-- = fp; if (fp == entryFrame) break; }
17814:     for (fsp = fstack; fsp < fspstop; ++fsp) {
17949:         fp = *fsp;
17949:         if (fp->callee) {
17923:             if (fsp == fstack) {
18425:                 if (size_t(p - &fp->argv[-2]) < size_t(2/*callee,this*/ + fp->fun->nargs))
18187:                     RETURN(offset + size_t(p - &fp->argv[-2]) * sizeof(double));
18425:                 offset += (2/*callee,this*/ + fp->fun->nargs) * sizeof(double);
17923:             }
17949:             if (size_t(p - &fp->slots[0]) < fp->script->nfixed)
17949:                 RETURN(offset + size_t(p - &fp->slots[0]) * sizeof(double));
17949:             offset += fp->script->nfixed * sizeof(double);
17949:         }
17949:         jsval* spbase = StackBase(fp);
17949:         if (size_t(p - spbase) < size_t(fp->regs->sp - spbase))
17923:             RETURN(offset + size_t(p - spbase) * sizeof(double));
17949:         offset += size_t(fp->regs->sp - spbase) * sizeof(double);
18119:         if (fsp < fspstop - 1) {
18119:             JSStackFrame* fp2 = fsp[1];
18119:             int missing = fp2->fun->nargs - fp2->argc;
18119:             if (missing > 0) {
18119:                 if (size_t(p - fp->regs->sp) < size_t(missing))
18119:                     RETURN(offset + size_t(p - fp->regs->sp) * sizeof(double));
18119:                 offset += size_t(missing) * sizeof(double);
18119:             }
18110:         }
17923:     }
17923: 
17923:     /*
17923:      * If it's not in a pending frame, it must be on the stack of the current frame above
17923:      * sp but below fp->slots + script->nslots.
17923:      */
17923:     JS_ASSERT(size_t(p - currentFrame->slots) < currentFrame->script->nslots);
17538:     offset += size_t(p - currentFrame->regs->sp) * sizeof(double);
17814:     RETURN(offset);
17814: #undef RETURN
17346: }
17346: 
17397: /* Track the maximum number of native frame slots we need during
17397:    execution. */
17397: void
17815: TraceRecorder::trackNativeStackUse(unsigned slots)
17815: {
17815:     if (slots > treeInfo->maxNativeStackSlots)
17815:         treeInfo->maxNativeStackSlots = slots;
17397: }
17397: 
21433: /* Unbox a jsval into a slot. Slots are wide enough to hold double values directly (instead of
21433:    storing a pointer to them). We now assert instead of type checking, the caller must ensure the
21433:    types are compatible. */
21433: static void
18296: ValueToNative(JSContext* cx, jsval v, uint8 type, double* slot)
18296: {
18296:     unsigned tag = JSVAL_TAG(v);
18296:     switch (type) {
18296:       case JSVAL_INT:
17482:         jsint i;
17482:         if (JSVAL_IS_INT(v))
17482:             *(jsint*)slot = JSVAL_TO_INT(v);
18296:         else if ((tag == JSVAL_DOUBLE) && JSDOUBLE_IS_INT(*JSVAL_TO_DOUBLE(v), i))
17469:             *(jsint*)slot = i;
21433:         else
21433:             JS_ASSERT(JSVAL_IS_INT(v));
18260:         debug_only_v(printf("int<%d> ", *(jsint*)slot);)
21433:         return;
18296:       case JSVAL_DOUBLE:
17482:         jsdouble d;
17482:         if (JSVAL_IS_INT(v))
17482:             d = JSVAL_TO_INT(v);
21433:         else
17482:             d = *JSVAL_TO_DOUBLE(v);
21433:         JS_ASSERT(JSVAL_IS_INT(v) || JSVAL_IS_DOUBLE(v));
17482:         *(jsdouble*)slot = d;
18260:         debug_only_v(printf("double<%g> ", d);)
21433:         return;
18296:       case JSVAL_BOOLEAN:
24846:         /* Watch out for pseudo-booleans. */
21433:         JS_ASSERT(tag == JSVAL_BOOLEAN);
24846:         *(JSBool*)slot = JSVAL_TO_PSEUDO_BOOLEAN(v);
20002:         debug_only_v(printf("boolean<%d> ", *(JSBool*)slot);)
21433:         return;
17387:       case JSVAL_STRING:
21433:         JS_ASSERT(tag == JSVAL_STRING);
17387:         *(JSString**)slot = JSVAL_TO_STRING(v);
25469:         debug_only_v(printf("string<%p> ", (void*)(*(JSString**)slot));)
21433:         return;
23075:       case JSVAL_TNULL:
23075:         JS_ASSERT(tag == JSVAL_OBJECT);
23075:         *(JSObject**)slot = NULL;
23075:         return;
17387:       default:
20399:         /* Note: we should never see JSVAL_BOXED in an entry type map. */
18296:         JS_ASSERT(type == JSVAL_OBJECT);
21433:         JS_ASSERT(tag == JSVAL_OBJECT);
17546:         *(JSObject**)slot = JSVAL_TO_OBJECT(v);
25469:         debug_only_v(printf("object<%p:%s> ", (void*)JSVAL_TO_OBJECT(v),
17611:                             JSVAL_IS_NULL(v)
17611:                             ? "null"
17611:                             : STOBJ_GET_CLASS(JSVAL_TO_OBJECT(v))->name);)
21433:         return;
17360:     }
18296: }
17360: 
23449: /* We maintain an emergency pool of doubles so we can recover safely if a trace runs
19987:    out of memory (doubles or objects). */
19991: static jsval
23449: AllocateDoubleFromReservedPool(JSContext* cx)
19987: {
19987:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
23449:     JS_ASSERT(tm->reservedDoublePoolPtr > tm->reservedDoublePool);
23449:     return *--tm->reservedDoublePoolPtr;
19987: }
19987: 
19987: static bool
23449: js_ReplenishReservedPool(JSContext* cx, JSTraceMonitor* tm)
19991: {
19991:     /* We should not be called with a full pool. */
23449:     JS_ASSERT((size_t) (tm->reservedDoublePoolPtr - tm->reservedDoublePool) <
19991:               MAX_NATIVE_STACK_SLOTS);
19991: 
19991:     /*
19991:      * When the GC runs in js_NewDoubleInRootedValue, it resets
23449:      * tm->reservedDoublePoolPtr back to tm->reservedDoublePool.
19991:      */
19991:     JSRuntime* rt = cx->runtime;
19991:     uintN gcNumber = rt->gcNumber;
25490:     uintN lastgcNumber = gcNumber;
23449:     jsval* ptr = tm->reservedDoublePoolPtr;
23449:     while (ptr < tm->reservedDoublePool + MAX_NATIVE_STACK_SLOTS) {
21514:         if (!js_NewDoubleInRootedValue(cx, 0.0, ptr))
21514:             goto oom;
25490: 
25490:         /* Check if the last call to js_NewDoubleInRootedValue GC'd. */
25490:         if (rt->gcNumber != lastgcNumber) {
25490:             lastgcNumber = rt->gcNumber;
23449:             JS_ASSERT(tm->reservedDoublePoolPtr == tm->reservedDoublePool);
23449:             ptr = tm->reservedDoublePool;
25490: 
25490:             /*
25490:              * Have we GC'd more than once? We're probably running really
25490:              * low on memory, bail now.
25490:              */
21514:             if (uintN(rt->gcNumber - gcNumber) > uintN(1))
21514:                 goto oom;
21514:             continue;
21514:         }
21514:         ++ptr;
21514:     }
23449:     tm->reservedDoublePoolPtr = ptr;
21514:     return true;
21514: 
21514: oom:
21514:     /*
21514:      * Already massive GC pressure, no need to hold doubles back.
21514:      * We won't run any native code anyway.
21514:      */
23449:     tm->reservedDoublePoolPtr = tm->reservedDoublePool;
21514:     return false;
19987: }
19987: 
17387: /* Box a value from the native stack back into the jsval format. Integers
17387:    that are too large to fit into a jsval are automatically boxed into
17387:    heap-allocated doubles. */
23446: static void
17857: NativeToValue(JSContext* cx, jsval& v, uint8 type, double* slot)
17841: {
17483:     jsint i;
17469:     jsdouble d;
17494:     switch (type) {
17361:       case JSVAL_BOOLEAN:
24846:         /* Watch out for pseudo-booleans. */
24846:         v = PSEUDO_BOOLEAN_TO_JSVAL(*(JSBool*)slot);
20002:         debug_only_v(printf("boolean<%d> ", *(JSBool*)slot);)
17393:         break;
17361:       case JSVAL_INT:
17483:         i = *(jsint*)slot;
18260:         debug_only_v(printf("int<%d> ", i);)
17483:       store_int:
17469:         if (INT_FITS_IN_JSVAL(i)) {
17492:             v = INT_TO_JSVAL(i);
17393:             break;
17469:         }
17469:         d = (jsdouble)i;
17483:         goto store_double;
17361:       case JSVAL_DOUBLE:
17469:         d = *slot;
18260:         debug_only_v(printf("double<%g> ", d);)
17483:         if (JSDOUBLE_IS_INT(d, i))
17483:             goto store_int;
19987:       store_double: {
19987:         /* Its not safe to trigger the GC here, so use an emergency heap if we are out of
19987:            double boxes. */
19987:         if (cx->doubleFreeList) {
19987: #ifdef DEBUG
25628:             JSBool ok =
19987: #endif
19987:                 js_NewDoubleInRootedValue(cx, d, &v);
19987:             JS_ASSERT(ok);
23446:             return;
19987:         }
23449:         v = AllocateDoubleFromReservedPool(cx);
19991:         JS_ASSERT(JSVAL_IS_DOUBLE(v) && *JSVAL_TO_DOUBLE(v) == 0.0);
19991:         *JSVAL_TO_DOUBLE(v) = d;
23446:         return;
19987:       }
17361:       case JSVAL_STRING:
17492:         v = STRING_TO_JSVAL(*(JSString**)slot);
20427:         JS_ASSERT(JSVAL_TAG(v) == JSVAL_STRING); /* if this fails the pointer was not aligned */
25469:         debug_only_v(printf("string<%p> ", (void*)(*(JSString**)slot));)
17393:         break;
20399:       case JSVAL_BOXED:
20399:         v = *(jsval*)slot;
24842:         JS_ASSERT(v != JSVAL_ERROR_COOKIE); /* don't leak JSVAL_ERROR_COOKIE */
25477:         debug_only_v(printf("box<%p> ", (void*)v));
20399:         break;
23075:       case JSVAL_TNULL:
23075:         JS_ASSERT(*(JSObject**)slot == NULL);
23075:         v = JSVAL_NULL;
25469:         debug_only_v(printf("null<%p> ", (void*)(*(JSObject**)slot)));
23075:         break;
17361:       default:
17841:         JS_ASSERT(type == JSVAL_OBJECT);
17492:         v = OBJECT_TO_JSVAL(*(JSObject**)slot);
20427:         JS_ASSERT(JSVAL_TAG(v) == JSVAL_OBJECT); /* if this fails the pointer was not aligned */
24590:         JS_ASSERT(v != JSVAL_ERROR_COOKIE); /* don't leak JSVAL_ERROR_COOKIE */
25469:         debug_only_v(printf("object<%p:%s> ", (void*)JSVAL_TO_OBJECT(v),
17812:                             JSVAL_IS_NULL(v)
17812:                             ? "null"
17812:                             : STOBJ_GET_CLASS(JSVAL_TO_OBJECT(v))->name);)
17393:         break;
17393:     }
17361: }
17361: 
21433: /* Attempt to unbox the given list of interned globals onto the native global frame. */
22652: static JS_REQUIRES_STACK void
17857: BuildNativeGlobalFrame(JSContext* cx, unsigned ngslots, uint16* gslots, uint8* mp, double* np)
17857: {
18260:     debug_only_v(printf("global: ");)
17815:     FORALL_GLOBAL_SLOTS(cx, ngslots, gslots,
21433:         ValueToNative(cx, *vp, *mp, np + gslots[n]);
17900:         ++mp;
17512:     );
18260:     debug_only_v(printf("\n");)
21433: }
21433: 
21433: /* Attempt to unbox the given JS frame onto a native frame. */
22652: static JS_REQUIRES_STACK void
17857: BuildNativeStackFrame(JSContext* cx, unsigned callDepth, uint8* mp, double* np)
17857: {
18260:     debug_only_v(printf("stack: ");)
18169:     FORALL_SLOTS_IN_PENDING_FRAMES(cx, callDepth,
18260:         debug_only_v(printf("%s%u=", vpname, vpnum);)
21433:         ValueToNative(cx, *vp, *mp, np);
17857:         ++mp; ++np;
17857:     );
18260:     debug_only_v(printf("\n");)
17857: }
17857: 
23446: /* Box the given native frame into a JS frame. This is infallible. */
22652: static JS_REQUIRES_STACK int
17857: FlushNativeGlobalFrame(JSContext* cx, unsigned ngslots, uint16* gslots, uint8* mp, double* np)
17857: {
17857:     uint8* mp_base = mp;
17815:     FORALL_GLOBAL_SLOTS(cx, ngslots, gslots,
25491:         debug_only_v(printf("%s%u=", vpname, vpnum);)
23446:         NativeToValue(cx, *vp, *mp, np + gslots[n]);
17900:         ++mp;
17815:     );
18260:     debug_only_v(printf("\n");)
18170:     return mp - mp_base;
17857: }
17857: 
19076: /**
23446:  * Box the given native stack frame into the virtual machine stack. This
23446:  * is infallible.
19076:  *
19076:  * @param callDepth the distance between the entry frame into our trace and
19076:  *                  cx->fp when we make this call.  If this is not called as a
19076:  *                  result of a nested exit, callDepth is 0.
19076:  * @param mp pointer to an array of type tags (JSVAL_INT, etc.) that indicate
19076:  *           what the types of the things on the stack are.
19076:  * @param np pointer to the native stack.  We want to copy values from here to
19076:  *           the JS stack as needed.
19076:  * @param stopFrame if non-null, this frame and everything above it should not
19076:  *                  be restored.
19076:  * @return the number of things we popped off of np.
19076:  */
22652: static JS_REQUIRES_STACK int
19076: FlushNativeStackFrame(JSContext* cx, unsigned callDepth, uint8* mp, double* np,
19076:                       JSStackFrame* stopFrame)
19076: {
19076:     jsval* stopAt = stopFrame ? &stopFrame->argv[-2] : NULL;
17857:     uint8* mp_base = mp;
17857:     /* Root all string and object references first (we don't need to call the GC for this). */
18169:     FORALL_SLOTS_IN_PENDING_FRAMES(cx, callDepth,
19987:         if (vp == stopAt) goto skip;
19987:         debug_only_v(printf("%s%u=", vpname, vpnum);)
23446:         NativeToValue(cx, *vp, *mp, np);
17857:         ++mp; ++np
17857:     );
19987: skip:
17925:     // Restore thisp from the now-restored argv[-1] in each pending frame.
19076:     // Keep in mind that we didn't restore frames at stopFrame and above!
19076:     // Scope to keep |fp| from leaking into the macros we're using.
19076:     {
19076:         unsigned n = callDepth+1; // +1 to make sure we restore the entry frame
19076:         JSStackFrame* fp = cx->fp;
19076:         if (stopFrame) {
19076:             for (; fp != stopFrame; fp = fp->down) {
19076:                 JS_ASSERT(n != 0);
19076:                 --n;
19076:             }
19076:             // Skip over stopFrame itself.
19076:             JS_ASSERT(n != 0);
19076:             --n;
19076:             fp = fp->down;
19076:         }
19076:         for (; n != 0; fp = fp->down) {
19076:             --n;
19076:             if (fp->callee) { // might not have it if the entry frame is global
19076:                 JS_ASSERT(JSVAL_IS_OBJECT(fp->argv[-1]));
17949:                 fp->thisp = JSVAL_TO_OBJECT(fp->argv[-1]);
19076:             }
19076:         }
19076:     }
18260:     debug_only_v(printf("\n");)
18170:     return mp - mp_base;
17361: }
17361: 
17363: /* Emit load instructions onto the trace that read the initial stack state. */
22652: JS_REQUIRES_STACK void
17815: TraceRecorder::import(LIns* base, ptrdiff_t offset, jsval* p, uint8& t,
18045:                       const char *prefix, uintN index, JSStackFrame *fp)
17319: {
17480:     LIns* ins;
17841:     if (t == JSVAL_INT) { /* demoted */
17482:         JS_ASSERT(isInt32(*p));
17480:         /* Ok, we have a valid demotion attempt pending, so insert an integer
17480:            read and promote it to double since all arithmetic operations expect
17480:            to see doubles on entry. The first op to use this slot will emit a
17480:            f2i cast which will cancel out the i2f we insert here. */
17803:         ins = lir->insLoadi(base, offset);
17803:         ins = lir->ins1(LIR_i2f, ins);
17480:     } else {
26286:         JS_ASSERT(t == JSVAL_BOXED || isNumber(*p) == (t == JSVAL_DOUBLE));
18232:         if (t == JSVAL_DOUBLE) {
18232:             ins = lir->insLoad(LIR_ldq, base, offset);
20393:         } else if (t == JSVAL_BOOLEAN) {
20393:             ins = lir->insLoad(LIR_ld, base, offset);
18232:         } else {
18232:             ins = lir->insLoad(LIR_ldp, base, offset);
18232:         }
17480:     }
24381:     checkForGlobalObjectReallocation();
17372:     tracker.set(p, ins);
17372: #ifdef DEBUG
17737:     char name[64];
17372:     JS_ASSERT(strlen(prefix) < 10);
17925:     void* mark = NULL;
17925:     jsuword* localNames = NULL;
18011:     const char* funName = NULL;
17925:     if (*prefix == 'a' || *prefix == 'v') {
17925:         mark = JS_ARENA_MARK(&cx->tempPool);
18597:         if (JS_GET_LOCAL_NAME_COUNT(fp->fun) != 0)
17925:             localNames = js_GetLocalNameArray(cx, fp->fun, &cx->tempPool);
18011:         funName = fp->fun->atom ? js_AtomToPrintableString(cx, fp->fun->atom) : "<anonymous>";
18011:     }
17737:     if (!strcmp(prefix, "argv")) {
18045:         if (index < fp->fun->nargs) {
17737:             JSAtom *atom = JS_LOCAL_NAME_TO_ATOM(localNames[index]);
18011:             JS_snprintf(name, sizeof name, "$%s.%s", funName, js_AtomToPrintableString(cx, atom));
18011:         } else {
18011:             JS_snprintf(name, sizeof name, "$%s.<arg%d>", funName, index);
18011:         }
17737:     } else if (!strcmp(prefix, "vars")) {
17925:         JSAtom *atom = JS_LOCAL_NAME_TO_ATOM(localNames[fp->fun->nargs + index]);
18011:         JS_snprintf(name, sizeof name, "$%s.%s", funName, js_AtomToPrintableString(cx, atom));
17737:     } else {
17379:         JS_snprintf(name, sizeof name, "$%s%d", prefix, index);
17737:     }
17925: 
17925:     if (mark)
17925:         JS_ARENA_RELEASE(&cx->tempPool, mark);
17721:     addName(ins, name);
17737: 
17587:     static const char* typestr[] = {
17587:         "object", "int", "double", "3", "string", "5", "boolean", "any"
17587:     };
21685:     debug_only_v(printf("import vp=%p name=%s type=%s flags=%d\n",
25469:                         (void*)p, name, typestr[t & 7], t >> 3);)
17372: #endif
17317: }
17317: 
22652: JS_REQUIRES_STACK void
24246: TraceRecorder::import(TreeInfo* treeInfo, LIns* sp, unsigned stackSlots, unsigned ngslots,
24246:                       unsigned callDepth, uint8* typeMap)
17997: {
18214:     /* If we get a partial list that doesn't have all the types (i.e. recording from a side
18214:        exit that was recorded but we added more global slots later), merge the missing types
18214:        from the entry type map. This is safe because at the loop edge we verify that we
18214:        have compatible types for all globals (entry type and loop edge type match). While
18214:        a different trace of the tree might have had a guard with a different type map for
18214:        these slots we just filled in here (the guard we continue from didn't know about them),
18214:        since we didn't take that particular guard the only way we could have ended up here
18214:        is if that other trace had at its end a compatible type distribution with the entry
18214:        map. Since thats exactly what we used to fill in the types our current side exit
18214:        didn't provide, this is always safe to do. */
24246: 
24246:     uint8* globalTypeMap = typeMap + stackSlots;
24491:     unsigned length = treeInfo->nGlobalTypes();
24246: 
24246:     /* This is potentially the typemap of the side exit and thus shorter than the tree's
24246:        global type map. */
24246:     if (ngslots < length)
24246:         mergeTypeMaps(&globalTypeMap/*out param*/, &ngslots/*out param*/,
24246:                       treeInfo->globalTypeMap(), length,
18214:                       (uint8*)alloca(sizeof(uint8) * length));
24491:     JS_ASSERT(ngslots == treeInfo->nGlobalTypes());
18214: 
17997:     /* the first time we compile a tree this will be empty as we add entries lazily */
24491:     uint16* gslots = treeInfo->globalSlots->data();
17997:     uint8* m = globalTypeMap;
17997:     FORALL_GLOBAL_SLOTS(cx, ngslots, gslots,
26282:         import(lirbuf->state, nativeGlobalOffset(vp), vp, *m, vpname, vpnum, NULL);
17997:         m++;
17997:     );
17997:     ptrdiff_t offset = -treeInfo->nativeStackBase;
24246:     m = typeMap;
18169:     FORALL_SLOTS_IN_PENDING_FRAMES(cx, callDepth,
18138:         import(sp, offset, vp, *m, vpname, vpnum, fp);
17997:         m++; offset += sizeof(double);
17997:     );
17997: }
17997: 
25938: JS_REQUIRES_STACK bool
25938: TraceRecorder::isValidSlot(JSScope* scope, JSScopeProperty* sprop)
25938: {
25938:     uint32 setflags = (js_CodeSpec[*cx->fp->regs->pc].format & (JOF_SET | JOF_INCDEC | JOF_FOR));
25938: 
25938:     if (setflags) {
25938:         if (!SPROP_HAS_STUB_SETTER(sprop))
25938:             ABORT_TRACE("non-stub setter");
25938:         if (sprop->attrs & JSPROP_READONLY)
25938:             ABORT_TRACE("writing to a read-only property");
25938:     }
25938:     /* This check applies even when setflags == 0. */
25938:     if (setflags != JOF_SET && !SPROP_HAS_STUB_GETTER(sprop))
25938:         ABORT_TRACE("non-stub getter");
25938: 
25938:     if (!SPROP_HAS_VALID_SLOT(sprop, scope))
25938:         ABORT_TRACE("slotless obj property");
25938: 
25938:     return true;
25938: }
25938: 
17894: /* Lazily import a global slot if we don't already have it in the tracker. */
22652: JS_REQUIRES_STACK bool
17892: TraceRecorder::lazilyImportGlobalSlot(unsigned slot)
17891: {
18712:     if (slot != uint16(slot)) /* we use a table of 16-bit ints, bail out if that's not enough */
17891:         return false;
17891:     jsval* vp = &STOBJ_GET_SLOT(globalObj, slot);
24381:     if (known(vp))
17891:         return true; /* we already have it */
24491:     unsigned index = treeInfo->globalSlots->length();
18210:     /* Add the slot to the list of interned global slots. */
25491:     JS_ASSERT(treeInfo->nGlobalTypes() == treeInfo->globalSlots->length());
24491:     treeInfo->globalSlots->add(slot);
17981:     uint8 type = getCoercedType(*vp);
22613:     if ((type == JSVAL_INT) && oracle.isGlobalSlotUndemotable(cx, slot))
17981:         type = JSVAL_DOUBLE;
24246:     treeInfo->typeMap.add(type);
26282:     import(lirbuf->state, sizeof(struct InterpState) + slot*sizeof(double),
26282:            vp, type, "global", index, NULL);
25491:     specializeTreesToMissingGlobals(cx, treeInfo);
17891:     return true;
17891: }
17891: 
18197: /* Write back a value onto the stack or global frames. */
18197: LIns*
18197: TraceRecorder::writeBack(LIns* i, LIns* base, ptrdiff_t offset)
18197: {
17792:     /* Sink all type casts targeting the stack into the side exit by simply storing the original
17792:        (uncasted) value. Each guard generates the side exit map based on the types of the
17792:        last stores to every stack location, so its safe to not perform them on-trace. */
17792:     if (isPromoteInt(i))
17792:         i = ::demote(lir, i);
18197:     return lir->insStorei(i, base, offset);
18197: }
18197: 
18197: /* Update the tracker, then issue a write back store. */
22652: JS_REQUIRES_STACK void
18197: TraceRecorder::set(jsval* p, LIns* i, bool initializing)
18197: {
26018:     JS_ASSERT(i != NULL);
24381:     JS_ASSERT(initializing || known(p));
24381:     checkForGlobalObjectReallocation();
18197:     tracker.set(p, i);
17803:     /* If we are writing to this location for the first time, calculate the offset into the
17803:        native frame manually, otherwise just look up the last load or store associated with
17803:        the same source address (p) and use the same offset/base. */
19068:     LIns* x = nativeFrameTracker.get(p);
19068:     if (!x) {
18197:         if (isGlobal(p))
26282:             x = writeBack(i, lirbuf->state, nativeGlobalOffset(p));
18197:         else
18197:             x = writeBack(i, lirbuf->sp, -treeInfo->nativeStackBase + nativeStackOffset(p));
17962:         nativeFrameTracker.set(p, x);
17803:     } else {
17815: #define ASSERT_VALID_CACHE_HIT(base, offset)                                  \
26282:     JS_ASSERT(base == lirbuf->sp || base == lirbuf->state);                   \
17815:     JS_ASSERT(offset == ((base == lirbuf->sp)                                 \
17962:         ? -treeInfo->nativeStackBase + nativeStackOffset(p)                   \
17815:         : nativeGlobalOffset(p)));                                            \
17815: 
18123:         if (x->isop(LIR_st) || x->isop(LIR_stq)) {
17815:             ASSERT_VALID_CACHE_HIT(x->oprnd2(), x->oprnd3()->constval());
18197:             writeBack(i, x->oprnd2(), x->oprnd3()->constval());
17803:         } else {
17811:             JS_ASSERT(x->isop(LIR_sti) || x->isop(LIR_stqi));
17815:             ASSERT_VALID_CACHE_HIT(x->oprnd2(), x->immdisp());
18197:             writeBack(i, x->oprnd2(), x->immdisp());
17803:         }
17803:     }
17815: #undef ASSERT_VALID_CACHE_HIT
17320: }
17320: 
22652: JS_REQUIRES_STACK LIns*
24381: TraceRecorder::get(jsval* p)
24381: {
24381:     checkForGlobalObjectReallocation();
17320:     return tracker.get(p);
17320: }
17320: 
24381: JS_REQUIRES_STACK bool
24381: TraceRecorder::known(jsval* p)
24381: {
24381:     checkForGlobalObjectReallocation();
24381:     return tracker.has(p);
24381: }
24381: 
24381: /*
24381:  * The dslots of the global object are sometimes reallocated by the interpreter.
24381:  * This function check for that condition and re-maps the entries of the tracker
24381:  * accordingly.
24381:  */
24381: JS_REQUIRES_STACK void
24381: TraceRecorder::checkForGlobalObjectReallocation()
24381: {
24381:     if (global_dslots != globalObj->dslots) {
24381:         debug_only_v(printf("globalObj->dslots relocated, updating tracker\n");)
24381:         jsval* src = global_dslots;
24381:         jsval* dst = globalObj->dslots;
24381:         jsuint length = globalObj->dslots[-1] - JS_INITIAL_NSLOTS;
24381:         LIns** map = (LIns**)alloca(sizeof(LIns*) * length);
24381:         for (jsuint n = 0; n < length; ++n) {
24381:             map[n] = tracker.get(src);
24381:             tracker.set(src++, NULL);
24381:         }
24381:         for (jsuint n = 0; n < length; ++n)
24381:             tracker.set(dst++, map[n]);
24381:         global_dslots = globalObj->dslots;
24381:     }
24381: }
24381: 
20416: /* Determine whether the current branch is a loop edge (taken or not taken). */
22652: static JS_REQUIRES_STACK bool
20416: js_IsLoopEdge(jsbytecode* pc, jsbytecode* header)
20416: {
20416:     switch (*pc) {
20416:       case JSOP_IFEQ:
20416:       case JSOP_IFNE:
20416:         return ((pc + GET_JUMP_OFFSET(pc)) == header);
20416:       case JSOP_IFEQX:
20416:       case JSOP_IFNEX:
20416:         return ((pc + GET_JUMPX_OFFSET(pc)) == header);
20416:       default:
20416:         JS_ASSERT((*pc == JSOP_AND) || (*pc == JSOP_ANDX) ||
20416:                   (*pc == JSOP_OR) || (*pc == JSOP_ORX));
20416:     }
20416:     return false;
20416: }
20416: 
18250: /* Promote slots if necessary to match the called tree' type map and report error if thats
18250:    impossible. */
22652: JS_REQUIRES_STACK bool
24246: TraceRecorder::adjustCallerTypes(Fragment* f)
18250: {
24491:     uint16* gslots = treeInfo->globalSlots->data();
24491:     unsigned ngslots = treeInfo->globalSlots->length();
24491:     JS_ASSERT(ngslots == treeInfo->nGlobalTypes());
24246:     TreeInfo* ti = (TreeInfo*)f->vmprivate;
18595:     bool ok = true;
24246:     uint8* map = ti->globalTypeMap();
24246:     uint8* m = map;
18250:     FORALL_GLOBAL_SLOTS(cx, ngslots, gslots,
18250:         LIns* i = get(vp);
18250:         bool isPromote = isPromoteInt(i);
18250:         if (isPromote && *m == JSVAL_DOUBLE)
26282:             lir->insStorei(get(vp), lirbuf->state, nativeGlobalOffset(vp));
18595:         else if (!isPromote && *m == JSVAL_INT) {
24246:             debug_only_v(printf("adjusting will fail, %s%d, slot %d\n", vpname, vpnum, m - map);)
24246:             oracle.markGlobalSlotUndemotable(cx, gslots[n]);
18595:             ok = false;
18595:         }
18250:         ++m;
18250:     );
24491:     JS_ASSERT(unsigned(m - map) == ti->nGlobalTypes());
24246:     map = ti->stackTypeMap();
18595:     m = map;
18265:     FORALL_SLOTS_IN_PENDING_FRAMES(cx, 0,
18250:         LIns* i = get(vp);
18250:         bool isPromote = isPromoteInt(i);
21433:         if (isPromote && *m == JSVAL_DOUBLE) {
18250:             lir->insStorei(get(vp), lirbuf->sp,
18250:                            -treeInfo->nativeStackBase + nativeStackOffset(vp));
21433:             /* Aggressively undo speculation so the inner tree will compile if this fails. */
24246:             oracle.markStackSlotUndemotable(cx, unsigned(m - map));
21433:         } else if (!isPromote && *m == JSVAL_INT) {
21433:             debug_only_v(printf("adjusting will fail, %s%d, slot %d\n", vpname, vpnum, m - map);)
18595:             ok = false;
24246:             oracle.markStackSlotUndemotable(cx, unsigned(m - map));
21433:         } else if (JSVAL_IS_INT(*vp) && *m == JSVAL_DOUBLE) {
21433:             /* Aggressively undo speculation so the inner tree will compile if this fails. */
24246:             oracle.markStackSlotUndemotable(cx, unsigned(m - map));
18595:         }
18250:         ++m;
18250:     );
24491:     JS_ASSERT(unsigned(m - map) == ti->nStackTypes);
18650:     JS_ASSERT(f == f->root);
18595:     return ok;
18250: }
18250: 
22652: JS_REQUIRES_STACK uint8
24381: TraceRecorder::determineSlotType(jsval* vp)
19084: {
19084:     uint8 m;
19084:     LIns* i = get(vp);
19084:     m = isNumber(*vp)
19084:         ? (isPromoteInt(i) ? JSVAL_INT : JSVAL_DOUBLE)
23075:         : JSVAL_IS_NULL(*vp)
23075:         ? JSVAL_TNULL
19084:         : JSVAL_TAG(*vp);
19084:     JS_ASSERT((m != JSVAL_INT) || isInt32(*vp));
19084:     return m;
19084: }
19084: 
22652: JS_REQUIRES_STACK LIns*
17850: TraceRecorder::snapshot(ExitType exitType)
17850: {
17923:     JSStackFrame* fp = cx->fp;
20969:     JSFrameRegs* regs = fp->regs;
20969:     jsbytecode* pc = regs->pc;
20969: 
20969:     /* Check for a return-value opcode that needs to restart at the next instruction. */
20969:     const JSCodeSpec& cs = js_CodeSpec[*pc];
20969: 
26286:     /* WARNING: don't return before restoring the original pc if (resumeAfter). */
20969:     bool resumeAfter = (pendingTraceableNative &&
24612:                         JSTN_ERRTYPE(pendingTraceableNative) == FAIL_STATUS);
20969:     if (resumeAfter) {
24384:         JS_ASSERT(*pc == JSOP_CALL || *pc == JSOP_APPLY);
20969:         pc += cs.length;
20969:         regs->pc = pc;
21476:         MUST_FLOW_THROUGH("restore_pc");
20969:     }
20969: 
20969:     /* Generate the entry map for the (possibly advanced) pc and stash it in the trace. */
18425:     unsigned stackSlots = js_NativeStackSlots(cx, callDepth);
20969: 
18226:     /* It's sufficient to track the native stack use here since all stores above the
17962:        stack watermark defined by guards are killed. */
17962:     trackNativeStackUse(stackSlots + 1);
20969: 
20947:     /* Capture the type map into a temporary location. */
24491:     unsigned ngslots = treeInfo->globalSlots->length();
20947:     unsigned typemap_size = (stackSlots + ngslots) * sizeof(uint8);
20947:     uint8* typemap = (uint8*)alloca(typemap_size);
20947:     uint8* m = typemap;
20969: 
20947:     /* Determine the type of a store by looking at the current type of the actual value the
20947:        interpreter is using. For numbers we have to check what kind of store we used last
20947:        (integer or double) to figure out what the side exit show reflect in its typemap. */
24491:     FORALL_SLOTS(cx, ngslots, treeInfo->globalSlots->data(), callDepth,
20947:         *m++ = determineSlotType(vp);
20947:     );
20947:     JS_ASSERT(unsigned(m - typemap) == ngslots + stackSlots);
20969: 
26286:     /* If we are capturing the stack state on a specific instruction, the value on
26286:        the top of the stack is a boxed value. */
26286:     if (resumeAfter) {
26286:         if (pendingTraceableNative->flags & JSTN_UNBOX_AFTER)
26284:             typemap[stackSlots - 1] = JSVAL_BOXED;
26284: 
26284:         /* Now restore the the original pc (after which early returns are ok). */
20969:         MUST_FLOW_LABEL(restore_pc);
20969:         regs->pc = pc - cs.length;
20969:     } else {
20947:         /* If we take a snapshot on a goto, advance to the target address. This avoids inner
20947:            trees returning on a break goto, which the outer recorder then would confuse with
20947:            a break in the outer tree. */
20947:         if (*pc == JSOP_GOTO)
20947:             pc += GET_JUMP_OFFSET(pc);
20947:         else if (*pc == JSOP_GOTOX)
20947:             pc += GET_JUMPX_OFFSET(pc);
20969:     }
20969: 
23918:     JS_STATIC_ASSERT (sizeof(GuardRecord) + sizeof(VMSideExit) < MAX_SKIP_BYTES);
23918: 
20947:     /* Check if we already have a matching side exit. If so use that side exit structure,
20947:        otherwise we have to create our own. */
21521:     VMSideExit** exits = treeInfo->sideExits.data();
20947:     unsigned nexits = treeInfo->sideExits.length();
20957:     if (exitType == LOOP_EXIT) {
20947:         for (unsigned n = 0; n < nexits; ++n) {
21521:             VMSideExit* e = exits[n];
25111:             if (e->pc == pc && e->imacpc == fp->imacpc &&
24246:                 !memcmp(getFullTypeMap(exits[n]), typemap, typemap_size)) {
24850:                 LIns* data = lir->skip(sizeof(GuardRecord));
20947:                 GuardRecord* rec = (GuardRecord*)data->payload();
20947:                 /* setup guard record structure with shared side exit */
20947:                 memset(rec, 0, sizeof(GuardRecord));
21521:                 VMSideExit* exit = exits[n];
20947:                 rec->exit = exit;
20947:                 exit->addGuard(rec);
20947:                 AUDIT(mergedLoopExits);
20947:                 return data;
20947:             }
20947:         }
20957:     }
20969: 
23918:     if (sizeof(GuardRecord) +
23918:         sizeof(VMSideExit) +
23918:         (stackSlots + ngslots) * sizeof(uint8) >= MAX_SKIP_BYTES) {
23918:         /**
23918:          * ::snapshot() is infallible in the sense that callers don't
23918:          * expect errors; but this is a trace-aborting error condition. So
23918:          * mangle the request to consume zero slots, and mark the tree as
23918:          * to-be-trashed. This should be safe as the trace will be aborted
23918:          * before assembly or execution due to the call to
23918:          * trackNativeStackUse above.
23918:          */
23918:         stackSlots = 0;
23918:         ngslots = 0;
23918:         trashSelf = true;
23918:     }
23918: 
20947:     /* We couldn't find a matching side exit, so create our own side exit structure. */
24850:     LIns* data = lir->skip(sizeof(GuardRecord) +
21521:                            sizeof(VMSideExit) +
20931:                            (stackSlots + ngslots) * sizeof(uint8));
20931:     GuardRecord* rec = (GuardRecord*)data->payload();
21521:     VMSideExit* exit = (VMSideExit*)(rec + 1);
20931:     /* setup guard record structure */
20931:     memset(rec, 0, sizeof(GuardRecord));
20931:     rec->exit = exit;
17381:     /* setup side exit structure */
21521:     memset(exit, 0, sizeof(VMSideExit));
20931:     exit->from = fragment;
20931:     exit->calldepth = callDepth;
20931:     exit->numGlobalSlots = ngslots;
20931:     exit->numStackSlots = stackSlots;
20931:     exit->numStackSlotsBelowCurrentFrame = cx->fp->callee
18624:         ? nativeStackOffset(&cx->fp->argv[-2])/sizeof(double)
18624:         : 0;
20931:     exit->exitType = exitType;
20946:     exit->addGuard(rec);
22925:     exit->block = fp->blockChain;
25111:     exit->pc = pc;
25111:     exit->imacpc = fp->imacpc;
20931:     exit->sp_adj = (stackSlots * sizeof(double)) - treeInfo->nativeStackBase;
23262:     exit->rp_adj = exit->calldepth * sizeof(FrameInfo*);
24246:     memcpy(getFullTypeMap(exit), typemap, typemap_size);
20969: 
20947:     /* BIG FAT WARNING: If compilation fails, we currently don't reset the lirbuf so its safe
20947:        to keep references to the side exits here. If we ever start rewinding those lirbufs,
20947:        we have to make sure we purge the side exits that then no longer will be in valid
20947:        memory. */
20957:     if (exitType == LOOP_EXIT)
20947:         treeInfo->sideExits.add(exit);
20931:     return data;
17381: }
17381: 
21083: /* Emit a guard for condition (cond), expecting to evaluate to boolean result (expected)
21083:    and using the supplied side exit if the conditon doesn't hold. */
26286: LIns*
21685: TraceRecorder::guard(bool expected, LIns* cond, LIns* exit)
21083: {
26265:     if (!cond->isCond()) {
26265:         expected = !expected;
26265:         cond = lir->ins_eq0(cond);
26265:     }
26286:     return lir->insGuard(expected ? LIR_xf : LIR_xt, cond, exit);
21083: }
21083: 
21083: /* Emit a guard for condition (cond), expecting to evaluate to boolean result (expected)
21083:    and generate a side exit with type exitType to jump to if the condition does not hold. */
26286: JS_REQUIRES_STACK LIns*
17850: TraceRecorder::guard(bool expected, LIns* cond, ExitType exitType)
17323: {
26286:     return guard(expected, cond, snapshot(exitType));
17336: }
17336: 
17894: /* Try to match the type of a slot to type t. checkType is used to verify that the type of
21433:  * values flowing into the loop edge is compatible with the type we expect in the loop header.
21433:  *
21433:  * @param v             Value.
21433:  * @param t             Typemap entry for value.
21433:  * @param stage_val     Outparam for set() address.
21433:  * @param stage_ins     Outparam for set() instruction.
21433:  * @param stage_count   Outparam for set() buffer count.
21433:  * @return              True if types are compatible, false otherwise.
21433:  */
22652: JS_REQUIRES_STACK bool
21433: TraceRecorder::checkType(jsval& v, uint8 t, jsval*& stage_val, LIns*& stage_ins,
21433:                          unsigned& stage_count)
17453: {
17841:     if (t == JSVAL_INT) { /* initially all whole numbers cause the slot to be demoted */
21433:         debug_only_v(printf("checkType(tag=1, t=%d, isnum=%d, i2f=%d) stage_count=%d\n",
21433:                             t,
21433:                             isNumber(v),
21433:                             isPromoteInt(get(&v)),
21433:                             stage_count);)
17841:         if (!isNumber(v))
17841:             return false; /* not a number? type mismatch */
17472:         LIns* i = get(&v);
21433:         /* This is always a type mismatch, we can't close a double to an int. */
21433:         if (!isPromoteInt(i))
21433:             return false;
20973:         /* Looks good, slot is an int32, the last instruction should be promotable. */
20973:         JS_ASSERT(isInt32(v) && isPromoteInt(i));
20973:         /* Overwrite the value in this slot with the argument promoted back to an integer. */
21433:         stage_val = &v;
21433:         stage_ins = f2i(i);
21433:         stage_count++;
17479:         return true;
17479:     }
18275:     if (t == JSVAL_DOUBLE) {
21433:         debug_only_v(printf("checkType(tag=2, t=%d, isnum=%d, promote=%d) stage_count=%d\n",
21433:                             t,
21433:                             isNumber(v),
21433:                             isPromoteInt(get(&v)),
21433:                             stage_count);)
18275:         if (!isNumber(v))
18275:             return false; /* not a number? type mismatch */
18275:         LIns* i = get(&v);
21685:         /* We sink i2f conversions into the side exit, but at the loop edge we have to make
18275:            sure we promote back to double if at loop entry we want a double. */
21433:         if (isPromoteInt(i)) {
21433:             stage_val = &v;
21433:             stage_ins = lir->ins1(LIR_i2f, i);
21433:             stage_count++;
21433:         }
18275:         return true;
18275:     }
23075:     if (t == JSVAL_TNULL && JSVAL_IS_NULL(v))
23075:         return true;
17479:     /* for non-number types we expect a precise match of the type */
23075:     uint8 vt = getCoercedType(v);
17500: #ifdef DEBUG
23075:     if (vt != t) {
23075:         debug_only_v(printf("Type mismatch: val %c, map %c ", typeChar[vt],
21685:                             typeChar[t]);)
17500:     }
17500: #endif
23075:     debug_only_v(printf("checkType(vt=%d, t=%d) stage_count=%d\n",
23075:                         (int) vt, t, stage_count);)
23075:     return vt == t;
17453: }
17453: 
21433: /**
21433:  * Make sure that the current values in the given stack frame and all stack frames
21433:  * up and including entryFrame are type-compatible with the entry map.
21433:  *
21433:  * @param root_peer         First fragment in peer list.
21433:  * @param stable_peer       Outparam for first type stable peer.
24246:  * @param demote            True if stability was achieved through demotion.
21433:  * @return                  True if type stable, false otherwise.
21433:  */
22652: JS_REQUIRES_STACK bool
24246: TraceRecorder::deduceTypeStability(Fragment* root_peer, Fragment** stable_peer, bool& demote)
21433: {
21433:     uint8* m;
21433:     uint8* typemap;
24491:     unsigned ngslots = treeInfo->globalSlots->length();
24491:     uint16* gslots = treeInfo->globalSlots->data();
24491:     JS_ASSERT(ngslots == treeInfo->nGlobalTypes());
21433: 
21433:     if (stable_peer)
21433:         *stable_peer = NULL;
21433: 
21519:     /*
21519:      * Rather than calculate all of this stuff twice, it gets cached locally.  The "stage" buffers
21433:      * are for calls to set() that will change the exit types.
21433:      */
21433:     bool success;
21433:     unsigned stage_count;
24246:     jsval** stage_vals = (jsval**)alloca(sizeof(jsval*) * (treeInfo->typeMap.length()));
24246:     LIns** stage_ins = (LIns**)alloca(sizeof(LIns*) * (treeInfo->typeMap.length()));
21433: 
21433:     /* First run through and see if we can close ourselves - best case! */
21433:     stage_count = 0;
21433:     success = false;
21433: 
25469:     debug_only_v(printf("Checking type stability against self=%p\n", (void*)fragment);)
21433: 
24246:     m = typemap = treeInfo->globalTypeMap();
17891:     FORALL_GLOBAL_SLOTS(cx, ngslots, gslots,
21433:         debug_only_v(printf("%s%d ", vpname, vpnum);)
21433:         if (!checkType(*vp, *m, stage_vals[stage_count], stage_ins[stage_count], stage_count)) {
21433:             /* If the failure was an int->double, tell the oracle. */
24246:             if (*m == JSVAL_INT && isNumber(*vp) && !isPromoteInt(get(vp))) {
22613:                 oracle.markGlobalSlotUndemotable(cx, gslots[n]);
24246:                 demote = true;
24246:             } else {
21433:                 goto checktype_fail_1;
21433:             }
24246:         }
21433:         ++m;
17891:     );
24246:     m = typemap = treeInfo->stackTypeMap();
21433:     FORALL_SLOTS_IN_PENDING_FRAMES(cx, 0,
21433:         debug_only_v(printf("%s%d ", vpname, vpnum);)
21433:         if (!checkType(*vp, *m, stage_vals[stage_count], stage_ins[stage_count], stage_count)) {
24246:             if (*m == JSVAL_INT && isNumber(*vp) && !isPromoteInt(get(vp))) {
24246:                 oracle.markStackSlotUndemotable(cx, unsigned(m - typemap));
24246:                 demote = true;
24246:             } else {
21433:                 goto checktype_fail_1;
21433:             }
24246:         }
21433:         ++m;
17512:     );
21433: 
21433:     success = true;
21433: 
21433: checktype_fail_1:
21433:     /* If we got a success and we don't need to recompile, we should just close here. */
24246:     if (success && !demote) {
21433:         for (unsigned i = 0; i < stage_count; i++)
21433:             set(stage_vals[i], stage_ins[i]);
21433:         return true;
21433:     /* If we need to trash, don't bother checking peers. */
22609:     } else if (trashSelf) {
21433:         return false;
24246:     }
24246: 
24246:     demote = false;
21433: 
21433:     /* At this point the tree is about to be incomplete, so let's see if we can connect to any
21433:      * peer fragment that is type stable.
21433:      */
21433:     Fragment* f;
21433:     TreeInfo* ti;
21433:     for (f = root_peer; f != NULL; f = f->peer) {
25469:         debug_only_v(printf("Checking type stability against peer=%p (code=%p)\n", (void*)f, f->code());)
21433:         if (!f->code())
21433:             continue;
21433:         ti = (TreeInfo*)f->vmprivate;
21433:         /* Don't allow varying stack depths */
24491:         if ((ti->nStackTypes != treeInfo->nStackTypes) ||
24491:             (ti->typeMap.length() != treeInfo->typeMap.length()) ||
24491:             (ti->globalSlots->length() != treeInfo->globalSlots->length()))
21433:             continue;
21433:         stage_count = 0;
21433:         success = false;
24246: 
24246:         m = ti->globalTypeMap();
24491:         FORALL_GLOBAL_SLOTS(cx, treeInfo->globalSlots->length(), treeInfo->globalSlots->data(),
24246:                 if (!checkType(*vp, *m, stage_vals[stage_count], stage_ins[stage_count], stage_count))
24246:                     goto checktype_fail_2;
24246:                 ++m;
24246:             );
24246: 
24246:         m = ti->stackTypeMap();
21433:         FORALL_SLOTS_IN_PENDING_FRAMES(cx, 0,
21433:                 if (!checkType(*vp, *m, stage_vals[stage_count], stage_ins[stage_count], stage_count))
21433:                     goto checktype_fail_2;
21433:                 ++m;
21433:             );
21433: 
21433:         success = true;
21433: 
21433: checktype_fail_2:
21433:         if (success) {
21519:             /*
21519:              * There was a successful match.  We don't care about restoring the saved staging, but
21519:              * we do need to clear the original undemote list.
21519:              */
21433:             for (unsigned i = 0; i < stage_count; i++)
21433:                 set(stage_vals[i], stage_ins[i]);
21433:             if (stable_peer)
21433:                 *stable_peer = f;
24246:             demote = false;
24246:             return false;
24246:         }
24246:     }
21519: 
21519:     /*
21519:      * If this is a loop trace and it would be stable with demotions, build an undemote list
21519:      * and return true.  Our caller should sniff this and trash the tree, recording a new one
21519:      * that will assumedly stabilize.
21519:      */
24246:     if (demote && fragment->kind == LoopTrace) {
24246:         typemap = m = treeInfo->globalTypeMap();
24491:         FORALL_GLOBAL_SLOTS(cx, treeInfo->globalSlots->length(), treeInfo->globalSlots->data(),
24246:             if (*m == JSVAL_INT) {
24246:                 JS_ASSERT(isNumber(*vp));
24246:                 if (!isPromoteInt(get(vp)))
24246:                     oracle.markGlobalSlotUndemotable(cx, gslots[n]);
24246:             } else if (*m == JSVAL_DOUBLE) {
24246:                 JS_ASSERT(isNumber(*vp));
24246:                 oracle.markGlobalSlotUndemotable(cx, gslots[n]);
24246:             } else {
24246:                 JS_ASSERT(*m == JSVAL_TAG(*vp));
24246:             }
24246:             m++;
24246:         );
24246: 
24246:         typemap = m = treeInfo->stackTypeMap();
21519:         FORALL_SLOTS_IN_PENDING_FRAMES(cx, 0,
21519:             if (*m == JSVAL_INT) {
21519:                 JS_ASSERT(isNumber(*vp));
21519:                 if (!isPromoteInt(get(vp)))
24246:                     oracle.markStackSlotUndemotable(cx, unsigned(m - typemap));
21519:             } else if (*m == JSVAL_DOUBLE) {
21519:                 JS_ASSERT(isNumber(*vp));
24246:                 oracle.markStackSlotUndemotable(cx, unsigned(m - typemap));
21519:             } else {
23075:                 JS_ASSERT((*m == JSVAL_TNULL) ? JSVAL_IS_NULL(*vp) : *m == JSVAL_TAG(*vp));
21519:             }
21519:             m++;
21519:         );
21519:         return true;
24246:     } else {
24246:         demote = false;
21519:     }
21519: 
21433:     return false;
17453: }
17453: 
18606: /* Compile the current fragment. */
22652: JS_REQUIRES_STACK void
24307: TraceRecorder::compile(JSTraceMonitor* tm)
24307: {
24307:     Fragmento* fragmento = tm->fragmento;
18118:     if (treeInfo->maxNativeStackSlots >= MAX_NATIVE_STACK_SLOTS) {
25627:         debug_only_v(printf("Blacklist: excessive stack use.\n"));
25895:         js_Blacklist(fragment->root);
18118:         return;
18118:     }
25099:     if (anchor && anchor->exitType != CASE_EXIT)
18781:         ++treeInfo->branchCount;
22662:     if (lirbuf->outOMem()) {
21483:         fragmento->assm()->setError(nanojit::OutOMem);
21483:         return;
21483:     }
18606:     ::compile(fragmento->assm(), fragment);
24244:     if (fragmento->assm()->error() == nanojit::OutOMem)
24244:         return;
22633:     if (fragmento->assm()->error() != nanojit::None) {
25627:         debug_only_v(printf("Blacklisted: error during compilation\n");)
25895:         js_Blacklist(fragment->root);
21483:         return;
22633:     }
25099:     if (anchor) {
25099: #ifdef NANOJIT_IA32
25099:         if (anchor->exitType == CASE_EXIT)
25099:             fragmento->assm()->patch(anchor, anchor->switchInfo);
25099:         else
25099: #endif
24244:             fragmento->assm()->patch(anchor);
25099:     }
18211:     JS_ASSERT(fragment->code());
18211:     JS_ASSERT(!fragment->vmprivate);
18211:     if (fragment == fragment->root)
18211:         fragment->vmprivate = treeInfo;
17884:     /* :TODO: windows support */
17884: #if defined DEBUG && !defined WIN32
21717:     const char* filename = cx->fp->script->filename;
21717:     char* label = (char*)malloc((filename ? strlen(filename) : 7) + 16);
21717:     sprintf(label, "%s:%u", filename ? filename : "<stdin>",
21685:             js_FramePCToLineNumber(cx, cx->fp));
17731:     fragmento->labels->add(fragment, sizeof(Fragment), 0, label);
18056:     free(label);
17731: #endif
18697:     AUDIT(traceCompleted);
18334: }
18334: 
21433: static bool
21521: js_JoinPeersIfCompatible(Fragmento* frago, Fragment* stableFrag, TreeInfo* stableTree,
21521:                          VMSideExit* exit)
21433: {
24491:     JS_ASSERT(exit->numStackSlots == stableTree->nStackTypes);
24246: 
21433:     /* Must have a matching type unstable exit. */
24246:     if ((exit->numGlobalSlots + exit->numStackSlots != stableTree->typeMap.length()) ||
24246:         memcmp(getFullTypeMap(exit), stableTree->typeMap.data(), stableTree->typeMap.length())) {
21433:        return false;
21433:     }
21433: 
21433:     exit->target = stableFrag;
21433:     frago->assm()->patch(exit);
21433: 
21463:     stableTree->dependentTrees.addUnique(exit->from->root);
25491:     ((TreeInfo*)exit->from->root->vmprivate)->linkedTrees.addUnique(stableFrag);
21463: 
21433:     return true;
21433: }
21433: 
18606: /* Complete and compile a trace and link it to the existing tree if appropriate. */
26557: JS_REQUIRES_STACK void
24307: TraceRecorder::closeLoop(JSTraceMonitor* tm, bool& demote)
21433: {
26557:     /*
26557:      * We should have arrived back at the loop header, and hence we don't want to be in an imacro
26557:      * here and the opcode should be either JSOP_LOOP, or in case this loop was blacklisted in the
26557:      * meantime JSOP_NOP.
26557:      */
26557:     JS_ASSERT((*cx->fp->regs->pc == JSOP_LOOP || *cx->fp->regs->pc == JSOP_NOP) && !cx->fp->imacpc);
26557: 
21433:     bool stable;
21433:     LIns* exitIns;
21433:     Fragment* peer;
21521:     VMSideExit* exit;
21433:     Fragment* peer_root;
24307:     Fragmento* fragmento = tm->fragmento;
21433: 
21433:     exitIns = snapshot(UNSTABLE_LOOP_EXIT);
21521:     exit = (VMSideExit*)((GuardRecord*)exitIns->payload())->exit;
21684: 
21684:     if (callDepth != 0) {
25627:         debug_only_v(printf("Blacklisted: stack depth mismatch, possible recursion.\n");)
25895:         js_Blacklist(fragment->root);
22609:         trashSelf = true;
26557:         return;
21684:     }
21684: 
24491:     JS_ASSERT(exit->numStackSlots == treeInfo->nStackTypes);
24491: 
24491:     peer_root = getLoop(traceMonitor, fragment->root->ip, treeInfo->globalShape);
21433:     JS_ASSERT(peer_root != NULL);
26557: 
24246:     stable = deduceTypeStability(peer_root, &peer, demote);
21433: 
21433: #if DEBUG
24246:     if (!stable)
18606:         AUDIT(unstableLoopVariable);
21433: #endif
21433: 
22609:     if (trashSelf) {
21433:         debug_only_v(printf("Trashing tree from type instability.\n");)
26557:         return;
21433:     }
21433: 
24246:     if (stable && demote) {
21519:         JS_ASSERT(fragment->kind == LoopTrace);
26557:         return;
21433:     }
21433: 
21433:     if (!stable) {
21433:         fragment->lastIns = lir->insGuard(LIR_x, lir->insImm(1), exitIns);
21433: 
21685:         /*
21685:          * If we didn't find a type stable peer, we compile the loop anyway and
21685:          * hope it becomes stable later.
21433:          */
21433:         if (!peer) {
21685:             /*
21685:              * If such a fragment does not exist, let's compile the loop ahead
21685:              * of time anyway.  Later, if the loop becomes type stable, we will
21685:              * connect these two fragments together.
21433:              */
21685:             debug_only_v(printf("Trace has unstable loop variable with no stable peer, "
21685:                                 "compiling anyway.\n");)
25102:             UnstableExit* uexit = new UnstableExit;
21433:             uexit->fragment = fragment;
21433:             uexit->exit = exit;
21433:             uexit->next = treeInfo->unstableExits;
21433:             treeInfo->unstableExits = uexit;
21433:         } else {
21433:             JS_ASSERT(peer->code());
21433:             exit->target = peer;
25469:             debug_only_v(printf("Joining type-unstable trace to target fragment %p.\n", (void*)peer);)
21433:             stable = true;
21783:             ((TreeInfo*)peer->vmprivate)->dependentTrees.addUnique(fragment->root);
25491:             treeInfo->linkedTrees.addUnique(peer);
21433:         }
21433:     } else {
21433:         exit->target = fragment->root;
21433:         fragment->lastIns = lir->insGuard(LIR_loop, lir->insImm(1), exitIns);
26557:     }
24307:     compile(tm);
26286: 
21483:     if (fragmento->assm()->error() != nanojit::None)
26557:         return;
21483: 
21433:     joinEdgesToEntry(fragmento, peer_root);
19588: 
25491:     debug_only_v(printf("updating specializations on dependent and linked trees\n"))
25491:     if (fragment->root->vmprivate)
25491:         specializeTreesToMissingGlobals(cx, (TreeInfo*)fragment->root->vmprivate);
25491: 
25627:     /* 
25627:      * If this is a newly formed tree, and the outer tree has not been compiled yet, we
25627:      * should try to compile the outer tree again.
25627:      */
25937:     if (outer)
25937:         js_AttemptCompilation(tm, globalObj, outer);
25627:     
21685:     debug_only_v(printf("recording completed at %s:%u@%u via closeLoop\n",
21685:                         cx->fp->script->filename,
21685:                         js_FramePCToLineNumber(cx, cx->fp),
21685:                         FramePCOffset(cx->fp));)
21433: }
21433: 
22652: JS_REQUIRES_STACK void
21685: TraceRecorder::joinEdgesToEntry(Fragmento* fragmento, Fragment* peer_root)
21433: {
21433:     if (fragment->kind == LoopTrace) {
21433:         TreeInfo* ti;
21433:         Fragment* peer;
21433:         uint8* t1, *t2;
21433:         UnstableExit* uexit, **unext;
24491:         uint32* stackDemotes = (uint32*)alloca(sizeof(uint32) * treeInfo->nStackTypes);
24491:         uint32* globalDemotes = (uint32*)alloca(sizeof(uint32) * treeInfo->nGlobalTypes());
24246: 
21433:         for (peer = peer_root; peer != NULL; peer = peer->peer) {
21433:             if (!peer->code())
21433:                 continue;
21433:             ti = (TreeInfo*)peer->vmprivate;
21433:             uexit = ti->unstableExits;
21433:             unext = &ti->unstableExits;
21433:             while (uexit != NULL) {
21433:                 bool remove = js_JoinPeersIfCompatible(fragmento, fragment, treeInfo, uexit->exit);
21433:                 JS_ASSERT(!remove || fragment != peer);
21433:                 debug_only_v(if (remove) {
21433:                              printf("Joining type-stable trace to target exit %p->%p.\n",
25469:                                     (void*)uexit->fragment, (void*)uexit->exit); });
21433:                 if (!remove) {
21433:                     /* See if this exit contains mismatch demotions, which imply trashing a tree.
21433:                        This is actually faster than trashing the original tree as soon as the
21433:                        instability is detected, since we could have compiled a fairly stable
21433:                        tree that ran faster with integers. */
24246:                     unsigned stackCount = 0;
24246:                     unsigned globalCount = 0;
24246:                     t1 = treeInfo->stackTypeMap();
24246:                     t2 = getStackTypeMap(uexit->exit);
21433:                     for (unsigned i = 0; i < uexit->exit->numStackSlots; i++) {
21433:                         if (t2[i] == JSVAL_INT && t1[i] == JSVAL_DOUBLE) {
24246:                             stackDemotes[stackCount++] = i;
21433:                         } else if (t2[i] != t1[i]) {
24246:                             stackCount = 0;
21433:                             break;
21433:                         }
21433:                     }
24246:                     t1 = treeInfo->globalTypeMap();
24246:                     t2 = getGlobalTypeMap(uexit->exit);
24246:                     for (unsigned i = 0; i < uexit->exit->numGlobalSlots; i++) {
24246:                         if (t2[i] == JSVAL_INT && t1[i] == JSVAL_DOUBLE) {
24246:                             globalDemotes[globalCount++] = i;
24246:                         } else if (t2[i] != t1[i]) {
24246:                             globalCount = 0;
24246:                             stackCount = 0;
24246:                             break;
24246:                         }
24246:                     }
24246:                     if (stackCount || globalCount) {
24246:                         for (unsigned i = 0; i < stackCount; i++)
24246:                             oracle.markStackSlotUndemotable(cx, stackDemotes[i]);
24246:                         for (unsigned i = 0; i < globalCount; i++)
24491:                             oracle.markGlobalSlotUndemotable(cx, ti->globalSlots->data()[globalDemotes[i]]);
22609:                         JS_ASSERT(peer == uexit->fragment->root);
22609:                         if (fragment == peer)
22609:                             trashSelf = true;
22609:                         else
22609:                             whichTreesToTrash.addUnique(uexit->fragment->root);
21433:                         break;
21433:                     }
21433:                 }
21433:                 if (remove) {
21433:                     *unext = uexit->next;
25102:                     delete uexit;
21433:                     uexit = *unext;
21433:                 } else {
21433:                     unext = &uexit->next;
21433:                     uexit = uexit->next;
21433:                 }
21433:             }
21433:         }
21433:     }
21433: 
24491:     debug_only_v(js_DumpPeerStability(traceMonitor, peer_root->ip, treeInfo->globalShape);)
18606: }
18606: 
18606: /* Emit an always-exit guard and compile the tree (used for break statements. */
22652: JS_REQUIRES_STACK void
24307: TraceRecorder::endLoop(JSTraceMonitor* tm)
18606: {
21684:     LIns* exitIns = snapshot(LOOP_EXIT);
21684: 
21684:     if (callDepth != 0) {
25627:         debug_only_v(printf("Blacklisted: stack depth mismatch, possible recursion.\n");)
25895:         js_Blacklist(fragment->root);
22609:         trashSelf = true;
21684:         return;
21684:     }
21684: 
21684:     fragment->lastIns = lir->insGuard(LIR_x, lir->insImm(1), exitIns);
24307:     compile(tm);
24307: 
24307:     if (tm->fragmento->assm()->error() != nanojit::None)
21483:         return;
21483: 
24491:     joinEdgesToEntry(tm->fragmento, getLoop(tm, fragment->root->ip, treeInfo->globalShape));
21433: 
25491:     /* Note: this must always be done, in case we added new globals on trace and haven't yet 
25491:        propagated those to linked and dependent trees. */
25491:     debug_only_v(printf("updating specializations on dependent and linked trees\n"))
25491:     if (fragment->root->vmprivate)
25491:         specializeTreesToMissingGlobals(cx, (TreeInfo*)fragment->root->vmprivate);
25491: 
25627:     /* 
25627:      * If this is a newly formed tree, and the outer tree has not been compiled yet, we
25627:      * should try to compile the outer tree again.
25627:      */
25937:     if (outer)
25937:         js_AttemptCompilation(tm, globalObj, outer);
25627:     
21685:     debug_only_v(printf("recording completed at %s:%u@%u via endLoop\n",
21685:                         cx->fp->script->filename,
21685:                         js_FramePCToLineNumber(cx, cx->fp),
21685:                         FramePCOffset(cx->fp));)
17334: }
17334: 
18241: /* Emit code to adjust the stack to match the inner tree's stack expectations. */
22652: JS_REQUIRES_STACK void
18250: TraceRecorder::prepareTreeCall(Fragment* inner)
17966: {
18121:     TreeInfo* ti = (TreeInfo*)inner->vmprivate;
18241:     inner_sp_ins = lirbuf->sp;
18215:     /* The inner tree expects to be called from the current frame. If the outer tree (this
18215:        trace) is currently inside a function inlining code (calldepth > 0), we have to advance
18007:        the native stack pointer such that we match what the inner trace expects to see. We
18007:        move it back when we come out of the inner tree call. */
18007:     if (callDepth > 0) {
18121:         /* Calculate the amount we have to lift the native stack pointer by to compensate for
18121:            any outer frames that the inner tree doesn't expect but the outer tree has. */
18280:         ptrdiff_t sp_adj = nativeStackOffset(&cx->fp->argv[-2]);
18133:         /* Calculate the amount we have to lift the call stack by */
23262:         ptrdiff_t rp_adj = callDepth * sizeof(FrameInfo*);
18121:         /* Guard that we have enough stack space for the tree we are trying to call on top
18121:            of the new value for sp. */
18260:         debug_only_v(printf("sp_adj=%d outer=%d inner=%d\n",
18187:                           sp_adj, treeInfo->nativeStackBase, ti->nativeStackBase));
18232:         LIns* sp_top = lir->ins2i(LIR_piadd, lirbuf->sp,
18184:                 - treeInfo->nativeStackBase /* rebase sp to beginning of outer tree's stack */
18184:                 + sp_adj /* adjust for stack in outer frame inner tree can't see */
18184:                 + ti->maxNativeStackSlots * sizeof(double)); /* plus the inner tree's stack */
18121:         guard(true, lir->ins2(LIR_lt, sp_top, eos_ins), OOM_EXIT);
18133:         /* Guard that we have enough call stack space. */
18232:         LIns* rp_top = lir->ins2i(LIR_piadd, lirbuf->rp, rp_adj +
23262:                 ti->maxCallDepth * sizeof(FrameInfo*));
18133:         guard(true, lir->ins2(LIR_lt, rp_top, eor_ins), OOM_EXIT);
18133:         /* We have enough space, so adjust sp and rp to their new level. */
18241:         lir->insStorei(inner_sp_ins = lir->ins2i(LIR_piadd, lirbuf->sp,
18184:                 - treeInfo->nativeStackBase /* rebase sp to beginning of outer tree's stack */
18184:                 + sp_adj /* adjust for stack in outer frame inner tree can't see */
18184:                 + ti->nativeStackBase), /* plus the inner tree's stack base */
18007:                 lirbuf->state, offsetof(InterpState, sp));
18232:         lir->insStorei(lir->ins2i(LIR_piadd, lirbuf->rp, rp_adj),
18133:                 lirbuf->state, offsetof(InterpState, rp));
18007:     }
18241: }
18241: 
18241: /* Record a call to an inner tree. */
22652: JS_REQUIRES_STACK void
21521: TraceRecorder::emitTreeCall(Fragment* inner, VMSideExit* exit)
18241: {
18241:     TreeInfo* ti = (TreeInfo*)inner->vmprivate;
18007:     /* Invoke the inner tree. */
18712:     LIns* args[] = { INS_CONSTPTR(inner), lirbuf->state }; /* reverse order */
20915:     LIns* ret = lir->insCall(&js_CallTree_ci, args);
18116:     /* Read back all registers, in case the called tree changed any of them. */
24246:     import(ti, inner_sp_ins, exit->numStackSlots, exit->numGlobalSlots,
24246:            exit->calldepth, getFullTypeMap(exit));
18159:     /* Restore sp and rp to their original values (we still have them in a register). */
18159:     if (callDepth > 0) {
18159:         lir->insStorei(lirbuf->sp, lirbuf->state, offsetof(InterpState, sp));
18159:         lir->insStorei(lirbuf->rp, lirbuf->state, offsetof(InterpState, rp));
18159:     }
18283:     /* Guard that we come out of the inner tree along the same side exit we came out when
18283:        we called the inner tree at recording time. */
20931:     guard(true, lir->ins2(LIR_eq, ret, INS_CONSTPTR(exit)), NESTED_EXIT);
18650:     /* Register us as a dependent tree of the inner tree. */
18650:     ((TreeInfo*)inner->vmprivate)->dependentTrees.addUnique(fragment->root);
25491:     treeInfo->linkedTrees.addUnique(inner);
18334: }
18334: 
18694: /* Add a if/if-else control-flow merge point to the list of known merge points. */
22652: JS_REQUIRES_STACK void
18694: TraceRecorder::trackCfgMerges(jsbytecode* pc)
18694: {
18694:     /* If we hit the beginning of an if/if-else, then keep track of the merge point after it. */
18694:     JS_ASSERT((*pc == JSOP_IFEQ) || (*pc == JSOP_IFEQX));
18694:     jssrcnote* sn = js_GetSrcNote(cx->fp->script, pc);
18694:     if (sn != NULL) {
18694:         if (SN_TYPE(sn) == SRC_IF) {
18694:             cfgMerges.add((*pc == JSOP_IFEQ)
18694:                           ? pc + GET_JUMP_OFFSET(pc)
18694:                           : pc + GET_JUMPX_OFFSET(pc));
18694:         } else if (SN_TYPE(sn) == SRC_IF_ELSE)
18694:             cfgMerges.add(pc + js_GetSrcNoteOffset(sn, 0));
18694:     }
18694: }
18694: 
20416: /* Invert the direction of the guard if this is a loop edge that is not
20416:    taken (thin loop). */
22652: JS_REQUIRES_STACK void
26557: TraceRecorder::emitIf(jsbytecode* pc, bool cond, LIns* x)
26557: {
26557:     ExitType exitType;
20416:     if (js_IsLoopEdge(pc, (jsbytecode*)fragment->root->ip)) {
26557:         exitType = LOOP_EXIT;
26557: 
26557:         /*
26557:          * If we are about to walk out of the loop, generate code for the inverse loop
26557:          * condition, pretending we recorded the case that stays on trace.
26557:          */
26557:         if ((*pc == JSOP_IFEQ || *pc == JSOP_IFEQX) == cond) {
26557:             JS_ASSERT(*pc == JSOP_IFNE || *pc == JSOP_IFNEX || *pc == JSOP_IFEQ || *pc == JSOP_IFEQX);
20416:             debug_only_v(printf("Walking out of the loop, terminating it anyway.\n");)
20416:             cond = !cond;
26557:         }
26557: 
26557:         /*
26557:          * Conditional guards do not have to be emitted if the condition is constant. We
26557:          * make a note whether the loop condition is true or false here, so we later know
26557:          * whether to emit a loop edge or a loop end.
21433:          */
26557:         if (x->isconst()) {
26557:             loop = (x->constval() == cond);
26557:             return;
26557:         }
26557:     } else {
26557:         exitType = BRANCH_EXIT;
26557:     }
26557:     if (!x->isconst())
26557:         guard(cond, x, exitType);
20416: }
20416: 
18694: /* Emit code for a fused IFEQ/IFNE. */
22652: JS_REQUIRES_STACK void
18694: TraceRecorder::fuseIf(jsbytecode* pc, bool cond, LIns* x)
18694: {
26557:     if (*pc == JSOP_IFEQ || *pc == JSOP_IFNE) {
26557:         emitIf(pc, cond, x);
26557:         if (*pc == JSOP_IFEQ)
18694:             trackCfgMerges(pc);
26557:     }
26557: }
26557: 
26557: /* Check whether we have reached the end of the trace. */
26557: JS_REQUIRES_STACK bool
26557: TraceRecorder::checkTraceEnd(jsbytecode *pc)
26557: {
26557:     if (js_IsLoopEdge(pc, (jsbytecode*)fragment->root->ip)) {
26557:         /*
26557:          * If we compile a loop, the trace should have a zero stack balance at the loop
26557:          * edge. Currently we are parked on a comparison op or IFNE/IFEQ, so advance
26557:          * pc to the loop header and adjust the stack pointer and pretend we have
26557:          * reached the loop header.
26557:          */
26557:         if (loop) {
26557:             JS_ASSERT(!cx->fp->imacpc && (pc == cx->fp->regs->pc || pc == cx->fp->regs->pc + 1));
26557:             bool fused = pc != cx->fp->regs->pc;
26557:             JSFrameRegs orig = *cx->fp->regs;
26557: 
26557:             cx->fp->regs->pc = (jsbytecode*)fragment->root->ip;
26557:             cx->fp->regs->sp -= fused ? 2 : 1;
26557: 
26557:             bool demote = false;
26557:             closeLoop(traceMonitor, demote);
26557: 
26557:             *cx->fp->regs = orig;
26557: 
26557:             /*
26557:              * If compiling this loop generated new oracle information which will likely
26557:              * lead to a different compilation result, immediately trigger another
26557:              * compiler run. This is guaranteed to converge since the oracle only
26557:              * accumulates adverse information but never drops it (except when we
26557:              * flush it during garbage collection.)
26557:              */
26557:             if (demote)
26557:                 js_AttemptCompilation(traceMonitor, globalObj, outer);
26557:         } else {
26557:             endLoop(traceMonitor);
26557:         }
26557:         return false;
26557:     }
26557:     return true;
17966: }
17966: 
21685: bool
21685: TraceRecorder::hasMethod(JSObject* obj, jsid id)
21685: {
21685:     if (!obj)
21685:         return false;
21685: 
21685:     JSObject* pobj;
21685:     JSProperty* prop;
21685:     int protoIndex = OBJ_LOOKUP_PROPERTY(cx, obj, id, &pobj, &prop);
21685:     if (protoIndex < 0 || !prop)
21685:         return false;
21685: 
21685:     bool found = false;
21685:     if (OBJ_IS_NATIVE(pobj)) {
21685:         JSScope* scope = OBJ_SCOPE(pobj);
21685:         JSScopeProperty* sprop = (JSScopeProperty*) prop;
21685: 
21685:         if (SPROP_HAS_STUB_GETTER(sprop) &&
21685:             SPROP_HAS_VALID_SLOT(sprop, scope)) {
21685:             jsval v = LOCKED_OBJ_GET_SLOT(pobj, sprop->slot);
21685:             if (VALUE_IS_FUNCTION(cx, v)) {
21685:                 found = true;
21685:                 if (!SCOPE_IS_BRANDED(scope)) {
21685:                     SCOPE_MAKE_UNIQUE_SHAPE(cx, scope);
21685:                     SCOPE_SET_BRANDED(scope);
21685:                 }
21685:             }
21685:         }
21685:     }
21685: 
21685:     OBJ_DROP_PROPERTY(cx, pobj, prop);
21685:     return found;
21685: }
21685: 
24299: JS_REQUIRES_STACK bool
21685: TraceRecorder::hasIteratorMethod(JSObject* obj)
21685: {
21685:     JS_ASSERT(cx->fp->regs->sp + 2 <= cx->fp->slots + cx->fp->script->nslots);
21685: 
21685:     return hasMethod(obj, ATOM_TO_JSID(cx->runtime->atomState.iteratorAtom));
21685: }
21685: 
17517: int
17517: nanojit::StackFilter::getTop(LInsp guard)
17517: {
21521:     VMSideExit* e = (VMSideExit*)guard->record()->exit;
20893:     if (sp == lirbuf->sp)
21521:         return e->sp_adj;
20893:     JS_ASSERT(sp == lirbuf->rp);
21521:     return e->rp_adj;
17517: }
17517: 
17517: #if defined NJ_VERBOSE
17517: void
17517: nanojit::LirNameMap::formatGuard(LIns *i, char *out)
17517: {
21521:     VMSideExit *x;
21521: 
21521:     x = (VMSideExit *)i->record()->exit;
17517:     sprintf(out,
25111:             "%s: %s %s -> pc=%p imacpc=%p sp%+ld rp%+ld",
17517:             formatRef(i),
17517:             lirNames[i->opcode()],
17517:             i->oprnd1()->isCond() ? formatRef(i->oprnd1()) : "",
25111:             (void *)x->pc,
25111:             (void *)x->imacpc,
18612:             (long int)x->sp_adj,
21685:             (long int)x->rp_adj);
17517: }
17517: #endif
17517: 
25102: void
25102: nanojit::Fragment::onDestroy()
25102: {
25102:     delete (TreeInfo *)vmprivate;
25102: }
25102: 
24499: static JS_REQUIRES_STACK bool
17410: js_DeleteRecorder(JSContext* cx)
17293: {
18782:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
18782: 
18702:     /* Aborting and completing a trace end up here. */
17410:     delete tm->recorder;
17410:     tm->recorder = NULL;
24486: 
24486:     /*
24486:      * If we ran out of memory, flush the code cache.
24486:      */
24486:     if (JS_TRACE_MONITOR(cx).fragmento->assm()->error() == OutOMem) {
24486:         js_FlushJITCache(cx);
24486:         return false;
24486:     }
24486: 
24486:     return true;
17410: }
17410: 
21514: /**
21688:  * Checks whether the shape of the global object has changed.
21514:  */
21514: static inline bool
24491: js_CheckGlobalObjectShape(JSContext* cx, JSTraceMonitor* tm, JSObject* globalObj,
24491:                           uint32 *shape=NULL, SlotList** slots=NULL)
24491: {
24491:     if (tm->needFlush) {
24491:         tm->needFlush = JS_FALSE;
24491:         return false;
24491:     }
24491: 
24491:     uint32 globalShape = OBJ_SHAPE(globalObj);
24491: 
24491:     if (tm->recorder) {
24491:         TreeInfo* ti = tm->recorder->getTreeInfo();
24491:         /* Check the global shape matches the recorder's treeinfo's shape. */
24491:         if (globalShape != ti->globalShape) {
21514:             AUDIT(globalShapeMismatchAtEntry);
21514:             debug_only_v(printf("Global shape mismatch (%u vs. %u), flushing cache.\n",
24491:                                 globalShape, ti->globalShape);)
24491:             return false;
24491:         }
24491:         if (shape)
24491:             *shape = globalShape;
24491:         if (slots)
24491:             *slots = ti->globalSlots;
24491:         return true;
24491:     }
24491: 
24491:     /* No recorder, search for a tracked global-state (or allocate one). */
24491:     for (size_t i = 0; i < MONITOR_N_GLOBAL_STATES; ++i) {
24491:         GlobalState &state = tm->globalStates[i];
24491: 
24608:         if (state.globalShape == (uint32) -1) {
24491:             state.globalShape = globalShape;
24491:             JS_ASSERT(state.globalSlots);
24491:             JS_ASSERT(state.globalSlots->length() == 0);
24491:         }
24491: 
24491:         if (tm->globalStates[i].globalShape == globalShape) {
24491:             if (shape)
24491:                 *shape = globalShape;
24491:             if (slots)
24491:                 *slots = state.globalSlots;
24491:             return true;
24491:         }
24491:     }
24491: 
24491:     /* No currently-tracked-global found and no room to allocate, abort. */
24491:     AUDIT(globalShapeMismatchAtEntry);
24491:     debug_only_v(printf("No global slotlist for global shape %u, flushing cache.\n",
24491:                         globalShape));
24491:     return false;
21514: }
21514: 
22652: static JS_REQUIRES_STACK bool
21521: js_StartRecorder(JSContext* cx, VMSideExit* anchor, Fragment* f, TreeInfo* ti,
24246:                  unsigned stackSlots, unsigned ngslots, uint8* typeMap,
25937:                  VMSideExit* expectedInnerExit, jsbytecode* outer)
17731: {
18782:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
24493:     JS_ASSERT(f->root != f || !cx->fp->imacpc);
18782: 
24613:     if (JS_TRACE_MONITOR(cx).prohibitRecording)
24613:         return false;
24613: 
17731:     /* start recording if no exception during construction */
19987:     tm->recorder = new (&gc) TraceRecorder(cx, anchor, f, ti,
24246:                                            stackSlots, ngslots, typeMap,
25484:                                            expectedInnerExit, outer);
25627: 
17731:     if (cx->throwing) {
20422:         js_AbortRecording(cx, "setting up recorder failed");
17731:         return false;
17731:     }
17978:     /* clear any leftover error state */
19987:     tm->fragmento->assm()->setError(None);
17731:     return true;
17731: }
17731: 
17853: static void
17889: js_TrashTree(JSContext* cx, Fragment* f)
17889: {
18211:     JS_ASSERT((!f->code()) == (!f->vmprivate));
18650:     JS_ASSERT(f == f->root);
18211:     if (!f->code())
18211:         return;
17889:     AUDIT(treesTrashed);
18260:     debug_only_v(printf("Trashing tree info.\n");)
18051:     Fragmento* fragmento = JS_TRACE_MONITOR(cx).fragmento;
18650:     TreeInfo* ti = (TreeInfo*)f->vmprivate;
17889:     f->vmprivate = NULL;
18051:     f->releaseCode(fragmento);
18650:     Fragment** data = ti->dependentTrees.data();
18650:     unsigned length = ti->dependentTrees.length();
18650:     for (unsigned n = 0; n < length; ++n)
18650:         js_TrashTree(cx, data[n]);
25102:     delete ti;
18650:     JS_ASSERT(!f->code() && !f->vmprivate);
17853: }
17853: 
19663: static int
17923: js_SynthesizeFrame(JSContext* cx, const FrameInfo& fi)
17923: {
22652:     VOUCH_DOES_NOT_REQUIRE_STACK();
22652: 
17923:     JS_ASSERT(HAS_FUNCTION_CLASS(fi.callee));
17923: 
17923:     JSFunction* fun = GET_FUNCTION_PRIVATE(cx, fi.callee);
17923:     JS_ASSERT(FUN_INTERPRETED(fun));
17923: 
19662:     /* Assert that we have a correct sp distance from cx->fp->slots in fi. */
22925:     JSStackFrame* fp = cx->fp;
25111:     JS_ASSERT_IF(!fi.imacpc,
25111:                  js_ReconstructStackDepth(cx, fp->script, fi.pc)
22925:                  == uintN(fi.s.spdist - fp->script->nfixed));
19662: 
19662:     uintN nframeslots = JS_HOWMANY(sizeof(JSInlineFrame), sizeof(jsval));
19662:     JSScript* script = fun->u.i.script;
19662:     size_t nbytes = (nframeslots + script->nslots) * sizeof(jsval);
19662: 
19662:     /* Code duplicated from inline_call: case in js_Interpret (FIXME). */
17923:     JSArena* a = cx->stackPool.current;
17923:     void* newmark = (void*) a->avail;
19662:     uintN argc = fi.s.argc & 0x7fff;
22925:     jsval* vp = fp->slots + fi.s.spdist - (2 + argc);
19662:     uintN missing = 0;
19662:     jsval* newsp;
19662: 
19662:     if (fun->nargs > argc) {
22925:         const JSFrameRegs& regs = *fp->regs;
19662: 
19662:         newsp = vp + 2 + fun->nargs;
19662:         JS_ASSERT(newsp > regs.sp);
19662:         if ((jsuword) newsp <= a->limit) {
19662:             if ((jsuword) newsp > a->avail)
19662:                 a->avail = (jsuword) newsp;
19662:             jsval* argsp = newsp;
19662:             do {
19662:                 *--argsp = JSVAL_VOID;
19662:             } while (argsp != regs.sp);
19662:             missing = 0;
19662:         } else {
19662:             missing = fun->nargs - argc;
19662:             nbytes += (2 + fun->nargs) * sizeof(jsval);
19662:         }
19662:     }
17923: 
17923:     /* Allocate the inline frame with its vars and operands. */
17923:     if (a->avail + nbytes <= a->limit) {
17923:         newsp = (jsval *) a->avail;
17923:         a->avail += nbytes;
19662:         JS_ASSERT(missing == 0);
17923:     } else {
23447:         /* This allocation is infallible: js_ExecuteTree reserved enough stack. */
17923:         JS_ARENA_ALLOCATE_CAST(newsp, jsval *, &cx->stackPool, nbytes);
23447:         JS_ASSERT(newsp);
19662: 
19662:         /*
19662:          * Move args if the missing ones overflow arena a, then push
19662:          * undefined for the missing args.
19662:          */
19662:         if (missing) {
19662:             memcpy(newsp, vp, (2 + argc) * sizeof(jsval));
19662:             vp = newsp;
19662:             newsp = vp + 2 + argc;
19662:             do {
19662:                 *newsp++ = JSVAL_VOID;
19662:             } while (--missing != 0);
19662:         }
17923:     }
17923: 
17923:     /* Claim space for the stack frame and initialize it. */
17923:     JSInlineFrame* newifp = (JSInlineFrame *) newsp;
17923:     newsp += nframeslots;
17923: 
17923:     newifp->frame.callobj = NULL;
17923:     newifp->frame.argsobj = NULL;
17923:     newifp->frame.varobj = NULL;
17923:     newifp->frame.script = script;
17923:     newifp->frame.callee = fi.callee;
17923:     newifp->frame.fun = fun;
17923: 
25628:     bool constructing = (fi.s.argc & 0x8000) != 0;
19577:     newifp->frame.argc = argc;
25111:     newifp->callerRegs.pc = fi.pc;
22925:     newifp->callerRegs.sp = fp->slots + fi.s.spdist;
25111:     fp->imacpc = fi.imacpc;
22925: 
22925: #ifdef DEBUG
22925:     if (fi.block != fp->blockChain) {
22925:         for (JSObject* obj = fi.block; obj != fp->blockChain; obj = STOBJ_GET_PARENT(obj))
22925:             JS_ASSERT(obj);
22925:     }
22925: #endif
22925:     fp->blockChain = fi.block;
21685: 
19981:     newifp->frame.argv = newifp->callerRegs.sp - argc;
19591:     JS_ASSERT(newifp->frame.argv);
19591: #ifdef DEBUG
19591:     // Initialize argv[-1] to a known-bogus value so we'll catch it if
19591:     // someone forgets to initialize it later.
19591:     newifp->frame.argv[-1] = JSVAL_HOLE;
19591: #endif
22925:     JS_ASSERT(newifp->frame.argv >= StackBase(fp) + 2);
17923: 
17923:     newifp->frame.rval = JSVAL_VOID;
22925:     newifp->frame.down = fp;
17923:     newifp->frame.annotation = NULL;
17923:     newifp->frame.scopeChain = OBJ_GET_PARENT(cx, fi.callee);
17923:     newifp->frame.sharpDepth = 0;
17923:     newifp->frame.sharpArray = NULL;
19577:     newifp->frame.flags = constructing ? JSFRAME_CONSTRUCTING : 0;
17923:     newifp->frame.dormantNext = NULL;
17923:     newifp->frame.xmlNamespace = NULL;
17923:     newifp->frame.blockChain = NULL;
17923:     newifp->mark = newmark;
17925:     newifp->frame.thisp = NULL; // will be set by js_ExecuteTree -> FlushNativeStackFrame
17923: 
22925:     newifp->frame.regs = fp->regs;
17923:     newifp->frame.regs->pc = script->code;
17923:     newifp->frame.regs->sp = newsp + script->nfixed;
21685:     newifp->frame.imacpc = NULL;
17923:     newifp->frame.slots = newsp;
18308:     if (script->staticDepth < JS_DISPLAY_SIZE) {
18308:         JSStackFrame **disp = &cx->display[script->staticDepth];
18308:         newifp->frame.displaySave = *disp;
18308:         *disp = &newifp->frame;
18308:     }
17923: #ifdef DEBUG
17923:     newifp->frame.pcDisabledSave = 0;
17923: #endif
17923: 
21141:     /*
22925:      * Note that fp->script is still the caller's script; set the callee
21141:      * inline frame's idea of caller version from its version.
21141:      */
22925:     newifp->callerVersion = (JSVersion) fp->script->version;
22925: 
22925:     // After this paragraph, fp and cx->fp point to the newly synthesized frame.
22925:     fp->regs = &newifp->callerRegs;
22925:     fp = cx->fp = &newifp->frame;
18226: 
19663:     if (fun->flags & JSFUN_HEAVYWEIGHT) {
21141:         /*
21141:          * Set hookData to null because the failure case for js_GetCallObject
21141:          * involves it calling the debugger hook.
23448:          *
23448:          * Allocating the Call object must not fail, so use an object
23448:          * previously reserved by js_ExecuteTrace if needed.
21141:          */
21141:         newifp->hookData = NULL;
23448:         JS_ASSERT(!JS_TRACE_MONITOR(cx).useReservedObjects);
23448:         JS_TRACE_MONITOR(cx).useReservedObjects = JS_TRUE;
23448: #ifdef DEBUG
23448:         JSObject *obj =
23448: #endif
25217:             js_GetCallObject(cx, &newifp->frame);
23448:         JS_ASSERT(obj);
23448:         JS_TRACE_MONITOR(cx).useReservedObjects = JS_FALSE;
19663:     }
19663: 
21141:     /*
21141:      * If there's a call hook, invoke it to compute the hookData used by
21141:      * debuggers that cooperate with the interpreter.
21141:      */
21141:     JSInterpreterHook hook = cx->debugHooks->callHook;
21141:     if (hook) {
21141:         newifp->hookData = hook(cx, &newifp->frame, JS_TRUE, 0,
21141:                                 cx->debugHooks->callHookData);
21141:     } else {
21141:         newifp->hookData = NULL;
21141:     }
21141: 
20404:     // FIXME? we must count stack slots from caller's operand stack up to (but not including)
18226:     // callee's, including missing arguments. Could we shift everything down to the caller's
18226:     // fp->slots (where vars start) and avoid some of the complexity?
22925:     return (fi.s.spdist - fp->down->script->nfixed) +
22925:            ((fun->nargs > fp->argc) ? fun->nargs - fp->argc : 0) +
18226:            script->nfixed;
17923: }
17923: 
22652: JS_REQUIRES_STACK bool
25937: js_RecordTree(JSContext* cx, JSTraceMonitor* tm, Fragment* f, jsbytecode* outer,
24491:               uint32 globalShape, SlotList* globalSlots)
17939: {
23091:     JS_ASSERT(f->root == f);
21796: 
18239:     /* Make sure the global type map didn't change on us. */
21688:     JSObject* globalObj = JS_GetGlobalForObject(cx, cx->fp->scopeChain);
21688:     if (!js_CheckGlobalObjectShape(cx, tm, globalObj)) {
18271:         js_FlushJITCache(cx);
18271:         return false;
18271:     }
18239: 
17939:     AUDIT(recorderStarted);
18213: 
18213:     /* Try to find an unused peer fragment, or allocate a new one. */
18213:     while (f->code() && f->peer)
18213:         f = f->peer;
18213:     if (f->code())
24491:         f = getAnchor(&JS_TRACE_MONITOR(cx), f->root->ip, globalShape);
24307: 
24307:     if (!f) {
24307:         js_FlushJITCache(cx);
24307:         return false;
24307:     }
18213: 
17939:     f->root = f;
23264:     f->lirbuf = tm->lirbuf;
17981: 
22662:     if (f->lirbuf->outOMem()) {
21483:         js_FlushJITCache(cx);
21483:         debug_only_v(printf("Out of memory recording new tree, flushing cache.\n");)
21483:         return false;
21483:     }
21483: 
18211:     JS_ASSERT(!f->code() && !f->vmprivate);
17981: 
17939:     /* setup the VM-private treeInfo structure for this fragment */
25102:     TreeInfo* ti = new (&gc) TreeInfo(f, globalShape, globalSlots);
17939: 
24246:     /* capture the coerced type of each active slot in the type map */
24491:     ti->typeMap.captureTypes(cx, *globalSlots, 0/*callDepth*/);
24491:     ti->nStackTypes = ti->typeMap.length() - globalSlots->length();
21433: 
21433:     /* Check for duplicate entry type maps.  This is always wrong and hints at trace explosion
21433:        since we are trying to stabilize something without properly connecting peer edges. */
21433:     #ifdef DEBUG
21433:     TreeInfo* ti_other;
24491:     for (Fragment* peer = getLoop(tm, f->root->ip, globalShape); peer != NULL; peer = peer->peer) {
21433:         if (!peer->code() || peer == f)
21433:             continue;
21433:         ti_other = (TreeInfo*)peer->vmprivate;
21433:         JS_ASSERT(ti_other);
24246:         JS_ASSERT(!ti->typeMap.matches(ti_other->typeMap));
21433:     }
25627:     ti->treeFileName = cx->fp->script->filename;
25627:     ti->treeLineNumber = js_FramePCToLineNumber(cx, cx->fp);
25627:     ti->treePCOffset = FramePCOffset(cx->fp);
21433:     #endif
21433: 
17939:     /* determine the native frame layout at the entry point */
24491:     unsigned entryNativeStackSlots = ti->nStackTypes;
18425:     JS_ASSERT(entryNativeStackSlots == js_NativeStackSlots(cx, 0/*callDepth*/));
17939:     ti->nativeStackBase = (entryNativeStackSlots -
17939:             (cx->fp->regs->sp - StackBase(cx->fp))) * sizeof(double);
17939:     ti->maxNativeStackSlots = entryNativeStackSlots;
17939:     ti->maxCallDepth = 0;
18595:     ti->script = cx->fp->script;
17939: 
17939:     /* recording primary trace */
21433:     if (!js_StartRecorder(cx, NULL, f, ti,
24491:                           ti->nStackTypes,
24491:                           ti->globalSlots->length(),
24246:                           ti->typeMap.data(), NULL, outer)) {
23918:         return false;
23918:     }
23918: 
23918:     return true;
23918: }
23918: 
24491: JS_REQUIRES_STACK static inline bool
24491: isSlotUndemotable(JSContext* cx, TreeInfo* ti, unsigned slot)
24491: {
24491:     if (slot < ti->nStackTypes)
24295:         return oracle.isStackSlotUndemotable(cx, slot);
24295: 
24491:     uint16* gslots = ti->globalSlots->data();
24491:     return oracle.isGlobalSlotUndemotable(cx, gslots[slot - ti->nStackTypes]);
17939: }
17939: 
22652: JS_REQUIRES_STACK static bool
25937: js_AttemptToStabilizeTree(JSContext* cx, VMSideExit* exit, jsbytecode* outer)
21433: {
21433:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
21456:     Fragment* from = exit->from->root;
24491:     TreeInfo* from_ti = (TreeInfo*)from->vmprivate;
21433: 
21463:     JS_ASSERT(exit->from->root->code());
21463: 
24246:     /* Make sure any doubles are not accidentally undemoted */
24246:     uint8* m = getStackTypeMap(exit);
21433:     for (unsigned i = 0; i < exit->numStackSlots; i++) {
24246:         if (m[i] == JSVAL_DOUBLE)
24246:             oracle.markStackSlotUndemotable(cx, i);
24246:     }
24246:     m = getGlobalTypeMap(exit);
24246:     for (unsigned i = 0; i < exit->numGlobalSlots; i++) {
24246:         if (m[i] == JSVAL_DOUBLE)
24491:             oracle.markGlobalSlotUndemotable(cx, from_ti->globalSlots->data()[i]);
24246:     }
24246: 
24246:     /* If this exit does not have enough globals, there might exist a peer with more globals that we
24246:      * can join to.
24246:      */
24246:     bool bound = false;
24491:     for (Fragment* f = from->first; f != NULL; f = f->peer) {
24246:         if (!f->code())
24246:             continue;
24491:         TreeInfo* ti = (TreeInfo*)f->vmprivate;
24491:         JS_ASSERT(exit->numStackSlots == ti->nStackTypes);
24246:         /* Check the minimum number of slots that need to be compared. */
24491:         unsigned checkSlots = JS_MIN(exit->numStackSlots + exit->numGlobalSlots, ti->typeMap.length());
24295:         m = getFullTypeMap(exit);
24491:         uint8* m2 = ti->typeMap.data();
24295:         /* Analyze the exit typemap against the peer typemap.
24295:          * Two conditions are important:
24295:          * 1) Typemaps are identical: these peers can be attached.
24295:          * 2) Typemaps do not match, but only contain I->D mismatches.
24295:          *    In this case, the original tree must be trashed because it
24295:          *    will never connect to any peer.
24295:          */
24491:         bool matched = true;
24491:         bool undemote = false;
24295:         for (uint32 i = 0; i < checkSlots; i++) {
24295:             /* If the types are equal we're okay. */
24295:             if (m[i] == m2[i])
24295:                 continue;
24295:             matched = false;
24295:             /* If there's an I->D that cannot be resolved, flag it.
24295:              * Otherwise, break and go to the next peer.
24295:              */
24295:             if (m[i] == JSVAL_INT && m2[i] == JSVAL_DOUBLE && isSlotUndemotable(cx, ti, i)) {
24295:                 undemote = true;
24295:             } else {
24295:                 undemote = false;
24295:                 break;
24295:             }
24295:         }
24295:         if (matched) {
24491:             JS_ASSERT(from_ti->globalSlots == ti->globalSlots);
24491:             JS_ASSERT(from_ti->nStackTypes == ti->nStackTypes);
24246:             /* Capture missing globals on both trees and link the fragments together. */
24246:             if (from != f) {
24246:                 ti->dependentTrees.addUnique(from);
25491:                 from_ti->linkedTrees.addUnique(f);
25491:             }
25491:             if (ti->nGlobalTypes() < ti->globalSlots->length())
25491:                 specializeTreesToMissingGlobals(cx, ti);
24246:             exit->target = f;
24246:             tm->fragmento->assm()->patch(exit);
24246:             /* Now erase this exit from the unstable exit list. */
24491:             UnstableExit** tail = &from_ti->unstableExits;
24491:             for (UnstableExit* uexit = from_ti->unstableExits; uexit != NULL; uexit = uexit->next) {
24246:                 if (uexit->exit == exit) {
24246:                     *tail = uexit->next;
25102:                     delete uexit;
24246:                     bound = true;
24246:                     break;
24246:                 }
24246:                 tail = &uexit->next;
24246:             }
24246:             JS_ASSERT(bound);
24491:             debug_only_v(js_DumpPeerStability(tm, f->ip, from_ti->globalShape);)
24283:             break;
24295:         } else if (undemote) {
24295:             /* The original tree is unconnectable, so trash it. */
24295:             js_TrashTree(cx, f);
24295:             /* We shouldn't attempt to record now, since we'll hit a duplicate. */
24295:             return false;
24246:         }
24246:     }
24246:     if (bound)
24246:         return false;
24246: 
24491:     return js_RecordTree(cx, tm, from->first, outer, from_ti->globalShape, from_ti->globalSlots);
21433: }
21433: 
22652: static JS_REQUIRES_STACK bool
25937: js_AttemptToExtendTree(JSContext* cx, VMSideExit* anchor, VMSideExit* exitedFrom, jsbytecode* outer)
18620: {
18620:     Fragment* f = anchor->from->root;
18619:     JS_ASSERT(f->vmprivate);
18781:     TreeInfo* ti = (TreeInfo*)f->vmprivate;
18781: 
18781:     /* Don't grow trees above a certain size to avoid code explosion due to tail duplication. */
18781:     if (ti->branchCount >= MAX_BRANCHES)
18781:         return false;
17939: 
17939:     Fragment* c;
18620:     if (!(c = anchor->target)) {
20931:         c = JS_TRACE_MONITOR(cx).fragmento->createBranch(anchor, cx->fp->regs->pc);
20931:         c->spawnedFrom = anchor;
17939:         c->parent = f;
18620:         anchor->target = c;
17939:         c->root = f;
17939:     }
17939: 
25627:     debug_only_v(printf("trying to attach another branch to the tree (hits = %d)\n", c->hits());)
25627: 
25627:     int32_t& hits = c->hits();
25937:     if (outer || hits++ >= HOTEXIT && hits <= HOTEXIT+MAXEXIT) {
17939:         /* start tracing secondary trace from this point */
17939:         c->lirbuf = f->lirbuf;
24246:         unsigned stackSlots;
18621:         unsigned ngslots;
24246:         uint8* typeMap;
18621:         TypeMap fullMap;
18621:         if (exitedFrom == NULL) {
18621:             /* If we are coming straight from a simple side exit, just use that exit's type map
18621:                as starting point. */
20931:             ngslots = anchor->numGlobalSlots;
24246:             stackSlots = anchor->numStackSlots;
24246:             typeMap = getFullTypeMap(anchor);
18621:         } else {
18621:             /* If we side-exited on a loop exit and continue on a nesting guard, the nesting
18621:                guard (anchor) has the type information for everything below the current scope,
18621:                and the actual guard we exited from has the types for everything in the current
18621:                scope (and whatever it inlined). We have to merge those maps here. */
21521:             VMSideExit* e1 = anchor;
21521:             VMSideExit* e2 = exitedFrom;
24246:             fullMap.add(getStackTypeMap(e1), e1->numStackSlotsBelowCurrentFrame);
24246:             fullMap.add(getStackTypeMap(e2), e2->numStackSlots);
24246:             stackSlots = fullMap.length();
24246:             fullMap.add(getGlobalTypeMap(e2), e2->numGlobalSlots);
18621:             ngslots = e2->numGlobalSlots;
24246:             typeMap = fullMap.data();
24246:         }
24246:         return js_StartRecorder(cx, anchor, c, (TreeInfo*)f->vmprivate, stackSlots,
24246:                                 ngslots, typeMap, exitedFrom, outer);
17939:     }
17939:     return false;
17939: }
17939: 
22652: static JS_REQUIRES_STACK VMSideExit*
21433: js_ExecuteTree(JSContext* cx, Fragment* f, uintN& inlineCallCount,
21521:                VMSideExit** innermostNestedGuardp);
17951: 
22652: JS_REQUIRES_STACK bool
20422: js_RecordLoopEdge(JSContext* cx, TraceRecorder* r, uintN& inlineCallCount)
17939: {
17939: #ifdef JS_THREADSAFE
17939:     if (OBJ_SCOPE(JS_GetGlobalForObject(cx, cx->fp->scopeChain))->title.ownercx != cx) {
20422:         js_AbortRecording(cx, "Global object not owned by this context");
17939:         return false; /* we stay away from shared global objects */
17939:     }
17939: #endif
26557: 
24307:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
24491:     TreeInfo* ti = r->getTreeInfo();
26557: 
22632:     /* Process deep abort requests. */
22632:     if (r->wasDeepAborted()) {
22632:         js_AbortRecording(cx, "deep abort requested");
22632:         return false;
22632:     }
26557: 
26557:     JS_ASSERT(r->getFragment() && !r->getFragment()->lastIns);
26557: 
26557:     /* Does this branch go to an inner loop? */
24491:     Fragment* f = getLoop(&JS_TRACE_MONITOR(cx), cx->fp->regs->pc, ti->globalShape);
26557:     if (!f) {
26557:         /* Not an inner loop we can call, abort trace. */
26557:         AUDIT(returnToDifferentLoopHeader);
26557:         JS_ASSERT(!cx->fp->imacpc);
26557:         debug_only_v(printf("loop edge to %d, header %d\n",
26557:                             cx->fp->regs->pc - cx->fp->script->code,
26557:                             (jsbytecode*)r->getFragment()->root->ip - cx->fp->script->code));
26557:         js_AbortRecording(cx, "Loop edge does not return to header");
26557:         return false;
26557:     }
21514: 
25880:     /* Cannot handle treecalls with callDepth > 0 and argc > nargs, see bug 480244. */
26557:     if (r->getCallDepth() > 0 && cx->fp->argc > cx->fp->fun->nargs) {
25880:         js_AbortRecording(cx, "Can't call inner tree with extra args in pending frame");
25880:         return false;
25880:     }
25880: 
21514:     /* Make sure inner tree call will not run into an out-of-memory condition. */
23449:     if (tm->reservedDoublePoolPtr < (tm->reservedDoublePool + MAX_NATIVE_STACK_SLOTS) &&
23449:         !js_ReplenishReservedPool(cx, tm)) {
21514:         js_AbortRecording(cx, "Couldn't call inner tree (out of memory)");
21514:         return false;
21514:     }
21514: 
21514:     /* Make sure the shape of the global object still matches (this might flush
21514:        the JIT cache). */
21514:     JSObject* globalObj = JS_GetGlobalForObject(cx, cx->fp->scopeChain);
24491:     uint32 globalShape = -1;
24491:     SlotList* globalSlots = NULL;
24491:     if (!js_CheckGlobalObjectShape(cx, tm, globalObj, &globalShape, &globalSlots)) {
21433:         js_AbortRecording(cx, "Couldn't call inner tree (prep failed)");
21433:         return false;
21433:     }
21433: 
21433:     debug_only_v(printf("Looking for type-compatible peer (%s:%d@%d)\n",
21433:                         cx->fp->script->filename,
21685:                         js_FramePCToLineNumber(cx, cx->fp),
21685:                         FramePCOffset(cx->fp));)
21433: 
21433:     /* Find an acceptable peer, make sure our types fit. */
21433:     Fragment* empty;
21433:     bool success = false;
21433: 
21433:     f = r->findNestedCompatiblePeer(f, &empty);
24246:     if (f && f->code())
24246:         success = r->adjustCallerTypes(f);
21433: 
21433:     if (!success) {
21433:         AUDIT(noCompatInnerTrees);
25937: 
25937:         jsbytecode* outer = (jsbytecode*)tm->recorder->getFragment()->root->ip;
21433:         js_AbortRecording(cx, "No compatible inner tree");
25627: 
24307:         f = empty;
24307:         if (!f) {
24491:             f = getAnchor(tm, cx->fp->regs->pc, globalShape);
24307:             if (!f) {
24307:                 js_FlushJITCache(cx);
24307:                 return false;
24307:             }
24307:         }
25937:         return js_RecordTree(cx, tm, f, outer, globalShape, globalSlots);
21433:     }
21433: 
18250:     r->prepareTreeCall(f);
21521:     VMSideExit* innermostNestedGuard = NULL;
21521:     VMSideExit* lr = js_ExecuteTree(cx, f, inlineCallCount, &innermostNestedGuard);
25489:     if (!lr || r->wasDeepAborted()) {
25489:         if (!lr)
20422:             js_AbortRecording(cx, "Couldn't call inner tree");
18051:         return false;
18051:     }
26557: 
25937:     jsbytecode* outer = (jsbytecode*)tm->recorder->getFragment()->root->ip;
20931:     switch (lr->exitType) {
18051:       case LOOP_EXIT:
18284:         /* If the inner tree exited on an unknown loop exit, grow the tree around it. */
18284:         if (innermostNestedGuard) {
25937:             js_AbortRecording(cx, "Inner tree took different side exit, abort current "
25937:                               "recording and grow nesting tree");
25937:             return js_AttemptToExtendTree(cx, innermostNestedGuard, lr, outer);
18284:         }
17997:         /* emit a call to the inner tree and continue recording the outer tree trace */
17997:         r->emitTreeCall(f, lr);
17997:         return true;
21433:       case UNSTABLE_LOOP_EXIT:
21433:         /* abort recording so the inner loop can become type stable. */
21433:         js_AbortRecording(cx, "Inner tree is trying to stabilize, abort outer recording");
25937:         return js_AttemptToStabilizeTree(cx, lr, outer);
18051:       case BRANCH_EXIT:
18051:         /* abort recording the outer tree, extend the inner tree */
20422:         js_AbortRecording(cx, "Inner tree is trying to grow, abort outer recording");
25937:         return js_AttemptToExtendTree(cx, lr, NULL, outer);
18051:       default:
20931:         debug_only_v(printf("exit_type=%d\n", lr->exitType);)
20422:             js_AbortRecording(cx, "Inner tree not suitable for calling");
18051:         return false;
18051:     }
17988: }
21433: 
21433: static bool
21433: js_IsEntryTypeCompatible(jsval* vp, uint8* m)
21433: {
21433:     unsigned tag = JSVAL_TAG(*vp);
21433: 
23075:     debug_only_v(printf("%c/%c ", tagChar[tag], typeChar[*m]);)
21433: 
21433:     switch (*m) {
21433:       case JSVAL_INT:
21433:         jsint i;
21433:         if (JSVAL_IS_INT(*vp))
21433:             return true;
21433:         if ((tag == JSVAL_DOUBLE) && JSDOUBLE_IS_INT(*JSVAL_TO_DOUBLE(*vp), i))
21433:             return true;
25469:         debug_only_v(printf("int != tag%u(value=%lu) ", tag, (unsigned long)*vp);)
21433:         return false;
21433:       case JSVAL_DOUBLE:
21433:         if (JSVAL_IS_INT(*vp) || tag == JSVAL_DOUBLE)
21433:             return true;
21433:         debug_only_v(printf("double != tag%u ", tag);)
21433:         return false;
21433:       case JSVAL_BOOLEAN:
21433:         if (tag == JSVAL_BOOLEAN)
21433:             return true;
21433:         debug_only_v(printf("bool != tag%u", tag);)
21433:         return false;
21433:       case JSVAL_STRING:
21828:         if (tag == JSVAL_STRING)
21433:             return true;
21433:         debug_only_v(printf("string != tag%u", tag);)
21433:         return false;
23075:       case JSVAL_TNULL:
23075:         return JSVAL_IS_NULL(*vp);
21433:       default:
21433:         JS_ASSERT(*m == JSVAL_OBJECT);
23075:         if (tag == JSVAL_OBJECT && !JSVAL_IS_NULL(*vp))
21433:             return true;
21433:         debug_only_v(printf("object != tag%u", tag);)
21433:         return false;
21433:     }
21433: }
21433: 
22652: JS_REQUIRES_STACK Fragment*
22652: TraceRecorder::findNestedCompatiblePeer(Fragment* f, Fragment** empty)
21433: {
21433:     Fragment* demote;
21433:     JSTraceMonitor* tm;
21433:     unsigned max_demotes;
21433: 
21433:     if (empty)
21433:         *empty = NULL;
21433:     demote = NULL;
21433: 
21433:     tm = &JS_TRACE_MONITOR(cx);
24491:     unsigned int ngslots = treeInfo->globalSlots->length();
24491:     uint16* gslots = treeInfo->globalSlots->data();
21433: 
21433:     /* We keep a maximum tally - we want to select the peer most likely to work so we don't keep
21433:      * recording.
21433:      */
21433:     max_demotes = 0;
21433: 
21433:     TreeInfo* ti;
21433:     for (; f != NULL; f = f->peer) {
21433:         if (!f->code()) {
21433:             if (empty)
21433:                 *empty = f;
21433:             continue;
21433:         }
21433: 
21433:         unsigned demotes = 0;
21433:         ti = (TreeInfo*)f->vmprivate;
21433: 
25469:         debug_only_v(printf("checking nested types %p: ", (void*)f);)
21433: 
24491:         if (ngslots > ti->nGlobalTypes())
25491:             specializeTreesToMissingGlobals(cx, ti);
24246: 
24246:         uint8* m = ti->typeMap.data();
24246: 
24246:         FORALL_SLOTS(cx, ngslots, gslots, 0,
21433:             debug_only_v(printf("%s%d=", vpname, vpnum);)
21433:             if (!js_IsEntryTypeCompatible(vp, m))
21433:                 goto check_fail;
21468:             if (*m == JSVAL_STRING && *vp == JSVAL_VOID)
21468:                 goto check_fail;
21433:             if (*m == JSVAL_INT && !isPromoteInt(get(vp)))
21433:                 demotes++;
21433:             m++;
21433:         );
24246:         JS_ASSERT(unsigned(m - ti->typeMap.data()) == ti->typeMap.length());
21433: 
21433:         debug_only_v(printf(" (demotes %d)\n", demotes);)
21433: 
21433:         if (demotes) {
21433:             if (demotes > max_demotes) {
21433:                 demote = f;
21433:                 max_demotes = demotes;
21433:             }
21433:             continue;
21433:         } else {
21433:             return f;
21433:         }
21433: 
21433: check_fail:
21433:         debug_only_v(printf("\n"));
21433:         continue;
21433:     }
21433: 
21433:     if (demote)
21433:         return demote;
21433: 
21433:    return NULL;
21433: }
21433: 
21433: /**
21433:  * Check if types are usable for trace execution.
21433:  *
21433:  * @param cx            Context.
21433:  * @param ti            Tree info of peer we're testing.
21433:  * @return              True if compatible (with or without demotions), false otherwise.
21433:  */
22652: static JS_REQUIRES_STACK bool
21433: js_CheckEntryTypes(JSContext* cx, TreeInfo* ti)
21433: {
24491:     unsigned int ngslots = ti->globalSlots->length();
24491:     uint16* gslots = ti->globalSlots->data();
24491: 
24491:     JS_ASSERT(ti->nStackTypes == js_NativeStackSlots(cx, 0));
24491: 
24491:     if (ngslots > ti->nGlobalTypes())
25491:         specializeTreesToMissingGlobals(cx, ti);
24246: 
24246:     uint8* m = ti->typeMap.data();
24246: 
24246:     JS_ASSERT(ti->typeMap.length() == js_NativeStackSlots(cx, 0) + ngslots);
24491:     JS_ASSERT(ti->typeMap.length() == ti->nStackTypes + ngslots);
24491:     JS_ASSERT(ti->nGlobalTypes() == ngslots);
24246:     FORALL_SLOTS(cx, ngslots, gslots, 0,
21433:         debug_only_v(printf("%s%d=", vpname, vpnum);)
24246:         JS_ASSERT(*m != 0xCD);
21433:         if (!js_IsEntryTypeCompatible(vp, m))
21433:             goto check_fail;
21433:         m++;
21433:     );
24246:     JS_ASSERT(unsigned(m - ti->typeMap.data()) == ti->typeMap.length());
21433: 
21433:     debug_only_v(printf("\n");)
21433:     return true;
21433: 
21433: check_fail:
21433:     debug_only_v(printf("\n");)
21433:     return false;
21433: }
21433: 
21433: /**
21433:  * Find an acceptable entry tree given a PC.
21433:  *
21433:  * @param cx            Context.
21433:  * @param f             First peer fragment.
21433:  * @param nodemote      If true, will try to find a peer that does not require demotion.
25627:  * @out   count         Number of fragments consulted.
21433:  */
22652: static JS_REQUIRES_STACK Fragment*
25627: js_FindVMCompatiblePeer(JSContext* cx, Fragment* f, uintN& count)
25627: {
25627:     count = 0;
21433:     for (; f != NULL; f = f->peer) {
21433:         if (f->vmprivate == NULL)
21433:             continue;
25469:         debug_only_v(printf("checking vm types %p (ip: %p): ", (void*)f, f->ip);)
21433:         if (js_CheckEntryTypes(cx, (TreeInfo*)f->vmprivate))
21433:             return f;
25627:         ++count;
21433:     }
21433:     return NULL;
21433: }
21433: 
24612: static void
24612: LeaveTree(InterpState&, VMSideExit* lr);
24612: 
21433: /**
21514:  * Executes a tree.
21433:  */
24612: static JS_REQUIRES_STACK VMSideExit*
21433: js_ExecuteTree(JSContext* cx, Fragment* f, uintN& inlineCallCount,
21521:                VMSideExit** innermostNestedGuardp)
18209: {
21433:     JS_ASSERT(f->code() && f->vmprivate);
24613:     JS_ASSERT(cx->builtinStatus == 0);
21433: 
21433:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
21514:     JSObject* globalObj = JS_GetGlobalForObject(cx, cx->fp->scopeChain);
18248:     TreeInfo* ti = (TreeInfo*)f->vmprivate;
24491:     unsigned ngslots = ti->globalSlots->length();
24491:     uint16* gslots = ti->globalSlots->data();
26282:     unsigned globalFrameSize = STOBJ_NSLOTS(globalObj);
21433: 
21433:     /* Make sure the global object is sane. */
24491:     JS_ASSERT(!ngslots || (OBJ_SHAPE(JS_GetGlobalForObject(cx, cx->fp->scopeChain)) == ti->globalShape));
21433:     /* Make sure our caller replenished the double pool. */
23449:     JS_ASSERT(tm->reservedDoublePoolPtr >= tm->reservedDoublePool + MAX_NATIVE_STACK_SLOTS);
21433: 
23448:     /* Reserve objects and stack space now, to make leaving the tree infallible. */
23448:     if (!js_ReserveObjects(cx, MAX_CALL_STACK_ENTRIES))
23448:         return NULL;
24612: 
26282:     /* Setup the interpreter state block, which is followed by the native global frame. */
26282:     InterpState* state = (InterpState*)alloca(sizeof(InterpState) + (globalFrameSize+1)*sizeof(double));
26282:     state->cx = cx;
26282:     state->globalObj = globalObj;
26282:     state->inlineCallCountp = &inlineCallCount;
26282:     state->innermostNestedGuardp = innermostNestedGuardp;
26282:     state->outermostTree = ti;
26282:     state->lastTreeExitGuard = NULL;
26282:     state->lastTreeCallGuard = NULL;
26282:     state->rpAtLastTreeCall = NULL;
26282: 
24612:     /* Setup the native global frame. */
26282:     double* global = (double*)(state+1);
24612: 
24612:     /* Setup the native stack frame. */
24612:     double stack_buffer[MAX_NATIVE_STACK_SLOTS];
26282:     state->stackBase = stack_buffer;
26282:     state->sp = stack_buffer + (ti->nativeStackBase/sizeof(double));
26282:     state->eos = stack_buffer + MAX_NATIVE_STACK_SLOTS;
24612: 
24612:     /* Setup the native call stack frame. */
24612:     FrameInfo* callstack_buffer[MAX_CALL_STACK_ENTRIES];
26282:     state->callstackBase = callstack_buffer;
26282:     state->rp = callstack_buffer;
26282:     state->eor = callstack_buffer + MAX_CALL_STACK_ENTRIES;
24612: 
24612:     void *reserve;
26282:     state->stackMark = JS_ARENA_MARK(&cx->stackPool);
23447:     JS_ARENA_ALLOCATE(reserve, &cx->stackPool, MAX_INTERP_STACK_BYTES);
23447:     if (!reserve)
23448:         return NULL;
23447: 
21514: #ifdef DEBUG
21433:     memset(stack_buffer, 0xCD, sizeof(stack_buffer));
26282:     memset(global, 0xCD, (globalFrameSize+1)*sizeof(double));
21514: #endif
21433: 
26282:     debug_only(*(uint64*)&global[globalFrameSize] = 0xdeadbeefdeadbeefLL;)
18773:     debug_only_v(printf("entering trace at %s:%u@%u, native stack slots: %u code: %p\n",
18260:                         cx->fp->script->filename,
21685:                         js_FramePCToLineNumber(cx, cx->fp),
21685:                         FramePCOffset(cx->fp),
21685:                         ti->maxNativeStackSlots,
21685:                         f->code());)
18210: 
24491:     JS_ASSERT(ti->nGlobalTypes() == ngslots);
24246: 
21433:     if (ngslots)
26282:         BuildNativeGlobalFrame(cx, ngslots, gslots, ti->globalTypeMap(), global);
24612:     BuildNativeStackFrame(cx, 0/*callDepth*/, ti->typeMap.data(), stack_buffer);
24612: 
17397:     union { NIns *code; GuardRecord* (FASTCALL *func)(InterpState*, Fragment*); } u;
17397:     u.code = f->code();
17923: 
24612: #ifdef EXECUTE_TREE_TIMER
26282:     state->startTime = rdtsc();
18776: #endif
17923: 
24376:     /* Set a flag that indicates to the runtime system that we are running in native code
24376:        now and we don't want automatic GC to happen. Instead we will get a silent failure,
24376:        which will cause a trace exit at which point the interpreter re-tries the operation
24376:        and eventually triggers the GC. */
24376:     JS_ASSERT(!tm->onTrace);
18782:     tm->onTrace = true;
26282:     cx->interpState = state;
18723: 
20893:     debug_only(fflush(NULL);)
20931:     GuardRecord* rec;
18723: #if defined(JS_NO_FASTCALL) && defined(NANOJIT_IA32)
26282:     SIMULATE_FASTCALL(rec, state, NULL, u.func);
18723: #else
26282:     rec = u.func(state, NULL);
18723: #endif
24376:     VMSideExit* lr = (VMSideExit*)rec->exit;
20931: 
21433:     AUDIT(traceTriggered);
21433: 
20931:     JS_ASSERT(lr->exitType != LOOP_EXIT || !lr->calldepth);
24376:     tm->onTrace = false;
26282:     LeaveTree(*state, lr);
26282:     return state->innermost;
24612: }
24612: 
24612: static JS_FORCES_STACK void
24612: LeaveTree(InterpState& state, VMSideExit* lr)
24612: {
24612:     VOUCH_DOES_NOT_REQUIRE_STACK();
24612: 
24612:     JSContext* cx = state.cx;
24612:     FrameInfo** callstack = state.callstackBase;
24612:     double* stack = state.stackBase;
17923: 
20429:     /* Except if we find that this is a nested bailout, the guard the call returned is the
20429:        one we have to use to adjust pc and sp. */
21521:     VMSideExit* innermost = lr;
20429: 
19588:     /* While executing a tree we do not update state.sp and state.rp even if they grow. Instead,
19588:        guards tell us by how much sp and rp should be incremented in case of a side exit. When
19588:        calling a nested tree, however, we actively adjust sp and rp. If we have such frames
19588:        from outer trees on the stack, then rp will have been adjusted. Before we can process
19588:        the stack of the frames of the tree we directly exited from, we have to first work our
19588:        way through the outer frames and generate interpreter frames for them. Once the call
19588:        stack (rp) is empty, we can process the final frames (which again are not directly
19588:        visible and only the guard we exited on will tells us about). */
23262:     FrameInfo** rp = (FrameInfo**)state.rp;
20931:     if (lr->exitType == NESTED_EXIT) {
21521:         VMSideExit* nested = state.lastTreeCallGuard;
20429:         if (!nested) {
20429:             /* If lastTreeCallGuard is not set in state, we only have a single level of
20429:                nesting in this exit, so lr itself is the innermost and outermost nested
20429:                guard, and hence we set nested to lr. The calldepth of the innermost guard
20429:                is not added to state.rp, so we do it here manually. For a nesting depth
20429:                greater than 1 the CallTree builtin already added the innermost guard's
20429:                calldepth to state.rpAtLastTreeCall. */
20429:             nested = lr;
20429:             rp += lr->calldepth;
20429:         } else {
20429:             /* During unwinding state.rp gets overwritten at every step and we restore
20429:                it here to its state at the innermost nested guard. The builtin already
20429:                added the calldepth of that innermost guard to rpAtLastTreeCall. */
23262:             rp = (FrameInfo**)state.rpAtLastTreeCall;
20429:         }
20429:         innermost = state.lastTreeExitGuard;
24612:         if (state.innermostNestedGuardp)
24612:             *state.innermostNestedGuardp = nested;
20429:         JS_ASSERT(nested);
20931:         JS_ASSERT(nested->exitType == NESTED_EXIT);
20429:         JS_ASSERT(state.lastTreeExitGuard);
20931:         JS_ASSERT(state.lastTreeExitGuard->exitType != NESTED_EXIT);
19590:     }
21433: 
24612:     int32_t bs = cx->builtinStatus;
24612:     cx->builtinStatus = 0;
24612:     bool bailed = innermost->exitType == STATUS_EXIT && (bs & JSBUILTIN_BAILED);
24870:     if (bailed) {
24612:         /*
24612:          * Deep-bail case.
24612:          *
24612:          * A _FAIL native already called LeaveTree. We already reconstructed
24612:          * the interpreter stack, in pre-call state, with pc pointing to the
24612:          * CALL/APPLY op, for correctness. Then we continued in native code.
24870:          */
24870:         if (!(bs & JSBUILTIN_ERROR)) {
24870:             /*
24612:              * The native succeeded (no exception or error). After it returned, the
24612:              * trace stored the return value (at the top of the native stack) and
24612:              * then immediately flunked the guard on cx->builtinStatus.
24612:              *
24612:              * Now LeaveTree has been called again from the tail of
24612:              * js_ExecuteTree. We are about to return to the interpreter. Adjust
24612:              * the top stack frame to resume on the next op.
24612:              */
24612:             JS_ASSERT(*cx->fp->regs->pc == JSOP_CALL || *cx->fp->regs->pc == JSOP_APPLY);
24612:             uintN argc = GET_ARGC(cx->fp->regs->pc);
24612:             cx->fp->regs->pc += JSOP_CALL_LENGTH;
24612:             cx->fp->regs->sp -= argc + 1;
24612:             JS_ASSERT_IF(!cx->fp->imacpc,
24612:                          cx->fp->slots + cx->fp->script->nfixed +
24612:                          js_ReconstructStackDepth(cx, cx->fp->script, cx->fp->regs->pc) ==
24612:                          cx->fp->regs->sp);
24612: 
24612:             /*
24612:              * The return value was not available when we reconstructed the stack,
24612:              * but we have it now. Box it.
24612:              */
24612:             uint8* typeMap = getStackTypeMap(innermost);
24612:             NativeToValue(cx,
24612:                           cx->fp->regs->sp[-1],
24612:                           typeMap[innermost->numStackSlots - 1],
24612:                           (jsdouble *) state.sp + innermost->sp_adj / sizeof(jsdouble) - 1);
24870:         }
24870:         JS_TRACE_MONITOR(cx).prohibitRecording = false;
24612:         return;
24612:     }
24612: 
24612:     JS_ARENA_RELEASE(&cx->stackPool, state.stackMark);
19588:     while (callstack < rp) {
19588:         /* Synthesize a stack frame and write out the values in it using the type map pointer
19588:            on the native call stack. */
23447:         js_SynthesizeFrame(cx, **callstack);
23262:         int slots = FlushNativeStackFrame(cx, 1/*callDepth*/, (uint8*)(*callstack+1), stack, cx->fp);
19588: #ifdef DEBUG
19588:         JSStackFrame* fp = cx->fp;
19588:         debug_only_v(printf("synthesized deep frame for %s:%u@%u, slots=%d\n",
21685:                             fp->script->filename,
21685:                             js_FramePCToLineNumber(cx, fp),
21685:                             FramePCOffset(fp),
21685:                             slots);)
19588: #endif
19588:         /* Keep track of the additional frames we put on the interpreter stack and the native
19588:            stack slots we consumed. */
24612:         ++*state.inlineCallCountp;
19588:         ++callstack;
18170:         stack += slots;
18164:     }
19588: 
18164:     /* We already synthesized the frames around the innermost guard. Here we just deal
18164:        with additional frames inside the tree we are bailing out from. */
19588:     JS_ASSERT(rp == callstack);
20429:     unsigned calldepth = innermost->calldepth;
18226:     unsigned calldepth_slots = 0;
19588:     for (unsigned n = 0; n < calldepth; ++n) {
23447:         calldepth_slots += js_SynthesizeFrame(cx, *callstack[n]);
24612:         ++*state.inlineCallCountp;
19588: #ifdef DEBUG
19588:         JSStackFrame* fp = cx->fp;
19588:         debug_only_v(printf("synthesized shallow frame for %s:%u@%u\n",
21685:                             fp->script->filename, js_FramePCToLineNumber(cx, fp),
21685:                             FramePCOffset(fp));)
19588: #endif
19588:     }
17923: 
21685:     /* Adjust sp and pc relative to the tree we exited from (not the tree we entered into).
21685:        These are our final values for sp and pc since js_SynthesizeFrame has already taken
22925:        care of all frames in between. But first we recover fp->blockChain, which comes from
22925:        the side exit struct. */
17923:     JSStackFrame* fp = cx->fp;
18226: 
22925:     fp->blockChain = innermost->block;
22925: 
18193:     /* If we are not exiting from an inlined frame the state->sp is spbase, otherwise spbase
18193:        is whatever slots frames around us consume. */
25111:     fp->regs->pc = innermost->pc;
25111:     fp->imacpc = innermost->imacpc;
20931:     fp->regs->sp = StackBase(fp) + (innermost->sp_adj / sizeof(double)) - calldepth_slots;
21685:     JS_ASSERT_IF(!fp->imacpc,
21685:                  fp->slots + fp->script->nfixed +
18772:                  js_ReconstructStackDepth(cx, fp->script, fp->regs->pc) == fp->regs->sp);
17923: 
24612: #ifdef EXECUTE_TREE_TIMER
24612:     uint64 cycles = rdtsc() - state.startTime;
21459: #elif defined(JS_JIT_SPEW)
19040:     uint64 cycles = 0;
18788: #endif
18788: 
20931:     debug_only_v(printf("leaving trace at %s:%u@%u, op=%s, lr=%p, exitType=%d, sp=%d, "
19588:                         "calldepth=%d, cycles=%llu\n",
21685:                         fp->script->filename,
21685:                         js_FramePCToLineNumber(cx, fp),
21685:                         FramePCOffset(fp),
21685:                         js_CodeName[fp->imacpc ? *fp->imacpc : *fp->regs->pc],
25469:                         (void*)lr,
20931:                         lr->exitType,
20931:                         fp->regs->sp - StackBase(fp),
19588:                         calldepth,
18788:                         cycles));
17923: 
18200:     /* If this trace is part of a tree, later branches might have added additional globals for
24246:        which we don't have any type information available in the side exit. We merge in this
18200:        information from the entry type-map. See also comment in the constructor of TraceRecorder
18200:        why this is always safe to do. */
24612:     TreeInfo* outermostTree = state.outermostTree;
24612:     uint16* gslots = outermostTree->globalSlots->data();
24612:     unsigned ngslots = outermostTree->globalSlots->length();
24612:     JS_ASSERT(ngslots == outermostTree->nGlobalTypes());
25491:     uint8* globalTypeMap;
25491: 
25491:     /* Are there enough globals? This is the ideal fast path. */
25491:     if (innermost->numGlobalSlots == ngslots) {
25491:         globalTypeMap = getGlobalTypeMap(innermost);
25491:     /* Otherwise, merge the typemap of the innermost entry and exit together.  This should always
25491:        work because it is invalid for nested trees or linked trees to have incompatible types.
25491:        Thus, whenever a new global type is lazily added into a tree, all dependent and linked
25491:        trees are immediately specialized (see bug 476653). */
25491:     } else {
25491:         TreeInfo* ti = (TreeInfo*)innermost->from->root->vmprivate;
25491:         JS_ASSERT(ti->nGlobalTypes() == ngslots);
25491:         JS_ASSERT(ti->nGlobalTypes() > innermost->numGlobalSlots);
25491:         globalTypeMap = (uint8*)alloca(ngslots * sizeof(uint8));
25491:         memcpy(globalTypeMap, getGlobalTypeMap(innermost), innermost->numGlobalSlots);
25491:         memcpy(globalTypeMap + innermost->numGlobalSlots,
25491:                ti->globalTypeMap() + innermost->numGlobalSlots,
25491:                ti->nGlobalTypes() - innermost->numGlobalSlots);
25491:     }
18200: 
18154:     /* write back interned globals */
26282:     double* global = (double*)(&state + 1);
26282:     FlushNativeGlobalFrame(cx, ngslots, gslots, globalTypeMap, global);
26282:     JS_ASSERT(*(uint64*)&global[STOBJ_NSLOTS(state.globalObj)] == 0xdeadbeefdeadbeefLL);
17726: 
18154:     /* write back native stack frame */
23446: #ifdef DEBUG
23446:     int slots =
23446: #endif
23446:         FlushNativeStackFrame(cx, innermost->calldepth,
24246:                               getStackTypeMap(innermost),
21685:                               stack, NULL);
20931:     JS_ASSERT(unsigned(slots) == innermost->numStackSlots);
18154: 
19591: #ifdef DEBUG
26268:     // Verify that our state restoration worked.
19591:     for (JSStackFrame* fp = cx->fp; fp; fp = fp->down) {
19591:         JS_ASSERT(!fp->callee || JSVAL_IS_OBJECT(fp->argv[-1]));
26264:         JS_ASSERT_IF(fp->callee && fp->thisp != JSVAL_TO_OBJECT(fp->argv[-1]),
26264:                      !(fp->flags & JSFRAME_COMPUTED_THIS) && !fp->thisp);
19591:     }
19591: #endif
23706: #ifdef JS_JIT_SPEW
23706:     if (innermost->exitType != TIMEOUT_EXIT)
17726:         AUDIT(sideExitIntoInterpreter);
23706:     else
23706:         AUDIT(timeoutIntoInterpreter);
23706: #endif
17397: 
24612:     state.innermost = innermost;
17772: }
17376: 
22652: JS_REQUIRES_STACK bool
20422: js_MonitorLoopEdge(JSContext* cx, uintN& inlineCallCount)
17939: {
17939:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
17939: 
18672:     /* Is the recorder currently active? */
17954:     if (tm->recorder) {
24493:         jsbytecode* innerLoopHeaderPC = cx->fp->regs->pc;
24493: 
20422:         if (js_RecordLoopEdge(cx, tm->recorder, inlineCallCount))
17954:             return true;
24493: 
24493:         /*
24493:          * js_RecordLoopEdge will invoke an inner tree if we have a matching one. If we
24493:          * arrive here, that tree didn't run to completion and instead we mis-matched
24493:          * or the inner tree took a side exit other than the loop exit. We are thus
24493:          * no longer guaranteed to be parked on the same loop header js_MonitorLoopEdge
24493:          * was called for. In fact, this might not even be a loop header at all. Hence
24493:          * if the program counter no longer hovers over the inner loop header, return to
24493:          * the interpreter and do not attempt to trigger or record a new tree at this
24493:          * location.
24493:          */
24493:         if (innerLoopHeaderPC != cx->fp->regs->pc)
24493:             return false;
17954:     }
18317:     JS_ASSERT(!tm->recorder);
17939: 
23449:     /* Check the pool of reserved doubles (this might trigger a GC). */
23449:     if (tm->reservedDoublePoolPtr < (tm->reservedDoublePool + MAX_NATIVE_STACK_SLOTS) &&
23449:         !js_ReplenishReservedPool(cx, tm)) {
21514:         return false; /* Out of memory, don't try to record now. */
21514:     }
21514: 
21514:     /* Make sure the shape of the global object still matches (this might flush the JIT cache). */
21514:     JSObject* globalObj = JS_GetGlobalForObject(cx, cx->fp->scopeChain);
24491:     uint32 globalShape = -1;
24491:     SlotList* globalSlots = NULL;
24491: 
24491:     if (!js_CheckGlobalObjectShape(cx, tm, globalObj, &globalShape, &globalSlots))
21688:         js_FlushJITCache(cx);
21514: 
25087:     /* Do not enter the JIT code with a pending operation callback. */
25087:     if (cx->operationCallbackFlag)
25087:         return false;
25087:     
17940:     jsbytecode* pc = cx->fp->regs->pc;
24290: 
24491:     Fragment* f = getLoop(tm, pc, globalShape);
18204:     if (!f)
24491:         f = getAnchor(tm, pc, globalShape);
24307: 
24307:     if (!f) {
24307:         js_FlushJITCache(cx);
24307:         return false;
24307:     }
21433: 
21433:     /* If we have no code in the anchor and no peers, we definitively won't be able to
24290:        activate any trees so, start compiling. */
21433:     if (!f->code() && !f->peer) {
25627:     record:
25627:         if (++f->hits() < HOTLOOP)
25627:             return false;
21433:         /* We can give RecordTree the root peer. If that peer is already taken, it will
21433:            walk the peer list and find us a free slot or allocate a new tree if needed. */
24491:         return js_RecordTree(cx, tm, f->first, NULL, globalShape, globalSlots);
21433:     }
25627: 
25627:     debug_only_v(printf("Looking for compat peer %d@%d, from %p (ip: %p)\n",
21685:                         js_FramePCToLineNumber(cx, cx->fp),
25627:                         FramePCOffset(cx->fp), (void*)f, f->ip);)
25627: 
25627:     uintN count;
25627:     Fragment* match = js_FindVMCompatiblePeer(cx, f, count);
25627:     if (!match) {
25627:         if (count < MAXPEERS)
25627:             goto record;
25627:         /* If we hit the max peers ceiling, don't try to lookup fragments all the time. Thats
25627:            expensive. This must be a rather type-unstable loop. */
25627:         debug_only_v(printf("Blacklisted: too many peer trees.\n");)
25895:         js_Blacklist(f->root);
25627:         return false;
25627:     }
21433: 
21521:     VMSideExit* lr = NULL;
21521:     VMSideExit* innermostNestedGuard = NULL;
21433: 
21433:     lr = js_ExecuteTree(cx, match, inlineCallCount, &innermostNestedGuard);
21433:     if (!lr)
21433:         return false;
21433: 
18284:     /* If we exit on a branch, or on a tree call guard, try to grow the inner tree (in case
18284:        of a branch exit), or the tree nested around the tree we exited from (in case of the
18284:        tree call guard). */
20931:     switch (lr->exitType) {
21433:       case UNSTABLE_LOOP_EXIT:
21456:         return js_AttemptToStabilizeTree(cx, lr, NULL);
18284:       case BRANCH_EXIT:
25099:       case CASE_EXIT:
21433:         return js_AttemptToExtendTree(cx, lr, NULL, NULL);
18284:       case LOOP_EXIT:
18284:         if (innermostNestedGuard)
21433:             return js_AttemptToExtendTree(cx, innermostNestedGuard, lr, NULL);
17951:         return false;
18284:       default:
18284:         /* No, this was an unusual exit (i.e. out of memory/GC), so just resume interpretation. */
18284:         return false;
18284:     }
17939: }
17939: 
23111: JS_REQUIRES_STACK JSMonitorRecordingStatus
23440: TraceRecorder::monitorRecording(JSContext* cx, TraceRecorder* tr, JSOp op)
23440: {
23111:     /* Process deepAbort() requests now. */
23440:     if (tr->wasDeepAborted()) {
22632:         js_AbortRecording(cx, "deep abort requested");
23111:         return JSMRS_STOP;
23111:     }
23111: 
26559:     JS_ASSERT(!tr->fragment->lastIns);
26559: 
26557:     /*
26557:      * Clear one-shot state used to communicate between record_JSOP_CALL and post-
26557:      * opcode-case-guts record hook (record_FastNativeCallComplete).
26557:      */
23440:     tr->pendingTraceableNative = NULL;
19068: 
26557:     debug_only_v(js_Disassemble1(cx, cx->fp->script, cx->fp->regs->pc,
26557:                                  (cx->fp->imacpc)
26557:                                  ? 0
26557:                                  : cx->fp->regs->pc - cx->fp->script->code,
26557:                                  !cx->fp->imacpc, stdout);)
19075: 
26011:     /* If op is not a break or a return from a loop, continue recording and follow the
26011:        trace. We check for imacro-calling bytecodes inside each switch case to resolve
26011:        the if (JSOP_IS_IMACOP(x)) conditions at compile time. */
26011: 
23111:     bool flag;
26563: #ifdef DEBUG
26563:     bool wasInImacro = (cx->fp->imacpc != NULL);
26563: #endif
23111:     switch (op) {
26557:       default: goto stop_recording;
23111: # define OPDEF(x,val,name,token,length,nuses,ndefs,prec,format)               \
23111:       case x:                                                                 \
23440:         flag = tr->record_##x();                                              \
26011:         if (JSOP_IS_IMACOP(x))                                                \
23111:             goto imacro;                                                      \
23111:         break;
23111: # include "jsopcode.tbl"
23111: # undef OPDEF
23111:     }
23111: 
26563:     JS_ASSERT_IF(!wasInImacro, cx->fp->imacpc == NULL);
26563: 
26557:     /* Process deepAbort() requests now. */
26557:     if (tr->wasDeepAborted()) {
26557:         js_AbortRecording(cx, "deep abort requested");
26557:         return JSMRS_STOP;
26557:     }
26557: 
26557:     if (JS_TRACE_MONITOR(cx).fragmento->assm()->error()) {
26557:         js_AbortRecording(cx, "error during recording");
26557:         return JSMRS_STOP;
26557:     }
26557: 
26557:     if (tr->lirbuf->outOMem()) {
26557:         js_AbortRecording(cx, "no more LIR memory");
26557:         js_FlushJITCache(cx);
26557:         return JSMRS_STOP;
26557:     }
26557: 
23111:     if (flag)
23111:         return JSMRS_CONTINUE;
26557: 
26557:     goto stop_recording;
23111: 
23111:   imacro:
23111:     /* We save macro-generated code size also via bool TraceRecorder::record_JSOP_*
23111:        return type, instead of a three-state: OK, ABORTED, IMACRO_STARTED. But the
23111:        price of this is the JSFRAME_IMACRO_START frame flag. We need one more bit
23111:        to detect that TraceRecorder::call_imacro was invoked by the record_JSOP_
23111:        method. */
23111:     if (flag)
23111:         return JSMRS_CONTINUE;
23111:     if (cx->fp->flags & JSFRAME_IMACRO_START) {
23111:         cx->fp->flags &= ~JSFRAME_IMACRO_START;
23111:         return JSMRS_IMACRO;
23111:     }
23111: 
26557:   stop_recording:
26557:     /* If we recorded the end of the trace, destroy the recorder now. */
26557:     if (tr->fragment->lastIns) {
26557:         js_DeleteRecorder(cx);
26557:         return JSMRS_STOP;
26557:     }
26557: 
26557:     /* Looks like we encountered an error condition. Abort recording. */
23111:     js_AbortRecording(cx, js_CodeName[op]);
23111:     return JSMRS_STOP;
18683: }
18683: 
22652: JS_REQUIRES_STACK void
20422: js_AbortRecording(JSContext* cx, const char* reason)
17350: {
18614:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
18614:     JS_ASSERT(tm->recorder != NULL);
20965:     AUDIT(recorderAborted);
21685: 
18614:     /* Abort the trace and blacklist its starting point. */
20965:     Fragment* f = tm->recorder->getFragment();
20965:     if (!f) {
20965:         js_DeleteRecorder(cx);
20965:         return;
20965:     }
20965:     JS_ASSERT(!f->vmprivate);
25627: 
25627: #ifdef DEBUG
25627:     TreeInfo* ti = tm->recorder->getTreeInfo();
26398:     debug_only_a(printf("Abort recording of tree %s:%d@%d at %s:%d@%d: %s.\n",
25627:                         ti->treeFileName,
25627:                         ti->treeLineNumber,
25627:                         ti->treePCOffset,
25627:                         cx->fp->script->filename,
25627:                         js_FramePCToLineNumber(cx, cx->fp),
25627:                         FramePCOffset(cx->fp),
25627:                         reason);)
25627: #endif
25627: 
25627:     js_Backoff(f->root, cx->fp->regs->pc);
24486: 
24486:     /*
24486:      * If js_DeleteRecorder flushed the code cache, we can't rely on f any more.
24486:      */
24486:     if (!js_DeleteRecorder(cx))
24486:         return;
24486: 
24486:     /*
24486:      * If this is the primary trace and we didn't succeed compiling, trash the
24486:      * TreeInfo object.
24486:      */
18614:     if (!f->code() && (f->root == f))
17889:         js_TrashTree(cx, f);
17350: }
17350: 
18333: #if defined NANOJIT_IA32
18333: static bool
18333: js_CheckForSSE2()
18333: {
18333:     int features = 0;
18333: #if defined _MSC_VER
18333:     __asm
18333:     {
18333:         pushad
18333:         mov eax, 1
18333:         cpuid
18333:         mov features, edx
18333:         popad
18333:     }
18333: #elif defined __GNUC__
21475:     asm("xchg %%esi, %%ebx\n" /* we can't clobber ebx on gcc (PIC register) */
20953:         "mov $0x01, %%eax\n"
20953:         "cpuid\n"
20953:         "mov %%edx, %0\n"
21475:         "xchg %%esi, %%ebx\n"
20953:         : "=m" (features)
21475:         : /* We have no inputs */
21475:         : "%eax", "%esi", "%ecx", "%edx"
18788:        );
19058: #elif defined __SUNPRO_C || defined __SUNPRO_CC
19058:     asm("push %%ebx\n"
19058:         "mov $0x01, %%eax\n"
19058:         "cpuid\n"
19058:         "pop %%ebx\n"
19058:         : "=d" (features)
18333:         : /* We have no inputs */
19058:         : "%eax", "%ecx"
18333:        );
18333: #endif
18333:     return (features & (1<<26)) != 0;
18333: }
18333: #endif
18333: 
26545: #if defined(NANOJIT_ARM)
26545: 
26545: #if defined(_MSC_VER) && defined(WINCE)
26545: 
26545: // these come in from jswince.asm
26545: extern "C" int js_arm_try_armv6t2_op();
26545: extern "C" int js_arm_try_vfp_op();
26545: 
26545: static bool
26545: js_arm_check_armv6t2() {
26545:     bool ret = false;
26545:     __try {
26545:         js_arm_try_armv6t2_op();
26545:         ret = true;
26545:     } __except(GetExceptionCode() == EXCEPTION_ILLEGAL_INSTRUCTION) {
26545:         ret = false;
26545:     }
26545:     return ret;
26545: }
26545: 
26545: static bool
26545: js_arm_check_vfp() {
26545:     bool ret = false;
26545:     __try {
26545:         js_arm_try_vfp_op();
26545:         ret = true;
26545:     } __except(GetExceptionCode() == EXCEPTION_ILLEGAL_INSTRUCTION) {
26545:         ret = false;
26545:     }
26545:     return ret;
26545: }
26545: 
26545: #elif defined(__GNUC__) && defined(AVMPLUS_LINUX)
26545: 
26545: #include <stdlib.h>
26545: #include <unistd.h>
26545: #include <sys/types.h>
26545: #include <sys/stat.h>
26545: #include <sys/mman.h>
26545: #include <fcntl.h>
26545: #include <string.h>
26545: #include <elf.h>
26545: 
26545: static bool arm_has_v7 = false;
26545: static bool arm_has_v6 = false;
26545: static bool arm_has_vfp = false;
26545: static bool arm_has_neon = false;
26545: static bool arm_has_iwmmxt = false;
26545: static bool arm_tests_initialized = false;
26545: 
26545: static void
26545: arm_read_auxv() {
26545:     int fd;
26545:     Elf32_auxv_t aux;
26545: 
26545:     fd = open("/proc/self/auxv", O_RDONLY);
26545:     if (fd > 0) {
26545:         while (read(fd, &aux, sizeof(Elf32_auxv_t))) {
26545:             if (aux.a_type == AT_HWCAP) {
26545:                 uint32_t hwcap = aux.a_un.a_val;
26545:                 if (getenv("ARM_FORCE_HWCAP"))
26545:                     hwcap = strtoul(getenv("ARM_FORCE_HWCAP"), NULL, 0);
26545:                 // hardcode these values to avoid depending on specific versions
26545:                 // of the hwcap header, e.g. HWCAP_NEON
26545:                 arm_has_vfp = (hwcap & 64) != 0;
26545:                 arm_has_iwmmxt = (hwcap & 512) != 0;
26545:                 // this flag is only present on kernel 2.6.29
26545:                 arm_has_neon = (hwcap & 4096) != 0;
26545:             } else if (aux.a_type == AT_PLATFORM) {
26545:                 const char *plat = (const char*) aux.a_un.a_val;
26545:                 if (getenv("ARM_FORCE_PLATFORM"))
26545:                     plat = getenv("ARM_FORCE_PLATFORM");
26545:                 if (strncmp(plat, "v7l", 3) == 0) {
26545:                     arm_has_v7 = true;
26545:                     arm_has_v6 = true;
26545:                 } else if (strncmp(plat, "v6l", 3) == 0) {
26545:                     arm_has_v6 = true;
26545:                 }
26545:             }
26545:         }
26545:         close (fd);
26545: 
26545:         // if we don't have 2.6.29, we have to do this hack; set
26545:         // the env var to trust HWCAP.
26545:         if (!getenv("ARM_TRUST_HWCAP") && arm_has_v7)
26545:             arm_has_neon = true;
26545:     }
26545: 
26545:     arm_tests_initialized = true;
26545: }
26545: 
26545: static bool
26545: js_arm_check_armv6t2() {
26545:     if (!arm_tests_initialized)
26545:         arm_read_auxv();
26545: 
26545:     return arm_has_v7;
26545: }
26545: 
26545: static bool
26545: js_arm_check_vfp() {
26545:     if (!arm_tests_initialized)
26545:         arm_read_auxv();
26545: 
26545:     return arm_has_vfp;
26545: }
26545: 
26545: #else
26545: #warning Not sure how to check for armv6t2 and vfp on your platform, assuming neither present.
26545: static bool
26545: js_arm_check_armv6t2() { return false; }
26548: static bool
26545: js_arm_check_vfp() { return false; }
26545: #endif
26545: 
26545: #endif /* NANOJIT_ARM */
26545: 
25940: void
18068: js_InitJIT(JSTraceMonitor *tm)
18068: {
26545:     if (!did_we_check_processor_features) {
18333: #if defined NANOJIT_IA32
22667:         avmplus::AvmCore::config.use_cmov =
22667:         avmplus::AvmCore::config.sse2 = js_CheckForSSE2();
18333: #endif
26545: #if defined NANOJIT_ARM
26545:         avmplus::AvmCore::config.vfp = js_arm_check_vfp();
26545:         avmplus::AvmCore::config.soft_float = !avmplus::AvmCore::config.vfp;
26545:         avmplus::AvmCore::config.v6t2 = js_arm_check_armv6t2();
26545: #endif
26545:         did_we_check_processor_features = true;
26545:     }
26545: 
17442:     if (!tm->fragmento) {
24491:         JS_ASSERT(!tm->reservedDoublePool);
17713:         Fragmento* fragmento = new (&gc) Fragmento(core, 24);
17884:         verbose_only(fragmento->labels = new (&gc) LabelMap(core, NULL);)
17442:         tm->fragmento = fragmento;
23264:         tm->lirbuf = new (&gc) LirBuffer(fragmento, NULL);
23264: #ifdef DEBUG
23264:         tm->lirbuf->names = new (&gc) LirNameMap(&gc, NULL, tm->fragmento->labels);
23264: #endif
24491:         for (size_t i = 0; i < MONITOR_N_GLOBAL_STATES; ++i) {
24491:             tm->globalStates[i].globalShape = -1;
24491:             JS_ASSERT(!tm->globalStates[i].globalSlots);
24491:             tm->globalStates[i].globalSlots = new (&gc) SlotList();
24491:         }
23449:         tm->reservedDoublePoolPtr = tm->reservedDoublePool = new jsval[MAX_NATIVE_STACK_SLOTS];
24307:         memset(tm->vmfragments, 0, sizeof(tm->vmfragments));
17442:     }
21491:     if (!tm->reFragmento) {
21502:         Fragmento* fragmento = new (&gc) Fragmento(core, 20);
21491:         verbose_only(fragmento->labels = new (&gc) LabelMap(core, NULL);)
21491:         tm->reFragmento = fragmento;
23237:         tm->reLirBuf = new (&gc) LirBuffer(fragmento, NULL);
21491:     }
17884: #if !defined XP_WIN
19623:     debug_only(memset(&jitstats, 0, sizeof(jitstats)));
17884: #endif
17726: }
17726: 
25940: void
18075: js_FinishJIT(JSTraceMonitor *tm)
18068: {
21459: #ifdef JS_JIT_SPEW
25882:     if (js_verboseStats && jitstats.recorderStarted) {
17726:         printf("recorder: started(%llu), aborted(%llu), completed(%llu), different header(%llu), "
18697:                "trees trashed(%llu), slot promoted(%llu), unstable loop variable(%llu), "
21433:                "breaks(%llu), returns(%llu), unstableInnerCalls(%llu)\n",
19623:                jitstats.recorderStarted, jitstats.recorderAborted, jitstats.traceCompleted,
19623:                jitstats.returnToDifferentLoopHeader, jitstats.treesTrashed, jitstats.slotPromoted,
21433:                jitstats.unstableLoopVariable, jitstats.breakLoopExits, jitstats.returnLoopExits,
21433:                jitstats.noCompatInnerTrees);
17728:         printf("monitor: triggered(%llu), exits(%llu), type mismatch(%llu), "
19623:                "global mismatch(%llu)\n", jitstats.traceTriggered, jitstats.sideExitIntoInterpreter,
19623:                jitstats.typeMapMismatchAtEntry, jitstats.globalShapeMismatchAtEntry);
22616:     }
17726: #endif
18079:     if (tm->fragmento != NULL) {
24491:         JS_ASSERT(tm->reservedDoublePool);
18056:         verbose_only(delete tm->fragmento->labels;)
23264: #ifdef DEBUG
23264:         delete tm->lirbuf->names;
23264:         tm->lirbuf->names = NULL;
23264: #endif
23264:         delete tm->lirbuf;
23264:         tm->lirbuf = NULL;
25102:         for (size_t i = 0; i < FRAGMENT_TABLE_SIZE; ++i) {
25102:             VMFragment* f = tm->vmfragments[i];
25102:             while(f) {
25102:                 VMFragment* next = f->next;
25102:                 tm->fragmento->clearFragment(f);
25102:                 f = next;
25102:             }
25102:             tm->vmfragments[i] = NULL;
25102:         }
18056:         delete tm->fragmento;
18079:         tm->fragmento = NULL;
24491:         for (size_t i = 0; i < MONITOR_N_GLOBAL_STATES; ++i) {
24491:             JS_ASSERT(tm->globalStates[i].globalSlots);
24491:             delete tm->globalStates[i].globalSlots;
24491:         }
23449:         delete[] tm->reservedDoublePool;
23449:         tm->reservedDoublePool = tm->reservedDoublePoolPtr = NULL;
18079:     }
21502:     if (tm->reFragmento != NULL) {
23237:         delete tm->reLirBuf;
21502:         verbose_only(delete tm->reFragmento->labels;)
21502:         delete tm->reFragmento;
21502:     }
17442: }
17442: 
21723: void
21723: TraceRecorder::pushAbortStack()
21723: {
21723:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
21723: 
21723:     JS_ASSERT(tm->abortStack != this);
21723: 
21723:     nextRecorderToAbort = tm->abortStack;
21723:     tm->abortStack = this;
21723: }
21723: 
21723: void
21723: TraceRecorder::popAbortStack()
21723: {
21723:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
21723: 
21723:     JS_ASSERT(tm->abortStack == this);
21723: 
21723:     tm->abortStack = nextRecorderToAbort;
21723:     nextRecorderToAbort = NULL;
21723: }
21723: 
25940: void
26569: js_PurgeJITOracle()
26569: {
26569:     oracle.clear();
26569: }
26569: 
26569: JS_REQUIRES_STACK void
26569: js_PurgeScriptFragments(JSContext* cx, JSScript* script)
18277: {
18725:     if (!TRACING_ENABLED(cx))
18725:         return;
26569:     debug_only_v(printf("Purging fragments for JSScript %p.\n", (void*)script);)
24879:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
24879:     for (size_t i = 0; i < FRAGMENT_TABLE_SIZE; ++i) {
25089:         for (VMFragment **f = &(tm->vmfragments[i]); *f; ) {
25089:             /* Disable future use of any script-associated VMFragment.*/
25089:             if (JS_UPTRDIFF((*f)->ip, script->code) < script->length) {
25089:                 debug_only_v(printf("Disconnecting VMFragment %p "
25089:                                     "with ip %p, in range [%p,%p).\n",
25469:                                     (void*)(*f), (*f)->ip, script->code,
25089:                                     script->code + script->length));
25102:                 VMFragment* next = (*f)->next;
25102:                 if (tm->fragmento)
25102:                     tm->fragmento->clearFragment(*f);
25102:                 *f = next;
25089:             } else {
25089:                 f = &((*f)->next);
25089:             }
25089:         }
24879:     }
24879: }
24879: 
25940: JS_REQUIRES_STACK void
17976: js_FlushJITCache(JSContext* cx)
17976: {
18725:     if (!TRACING_ENABLED(cx))
18725:         return;
21685:     debug_only_v(printf("Flushing cache.\n");)
17976:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
17976:     if (tm->recorder)
20422:         js_AbortRecording(cx, "flush cache");
21723:     TraceRecorder* tr;
21723:     while ((tr = tm->abortStack) != NULL) {
21723:         tr->removeFragmentoReferences();
21723:         tr->deepAbort();
21723:         tr->popAbortStack();
21723:     }
17976:     Fragmento* fragmento = tm->fragmento;
17976:     if (fragmento) {
17976:         fragmento->clearFrags();
17976: #ifdef DEBUG
17976:         JS_ASSERT(fragmento->labels);
24618:         fragmento->labels->clear();
17976: #endif
23264:         tm->lirbuf->rewind();
25102:         for (size_t i = 0; i < FRAGMENT_TABLE_SIZE; ++i) {
25102:             VMFragment* f = tm->vmfragments[i];
25102:             while(f) {
25102:                 VMFragment* next = f->next;
25102:                 fragmento->clearFragment(f);
25102:                 f = next;
25102:             }
25102:             tm->vmfragments[i] = NULL;
25102:         }
24491:         for (size_t i = 0; i < MONITOR_N_GLOBAL_STATES; ++i) {
24491:             tm->globalStates[i].globalShape = -1;
24491:             tm->globalStates[i].globalSlots->clear();
24491:         }
23918:     }
17976: }
17976: 
25214: JS_FORCES_STACK JS_FRIEND_API(void)
25214: js_DeepBail(JSContext *cx)
25214: {
25214:     JS_ASSERT(JS_ON_TRACE(cx));
25214: 
24612:     /* It's a bug if a non-FAIL_STATUS builtin gets here. */
24612:     JS_ASSERT(cx->bailExit);
24612: 
24612:     JS_TRACE_MONITOR(cx).onTrace = false;
24613:     JS_TRACE_MONITOR(cx).prohibitRecording = true;
24612:     LeaveTree(*cx->interpState, cx->bailExit);
24612: #ifdef DEBUG
24612:     cx->bailExit = NULL;
22652: #endif
24612:     cx->builtinStatus |= JSBUILTIN_BAILED;
22652: }
22652: 
22652: JS_REQUIRES_STACK jsval&
17412: TraceRecorder::argval(unsigned n) const
17412: {
17799:     JS_ASSERT(n < cx->fp->fun->nargs);
17412:     return cx->fp->argv[n];
17412: }
17412: 
22652: JS_REQUIRES_STACK jsval&
17412: TraceRecorder::varval(unsigned n) const
17412: {
18137:     JS_ASSERT(n < cx->fp->script->nslots);
17807:     return cx->fp->slots[n];
17412: }
17412: 
22652: JS_REQUIRES_STACK jsval&
17412: TraceRecorder::stackval(int n) const
17412: {
17520:     jsval* sp = cx->fp->regs->sp;
17520:     return sp[n];
17412: }
17412: 
22652: JS_REQUIRES_STACK LIns*
18286: TraceRecorder::scopeChain() const
18286: {
18286:     return lir->insLoad(LIR_ldp,
18286:                         lir->insLoad(LIR_ldp, cx_ins, offsetof(JSContext, fp)),
18286:                         offsetof(JSStackFrame, scopeChain));
18286: }
18286: 
18286: static inline bool
18286: FrameInRange(JSStackFrame* fp, JSStackFrame *target, unsigned callDepth)
18286: {
18286:     while (fp != target) {
18286:         if (callDepth-- == 0)
18286:             return false;
18286:         if (!(fp = fp->down))
18286:             return false;
18286:     }
18286:     return true;
18286: }
18286: 
22652: JS_REQUIRES_STACK bool
18286: TraceRecorder::activeCallOrGlobalSlot(JSObject* obj, jsval*& vp)
18286: {
23096:     // Lookup a name in the scope chain, arriving at a property either in the
23096:     // global object or some call object's fp->slots, and import that property
23096:     // into the trace's native stack frame. This could theoretically do *lookup*
23096:     // through the property cache, but there is little performance to be gained
23096:     // by doing so since at trace-execution time the underlying object (call
23096:     // object or global object) will not be consulted at all: the jsval*
23096:     // returned from this function will map (in the tracker) to a LIns* directly
23096:     // defining a slot in the trace's native stack.
23096: 
18286:     JS_ASSERT(obj != globalObj);
18286: 
18286:     JSAtom* atom = atoms[GET_INDEX(cx->fp->regs->pc)];
18286:     JSObject* obj2;
18286:     JSProperty* prop;
18286:     if (js_FindProperty(cx, ATOM_TO_JSID(atom), &obj, &obj2, &prop) < 0 || !prop)
18286:         ABORT_TRACE("failed to find name in non-global scope chain");
18286: 
18286:     if (obj == globalObj) {
18286:         JSScopeProperty* sprop = (JSScopeProperty*) prop;
23096: 
25938:         if (obj2 != obj) {
18286:             OBJ_DROP_PROPERTY(cx, obj2, prop);
25938:             ABORT_TRACE("prototype property");
25938:         }
25938:         if (!isValidSlot(OBJ_SCOPE(obj), sprop)) {
25938:             OBJ_DROP_PROPERTY(cx, obj2, prop);
25938:             return false;
25938:         }
25938:         if (!lazilyImportGlobalSlot(sprop->slot)) {
25938:             OBJ_DROP_PROPERTY(cx, obj2, prop);
18286:             ABORT_TRACE("lazy import of global slot failed");
25938:         }
18286:         vp = &STOBJ_GET_SLOT(obj, sprop->slot);
18286:         OBJ_DROP_PROPERTY(cx, obj2, prop);
18286:         return true;
18286:     }
18286: 
22618:     if (wasDeepAborted())
22618:         ABORT_TRACE("deep abort from property lookup");
22618: 
18286:     if (obj == obj2 && OBJ_GET_CLASS(cx, obj) == &js_CallClass) {
18286:         JSStackFrame* cfp = (JSStackFrame*) JS_GetPrivate(cx, obj);
18286:         if (cfp && FrameInRange(cx->fp, cfp, callDepth)) {
18286:             JSScopeProperty* sprop = (JSScopeProperty*) prop;
18286:             uintN slot = sprop->shortid;
18286: 
18426:             vp = NULL;
18286:             if (sprop->getter == js_GetCallArg) {
18286:                 JS_ASSERT(slot < cfp->fun->nargs);
18286:                 vp = &cfp->argv[slot];
18426:             } else if (sprop->getter == js_GetCallVar) {
18286:                 JS_ASSERT(slot < cfp->script->nslots);
18286:                 vp = &cfp->slots[slot];
18286:             }
18286:             OBJ_DROP_PROPERTY(cx, obj2, prop);
18426:             if (!vp)
18426:                 ABORT_TRACE("dynamic property of Call object");
18286:             return true;
18286:         }
18286:     }
18286: 
18286:     OBJ_DROP_PROPERTY(cx, obj2, prop);
18286:     ABORT_TRACE("fp->scopeChain is not global or active call object");
18286: }
18286: 
22652: JS_REQUIRES_STACK LIns*
17412: TraceRecorder::arg(unsigned n)
17412: {
17412:     return get(&argval(n));
17412: }
17412: 
22652: JS_REQUIRES_STACK void
17415: TraceRecorder::arg(unsigned n, LIns* i)
17415: {
17415:     set(&argval(n), i);
17415: }
17415: 
22652: JS_REQUIRES_STACK LIns*
17412: TraceRecorder::var(unsigned n)
17412: {
17412:     return get(&varval(n));
17412: }
17412: 
22652: JS_REQUIRES_STACK void
17415: TraceRecorder::var(unsigned n, LIns* i)
17415: {
17415:     set(&varval(n), i);
17415: }
17415: 
22652: JS_REQUIRES_STACK LIns*
17412: TraceRecorder::stack(int n)
17412: {
17412:     return get(&stackval(n));
17412: }
17412: 
22652: JS_REQUIRES_STACK void
17412: TraceRecorder::stack(int n, LIns* i)
17412: {
17788:     set(&stackval(n), i, n >= 0);
17412: }
17412: 
23456: JS_REQUIRES_STACK LIns*
21799: TraceRecorder::alu(LOpcode v, jsdouble v0, jsdouble v1, LIns* s0, LIns* s1)
21799: {
21799:     if (v == LIR_fadd || v == LIR_fsub) {
21799:         jsdouble r;
21799:         if (v == LIR_fadd)
21799:             r = v0 + v1;
21799:         else
21799:             r = v0 - v1;
21799:         /*
21799:          * Calculate the result of the addition for the current values. If the
21799:          * value is not within the integer range, don't even try to demote
21799:          * here.
21799:          */
21799:         if (!JSDOUBLE_IS_NEGZERO(r) && (jsint(r) == r) && isPromoteInt(s0) && isPromoteInt(s1)) {
21799:             LIns* d0 = ::demote(lir, s0);
21799:             LIns* d1 = ::demote(lir, s1);
21799:             /*
21799:              * If the inputs are constant, generate an integer constant for
21799:              * this operation.
21799:              */
21799:             if (d0->isconst() && d1->isconst())
21799:                 return lir->ins1(LIR_i2f, lir->insImm(jsint(r)));
21799:             /*
21799:              * Speculatively generate code that will perform the addition over
21799:              * the integer inputs as an integer addition/subtraction and exit
21799:              * if that fails.
21799:              */
21799:             v = (LOpcode)((int)v & ~LIR64);
21799:             LIns* result = lir->ins2(v, d0, d1);
26117:             if (!result->isconst() && (!overflowSafe(d0) || !overflowSafe(d1)))
26117:                 lir->insGuard(LIR_xt, lir->ins1(LIR_ov, result), snapshot(OVERFLOW_EXIT));
21799:             return lir->ins1(LIR_i2f, result);
21799:         }
21799:         /*
21799:          * The result doesn't fit into the integer domain, so either generate
21799:          * a floating point constant or a floating point operation.
21799:          */
26265:         if (s0->isconst() && s1->isconst())
26265:             return lir->insImmf(r);
21799:         return lir->ins2(v, s0, s1);
21799:     }
21799:     return lir->ins2(v, s0, s1);
21799: }
21799: 
21799: LIns*
21799: TraceRecorder::f2i(LIns* f)
17469: {
20915:     return lir->insCall(&js_DoubleToInt32_ci, &f);
17469: }
17469: 
22652: JS_REQUIRES_STACK LIns*
21799: TraceRecorder::makeNumberInt32(LIns* f)
19979: {
19979:     JS_ASSERT(f->isQuad());
19979:     LIns* x;
19979:     if (!isPromote(f)) {
19979:         x = f2i(f);
19979:         guard(true, lir->ins2(LIR_feq, f, lir->ins1(LIR_i2f, x)), MISMATCH_EXIT);
19979:     } else {
19979:         x = ::demote(lir, f);
19979:     }
19979:     return x;
19979: }
19979: 
23456: JS_REQUIRES_STACK LIns*
21685: TraceRecorder::stringify(jsval& v)
21685: {
21685:     LIns* v_ins = get(&v);
21447:     if (JSVAL_IS_STRING(v))
21447:         return v_ins;
21447: 
21447:     LIns* args[] = { v_ins, cx_ins };
21447:     const CallInfo* ci;
21447:     if (JSVAL_IS_NUMBER(v)) {
21447:         ci = &js_NumberToString_ci;
21447:     } else if (JSVAL_TAG(v) == JSVAL_BOOLEAN) {
21447:         ci = &js_BooleanOrUndefinedToString_ci;
21447:     } else {
26036:         /*
26036:          * Callers must deal with non-primitive (non-null object) values by
26036:          * calling an imacro. We don't try to guess about which imacro, with
26036:          * what valueOf hint, here.
26036:          */
26036:         JS_ASSERT(JSVAL_IS_NULL(v));
26078:         return INS_CONSTPTR(ATOM_TO_STRING(cx->runtime->atomState.nullAtom));
26036:     }
26036: 
21447:     v_ins = lir->insCall(ci, args);
21447:     guard(false, lir->ins_eq0(v_ins), OOM_EXIT);
21447:     return v_ins;
21447: }
21447: 
23456: JS_REQUIRES_STACK bool
21685: TraceRecorder::call_imacro(jsbytecode* imacro)
21685: {
21685:     JSStackFrame* fp = cx->fp;
21685:     JSFrameRegs* regs = fp->regs;
21685: 
21808:     if (!fp->imacpc) {
21685:         fp->imacpc = regs->pc;
21719:         fp->flags |= JSFRAME_IMACRO_START;
21685:         regs->pc = imacro;
21685:         atoms = COMMON_ATOMS_START(&cx->runtime->atomState);
21808:     }
21685:     return false;
21685: }
21685: 
22652: JS_REQUIRES_STACK bool
20416: TraceRecorder::ifop()
17452: {
17452:     jsval& v = stackval(-1);
19604:     LIns* v_ins = get(&v);
20410:     bool cond;
20410:     LIns* x;
23075: 
26557:     /* Objects always evaluate to true since we specialize the Null type on trace. */
26557:     if (JSVAL_TAG(v) == JSVAL_OBJECT) {
26557:         cond = true;
26557:         x = lir->insImm(1);
26557:     } else if (JSVAL_TAG(v) == JSVAL_BOOLEAN) {
23075:         /* Test for boolean is true, negate later if we are testing for false. */
24846:         cond = JSVAL_TO_PSEUDO_BOOLEAN(v) == JS_TRUE;
20410:         x = lir->ins2i(LIR_eq, v_ins, 1);
17749:     } else if (isNumber(v)) {
17749:         jsdouble d = asNumber(v);
20411:         cond = !JSDOUBLE_IS_NaN(d) && d;
20410:         x = lir->ins2(LIR_and,
20410:                       lir->ins2(LIR_feq, v_ins, v_ins),
26265:                       lir->ins_eq0(lir->ins2(LIR_feq, v_ins, lir->insImmq(0))));
17749:     } else if (JSVAL_IS_STRING(v)) {
20411:         cond = JSSTRING_LENGTH(JSVAL_TO_STRING(v)) != 0;
20411:         x = lir->ins2(LIR_piand,
18254:                       lir->insLoad(LIR_ldp,
19604:                                    v_ins,
18254:                                    (int)offsetof(JSString, length)),
26397:                       INS_CONSTPTR(reinterpret_cast<void *>(JSSTRING_LENGTH_MASK)));
17452:     } else {
17927:         JS_NOT_REACHED("ifop");
20410:         return false;
20410:     }
26557: 
26557:     jsbytecode* pc = cx->fp->regs->pc;
26557:     emitIf(pc, cond, x);
26557:     return checkTraceEnd(pc);
17452: }
17452: 
25099: #ifdef NANOJIT_IA32
26011: /* Record LIR for a tableswitch or tableswitchx op. We record LIR only the
26011:    "first" time we hit the op. Later, when we start traces after exiting that
26011:    trace, we just patch. */
25099: JS_REQUIRES_STACK LIns*
25099: TraceRecorder::tableswitch()
25099: {
25099:     jsval& v = stackval(-1);
25107:     if (!isNumber(v))
25107:         return NULL;
25107: 
25099:     /* no need to guard if condition is constant */
25107:     LIns* v_ins = f2i(get(&v));
25099:     if (v_ins->isconst() || v_ins->isconstq())
25099:         return NULL;
25099: 
25099:     jsbytecode* pc = cx->fp->regs->pc;
25099:     /* Starting a new trace after exiting a trace via switch. */
26011:     if (anchor &&
26011:         (anchor->exitType == CASE_EXIT || anchor->exitType == DEFAULT_EXIT) &&
26011:         fragment->ip == pc) {
25099:         return NULL;
26011:     }
25099: 
25099:     /* Decode jsop. */
25099:     jsint low, high;
25099:     if (*pc == JSOP_TABLESWITCH) {
25099:         pc += JUMP_OFFSET_LEN;
25099:         low = GET_JUMP_OFFSET(pc);
25099:         pc += JUMP_OFFSET_LEN;
25099:         high = GET_JUMP_OFFSET(pc);
25099:     } else {
25099:         pc += JUMPX_OFFSET_LEN;
25099:         low = GET_JUMPX_OFFSET(pc);
25099:         pc += JUMPX_OFFSET_LEN;
25099:         high = GET_JUMPX_OFFSET(pc);
25099:     }
25099: 
26011:     /* Really large tables won't fit in a page. This is a conservative check.
26011:        If it matters in practice we need to go off-page. */
25099:     if ((high + 1 - low) * sizeof(intptr_t*) + 128 > (unsigned) LARGEST_UNDERRUN_PROT) {
25099:         // This throws away the return value of switchop but it seems
25099:         // ok because switchop always returns true.
25099:         (void) switchop();
25099:         return NULL;
25099:     }
25099: 
25099:     /* Generate switch LIR. */
25099:     LIns* si_ins = lir_buf_writer->skip(sizeof(SwitchInfo));
25099:     SwitchInfo* si = (SwitchInfo*) si_ins->payload();
25099:     si->count = high + 1 - low;
25099:     si->table = 0;
25099:     si->index = (uint32) -1;
25107:     LIns* diff = lir->ins2(LIR_sub, v_ins, lir->insImm(low));
25099:     LIns* cmp = lir->ins2(LIR_ult, diff, lir->insImm(si->count));
25099:     lir->insGuard(LIR_xf, cmp, snapshot(DEFAULT_EXIT));
25099:     lir->insStore(diff, lir->insImmPtr(&si->index), lir->insImm(0));
25099:     LIns* exit = snapshot(CASE_EXIT);
25099:     ((GuardRecord*) exit->payload())->exit->switchInfo = si;
25099:     return lir->insGuard(LIR_xtbl, diff, exit);
25099: }
25099: #endif
25099: 
22652: JS_REQUIRES_STACK bool
18687: TraceRecorder::switchop()
18687: {
18687:     jsval& v = stackval(-1);
19604:     LIns* v_ins = get(&v);
19604:     /* no need to guard if condition is constant */
19604:     if (v_ins->isconst() || v_ins->isconstq())
19604:         return true;
18687:     if (isNumber(v)) {
18687:         jsdouble d = asNumber(v);
18687:         guard(true,
26265:               addName(lir->ins2(LIR_feq, v_ins, lir->insImmf(d)),
18687:                       "guard(switch on numeric)"),
18687:               BRANCH_EXIT);
18687:     } else if (JSVAL_IS_STRING(v)) {
19604:         LIns* args[] = { v_ins, INS_CONSTPTR(JSVAL_TO_STRING(v)) };
18687:         guard(true,
20915:               addName(lir->ins_eq0(lir->ins_eq0(lir->insCall(&js_EqualStrings_ci, args))),
18687:                       "guard(switch on string)"),
18687:               BRANCH_EXIT);
19995:     } else if (JSVAL_TAG(v) == JSVAL_BOOLEAN) {
18687:         guard(true,
24867:               addName(lir->ins2(LIR_eq, v_ins, lir->insImm(JSVAL_TO_PUBLIC_PSEUDO_BOOLEAN(v))),
18687:                       "guard(switch on boolean)"),
18687:               BRANCH_EXIT);
18687:     } else {
19995:         ABORT_TRACE("switch on object or null");
18687:     }
18687:     return true;
18687: }
18687: 
22652: JS_REQUIRES_STACK bool
17412: TraceRecorder::inc(jsval& v, jsint incr, bool pre)
17412: {
17782:     LIns* v_ins = get(&v);
17782:     if (!inc(v, v_ins, incr, pre))
17782:         return false;
17782:     set(&v, v_ins);
17782:     return true;
17782: }
17782: 
17782: /*
17782:  * On exit, v_ins is the incremented unboxed value, and the appropriate
17782:  * value (pre- or post-increment as described by pre) is stacked.
17782:  */
22652: JS_REQUIRES_STACK bool
17782: TraceRecorder::inc(jsval& v, LIns*& v_ins, jsint incr, bool pre)
17758: {
17758:     if (!isNumber(v))
17782:         ABORT_TRACE("can only inc numbers");
17758: 
26265:     LIns* v_after = alu(LIR_fadd, asNumber(v), incr, v_ins, lir->insImmf(incr));
17544: 
17544:     const JSCodeSpec& cs = js_CodeSpec[*cx->fp->regs->pc];
17544:     JS_ASSERT(cs.ndefs == 1);
17782:     stack(-cs.nuses, pre ? v_after : v_ins);
17782:     v_ins = v_after;
17412:     return true;
17412: }
17758: 
22652: JS_REQUIRES_STACK bool
17758: TraceRecorder::incProp(jsint incr, bool pre)
17758: {
17758:     jsval& l = stackval(-1);
17758:     if (JSVAL_IS_PRIMITIVE(l))
17758:         ABORT_TRACE("incProp on primitive");
17758: 
17758:     JSObject* obj = JSVAL_TO_OBJECT(l);
17758:     LIns* obj_ins = get(&l);
17758: 
17761:     uint32 slot;
17758:     LIns* v_ins;
17761:     if (!prop(obj, obj_ins, slot, v_ins))
17412:         return false;
17761: 
18666:     if (slot == SPROP_INVALID_SLOT)
18666:         ABORT_TRACE("incProp on invalid slot");
18666: 
17761:     jsval& v = STOBJ_GET_SLOT(obj, slot);
17761:     if (!inc(v, v_ins, incr, pre))
17761:         return false;
17761: 
23708:     box_jsval(v, v_ins);
17761: 
17761:     LIns* dslots_ins = NULL;
17782:     stobj_set_slot(obj_ins, slot, dslots_ins, v_ins);
17761:     return true;
17758: }
17758: 
22652: JS_REQUIRES_STACK bool
17758: TraceRecorder::incElem(jsint incr, bool pre)
17758: {
17758:     jsval& r = stackval(-1);
17758:     jsval& l = stackval(-2);
17758:     jsval* vp;
17758:     LIns* v_ins;
17782:     LIns* addr_ins;
17782:     if (!elem(l, r, vp, v_ins, addr_ins))
17758:         return false;
20972:     if (!addr_ins) // if we read a hole, abort
20972:         return false;
17782:     if (!inc(*vp, v_ins, incr, pre))
17782:         return false;
23708:     box_jsval(*vp, v_ins);
17782:     lir->insStorei(v_ins, addr_ins, 0);
17782:     return true;
17412: }
17412: 
19576: static bool
19576: evalCmp(LOpcode op, double result)
19576: {
18017:     bool cond;
18017:     switch (op) {
19576:       case LIR_feq:
19576:         cond = (result == 0);
19576:         break;
18017:       case LIR_flt:
18017:         cond = result < 0;
18017:         break;
18017:       case LIR_fgt:
18017:         cond = result > 0;
18017:         break;
18017:       case LIR_fle:
18017:         cond = result <= 0;
18017:         break;
18017:       case LIR_fge:
18017:         cond = result >= 0;
18017:         break;
18017:       default:
19576:         JS_NOT_REACHED("unexpected comparison op");
19576:         return false;
19576:     }
19576:     return cond;
19576: }
19576: 
19576: static bool
19576: evalCmp(LOpcode op, double l, double r)
19576: {
19576:     return evalCmp(op, l - r);
19576: }
19576: 
19576: static bool
19576: evalCmp(LOpcode op, JSString* l, JSString* r)
19576: {
19576:     if (op == LIR_feq)
19576:         return js_EqualStrings(l, r);
19576:     return evalCmp(op, js_CompareStrings(l, r));
19576: }
19576: 
22705: JS_REQUIRES_STACK void
23093: TraceRecorder::strictEquality(bool equal, bool cmpCase)
22705: {
22705:     jsval& r = stackval(-1);
22705:     jsval& l = stackval(-2);
22705:     LIns* l_ins = get(&l);
22705:     LIns* r_ins = get(&r);
23115:     LIns* x;
23115:     bool cond;
22705: 
22705:     uint8 ltag = getPromotedType(l);
22705:     if (ltag != getPromotedType(r)) {
23115:         cond = !equal;
23115:         x = lir->insImm(cond);
23115:     } else if (ltag == JSVAL_STRING) {
22705:         LIns* args[] = { r_ins, l_ins };
22705:         x = lir->ins2i(LIR_eq, lir->insCall(&js_EqualStrings_ci, args), equal);
23093:         cond = js_EqualStrings(JSVAL_TO_STRING(l), JSVAL_TO_STRING(r));
22705:     } else {
22705:         LOpcode op = (ltag != JSVAL_DOUBLE) ? LIR_eq : LIR_feq;
22705:         x = lir->ins2(op, l_ins, r_ins);
22705:         if (!equal)
22705:             x = lir->ins_eq0(x);
23117:         cond = (ltag == JSVAL_DOUBLE)
23118:                ? asNumber(l) == asNumber(r)
23117:                : l == r;
23093:     }
23093:     cond = (cond == equal);
23093: 
23093:     if (cmpCase) {
23093:         /* Only guard if the same path may not always be taken. */
23093:         if (!x->isconst())
23093:             guard(cond, x, BRANCH_EXIT);
23093:         return;
22705:     }
22705: 
22705:     set(&l, x);
22705: }
22705: 
22705: JS_REQUIRES_STACK bool
23093: TraceRecorder::equality(bool negate, bool tryBranchAfterCond)
19576: {
23223:     jsval& rval = stackval(-1);
23223:     jsval& lval = stackval(-2);
23223:     LIns* l_ins = get(&lval);
23223:     LIns* r_ins = get(&rval);
23223: 
23223:     return equalityHelper(lval, rval, l_ins, r_ins, negate, tryBranchAfterCond, lval);
23223: }
23223: 
23223: JS_REQUIRES_STACK bool
23223: TraceRecorder::equalityHelper(jsval l, jsval r, LIns* l_ins, LIns* r_ins,
23223:                               bool negate, bool tryBranchAfterCond,
23223:                               jsval& rval)
23223: {
23223:     bool fp = false;
19576:     bool cond;
23223:     LIns* args[] = { NULL, NULL };
23223: 
23223:     /*
23223:      * The if chain below closely mirrors that found in 11.9.3, in general
23223:      * deviating from that ordering of ifs only to account for SpiderMonkey's
23223:      * conflation of booleans and undefined and for the possibility of
23223:      * confusing objects and null.  Note carefully the spec-mandated recursion
23223:      * in the final else clause, which terminates because Number == T recurs
23223:      * only if T is Object, but that must recur again to convert Object to
23223:      * primitive, and ToPrimitive throws if the object cannot be converted to
23223:      * a primitive value (which would terminate recursion).
23223:      */
23223: 
23223:     if (getPromotedType(l) == getPromotedType(r)) {
23223:         if (JSVAL_TAG(l) == JSVAL_OBJECT || JSVAL_TAG(l) == JSVAL_BOOLEAN) {
23223:             cond = (l == r);
23223:         } else if (JSVAL_IS_STRING(l)) {
23223:             args[0] = r_ins, args[1] = l_ins;
23223:             l_ins = lir->insCall(&js_EqualStrings_ci, args);
23223:             r_ins = lir->insImm(1);
23223:             cond = js_EqualStrings(JSVAL_TO_STRING(l), JSVAL_TO_STRING(r));
21719:         } else {
23223:             JS_ASSERT(isNumber(l) && isNumber(r));
23223:             cond = (asNumber(l) == asNumber(r));
20392:             fp = true;
23223:         }
23223:     } else if (JSVAL_IS_NULL(l) && JSVAL_TAG(r) == JSVAL_BOOLEAN) {
24846:         l_ins = lir->insImm(JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID));
23223:         cond = (r == JSVAL_VOID);
23223:     } else if (JSVAL_TAG(l) == JSVAL_BOOLEAN && JSVAL_IS_NULL(r)) {
24846:         r_ins = lir->insImm(JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID));
23223:         cond = (l == JSVAL_VOID);
23223:     } else if (isNumber(l) && JSVAL_IS_STRING(r)) {
23223:         args[0] = r_ins, args[1] = cx_ins;
23223:         r_ins = lir->insCall(&js_StringToNumber_ci, args);
23223:         cond = (asNumber(l) == js_StringToNumber(cx, JSVAL_TO_STRING(r)));
23223:         fp = true;
23223:     } else if (JSVAL_IS_STRING(l) && isNumber(r)) {
23223:         args[0] = l_ins, args[1] = cx_ins;
20915:         l_ins = lir->insCall(&js_StringToNumber_ci, args);
23223:         cond = (js_StringToNumber(cx, JSVAL_TO_STRING(l)) == asNumber(r));
23223:         fp = true;
23223:     } else {
23223:         if (JSVAL_TAG(l) == JSVAL_BOOLEAN) {
25478:             bool isVoid = JSVAL_IS_VOID(l);
25478:             guard(isVoid,
25478:                   lir->ins2(LIR_eq, l_ins, INS_CONST(JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID))),
25478:                   BRANCH_EXIT);
25478:             if (!isVoid) {
23223:                 args[0] = l_ins, args[1] = cx_ins;
21447:                 l_ins = lir->insCall(&js_BooleanOrUndefinedToNumber_ci, args);
23223:                 l = (l == JSVAL_VOID)
24315:                     ? DOUBLE_TO_JSVAL(cx->runtime->jsNaN)
23223:                     : INT_TO_JSVAL(l == JSVAL_TRUE);
23223:                 return equalityHelper(l, r, l_ins, r_ins, negate,
23223:                                       tryBranchAfterCond, rval);
23223:             }
25478:         } else if (JSVAL_TAG(r) == JSVAL_BOOLEAN) {
25478:             bool isVoid = JSVAL_IS_VOID(r);
25478:             guard(isVoid,
25478:                   lir->ins2(LIR_eq, r_ins, INS_CONST(JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID))),
25478:                   BRANCH_EXIT);
25478:             if (!isVoid) {
23223:                 args[0] = r_ins, args[1] = cx_ins;
21447:                 r_ins = lir->insCall(&js_BooleanOrUndefinedToNumber_ci, args);
23223:                 r = (r == JSVAL_VOID)
24315:                     ? DOUBLE_TO_JSVAL(cx->runtime->jsNaN)
23223:                     : INT_TO_JSVAL(r == JSVAL_TRUE);
23223:                 return equalityHelper(l, r, l_ins, r_ins, negate,
23223:                                       tryBranchAfterCond, rval);
23223:             }
25478:         } else {
23223:             if ((JSVAL_IS_STRING(l) || isNumber(l)) && !JSVAL_IS_PRIMITIVE(r))
23223:                 return call_imacro(equality_imacros.any_obj);
23223:             if (!JSVAL_IS_PRIMITIVE(l) && (JSVAL_IS_STRING(r) || isNumber(r)))
23223:                 return call_imacro(equality_imacros.obj_any);
25478:         }
23223: 
23223:         l_ins = lir->insImm(0);
23223:         r_ins = lir->insImm(1);
23223:         cond = false;
23223:     }
23223: 
23223:     /* If the operands aren't numbers, compare them as integers. */
22705:     LOpcode op = fp ? LIR_feq : LIR_eq;
23223:     LIns* x = lir->ins2(op, l_ins, r_ins);
22705:     if (negate) {
22705:         x = lir->ins_eq0(x);
22705:         cond = !cond;
22705:     }
22705: 
26557:     jsbytecode* pc = cx->fp->regs->pc;
26557: 
22705:     /*
26118:      * Don't guard if the same path is always taken.  If it isn't, we have to
26118:      * fuse comparisons and the following branch, because the interpreter does
26118:      * that.
22705:      */
26557:     if (tryBranchAfterCond)
26557:         fuseIf(pc + 1, cond, x);
26557: 
26557:     /*
26557:      * There is no need to write out the result of this comparison if the trace
26557:      * ends on this operation.
26557:      */
26557:     if ((pc[1] == JSOP_IFNE || pc[1] == JSOP_IFEQ) && !checkTraceEnd(pc + 1))
26557:         return false;
22705: 
22705:     /*
22705:      * We update the stack after the guard. This is safe since the guard bails
22705:      * out at the comparison and the interpreter will therefore re-execute the
22705:      * comparison. This way the value of the condition doesn't have to be
22705:      * calculated and saved on the stack in most cases.
22705:      */
23223:     set(&rval, x);
26557: 
22705:     return true;
22705: }
22705: 
22705: JS_REQUIRES_STACK bool
23093: TraceRecorder::relational(LOpcode op, bool tryBranchAfterCond)
22705: {
22705:     jsval& r = stackval(-1);
22705:     jsval& l = stackval(-2);
22705:     LIns* x = NULL;
22705:     bool cond;
22705:     LIns* l_ins = get(&l);
22705:     LIns* r_ins = get(&r);
22705:     bool fp = false;
22705:     jsdouble lnum, rnum;
22705: 
22705:     /*
22705:      * 11.8.5 if either argument is an object with a function-valued valueOf
22705:      * property; if both arguments are objects with non-function-valued valueOf
22705:      * properties, abort.
22705:      */
23075:     if (!JSVAL_IS_PRIMITIVE(l)) {
23075:         if (!JSVAL_IS_PRIMITIVE(r))
22705:             return call_imacro(binary_imacros.obj_obj);
22705:         return call_imacro(binary_imacros.obj_any);
22705:     }
23075:     if (!JSVAL_IS_PRIMITIVE(r))
22705:         return call_imacro(binary_imacros.any_obj);
22705: 
22705:     /* 11.8.5 steps 3, 16-21. */
22705:     if (JSVAL_IS_STRING(l) && JSVAL_IS_STRING(r)) {
22705:         LIns* args[] = { r_ins, l_ins };
22705:         l_ins = lir->insCall(&js_CompareStrings_ci, args);
22705:         r_ins = lir->insImm(0);
22705:         cond = evalCmp(op, JSVAL_TO_STRING(l), JSVAL_TO_STRING(r));
22705:         goto do_comparison;
22705:     }
22705: 
22705:     /* 11.8.5 steps 4-5. */
22705:     if (!JSVAL_IS_NUMBER(l)) {
22705:         LIns* args[] = { l_ins, cx_ins };
22705:         switch (JSVAL_TAG(l)) {
22705:           case JSVAL_BOOLEAN:
22705:             l_ins = lir->insCall(&js_BooleanOrUndefinedToNumber_ci, args);
22705:             break;
22705:           case JSVAL_STRING:
22705:             l_ins = lir->insCall(&js_StringToNumber_ci, args);
22705:             break;
23075:           case JSVAL_OBJECT:
23075:             if (JSVAL_IS_NULL(l)) {
23075:                 l_ins = lir->insImmq(0);
23075:                 break;
23075:             }
23075:             // FALL THROUGH
22705:           case JSVAL_INT:
22705:           case JSVAL_DOUBLE:
22705:           default:
22705:             JS_NOT_REACHED("JSVAL_IS_NUMBER if int/double, objects should "
22705:                            "have been handled at start of method");
22705:             ABORT_TRACE("safety belt");
22705:         }
22705:     }
22705:     if (!JSVAL_IS_NUMBER(r)) {
22705:         LIns* args[] = { r_ins, cx_ins };
22705:         switch (JSVAL_TAG(r)) {
22705:           case JSVAL_BOOLEAN:
22705:             r_ins = lir->insCall(&js_BooleanOrUndefinedToNumber_ci, args);
22705:             break;
22705:           case JSVAL_STRING:
22705:             r_ins = lir->insCall(&js_StringToNumber_ci, args);
22705:             break;
23075:           case JSVAL_OBJECT:
23075:             if (JSVAL_IS_NULL(r)) {
23075:                 r_ins = lir->insImmq(0);
23075:                 break;
23075:             }
23075:             // FALL THROUGH
22705:           case JSVAL_INT:
22705:           case JSVAL_DOUBLE:
22705:           default:
22705:             JS_NOT_REACHED("JSVAL_IS_NUMBER if int/double, objects should "
22705:                            "have been handled at start of method");
22705:             ABORT_TRACE("safety belt");
22705:         }
22705:     }
22705:     {
22705:         jsval tmp = JSVAL_NULL;
22705:         JSAutoTempValueRooter tvr(cx, 1, &tmp);
22705: 
22705:         tmp = l;
22705:         lnum = js_ValueToNumber(cx, &tmp);
22705:         tmp = r;
22705:         rnum = js_ValueToNumber(cx, &tmp);
22705:     }
22705:     cond = evalCmp(op, lnum, rnum);
22705:     fp = true;
22705: 
22705:     /* 11.8.5 steps 6-15. */
22705:   do_comparison:
22705:     /* If the result is not a number or it's not a quad, we must use an integer compare. */
22651:     if (!fp) {
22651:         JS_ASSERT(op >= LIR_feq && op <= LIR_fge);
22651:         op = LOpcode(op + (LIR_eq - LIR_feq));
22651:     }
22650:     x = lir->ins2(op, l_ins, r_ins);
22705: 
26557:     jsbytecode* pc = cx->fp->regs->pc;
26557: 
22705:     /*
26118:      * Don't guard if the same path is always taken.  If it isn't, we have to
26118:      * fuse comparisons and the following branch, because the interpreter does
26118:      * that.
22705:      */
26557:     if (tryBranchAfterCond)
26557:         fuseIf(pc + 1, cond, x);
26557: 
26557:     /*
26557:      * There is no need to write out the result of this comparison if the trace
26557:      * ends on this operation.
26557:      */
26557:     if ((pc[1] == JSOP_IFNE || pc[1] == JSOP_IFEQ) && !checkTraceEnd(pc + 1))
26557:         return false;
22705: 
22705:     /*
22705:      * We update the stack after the guard. This is safe since the guard bails
22705:      * out at the comparison and the interpreter will therefore re-execute the
22705:      * comparison. This way the value of the condition doesn't have to be
22705:      * calculated and saved on the stack in most cases.
22705:      */
17413:     set(&l, x);
26557: 
17412:     return true;
17412: }
17412: 
22652: JS_REQUIRES_STACK bool
17467: TraceRecorder::unary(LOpcode op)
17467: {
17467:     jsval& v = stackval(-1);
17467:     bool intop = !(op & LIR64);
17467:     if (isNumber(v)) {
17467:         LIns* a = get(&v);
17467:         if (intop)
17469:             a = f2i(a);
17467:         a = lir->ins1(op, a);
17467:         if (intop)
17467:             a = lir->ins1(LIR_i2f, a);
17467:         set(&v, a);
17467:         return true;
17467:     }
17467:     return false;
17467: }
17467: 
22652: JS_REQUIRES_STACK bool
17467: TraceRecorder::binary(LOpcode op)
17467: {
17467:     jsval& r = stackval(-1);
17467:     jsval& l = stackval(-2);
21685: 
23075:     if (!JSVAL_IS_PRIMITIVE(l)) {
23075:         if (!JSVAL_IS_PRIMITIVE(r))
21685:             return call_imacro(binary_imacros.obj_obj);
21685:         return call_imacro(binary_imacros.obj_any);
21685:     }
23075:     if (!JSVAL_IS_PRIMITIVE(r))
21685:         return call_imacro(binary_imacros.any_obj);
21685: 
17467:     bool intop = !(op & LIR64);
17467:     LIns* a = get(&l);
17467:     LIns* b = get(&r);
21799: 
21799:     bool leftIsNumber = isNumber(l);
21799:     jsdouble lnum = leftIsNumber ? asNumber(l) : 0;
21799: 
21799:     bool rightIsNumber = isNumber(r);
21799:     jsdouble rnum = rightIsNumber ? asNumber(r) : 0;
21799: 
17910:     if ((op >= LIR_sub && op <= LIR_ush) ||  // sub, mul, (callh), or, xor, (not,) lsh, rsh, ush
17910:         (op >= LIR_fsub && op <= LIR_fdiv)) { // fsub, fmul, fdiv
18693:         LIns* args[2];
17910:         if (JSVAL_IS_STRING(l)) {
17910:             args[0] = a;
18693:             args[1] = cx_ins;
20915:             a = lir->insCall(&js_StringToNumber_ci, args);
21799:             lnum = js_StringToNumber(cx, JSVAL_TO_STRING(l));
21799:             leftIsNumber = true;
17910:         }
17910:         if (JSVAL_IS_STRING(r)) {
17910:             args[0] = b;
18693:             args[1] = cx_ins;
20915:             b = lir->insCall(&js_StringToNumber_ci, args);
21799:             rnum = js_StringToNumber(cx, JSVAL_TO_STRING(r));
21799:             rightIsNumber = true;
17910:         }
17910:     }
21438:     if (JSVAL_TAG(l) == JSVAL_BOOLEAN) {
21438:         LIns* args[] = { a, cx_ins };
21447:         a = lir->insCall(&js_BooleanOrUndefinedToNumber_ci, args);
24846:         lnum = js_BooleanOrUndefinedToNumber(cx, JSVAL_TO_PSEUDO_BOOLEAN(l));
21799:         leftIsNumber = true;
20972:     }
21438:     if (JSVAL_TAG(r) == JSVAL_BOOLEAN) {
21438:         LIns* args[] = { b, cx_ins };
21447:         b = lir->insCall(&js_BooleanOrUndefinedToNumber_ci, args);
24846:         rnum = js_BooleanOrUndefinedToNumber(cx, JSVAL_TO_PSEUDO_BOOLEAN(r));
21799:         rightIsNumber = true;
21799:     }
21799:     if (leftIsNumber && rightIsNumber) {
17467:         if (intop) {
18693:             LIns *args[] = { a };
20915:             a = lir->insCall(op == LIR_ush ? &js_DoubleToUint32_ci : &js_DoubleToInt32_ci, args);
17469:             b = f2i(b);
17467:         }
21799:         a = alu(op, lnum, rnum, a, b);
17467:         if (intop)
17467:             a = lir->ins1(op == LIR_ush ? LIR_u2f : LIR_i2f, a);
17467:         set(&l, a);
17467:         return true;
17467:     }
17467:     return false;
17467: }
17467: 
18026: JS_STATIC_ASSERT(offsetof(JSObjectOps, newObjectMap) == 0);
18026: 
18026: bool
18026: TraceRecorder::map_is_native(JSObjectMap* map, LIns* map_ins, LIns*& ops_ins, size_t op_offset)
17899: {
25636: #define OP(ops) (*(JSObjectOp*) ((char*)(ops) + op_offset))
25636:     if (OP(map->ops) != OP(&js_ObjectOps))
25636:         return false;
25636: 
18230:     ops_ins = addName(lir->insLoad(LIR_ldp, map_ins, offsetof(JSObjectMap, ops)), "ops");
18230:     LIns* n = lir->insLoad(LIR_ldp, ops_ins, op_offset);
25636:     guard(true,
25636:           addName(lir->ins2(LIR_eq, n, INS_CONSTFUNPTR(OP(&js_ObjectOps))), "guard(native-map)"),
25636:           BRANCH_EXIT);
18026: #undef OP
25636: 
25636:     return true;
17417: }
17417: 
22652: JS_REQUIRES_STACK bool
17746: TraceRecorder::test_property_cache(JSObject* obj, LIns* obj_ins, JSObject*& obj2, jsuword& pcval)
17459: {
19093:     jsbytecode* pc = cx->fp->regs->pc;
19093:     JS_ASSERT(*pc != JSOP_INITPROP && *pc != JSOP_SETNAME && *pc != JSOP_SETPROP);
19093: 
18439:     // Mimic the interpreter's special case for dense arrays by skipping up one
18439:     // hop along the proto chain when accessing a named (not indexed) property,
18439:     // typically to find Array.prototype methods.
18026:     JSObject* aobj = obj;
18034:     if (OBJ_IS_DENSE_ARRAY(cx, obj)) {
18026:         aobj = OBJ_GET_PROTO(cx, obj);
18026:         obj_ins = stobj_get_fslot(obj_ins, JSSLOT_PROTO);
18026:     }
18026: 
18230:     LIns* map_ins = lir->insLoad(LIR_ldp, obj_ins, (int)offsetof(JSObject, map));
17899:     LIns* ops_ins;
18026: 
18026:     // Interpreter calls to PROPERTY_CACHE_TEST guard on native object ops
18026:     // (newObjectMap == js_ObjectOps.newObjectMap) which is required to use
18026:     // native objects (those whose maps are scopes), or even more narrow
18026:     // conditions required because the cache miss case will call a particular
18026:     // object-op (js_GetProperty, js_SetProperty).
18026:     //
18026:     // We parameterize using offsetof and guard on match against the hook at
18026:     // the given offset in js_ObjectOps. TraceRecorder::record_JSOP_SETPROP
18026:     // guards the js_SetProperty case.
19093:     uint32 format = js_CodeSpec[*pc].format;
18026:     uint32 mode = JOF_MODE(format);
19090: 
19090:     // No need to guard native-ness of global object.
19090:     JS_ASSERT(OBJ_IS_NATIVE(globalObj));
19090:     if (aobj != globalObj) {
18026:         size_t op_offset = 0;
18026:         if (mode == JOF_PROP || mode == JOF_VARPROP) {
18026:             JS_ASSERT(!(format & JOF_SET));
18026:             op_offset = offsetof(JSObjectOps, getProperty);
18026:         } else {
18026:             JS_ASSERT(mode == JOF_NAME);
18026:         }
18026: 
18026:         if (!map_is_native(aobj->map, map_ins, ops_ins, op_offset))
25636:             ABORT_TRACE("non-native map");
19090:     }
17459: 
17459:     JSAtom* atom;
17746:     JSPropCacheEntry* entry;
19093:     PROPERTY_CACHE_TEST(cx, pc, aobj, obj2, entry, atom);
24876:     if (!atom) {
24876:         // Null atom means that obj2 is locked and must now be unlocked.
24876:         JS_UNLOCK_OBJ(cx, obj2);
24876:     } else {
18439:         // Miss: pre-fill the cache for the interpreter, as well as for our needs.
18439:         // FIXME: 452357 - correctly propagate exceptions into the interpreter from
18439:         // js_FindPropertyHelper, js_LookupPropertyWithFlags, and elsewhere.
18439:         jsid id = ATOM_TO_JSID(atom);
17747:         JSProperty* prop;
19093:         if (JOF_OPMODE(*pc) == JOF_NAME) {
18112:             JS_ASSERT(aobj == obj);
17878:             if (js_FindPropertyHelper(cx, id, &obj, &obj2, &prop, &entry) < 0)
17747:                 ABORT_TRACE("failed to find name");
17747:         } else {
19712:             int protoIndex = js_LookupPropertyWithFlags(cx, aobj, id,
19712:                                                         cx->resolveFlags,
19712:                                                         &obj2, &prop);
17878:             if (protoIndex < 0)
17747:                 ABORT_TRACE("failed to lookup property");
17878: 
17998:             if (prop) {
19020:                 js_FillPropertyCache(cx, aobj, OBJ_SHAPE(aobj), 0, protoIndex, obj2,
18439:                                      (JSScopeProperty*) prop, &entry);
17998:             }
17998:         }
17998: 
17998:         if (!prop) {
17998:             // Propagate obj from js_FindPropertyHelper to record_JSOP_BINDNAME
18712:             // via our obj2 out-parameter. If we are recording JSOP_SETNAME and
18712:             // the global it's assigning does not yet exist, create it.
17998:             obj2 = obj;
19093: 
18712:             // Use PCVAL_NULL to return "no such property" to our caller.
17998:             pcval = PCVAL_NULL;
25633:             return true;
17998:         }
17998: 
17998:         OBJ_DROP_PROPERTY(cx, obj2, prop);
18439:         if (!entry)
17878:             ABORT_TRACE("failed to fill property cache");
17998:     }
17878: 
22618:     if (wasDeepAborted())
22618:         ABORT_TRACE("deep abort from property lookup");
22618: 
18439: #ifdef JS_THREADSAFE
18439:     // There's a potential race in any JS_THREADSAFE embedding that's nuts
18439:     // enough to share mutable objects on the scope or proto chain, but we
18439:     // don't care about such insane embeddings. Anyway, the (scope, proto)
18439:     // entry->vcap coordinates must reach obj2 from aobj at this point.
18439:     JS_ASSERT(cx->requestDepth);
18439: #endif
18439: 
18439:     // Emit guard(s), common code for both hit and miss cases.
18439:     // Check for first-level cache hit and guard on kshape if possible.
18439:     // Otherwise guard on key object exact match.
18061:     if (PCVCAP_TAG(entry->vcap) <= 1) {
18112:         if (aobj != globalObj) {
18286:             LIns* shape_ins = addName(lir->insLoad(LIR_ld, map_ins, offsetof(JSScope, shape)),
18286:                                       "shape");
18439:             guard(true, addName(lir->ins2i(LIR_eq, shape_ins, entry->kshape), "guard(kshape)"),
23721:                   BRANCH_EXIT);
18112:         }
18061:     } else {
18680: #ifdef DEBUG
19093:         JSOp op = JSOp(*pc);
18689:         ptrdiff_t pcoff = (op == JSOP_GETARGPROP) ? ARGNO_LEN :
18689:                           (op == JSOP_GETLOCALPROP) ? SLOTNO_LEN : 0;
19093:         jsatomid index = js_GetIndexFromBytecode(cx, cx->fp->script, pc, pcoff);
18680:         JS_ASSERT(entry->kpc == (jsbytecode*) atoms[index]);
18112:         JS_ASSERT(entry->kshape == jsuword(aobj));
18680: #endif
23075:         if (aobj != globalObj && !obj_ins->isconstp()) {
18439:             guard(true, addName(lir->ins2i(LIR_eq, obj_ins, entry->kshape), "guard(kobj)"),
23721:                   BRANCH_EXIT);
18439:         }
18439:     }
18439: 
21685:     // For any hit that goes up the scope and/or proto chains, we will need to
18439:     // guard on the shape of the object containing the property.
18061:     if (PCVCAP_TAG(entry->vcap) >= 1) {
18439:         jsuword vcap = entry->vcap;
18439:         uint32 vshape = PCVCAP_SHAPE(vcap);
19020:         JS_ASSERT(OBJ_SHAPE(obj2) == vshape);
18439: 
20979:         LIns* obj2_ins;
20979:         if (PCVCAP_TAG(entry->vcap) == 1) {
20979:             // Duplicate the special case in PROPERTY_CACHE_TEST.
20979:             obj2_ins = stobj_get_fslot(obj_ins, JSSLOT_PROTO);
23721:             guard(false, lir->ins_eq0(obj2_ins), BRANCH_EXIT);
20979:         } else {
20979:             obj2_ins = INS_CONSTPTR(obj2);
20979:         }
18230:         map_ins = lir->insLoad(LIR_ldp, obj2_ins, (int)offsetof(JSObject, map));
18439:         if (!map_is_native(obj2->map, map_ins, ops_ins))
25636:             ABORT_TRACE("non-native map");
18439: 
18439:         LIns* shape_ins = addName(lir->insLoad(LIR_ld, map_ins, offsetof(JSScope, shape)),
18439:                                   "shape");
18086:         guard(true,
18439:               addName(lir->ins2i(LIR_eq, shape_ins, vshape), "guard(vshape)"),
23721:               BRANCH_EXIT);
17878:     }
17747: 
18439:     pcval = entry->vword;
17459:     return true;
17459: }
17459: 
22652: JS_REQUIRES_STACK bool
17541: TraceRecorder::test_property_cache_direct_slot(JSObject* obj, LIns* obj_ins, uint32& slot)
17541: {
17541:     JSObject* obj2;
17746:     jsuword pcval;
17541: 
17541:     /*
17541:      * Property cache ensures that we are dealing with an existing property,
17541:      * and guards the shape for us.
17541:      */
17746:     if (!test_property_cache(obj, obj_ins, obj2, pcval))
17541:         return false;
17541: 
17998:     /* No such property means invalid slot, which callers must check for first. */
17998:     if (PCVAL_IS_NULL(pcval)) {
17998:         slot = SPROP_INVALID_SLOT;
17998:         return true;
17998:     }
17998: 
19558:     /* Insist on obj being the directly addressed object. */
19558:     if (obj2 != obj)
19558:         ABORT_TRACE("test_property_cache_direct_slot hit prototype chain");
17541: 
17969:     /* Don't trace getter or setter calls, our caller wants a direct slot. */
17746:     if (PCVAL_IS_SPROP(pcval)) {
17746:         JSScopeProperty* sprop = PCVAL_TO_SPROP(pcval);
17545: 
25938:         if (!isValidSlot(OBJ_SCOPE(obj), sprop))
25938:             return false;
25938: 
17545:         slot = sprop->slot;
17545:     } else {
17746:         if (!PCVAL_IS_SLOT(pcval))
17630:             ABORT_TRACE("PCE is not a slot");
17746:         slot = PCVAL_TO_SLOT(pcval);
17545:     }
17541:     return true;
17541: }
17541: 
17429: void
22626: TraceRecorder::stobj_set_dslot(LIns *obj_ins, unsigned slot, LIns*& dslots_ins, LIns* v_ins,
22626:                                const char *name)
22626: {
22626:     if (!dslots_ins)
22626:         dslots_ins = lir->insLoad(LIR_ldp, obj_ins, offsetof(JSObject, dslots));
22626:     addName(lir->insStorei(v_ins, dslots_ins, slot * sizeof(jsval)), name);
22626: }
22626: 
22626: void
17429: TraceRecorder::stobj_set_slot(LIns* obj_ins, unsigned slot, LIns*& dslots_ins, LIns* v_ins)
17426: {
17487:     if (slot < JS_INITIAL_NSLOTS) {
17721:         addName(lir->insStorei(v_ins, obj_ins,
17721:                                offsetof(JSObject, fslots) + slot * sizeof(jsval)),
17721:                 "set_slot(fslots)");
17487:     } else {
22626:         stobj_set_dslot(obj_ins, slot - JS_INITIAL_NSLOTS, dslots_ins, v_ins,
22626:                         "set_slot(dslots)");
17429:     }
17426: }
17426: 
17459: LIns*
17899: TraceRecorder::stobj_get_fslot(LIns* obj_ins, unsigned slot)
17899: {
17899:     JS_ASSERT(slot < JS_INITIAL_NSLOTS);
18230:     return lir->insLoad(LIR_ldp, obj_ins, offsetof(JSObject, fslots) + slot * sizeof(jsval));
17899: }
17899: 
17899: LIns*
17459: TraceRecorder::stobj_get_slot(LIns* obj_ins, unsigned slot, LIns*& dslots_ins)
17426: {
17899:     if (slot < JS_INITIAL_NSLOTS)
17899:         return stobj_get_fslot(obj_ins, slot);
17459: 
17429:     if (!dslots_ins)
18230:         dslots_ins = lir->insLoad(LIR_ldp, obj_ins, offsetof(JSObject, dslots));
18230:     return lir->insLoad(LIR_ldp, dslots_ins, (slot - JS_INITIAL_NSLOTS) * sizeof(jsval));
17426: }
17426: 
17426: bool
17429: TraceRecorder::native_set(LIns* obj_ins, JSScopeProperty* sprop, LIns*& dslots_ins, LIns* v_ins)
17426: {
17426:     if (SPROP_HAS_STUB_SETTER(sprop) && sprop->slot != SPROP_INVALID_SLOT) {
17429:         stobj_set_slot(obj_ins, sprop->slot, dslots_ins, v_ins);
17429:         return true;
17426:     }
17721:     ABORT_TRACE("unallocated or non-stub sprop");
17426: }
17426: 
17426: bool
17429: TraceRecorder::native_get(LIns* obj_ins, LIns* pobj_ins, JSScopeProperty* sprop,
17429:         LIns*& dslots_ins, LIns*& v_ins)
17426: {
17459:     if (!SPROP_HAS_STUB_GETTER(sprop))
17459:         return false;
17459: 
17426:     if (sprop->slot != SPROP_INVALID_SLOT)
17459:         v_ins = stobj_get_slot(pobj_ins, sprop->slot, dslots_ins);
17426:     else
24846:         v_ins = INS_CONST(JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID));
24846:     return true;
24846: }
18001: 
23708: JS_REQUIRES_STACK void
17468: TraceRecorder::box_jsval(jsval v, LIns*& v_ins)
17468: {
17470:     if (isNumber(v)) {
17477:         LIns* args[] = { v_ins, cx_ins };
20915:         v_ins = lir->insCall(&js_BoxDouble_ci, args);
18680:         guard(false, lir->ins2(LIR_eq, v_ins, INS_CONST(JSVAL_ERROR_COOKIE)),
17850:               OOM_EXIT);
23708:         return;
17468:     }
17470:     switch (JSVAL_TAG(v)) {
17470:       case JSVAL_BOOLEAN:
18645:         v_ins = lir->ins2i(LIR_pior, lir->ins2i(LIR_pilsh, v_ins, JSVAL_TAGBITS), JSVAL_BOOLEAN);
23708:         return;
18001:       case JSVAL_OBJECT:
23708:         return;
23708:       default:
23708:         JS_ASSERT(JSVAL_TAG(v) == JSVAL_STRING);
18645:         v_ins = lir->ins2(LIR_pior, v_ins, INS_CONST(JSVAL_STRING));
23708:         return;
23708:     }
17470: }
17468: 
23710: JS_REQUIRES_STACK void
26286: TraceRecorder::unbox_jsval(jsval v, LIns*& v_ins)
17460: {
17470:     if (isNumber(v)) {
17470:         // JSVAL_IS_NUMBER(v)
17758:         guard(false,
18645:               lir->ins_eq0(lir->ins2(LIR_pior,
18648:                                      lir->ins2(LIR_piand, v_ins, INS_CONST(JSVAL_INT)),
17758:                                      lir->ins2i(LIR_eq,
18232:                                                 lir->ins2(LIR_piand, v_ins,
18648:                                                           INS_CONST(JSVAL_TAGMASK)),
18095:                                                 JSVAL_DOUBLE))),
26286:               MISMATCH_EXIT);
18693:         LIns* args[] = { v_ins };
20915:         v_ins = lir->insCall(&js_UnboxDouble_ci, args);
23710:         return;
17460:     }
17470:     switch (JSVAL_TAG(v)) {
17470:       case JSVAL_BOOLEAN:
17541:         guard(true,
17541:               lir->ins2i(LIR_eq,
18648:                          lir->ins2(LIR_piand, v_ins, INS_CONST(JSVAL_TAGMASK)),
18095:                          JSVAL_BOOLEAN),
26286:               MISMATCH_EXIT);
17470:         v_ins = lir->ins2i(LIR_ush, v_ins, JSVAL_TAGBITS);
23710:         return;
17630:       case JSVAL_OBJECT:
23075:         if (JSVAL_IS_NULL(v)) {
23075:             // JSVAL_NULL maps to type JSVAL_TNULL, so insist that v_ins == 0 here.
26286:             guard(true, lir->ins_eq0(v_ins), MISMATCH_EXIT);
23075:         } else {
23075:             // We must guard that v_ins has JSVAL_OBJECT tag but is not JSVAL_NULL.
26286:             LIns* exit = snapshot(MISMATCH_EXIT);
17630:             guard(true,
17630:                   lir->ins2i(LIR_eq,
18648:                              lir->ins2(LIR_piand, v_ins, INS_CONST(JSVAL_TAGMASK)),
18095:                              JSVAL_OBJECT),
23075:                   exit);
23075:             guard(false, lir->ins_eq0(v_ins), exit);
23075:         }
23710:         return;
23710:       default:
23710:         JS_ASSERT(JSVAL_TAG(v) == JSVAL_STRING);
17870:         guard(true,
17870:               lir->ins2i(LIR_eq,
18648:                         lir->ins2(LIR_piand, v_ins, INS_CONST(JSVAL_TAGMASK)),
18095:                         JSVAL_STRING),
26286:               MISMATCH_EXIT);
18645:         v_ins = lir->ins2(LIR_piand, v_ins, INS_CONST(~JSVAL_TAGMASK));
23710:         return;
23710:     }
17470: }
17460: 
22652: JS_REQUIRES_STACK bool
17688: TraceRecorder::getThis(LIns*& this_ins)
17688: {
17688:     if (cx->fp->callee) { /* in a function */
17688:         if (JSVAL_IS_NULL(cx->fp->argv[-1]))
17688:             return false;
17688:         this_ins = get(&cx->fp->argv[-1]);
18053:         guard(false, lir->ins_eq0(this_ins), MISMATCH_EXIT);
17688:     } else { /* in global code */
18286:         this_ins = scopeChain();
17688:     }
17688:     return true;
17688: }
17688: 
22652: JS_REQUIRES_STACK bool
25879: TraceRecorder::guardClass(JSObject* obj, LIns* obj_ins, JSClass* clasp, LIns* exit)
17899: {
20974:     bool cond = STOBJ_GET_CLASS(obj) == clasp;
17899: 
19020:     LIns* class_ins = lir->insLoad(LIR_ldp, obj_ins, offsetof(JSObject, classword));
18645:     class_ins = lir->ins2(LIR_piand, class_ins, lir->insImm(~3));
17899: 
17899:     char namebuf[32];
17899:     JS_snprintf(namebuf, sizeof namebuf, "guard(class is %s)", clasp->name);
25879:     guard(cond, addName(lir->ins2(LIR_eq, class_ins, INS_CONSTPTR(clasp)), namebuf), exit);
20974:     return cond;
17630: }
17630: 
22652: JS_REQUIRES_STACK bool
21685: TraceRecorder::guardDenseArray(JSObject* obj, LIns* obj_ins, ExitType exitType)
21685: {
25879:     return guardClass(obj, obj_ins, &js_ArrayClass, snapshot(exitType));
17899: }
17899: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::guardDenseArrayIndex(JSObject* obj, jsint idx, LIns* obj_ins,
20972:                                     LIns* dslots_ins, LIns* idx_ins, ExitType exitType)
17429: {
25475:     jsuint capacity = js_DenseArrayCapacity(obj);
25475: 
25475:     bool cond = (jsuint(idx) < jsuint(obj->fslots[JSSLOT_ARRAY_LENGTH]) && jsuint(idx) < capacity);
21083:     if (cond) {
21083:         /* Guard array length */
26286:         LIns* exit = guard(true,
21083:                            lir->ins2(LIR_ult, idx_ins, stobj_get_fslot(obj_ins, JSSLOT_ARRAY_LENGTH)),
26286:                            exitType)->oprnd2();
21083:         /* dslots must not be NULL */
21083:         guard(false,
21083:               lir->ins_eq0(dslots_ins),
21083:               exit);
21083:         /* Guard array capacity */
21083:         guard(true,
21083:               lir->ins2(LIR_ult,
21083:                         idx_ins,
21083:                         lir->insLoad(LIR_ldp, dslots_ins, 0 - (int)sizeof(jsval))),
21083:               exit);
21083:     } else {
21083:         /* If not idx < length, stay on trace (and read value as undefined). */
21083:         LIns* br1 = lir->insBranch(LIR_jf,
21083:                                    lir->ins2(LIR_ult,
21083:                                              idx_ins,
21083:                                              stobj_get_fslot(obj_ins, JSSLOT_ARRAY_LENGTH)),
21083:                                    NULL);
21083:         /* If dslots is NULL, stay on trace (and read value as undefined). */
21083:         LIns* br2 = lir->insBranch(LIR_jt, lir->ins_eq0(dslots_ins), NULL);
21083:         /* If not idx < capacity, stay on trace (and read value as undefined). */
21083:         LIns* br3 = lir->insBranch(LIR_jf,
21083:                                    lir->ins2(LIR_ult,
21083:                                              idx_ins,
21083:                                              lir->insLoad(LIR_ldp, dslots_ins, 0 - (int)sizeof(jsval))),
21083:                                    NULL);
21083:         lir->insGuard(LIR_x, lir->insImm(1), snapshot(exitType));
21083:         LIns* label = lir->ins0(LIR_label);
21083:         br1->target(label);
21083:         br2->target(label);
21083:         br3->target(label);
21083:     }
20974:     return cond;
17427: }
17427: 
26274: bool
26274: TraceRecorder::guardNotGlobalObject(JSObject* obj, LIns* obj_ins)
26274: {
26274:     if (obj == globalObj)
26274:         ABORT_TRACE("reference aliases global object");
26274:     guard(false, lir->ins2(LIR_eq, obj_ins, globalObj_ins), MISMATCH_EXIT);
19983:     return true;
19983: }
19983: 
22652: JS_REQUIRES_STACK void
17818: TraceRecorder::clearFrameSlotsFromCache()
17818: {
17815:     /* Clear out all slots of this frame in the nativeFrameTracker. Different locations on the
17811:        VM stack might map to different locations on the native stack depending on the
17811:        number of arguments (i.e.) of the next call, so we have to make sure we map
17811:        those in to the cache with the right offsets. */
17811:     JSStackFrame* fp = cx->fp;
17811:     jsval* vp;
17811:     jsval* vpstop;
18136:     if (fp->callee) {
18187:         vp = &fp->argv[-2];
18425:         vpstop = &fp->argv[fp->fun->nargs];
18136:         while (vp < vpstop)
18136:             nativeFrameTracker.set(vp++, (LIns*)0);
18136:     }
18136:     vp = &fp->slots[0];
18136:     vpstop = &fp->slots[fp->script->nslots];
18136:     while (vp < vpstop)
18136:         nativeFrameTracker.set(vp++, (LIns*)0);
17818: }
17818: 
22652: JS_REQUIRES_STACK bool
17818: TraceRecorder::record_EnterFrame()
17818: {
19078:     JSStackFrame* fp = cx->fp;
19078: 
17852:     if (++callDepth >= MAX_CALLDEPTH)
17852:         ABORT_TRACE("exceeded maximum call depth");
20899:     // FIXME: Allow and attempt to inline a single level of recursion until we compile
20899:     //        recursive calls as independent trees (459301).
20899:     if (fp->script == fp->down->script && fp->down->down && fp->down->down->script == fp->script)
19078:         ABORT_TRACE("recursive call");
19078: 
18260:     debug_only_v(printf("EnterFrame %s, callDepth=%d\n",
18140:                         js_AtomToPrintableString(cx, cx->fp->fun->atom),
21685:                         callDepth);)
23450:     debug_only_v(
23450:         js_Disassemble(cx, cx->fp->script, JS_TRUE, stdout);
23450:         printf("----\n");)
24846:     LIns* void_ins = INS_CONST(JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID));
18119: 
18119:     jsval* vp = &fp->argv[fp->argc];
19567:     jsval* vpstop = vp + ptrdiff_t(fp->fun->nargs) - ptrdiff_t(fp->argc);
18119:     while (vp < vpstop) {
18119:         if (vp >= fp->down->regs->sp)
18119:             nativeFrameTracker.set(vp, (LIns*)0);
18119:         set(vp++, void_ins, true);
18119:     }
18119: 
18119:     vp = &fp->slots[0];
18119:     vpstop = vp + fp->script->nfixed;
18119:     while (vp < vpstop)
18119:         set(vp++, void_ins, true);
17811:     return true;
17811: }
17818: 
22652: JS_REQUIRES_STACK bool
17818: TraceRecorder::record_LeaveFrame()
17818: {
18260:     debug_only_v(
18142:         if (cx->fp->fun)
18150:             printf("LeaveFrame (back to %s), callDepth=%d\n",
18140:                    js_AtomToPrintableString(cx, cx->fp->fun->atom),
18140:                    callDepth);
18260:         );
26118:     if (callDepth-- <= 0)
26118:         ABORT_TRACE("returned out of a loop we started tracing");
18001: 
18001:     // LeaveFrame gets called after the interpreter popped the frame and
18001:     // stored rval, so cx->fp not cx->fp->down, and -1 not 0.
24293:     atoms = FrameAtomBase(cx, cx->fp);
18150:     set(&stackval(-1), rval_ins, true);
17818:     return true;
17805: }
17805: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_PUSH()
17409: {
24846:     stack(0, INS_CONST(JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID)));
17456:     return true;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_POPV()
17926: {
20907:     jsval& rval = stackval(-1);
20907:     LIns *rval_ins = get(&rval);
23708:     box_jsval(rval, rval_ins);
20907: 
20907:     // Store it in cx->fp->rval. NB: Tricky dependencies. cx->fp is the right
20907:     // frame because POPV appears only in global and eval code and we don't
20907:     // trace JSOP_EVAL or leaving the frame where tracing started.
20907:     LIns *fp_ins = lir->insLoad(LIR_ldp, cx_ins, offsetof(JSContext, fp));
20907:     lir->insStorei(rval_ins, fp_ins, offsetof(JSStackFrame, rval));
20907:     return true;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
21685: TraceRecorder::record_JSOP_ENTERWITH()
21685: {
21685:     return false;
21685: }
21685: 
22652: JS_REQUIRES_STACK bool
21685: TraceRecorder::record_JSOP_LEAVEWITH()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_RETURN()
17409: {
26557:     /* A return from callDepth 0 terminates the current loop. */
26557:     if (callDepth == 0) {
26557:         AUDIT(returnLoopExits);
26557:         endLoop(traceMonitor);
26557:         return false;
26557:     }
26557: 
26557:     /* If we inlined this function call, make the return value available to the caller code. */
18001:     jsval& rval = stackval(-1);
18661:     JSStackFrame *fp = cx->fp;
18785:     if ((cx->fp->flags & JSFRAME_CONSTRUCTING) && JSVAL_IS_PRIMITIVE(rval)) {
18661:         JS_ASSERT(OBJECT_TO_JSVAL(fp->thisp) == fp->argv[-1]);
18661:         rval_ins = get(&fp->argv[-1]);
18661:     } else {
18661:         rval_ins = get(&rval);
18001:     }
21685:     debug_only_v(printf("returning from %s\n", js_AtomToPrintableString(cx, cx->fp->fun->atom));)
17818:     clearFrameSlotsFromCache();
26557: 
17689:     return true;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_GOTO()
17409: {
26557:     /*
26557:      * If we hit a break, end the loop and generate an always taken loop exit guard.
26557:      * For other downward gotos (like if/else) continue recording.
26557:      */
26557:     jssrcnote* sn = js_GetSrcNote(cx->fp->script, cx->fp->regs->pc);
26557: 
26557:     if (sn && SN_TYPE(sn) == SRC_BREAK) {
26557:         AUDIT(breakLoopExits);
26557:         endLoop(traceMonitor);
26557:         return false;
26557:     }
17577:     return true;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_IFEQ()
17409: {
18694:     trackCfgMerges(cx->fp->regs->pc);
20416:     return ifop();
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_IFNE()
17409: {
20416:     return ifop();
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_ARGUMENTS()
17409: {
19978: #if 1
19978:     ABORT_TRACE("can't trace arguments yet");
19978: #else
19068:     LIns* args[] = { cx_ins };
20915:     LIns* a_ins = lir->insCall(&js_Arguments_ci, args);
19068:     guard(false, lir->ins_eq0(a_ins), OOM_EXIT);
19068:     stack(0, a_ins);
19068:     return true;
19978: #endif
17409: }
17899: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_DUP()
17409: {
17448:     stack(0, get(&stackval(-1)));
17448:     return true;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_DUP2()
17409: {
17448:     stack(0, get(&stackval(-2)));
17448:     stack(1, get(&stackval(-1)));
17448:     return true;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
21685: TraceRecorder::record_JSOP_SWAP()
21685: {
21685:     jsval& l = stackval(-2);
21685:     jsval& r = stackval(-1);
21685:     LIns* l_ins = get(&l);
21685:     LIns* r_ins = get(&r);
21685:     set(&r, l_ins);
21685:     set(&l, r_ins);
21685:     return true;
21685: }
21685: 
22652: JS_REQUIRES_STACK bool
23097: TraceRecorder::record_JSOP_PICK()
23097: {
23097:     jsval* sp = cx->fp->regs->sp;
23102:     jsint n = cx->fp->regs->pc[1];
23097:     JS_ASSERT(sp - (n+1) >= StackBase(cx->fp));
24381:     LIns* top = get(sp - (n+1));
23097:     for (jsint i = 0; i < n; ++i)
24381:         set(sp - (n+1) + i, get(sp - n + i));
23097:     set(&sp[-1], top);
23097:     return true;
23097: }
23097: 
23097: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_SETCONST()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_BITOR()
17409: {
17469:     return binary(LIR_or);
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_BITXOR()
17409: {
17469:     return binary(LIR_xor);
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_BITAND()
17409: {
17469:     return binary(LIR_and);
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_EQ()
17409: {
23093:     return equality(false, true);
18687: }
18687: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_NE()
17409: {
23093:     return equality(true, true);
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_LT()
17409: {
23093:     return relational(LIR_flt, true);
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_LE()
17409: {
23093:     return relational(LIR_fle, true);
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_GT()
17409: {
23093:     return relational(LIR_fgt, true);
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_GE()
17409: {
23093:     return relational(LIR_fge, true);
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_LSH()
17409: {
17469:     return binary(LIR_lsh);
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_RSH()
17409: {
17469:     return binary(LIR_rsh);
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_URSH()
17409: {
17469:     return binary(LIR_ush);
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_ADD()
17409: {
17872:     jsval& r = stackval(-1);
17872:     jsval& l = stackval(-2);
21685: 
23075:     if (!JSVAL_IS_PRIMITIVE(l)) {
23075:         if (!JSVAL_IS_PRIMITIVE(r))
21685:             return call_imacro(add_imacros.obj_obj);
21685:         return call_imacro(add_imacros.obj_any);
21685:     }
23075:     if (!JSVAL_IS_PRIMITIVE(r))
21685:         return call_imacro(add_imacros.any_obj);
21685: 
21447:     if (JSVAL_IS_STRING(l) || JSVAL_IS_STRING(r)) {
21685:         LIns* args[] = { stringify(r), stringify(l), cx_ins };
20915:         LIns* concat = lir->insCall(&js_ConcatStrings_ci, args);
17873:         guard(false, lir->ins_eq0(concat), OOM_EXIT);
17872:         set(&l, concat);
17872:         return true;
17872:     }
21685: 
17469:     return binary(LIR_fadd);
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_SUB()
17409: {
17469:     return binary(LIR_fsub);
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_MUL()
17409: {
17469:     return binary(LIR_fmul);
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_DIV()
17409: {
17469:     return binary(LIR_fdiv);
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_MOD()
17409: {
17683:     jsval& r = stackval(-1);
17683:     jsval& l = stackval(-2);
21685: 
23075:     if (!JSVAL_IS_PRIMITIVE(l)) {
23075:         if (!JSVAL_IS_PRIMITIVE(r))
21685:             return call_imacro(binary_imacros.obj_obj);
21685:         return call_imacro(binary_imacros.obj_any);
21685:     }
23075:     if (!JSVAL_IS_PRIMITIVE(r))
21685:         return call_imacro(binary_imacros.any_obj);
21685: 
17683:     if (isNumber(l) && isNumber(r)) {
19974:         LIns* l_ins = get(&l);
19974:         LIns* r_ins = get(&r);
19974:         LIns* x;
19974:         /* We can't demote this in a filter since we need the actual values of l and r. */
19974:         if (isPromote(l_ins) && isPromote(r_ins) && asNumber(l) >= 0 && asNumber(r) > 0) {
19974:             LIns* args[] = { ::demote(lir, r_ins), ::demote(lir, l_ins) };
20915:             x = lir->insCall(&js_imod_ci, args);
19974:             guard(false, lir->ins2(LIR_eq, x, lir->insImm(-1)), BRANCH_EXIT);
19974:             x = lir->ins1(LIR_i2f, x);
19974:         } else {
19974:             LIns* args[] = { r_ins, l_ins };
20915:             x = lir->insCall(&js_dmod_ci, args);
19974:         }
19974:         set(&l, x);
17683:         return true;
17683:     }
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_NOT()
17409: {
17436:     jsval& v = stackval(-1);
19070:     if (JSVAL_TAG(v) == JSVAL_BOOLEAN) {
19070:         set(&v, lir->ins_eq0(lir->ins2i(LIR_eq, get(&v), 1)));
19070:         return true;
19070:     }
19554:     if (isNumber(v)) {
21776:         LIns* v_ins = get(&v);
21776:         set(&v, lir->ins2(LIR_or, lir->ins2(LIR_feq, v_ins, lir->insImmq(0)),
21776:                                   lir->ins_eq0(lir->ins2(LIR_feq, v_ins, v_ins))));
19554:         return true;
19554:     }
23075:     if (JSVAL_TAG(v) == JSVAL_OBJECT) {
19554:         set(&v, lir->ins_eq0(get(&v)));
18769:         return true;
18769:     }
20435:     JS_ASSERT(JSVAL_IS_STRING(v));
19580:     set(&v, lir->ins_eq0(lir->ins2(LIR_piand,
19580:                                    lir->insLoad(LIR_ldp, get(&v), (int)offsetof(JSString, length)),
26397:                                    INS_CONSTPTR(reinterpret_cast<void *>(JSSTRING_LENGTH_MASK)))));
17436:     return true;
17436: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_BITNOT()
17409: {
17469:     return unary(LIR_not);
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_NEG()
17409: {
18787:     jsval& v = stackval(-1);
23106: 
23106:     if (!JSVAL_IS_PRIMITIVE(v))
23106:         return call_imacro(unary_imacros.sign);
23106: 
18787:     if (isNumber(v)) {
18787:         LIns* a = get(&v);
18787: 
18787:         /* If we're a promoted integer, we have to watch out for 0s since -0 is a double.
18787:            Only follow this path if we're not an integer that's 0 and we're not a double
18787:            that's zero.
18787:          */
18787:         if (isPromoteInt(a) &&
18787:             (!JSVAL_IS_INT(v) || JSVAL_TO_INT(v) != 0) &&
22610:             (!JSVAL_IS_DOUBLE(v) || !JSDOUBLE_IS_NEGZERO(*JSVAL_TO_DOUBLE(v))) &&
22610:             -asNumber(v) == (int)-asNumber(v)) {
18787:             a = lir->ins1(LIR_neg, ::demote(lir, a));
26117:             if (!a->isconst()) {
26117:                 LIns* exit = snapshot(OVERFLOW_EXIT);
26117:                 lir->insGuard(LIR_xt, lir->ins1(LIR_ov, a), exit);
26117:                 lir->insGuard(LIR_xt, lir->ins2(LIR_eq, a, lir->insImm(0)), exit);
26117:             }
18787:             a = lir->ins1(LIR_i2f, a);
18787:         } else {
18787:             a = lir->ins1(LIR_fneg, a);
18787:         }
18787: 
18787:         set(&v, a);
18787:         return true;
18787:     }
23106: 
23106:     if (JSVAL_IS_NULL(v)) {
26265:         set(&v, lir->insImmf(-0.0));
23106:         return true;
23106:     }
23106: 
23225:     JS_ASSERT(JSVAL_TAG(v) == JSVAL_STRING || JSVAL_TAG(v) == JSVAL_BOOLEAN);
23225: 
23106:     LIns* args[] = { get(&v), cx_ins };
23225:     set(&v, lir->ins1(LIR_fneg,
23225:                       lir->insCall(JSVAL_IS_STRING(v)
23225:                                    ? &js_StringToNumber_ci
23225:                                    : &js_BooleanOrUndefinedToNumber_ci,
23225:                                    args)));
23106:     return true;
23106: }
23106: 
23106: JS_REQUIRES_STACK bool
23106: TraceRecorder::record_JSOP_POS()
23106: {
23225:     jsval& v = stackval(-1);
23225: 
23225:     if (!JSVAL_IS_PRIMITIVE(v))
23106:         return call_imacro(unary_imacros.sign);
23106: 
23225:     if (isNumber(v))
23225:         return true;
23225: 
23225:     if (JSVAL_IS_NULL(v)) {
23225:         set(&v, lir->insImmq(0));
23225:         return true;
23225:     }
23225: 
23225:     JS_ASSERT(JSVAL_TAG(v) == JSVAL_STRING || JSVAL_TAG(v) == JSVAL_BOOLEAN);
23225: 
23225:     LIns* args[] = { get(&v), cx_ins };
23225:     set(&v, lir->insCall(JSVAL_IS_STRING(v)
23106:                          ? &js_StringToNumber_ci
23106:                          : &js_BooleanOrUndefinedToNumber_ci,
23106:                          args));
23106:     return true;
23106: }
23106: 
23106: JS_REQUIRES_STACK bool
23106: TraceRecorder::record_JSOP_PRIMTOP()
23106: {
23106:     // Either this opcode does nothing or we couldn't have traced here, because
23106:     // we'd have thrown an exception -- so do nothing if we actually hit this.
23106:     return true;
17409: }
17926: 
24873: JS_REQUIRES_STACK bool
24873: TraceRecorder::record_JSOP_OBJTOP()
24873: {
24873:     // See the comment in record_JSOP_PRIMTOP.
24873:     return true;
24873: }
24873: 
18300: JSBool
18300: js_Array(JSContext* cx, JSObject* obj, uintN argc, jsval* argv, jsval* rval);
18300: 
18712: JSBool
18712: js_Object(JSContext *cx, JSObject *obj, uintN argc, jsval *argv, jsval *rval);
18712: 
20402: JSBool
20402: js_Date(JSContext *cx, JSObject *obj, uintN argc, jsval *argv, jsval *rval);
20402: 
21526: bool
22626: TraceRecorder::getClassPrototype(JSObject* ctor, LIns*& proto_ins)
22626: {
22626:     jsval pval;
22626: 
22626:     if (!OBJ_GET_PROPERTY(cx, ctor,
22626:                           ATOM_TO_JSID(cx->runtime->atomState
22626:                                        .classPrototypeAtom),
22626:                           &pval)) {
22626:         ABORT_TRACE("error getting prototype from constructor");
22626:     }
23075:     if (JSVAL_TAG(pval) != JSVAL_OBJECT)
22626:         ABORT_TRACE("got primitive prototype from constructor");
25887: #ifdef DEBUG
25887:     JSBool ok, found;
25887:     uintN attrs;
25887:     ok = JS_GetPropertyAttributes(cx, ctor, js_class_prototype_str, &attrs, &found);
25887:     JS_ASSERT(ok);
25887:     JS_ASSERT(found);
25887:     JS_ASSERT((~attrs & (JSPROP_READONLY | JSPROP_PERMANENT)) == 0);
25887: #endif
22626:     proto_ins = INS_CONSTPTR(JSVAL_TO_OBJECT(pval));
22626:     return true;
22626: }
22626: 
22626: bool
22626: TraceRecorder::newArray(JSObject *ctor, uint32 argc, jsval *argv, jsval *rval)
22626: {
22626:     LIns *proto_ins, *arr_ins;
22626:     if (!getClassPrototype(ctor, proto_ins))
22626:         return false;
22626: 
22626:     if (argc == 0) {
22626:         LIns *args[] = { proto_ins, cx_ins };
22626:         arr_ins = lir->insCall(&js_FastNewArray_ci, args);
22626:         guard(false, lir->ins_eq0(arr_ins), OOM_EXIT);
22626:     } else if (argc == 1 && (JSVAL_IS_NUMBER(argv[0]) || JSVAL_IS_STRING(argv[0]))) {
22626:         bool num = JSVAL_IS_NUMBER(argv[0]);
22626:         LIns *arg_ins = get(argv);
22626:         if (num)
22626:             arg_ins = f2i(arg_ins);
22626:         LIns *args[] = { arg_ins, proto_ins, cx_ins };
22626:         arr_ins = lir->insCall(num ? &js_FastNewArrayWithLength_ci : &js_Array_1str_ci, args);
22626:         guard(false, lir->ins_eq0(arr_ins), OOM_EXIT);
22626:     } else {
22626:         // arr_ins = js_NewUninitializedArray(cx, Array.prototype, argc)
22626:         LIns *args[] = { INS_CONST(argc), proto_ins, cx_ins };
22626:         arr_ins = lir->insCall(&js_NewUninitializedArray_ci, args);
22626:         guard(false, lir->ins_eq0(arr_ins), OOM_EXIT);
22626: 
22626:         // arr->dslots[i] = box_jsval(vp[i]);  for i in 0..argc
22626:         LIns *dslots_ins = NULL;
22626:         for (uint32 i = 0; i < argc; i++) {
22626:             LIns *elt_ins = get(argv + i);
23708:             box_jsval(argv[i], elt_ins);
22626:             stobj_set_dslot(arr_ins, i, dslots_ins, elt_ins, "set_array_elt");
22626:         }
22626:     }
22626:     set(rval, arr_ins);
22626:     return true;
22626: }
22626: 
26563: bool
26563: TraceRecorder::newString(JSObject* ctor, jsval& arg, jsval* rval)
26563: {
26563:     if (!JSVAL_IS_PRIMITIVE(arg))
26563:         return call_imacro(new_imacros.String);
26563: 
26563:     LIns* proto_ins;
26563:     if (!getClassPrototype(ctor, proto_ins))
26563:         return false;
26563: 
26563:     LIns* args[] = { stringify(arg), proto_ins, cx_ins };
26563:     LIns* obj_ins = lir->insCall(&js_String_tn_ci, args);
26563:     guard(false, lir->ins_eq0(obj_ins), OOM_EXIT);
26563: 
26563:     set(rval, obj_ins);
26563:     return true;
26563: }
26563: 
22652: JS_REQUIRES_STACK bool
26552: TraceRecorder::emitNativeCall(JSTraceableNative* known, uintN argc, LIns* args[])
26552: {
26552:     bool constructing = known->flags & JSTN_CONSTRUCTOR;
26552: 
26552:     if (JSTN_ERRTYPE(known) == FAIL_STATUS) {
26552:         // This needs to capture the pre-call state of the stack. So do not set
26552:         // pendingTraceableNative before taking this snapshot.
26552:         JS_ASSERT(!pendingTraceableNative);
26552: 
26552:         // Take snapshot for deep LeaveTree and store it in cx->bailExit.
26552:         LIns* rec_ins = snapshot(DEEP_BAIL_EXIT);
26552:         GuardRecord* rec = (GuardRecord *) rec_ins->payload();
26552:         JS_ASSERT(rec->exit);
26552:         lir->insStorei(INS_CONSTPTR(rec->exit), cx_ins, offsetof(JSContext, bailExit));
26552: 
26552:         // Tell nanojit not to discard or defer stack writes before this call.
26552:         lir->insGuard(LIR_xbarrier, rec_ins, rec_ins);
26552:     }
26552: 
26552:     LIns* res_ins = lir->insCall(known->builtin, args);
26552:     if (!constructing)
26552:         rval_ins = res_ins;
26552:     switch (JSTN_ERRTYPE(known)) {
26552:       case FAIL_NULL:
26552:         guard(false, lir->ins_eq0(res_ins), OOM_EXIT);
26552:         break;
26552:       case FAIL_NEG:
26552:         res_ins = lir->ins1(LIR_i2f, res_ins);
26552:         guard(false, lir->ins2(LIR_flt, res_ins, lir->insImmq(0)), OOM_EXIT);
26552:         break;
26552:       case FAIL_VOID:
26552:         guard(false, lir->ins2i(LIR_eq, res_ins, JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID)), OOM_EXIT);
26552:         break;
26552:       case FAIL_COOKIE:
26552:         guard(false, lir->ins2(LIR_eq, res_ins, INS_CONST(JSVAL_ERROR_COOKIE)), OOM_EXIT);
26552:         break;
26552:       default:;
26552:     }
26552: 
26552:     set(&stackval(0 - (2 + argc)), res_ins);
26552: 
26552:     if (!constructing) {
26552:         /*
26552:          * The return value will be processed by FastNativeCallComplete since
26552:          * we have to know the actual return value type for calls that return
26552:          * jsval (like Array_p_pop).
26552:          */
26552:         pendingTraceableNative = known;
26552:     }
26552: 
26552:     return true;
26552: }
26552: 
26552: /*
26552:  * Check whether we have a specialized implementation for this fast native invocation.
26552:  */
26552: JS_REQUIRES_STACK bool
26552: TraceRecorder::callTraceableNative(JSFunction* fun, uintN argc, bool constructing)
26552: {
26552:     JSTraceableNative* known = FUN_TRCINFO(fun);
26552:     JS_ASSERT(known && (JSFastNative)fun->u.n.native == known->native);
26552: 
20431:     JSStackFrame* fp = cx->fp;
20431:     jsbytecode *pc = fp->regs->pc;
21511: 
18300:     jsval& fval = stackval(0 - (2 + argc));
26552:     jsval& tval = stackval(0 - (1 + argc));
26552: 
18641:     LIns* this_ins = get(&tval);
20431: 
26676:     LIns* args[nanojit::MAXARGS];
20431:     do {
25887:         if (((known->flags & JSTN_CONSTRUCTOR) != 0) != constructing)
17651:             continue;
17634: 
17871:         uintN knownargc = strlen(known->argtypes);
17870:         if (argc != knownargc)
18115:             continue;
17870: 
17870:         intN prefixc = strlen(known->prefix);
20431:         JS_ASSERT(prefixc <= 3);
17870:         LIns** argp = &args[argc + prefixc - 1];
17870:         char argtype;
17870: 
18172: #if defined _DEBUG
18172:         memset(args, 0xCD, sizeof(args));
18172: #endif
18172: 
20431:         uintN i;
20431:         for (i = prefixc; i--; ) {
20431:             argtype = known->prefix[i];
20431:             if (argtype == 'C') {
20431:                 *argp = cx_ins;
20431:             } else if (argtype == 'T') {   /* this, as an object */
23075:                 if (JSVAL_IS_PRIMITIVE(tval))
20431:                     goto next_specialization;
20431:                 *argp = this_ins;
20431:             } else if (argtype == 'S') {   /* this, as a string */
23228:                 if (!JSVAL_IS_STRING(tval))
23228:                     goto next_specialization;
23226:                 *argp = this_ins;
20431:             } else if (argtype == 'f') {
20431:                 *argp = INS_CONSTPTR(JSVAL_TO_OBJECT(fval));
20431:             } else if (argtype == 'p') {
22626:                 if (!getClassPrototype(JSVAL_TO_OBJECT(fval), *argp))
22626:                     return false;
20431:             } else if (argtype == 'R') {
20431:                 *argp = INS_CONSTPTR(cx->runtime);
20431:             } else if (argtype == 'P') {
24600:                 // FIXME: Set pc to imacpc when recording JSOP_CALL inside the
24600:                 //        JSOP_GETELEM imacro (bug 476559).
24600:                 if (*pc == JSOP_CALL && fp->imacpc && *fp->imacpc == JSOP_GETELEM)
24600:                     *argp = INS_CONSTPTR(fp->imacpc);
24600:                 else
20431:                     *argp = INS_CONSTPTR(pc);
20431:             } else if (argtype == 'D') {  /* this, as a number */
22634:                 if (!isNumber(tval))
20431:                     goto next_specialization;
20431:                 *argp = this_ins;
20431:             } else {
20431:                 JS_NOT_REACHED("unknown prefix arg type");
20431:             }
20431:             argp--;
20431:         }
20431: 
20431:         for (i = knownargc; i--; ) {
22634:             jsval& arg = stackval(0 - (i + 1));
22634:             *argp = get(&arg);
20431: 
20431:             argtype = known->argtypes[i];
20431:             if (argtype == 'd' || argtype == 'i') {
20431:                 if (!isNumber(arg))
20431:                     goto next_specialization;
20431:                 if (argtype == 'i')
20431:                     *argp = f2i(*argp);
20431:             } else if (argtype == 'o') {
23075:                 if (JSVAL_IS_PRIMITIVE(arg))
20431:                     goto next_specialization;
20431:             } else if (argtype == 's') {
20431:                 if (!JSVAL_IS_STRING(arg))
20431:                     goto next_specialization;
20431:             } else if (argtype == 'r') {
20431:                 if (!VALUE_IS_REGEXP(cx, arg))
20431:                     goto next_specialization;
20431:             } else if (argtype == 'f') {
20431:                 if (!VALUE_IS_FUNCTION(cx, arg))
20431:                     goto next_specialization;
20431:             } else if (argtype == 'v') {
23708:                 box_jsval(arg, *argp);
20431:             } else {
20431:                 goto next_specialization;
20431:             }
20431:             argp--;
20431:         }
20431:         goto success;
20431: 
20431: next_specialization:;
20431:     } while ((known++)->flags & JSTN_MORE);
20431: 
26552:     return false;
20431: 
20431: success:
18172: #if defined _DEBUG
18172:     JS_ASSERT(args[0] != (LIns *)0xcdcdcdcd);
18172: #endif
18172: 
26552:     return emitNativeCall(known, argc, args);
26552: }
26552: 
26552: bool
26552: TraceRecorder::callNative(JSFunction* fun, uintN argc, bool constructing)
26552: {
26552:     if (fun->flags & JSFUN_TRACEABLE) {
26552:         if (callTraceableNative(fun, argc, constructing))
26552:             return true;
26552:     }
26552: 
26552:     if (!(fun->flags & JSFUN_FAST_NATIVE))
26552:         ABORT_TRACE("untraceable slow native");
26552: 
26556:     if (constructing)
26556:         ABORT_TRACE("untraceable fast native constructor");
26556: 
26552:     jsval* vp = &stackval(0 - (2 + argc));
26552:     invokevp_ins = lir->insAlloc((2 + argc) * sizeof(jsval));
26552: 
20431:     /*
26552:      * For a very long argument list we might run out of LIR space, so better check while
26552:      * looping over the argument list.
20431:      */
26552:     for (jsint n = 0; n < jsint(2 + argc) && !lirbuf->outOMem(); ++n) {
26552:         LIns* i = get(&vp[n]);
26552:         box_jsval(vp[n], i);
26552:         lir->insStorei(i, invokevp_ins, n * sizeof(jsval));
26552:     }
26552: 
26552:     LIns* args[] = { invokevp_ins, lir->insImm(argc), cx_ins };
26552: 
26552:     CallInfo* ci = (CallInfo*) lir->skip(sizeof(struct CallInfo))->payload();
26552:     ci->_address = uintptr_t(fun->u.n.native);
26552:     ci->_argtypes = ARGSIZE_LO | ARGSIZE_LO << 2 | ARGSIZE_LO << 4 | ARGSIZE_LO << 6;
26552:     ci->_cse = ci->_fold = 0;
26552:     ci->_abi = ABI_CDECL;
26552: #ifdef DEBUG
26552:     ci->_name = "JSFastNative";
26552: #endif
26552: 
26552:     // Generate a JSTraceableNative structure on the fly.
26552:     generatedTraceableNative->builtin = ci;
26552:     generatedTraceableNative->native = (JSFastNative)fun->u.n.native;
26552:     generatedTraceableNative->flags = FAIL_STATUS | JSTN_UNBOX_AFTER;
26552:     generatedTraceableNative->prefix = generatedTraceableNative->argtypes = NULL;
26552: 
26552:     // argc is the original argc here. It is used to calculate where to place the return value.
26552:     return emitNativeCall(generatedTraceableNative, argc, args);
26552: }
26552: 
26552: JS_REQUIRES_STACK bool
26552: TraceRecorder::functionCall(bool constructing, uintN argc)
26552: {
26552:     jsval& fval = stackval(0 - (2 + argc));
26552:     JS_ASSERT(&fval >= StackBase(cx->fp));
26552: 
26552:     if (!VALUE_IS_FUNCTION(cx, fval))
26552:         ABORT_TRACE("callee is not a function");
26552: 
26552:     jsval& tval = stackval(0 - (1 + argc));
26552: 
26552:     /*
26552:      * If callee is not constant, it's a shapeless call and we have to guard
26552:      * explicitly that we will get this callee again at runtime.
26552:      */
26552:     if (!get(&fval)->isconst() && !guardCallee(fval))
26552:         return false;
26552: 
26552:     /*
26552:      * Require that the callee be a function object, to avoid guarding on its
26552:      * class here. We know if the callee and this were pushed by JSOP_CALLNAME
26552:      * or JSOP_CALLPROP that callee is a *particular* function, since these hit
26552:      * the property cache and guard on the object (this) in which the callee
26552:      * was found. So it's sufficient to test here that the particular function
26552:      * is interpreted, not guard on that condition.
26552:      *
26552:      * Bytecode sequences that push shapeless callees must guard on the callee
26552:      * class being Function and the function being interpreted.
26552:      */
26552:     JSFunction* fun = GET_FUNCTION_PRIVATE(cx, JSVAL_TO_OBJECT(fval));
26552: 
26552:     if (FUN_INTERPRETED(fun)) {
26552:         if (constructing) {
26552:             LIns* args[] = { get(&fval), cx_ins };
26552:             LIns* tv_ins = lir->insCall(&js_NewInstance_ci, args);
26552:             guard(false, lir->ins_eq0(tv_ins), OOM_EXIT);
26552:             set(&tval, tv_ins);
26552:         }
26552:         return interpretedFunctionCall(fval, fun, argc, constructing);
26552:     }
26552: 
26552:     if (FUN_SLOW_NATIVE(fun)) {
26552:         JSNative native = fun->u.n.native;
26552:         if (native == js_Array)
26563:             return newArray(JSVAL_TO_OBJECT(fval), argc, &tval + 1, &fval);
26563:         if (native == js_String) {
26563:             if (argc != 1)
26563:                 ABORT_TRACE("can't trace String when not called with a single argument");
26563: 
26552:             jsval& v = stackval(0 - argc);
26563:             if (constructing)
26563:                 return newString(JSVAL_TO_OBJECT(fval), v, &fval);
26552:             if (!JSVAL_IS_PRIMITIVE(v))
26552:                 return call_imacro(call_imacros.String);
26552:             set(&fval, stringify(v));
26552:             return true;
26552:         }
26552:     }
26552: 
26552:     return callNative(fun, argc, constructing);
20431: }
20431: 
22652: JS_REQUIRES_STACK bool
20431: TraceRecorder::record_JSOP_NEW()
20431: {
22634:     return functionCall(true, GET_ARGC(cx->fp->regs->pc));
18300: }
18300: 
22652: JS_REQUIRES_STACK bool
18300: TraceRecorder::record_JSOP_DELNAME()
18300: {
18300:     return false;
18300: }
18300: 
22652: JS_REQUIRES_STACK bool
18300: TraceRecorder::record_JSOP_DELPROP()
18300: {
18300:     return false;
18300: }
18300: 
22652: JS_REQUIRES_STACK bool
18300: TraceRecorder::record_JSOP_DELELEM()
18300: {
18300:     return false;
18300: }
18300: 
22652: JS_REQUIRES_STACK bool
18300: TraceRecorder::record_JSOP_TYPEOF()
18300: {
18300:     jsval& r = stackval(-1);
18300:     LIns* type;
18300:     if (JSVAL_IS_STRING(r)) {
18663:         type = INS_CONSTPTR(ATOM_TO_STRING(cx->runtime->atomState.typeAtoms[JSTYPE_STRING]));
18300:     } else if (isNumber(r)) {
18663:         type = INS_CONSTPTR(ATOM_TO_STRING(cx->runtime->atomState.typeAtoms[JSTYPE_NUMBER]));
18300:     } else {
18300:         LIns* args[] = { get(&r), cx_ins };
18300:         if (JSVAL_TAG(r) == JSVAL_BOOLEAN) {
18300:             // We specialize identically for boolean and undefined. We must not have a hole here.
18300:             // Pass the unboxed type here, since TypeOfBoolean knows how to handle it.
24846:             JS_ASSERT(r == JSVAL_TRUE || r == JSVAL_FALSE || r == JSVAL_VOID);
20915:             type = lir->insCall(&js_TypeOfBoolean_ci, args);
18300:         } else {
23075:             JS_ASSERT(JSVAL_TAG(r) == JSVAL_OBJECT);
20915:             type = lir->insCall(&js_TypeOfObject_ci, args);
18300:         }
18300:     }
18300:     set(&r, type);
18300:     return true;
18300: }
18300: 
22652: JS_REQUIRES_STACK bool
18300: TraceRecorder::record_JSOP_VOID()
18300: {
24846:     stack(-1, INS_CONST(JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID)));
18300:     return true;
18300: }
18300: 
22652: JS_REQUIRES_STACK bool
18300: TraceRecorder::record_JSOP_INCNAME()
18300: {
18300:     return incName(1);
18300: }
18300: 
22652: JS_REQUIRES_STACK bool
18300: TraceRecorder::record_JSOP_INCPROP()
18300: {
18300:     return incProp(1);
18300: }
18300: 
22652: JS_REQUIRES_STACK bool
18300: TraceRecorder::record_JSOP_INCELEM()
18300: {
18300:     return incElem(1);
18300: }
18300: 
22652: JS_REQUIRES_STACK bool
18300: TraceRecorder::record_JSOP_DECNAME()
18300: {
18300:     return incName(-1);
18300: }
18300: 
22652: JS_REQUIRES_STACK bool
18300: TraceRecorder::record_JSOP_DECPROP()
18300: {
18300:     return incProp(-1);
18300: }
18300: 
22652: JS_REQUIRES_STACK bool
18300: TraceRecorder::record_JSOP_DECELEM()
18300: {
18300:     return incElem(-1);
18300: }
18300: 
22652: JS_REQUIRES_STACK bool
18300: TraceRecorder::incName(jsint incr, bool pre)
18300: {
18300:     jsval* vp;
18300:     if (!name(vp))
18300:         return false;
18300:     LIns* v_ins = get(vp);
18300:     if (!inc(*vp, v_ins, incr, pre))
18300:         return false;
18300:     set(vp, v_ins);
18300:     return true;
18300: }
18300: 
22652: JS_REQUIRES_STACK bool
18300: TraceRecorder::record_JSOP_NAMEINC()
18300: {
18300:     return incName(1, false);
18300: }
18300: 
22652: JS_REQUIRES_STACK bool
18300: TraceRecorder::record_JSOP_PROPINC()
18300: {
18300:     return incProp(1, false);
18300: }
18300: 
18300: // XXX consolidate with record_JSOP_GETELEM code...
22652: JS_REQUIRES_STACK bool
18300: TraceRecorder::record_JSOP_ELEMINC()
18300: {
18300:     return incElem(1, false);
18300: }
18300: 
22652: JS_REQUIRES_STACK bool
18300: TraceRecorder::record_JSOP_NAMEDEC()
18300: {
21805:     return incName(-1, false);
18300: }
18300: 
22652: JS_REQUIRES_STACK bool
18300: TraceRecorder::record_JSOP_PROPDEC()
18300: {
18300:     return incProp(-1, false);
18300: }
18300: 
22652: JS_REQUIRES_STACK bool
18300: TraceRecorder::record_JSOP_ELEMDEC()
18300: {
18300:     return incElem(-1, false);
18300: }
18300: 
22652: JS_REQUIRES_STACK bool
18300: TraceRecorder::record_JSOP_GETPROP()
18300: {
18300:     return getProp(stackval(-1));
18300: }
18300: 
22652: JS_REQUIRES_STACK bool
18300: TraceRecorder::record_JSOP_SETPROP()
18300: {
19093:     jsval& l = stackval(-2);
19093:     if (JSVAL_IS_PRIMITIVE(l))
19093:         ABORT_TRACE("primitive this for SETPROP");
19093: 
19093:     JSObject* obj = JSVAL_TO_OBJECT(l);
19093:     if (obj->map->ops->setProperty != js_SetProperty)
19093:         ABORT_TRACE("non-native JSObjectOps::setProperty");
19093:     return true;
19093: }
19093: 
22652: JS_REQUIRES_STACK bool
19171: TraceRecorder::record_SetPropHit(JSPropCacheEntry* entry, JSScopeProperty* sprop)
19093: {
19093:     jsbytecode* pc = cx->fp->regs->pc;
18300:     jsval& r = stackval(-1);
18300:     jsval& l = stackval(-2);
18300: 
19093:     JS_ASSERT(!JSVAL_IS_PRIMITIVE(l));
18300:     JSObject* obj = JSVAL_TO_OBJECT(l);
18300:     LIns* obj_ins = get(&l);
18300: 
25938:     if (!isValidSlot(OBJ_SCOPE(obj), sprop))
25938:         return false;
25938: 
19093:     if (obj == globalObj) {
19093:         JS_ASSERT(SPROP_HAS_VALID_SLOT(sprop, OBJ_SCOPE(obj)));
19093:         uint32 slot = sprop->slot;
19093:         if (!lazilyImportGlobalSlot(slot))
19093:             ABORT_TRACE("lazy import of global slot failed");
19093: 
19093:         LIns* r_ins = get(&r);
25879: 
25879:         if (JSVAL_IS_OBJECT(r)) {
25879:             /*
25933:              * Writing a function into the global object might rebrand it. We don't trace
25933:              * that case.
25879:              */
25933:             if (VALUE_IS_FUNCTION(cx, r))
25933:                 ABORT_TRACE("potential rebranding of the global object");
25933: 
25879:             /*
25879:              * If a regular object was written, we have to guard that it's not a function
25879:              * at execution time either. FIXME: We should split function and object into
25879:              * separate types when on trace (bug 481273).
25879:              */
25933:             guardClass(obj, obj_ins, &js_FunctionClass, snapshot(MISMATCH_EXIT));
19093:             set(&STOBJ_GET_SLOT(obj, slot), r_ins);
25879:         } else {
25879:             set(&STOBJ_GET_SLOT(obj, slot), r_ins);
25879:         }
19093: 
19093:         JS_ASSERT(*pc != JSOP_INITPROP);
19093:         if (pc[JSOP_SETPROP_LENGTH] != JSOP_POP)
19093:             set(&l, r_ins);
19093:         return true;
19093:     }
19093: 
19093:     // The global object's shape is guarded at trace entry, all others need a guard here.
18300:     LIns* map_ins = lir->insLoad(LIR_ldp, obj_ins, (int)offsetof(JSObject, map));
18300:     LIns* ops_ins;
18300:     if (!map_is_native(obj->map, map_ins, ops_ins, offsetof(JSObjectOps, setProperty)))
25636:         ABORT_TRACE("non-native map");
18300: 
18300:     LIns* shape_ins = addName(lir->insLoad(LIR_ld, map_ins, offsetof(JSScope, shape)), "shape");
19171:     guard(true, addName(lir->ins2i(LIR_eq, shape_ins, entry->kshape), "guard(shape)"),
23721:           BRANCH_EXIT);
19171: 
19171:     if (entry->kshape != PCVCAP_SHAPE(entry->vcap)) {
18712:         LIns* args[] = { INS_CONSTPTR(sprop), obj_ins, cx_ins };
20915:         LIns* ok_ins = lir->insCall(&js_AddProperty_ci, args);
19171:         guard(false, lir->ins_eq0(ok_ins), OOM_EXIT);
18300:     }
18300: 
18300:     LIns* dslots_ins = NULL;
18300:     LIns* v_ins = get(&r);
18300:     LIns* boxed_ins = v_ins;
23708:     box_jsval(r, boxed_ins);
18300:     if (!native_set(obj_ins, sprop, dslots_ins, boxed_ins))
18300:         return false;
19093: 
19093:     if (*pc != JSOP_INITPROP && pc[JSOP_SETPROP_LENGTH] != JSOP_POP)
19093:         set(&l, v_ins);
19093:     return true;
19093: }
19093: 
22652: JS_REQUIRES_STACK bool
19093: TraceRecorder::record_SetPropMiss(JSPropCacheEntry* entry)
19093: {
19171:     if (entry->kpc != cx->fp->regs->pc || !PCVAL_IS_SPROP(entry->vword))
19148:         ABORT_TRACE("can't trace uncacheable property set");
19148: 
19171:     JSScopeProperty* sprop = PCVAL_TO_SPROP(entry->vword);
19171: 
19171: #ifdef DEBUG
19171:     jsval& l = stackval(-2);
19171:     JSObject* obj = JSVAL_TO_OBJECT(l);
19171:     JSScope* scope = OBJ_SCOPE(obj);
19171:     JS_ASSERT(scope->object == obj);
19171:     JS_ASSERT(scope->shape == PCVCAP_SHAPE(entry->vcap));
19171:     JS_ASSERT(SCOPE_HAS_PROPERTY(scope, sprop));
19171: #endif
19171: 
19171:     return record_SetPropHit(entry, sprop);
18300: }
18300: 
24489: /* Functions used by JSOP_GETELEM. */
24489: 
24489: static JSBool
24489: GetProperty(JSContext *cx, uintN argc, jsval *vp)
24489: {
24489:     jsval *argv;
24489:     jsid id;
24489: 
25213:     JS_ASSERT_NOT_ON_TRACE(cx);
25213:     JS_ASSERT(cx->fp->imacpc && argc == 1);
24489:     argv = JS_ARGV(cx, vp);
24489:     JS_ASSERT(JSVAL_IS_STRING(argv[0]));
24489:     if (!js_ValueToStringId(cx, argv[0], &id))
24489:         return JS_FALSE;
24489:     argv[0] = ID_TO_VALUE(id);
24489:     return OBJ_GET_PROPERTY(cx, JS_THIS_OBJECT(cx, vp), id, &JS_RVAL(cx, vp));
24489: }
24489: 
24489: static jsval FASTCALL
24598: GetProperty_tn(JSContext *cx, jsbytecode *pc, JSObject *obj, JSString *name)
24489: {
25094:     JSAutoTempIdRooter idr(cx);
25094:     JSAutoTempValueRooter tvr(cx);
24489: 
25094:     if (!js_ValueToStringId(cx, STRING_TO_JSVAL(name), idr.addr()) ||
25094:         !OBJ_GET_PROPERTY(cx, obj, idr.id(), tvr.addr())) {
24612:         cx->builtinStatus |= JSBUILTIN_ERROR;
25094:         *tvr.addr() = JSVAL_ERROR_COOKIE;
24600:     }
25094:     return tvr.value();
24489: }
24489: 
24489: static JSBool
24489: GetElement(JSContext *cx, uintN argc, jsval *vp)
24489: {
24489:     jsval *argv;
24489:     jsid id;
24489: 
25213:     JS_ASSERT_NOT_ON_TRACE(cx);
25213:     JS_ASSERT(cx->fp->imacpc && argc == 1);
24489:     argv = JS_ARGV(cx, vp);
24489:     JS_ASSERT(JSVAL_IS_NUMBER(argv[0]));
24489:     if (!JS_ValueToId(cx, argv[0], &id))
24489:         return JS_FALSE;
24489:     argv[0] = ID_TO_VALUE(id);
24489:     return OBJ_GET_PROPERTY(cx, JS_THIS_OBJECT(cx, vp), id, &JS_RVAL(cx, vp));
24489: }
24489: 
24489: static jsval FASTCALL
24598: GetElement_tn(JSContext* cx, jsbytecode *pc, JSObject* obj, int32 index)
24489: {
25094:     JSAutoTempValueRooter tvr(cx);
25094:     JSAutoTempIdRooter idr(cx);
25094: 
25094:     if (!js_Int32ToId(cx, index, idr.addr())) {
24612:         cx->builtinStatus |= JSBUILTIN_ERROR;
24489:         return JSVAL_ERROR_COOKIE;
24612:     }
25094:     if (!OBJ_GET_PROPERTY(cx, obj, idr.id(), tvr.addr())) {
24612:         cx->builtinStatus |= JSBUILTIN_ERROR;
25094:         *tvr.addr() = JSVAL_ERROR_COOKIE;
24612:     }
25094:     return tvr.value();
24489: }
24489: 
24489: JS_DEFINE_TRCINFO_1(GetProperty,
24598:     (4, (static, JSVAL_FAIL,    GetProperty_tn, CONTEXT, PC, THIS, STRING,      0, 0)))
24489: JS_DEFINE_TRCINFO_1(GetElement,
24598:     (4, (extern, JSVAL_FAIL,    GetElement_tn,  CONTEXT, PC, THIS, INT32,       0, 0)))
24489: 
22652: JS_REQUIRES_STACK bool
18300: TraceRecorder::record_JSOP_GETELEM()
18300: {
26552:     bool call = *cx->fp->regs->pc == JSOP_CALLELEM;
26551: 
19979:     jsval& idx = stackval(-1);
19983:     jsval& lval = stackval(-2);
19983: 
19983:     LIns* obj_ins = get(&lval);
19979:     LIns* idx_ins = get(&idx);
19979: 
26274:     // Special case for array-like access of strings.
26274:     if (JSVAL_IS_STRING(lval) && isInt32(idx)) {
26551:         if (call)
26551:             ABORT_TRACE("JSOP_CALLELEM on a string");
26274:         int i = asInt32(idx);
26274:         if (size_t(i) >= JSSTRING_LENGTH(JSVAL_TO_STRING(lval)))
18692:             ABORT_TRACE("Invalid string index in JSOP_GETELEM");
19979:         idx_ins = makeNumberInt32(idx_ins);
19979:         LIns* args[] = { idx_ins, obj_ins, cx_ins };
20915:         LIns* unitstr_ins = lir->insCall(&js_String_getelem_ci, args);
18300:         guard(false, lir->ins_eq0(unitstr_ins), MISMATCH_EXIT);
19983:         set(&lval, unitstr_ins);
19983:         return true;
19983:     }
19983: 
19983:     if (JSVAL_IS_PRIMITIVE(lval))
19979:         ABORT_TRACE("JSOP_GETLEM on a primitive");
19979: 
19983:     JSObject* obj = JSVAL_TO_OBJECT(lval);
19979:     jsval id;
19979:     LIns* v_ins;
19979: 
26274:     /* Property access using a string name or something we have to stringify. */
26274:     if (!JSVAL_IS_INT(idx)) {
26374:         if (!JSVAL_IS_PRIMITIVE(idx))
26374:             ABORT_TRACE("non-primitive index");
26274:         // If index is not a string, turn it into a string.
26274:         if (!js_InternNonIntElementId(cx, obj, idx, &id))
26274:             ABORT_TRACE("failed to intern non-int element id");
26274:         set(&idx, stringify(idx));
26274: 
19983:         // Store the interned string to the stack to save the interpreter from redoing this work.
19983:         idx = ID_TO_VALUE(id);
26274: 
26274:         // The object is not guaranteed to be a dense array at this point, so it might be the
26274:         // global object, which we have to guard against.
26274:         if (!guardNotGlobalObject(obj, obj_ins))
26274:             return false;
26274: 
26551:         return call_imacro(call ? callelem_imacros.callprop : getelem_imacros.getprop);
19979:     }
19979: 
26274:     // Invalid dense array index or not a dense array.
26274:     if (JSVAL_TO_INT(idx) < 0 || !OBJ_IS_DENSE_ARRAY(cx, obj)) {
26274:         if (!guardNotGlobalObject(obj, obj_ins))
26274:             return false;
26274: 
26551:         return call_imacro(call ? callelem_imacros.callelem : getelem_imacros.getelem);
18300:     }
18300: 
26274:     // Fast path for dense arrays accessed with a non-negative integer index.
18300:     jsval* vp;
18300:     LIns* addr_ins;
19983:     if (!elem(lval, idx, vp, v_ins, addr_ins))
19983:         return false;
19983:     set(&lval, v_ins);
26551:     if (call)
26551:         set(&idx, obj_ins);
18300:     return true;
18300: }
18300: 
24489: /* Functions used by JSOP_SETELEM */
24489: 
24489: static JSBool
24489: SetProperty(JSContext *cx, uintN argc, jsval *vp)
24489: {
24489:     jsval *argv;
24489:     jsid id;
24489: 
24489:     JS_ASSERT(argc == 2);
24489:     argv = JS_ARGV(cx, vp);
24489:     JS_ASSERT(JSVAL_IS_STRING(argv[0]));
24489:     if (!js_ValueToStringId(cx, argv[0], &id))
24489:         return JS_FALSE;
24489:     argv[0] = ID_TO_VALUE(id);
24489:     if (!OBJ_SET_PROPERTY(cx, JS_THIS_OBJECT(cx, vp), id, &argv[1]))
24489:         return JS_FALSE;
24489:     JS_SET_RVAL(cx, vp, JSVAL_VOID);
24489:     return JS_TRUE;
24489: }
24489: 
26395: static JSBool FASTCALL
24489: SetProperty_tn(JSContext* cx, JSObject* obj, JSString* idstr, jsval v)
24489: {
25094:     JSAutoTempValueRooter tvr(cx, v);
25094:     JSAutoTempIdRooter idr(cx);
25094: 
25094:     if (!js_ValueToStringId(cx, STRING_TO_JSVAL(idstr), idr.addr()) ||
25094:         !OBJ_SET_PROPERTY(cx, obj, idr.id(), tvr.addr())) {
24612:         cx->builtinStatus |= JSBUILTIN_ERROR;
24612:     }
24846:     return JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID);
24489: }
24489: 
24489: static JSBool
24489: SetElement(JSContext *cx, uintN argc, jsval *vp)
24489: {
24489:     jsval *argv;
24489:     jsid id;
24489: 
24489:     JS_ASSERT(argc == 2);
24489:     argv = JS_ARGV(cx, vp);
24489:     JS_ASSERT(JSVAL_IS_NUMBER(argv[0]));
24489:     if (!JS_ValueToId(cx, argv[0], &id))
24489:         return JS_FALSE;
24489:     argv[0] = ID_TO_VALUE(id);
24489:     if (!OBJ_SET_PROPERTY(cx, JS_THIS_OBJECT(cx, vp), id, &argv[1]))
24489:         return JS_FALSE;
24489:     JS_SET_RVAL(cx, vp, JSVAL_VOID);
24489:     return JS_TRUE;
24489: }
24489: 
26395: static JSBool FASTCALL
24489: SetElement_tn(JSContext* cx, JSObject* obj, int32 index, jsval v)
24489: {
25094:     JSAutoTempIdRooter idr(cx);
25094:     JSAutoTempValueRooter tvr(cx, v);
25094: 
25094:     if (!js_Int32ToId(cx, index, idr.addr()) ||
25094:         !OBJ_SET_PROPERTY(cx, obj, idr.id(), tvr.addr())) {
24612:         cx->builtinStatus |= JSBUILTIN_ERROR;
25094:     }
24846:     return JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID);
24489: }
24489: 
24489: JS_DEFINE_TRCINFO_1(SetProperty,
24489:     (4, (extern, BOOL_FAIL,     SetProperty_tn, CONTEXT, THIS, STRING, JSVAL,   0, 0)))
24489: JS_DEFINE_TRCINFO_1(SetElement,
24489:     (4, (extern, BOOL_FAIL,     SetElement_tn,  CONTEXT, THIS, INT32, JSVAL,    0, 0)))
24489: 
22652: JS_REQUIRES_STACK bool
18300: TraceRecorder::record_JSOP_SETELEM()
18300: {
18300:     jsval& v = stackval(-1);
19979:     jsval& idx = stackval(-2);
19983:     jsval& lval = stackval(-3);
18300: 
18300:     /* no guards for type checks, trace specialized this already */
19983:     if (JSVAL_IS_PRIMITIVE(lval))
18300:         ABORT_TRACE("left JSOP_SETELEM operand is not an object");
19979: 
19983:     JSObject* obj = JSVAL_TO_OBJECT(lval);
19983:     LIns* obj_ins = get(&lval);
19979:     LIns* idx_ins = get(&idx);
18300:     LIns* v_ins = get(&v);
20000:     jsid id;
19979: 
26274:     if (!JSVAL_IS_INT(idx)) {
26274:         // If index is not a string, turn it into a string.
26274:         if (!js_InternNonIntElementId(cx, obj, idx, &id))
26274:             ABORT_TRACE("failed to intern non-int element id");
26274:         set(&idx, stringify(idx));
26274: 
26274:         // Store the interned string to the stack to save the interpreter from redoing this work.
26274:         idx = ID_TO_VALUE(id);
26274: 
26274:         // The object is not guaranteed to be a dense array at this point, so it might be the
26274:         // global object, which we have to guard against.
26274:         if (!guardNotGlobalObject(obj, obj_ins))
26274:             return false;
26274: 
26376:         return call_imacro((*cx->fp->regs->pc == JSOP_INITELEM)
26376:                            ? initelem_imacros.initprop
26376:                            : setelem_imacros.setprop);
26274:     }
26274: 
26274:     if (JSVAL_TO_INT(idx) < 0 || !OBJ_IS_DENSE_ARRAY(cx, obj)) {
26274:         if (!guardNotGlobalObject(obj, obj_ins))
26274:             return false;
26274: 
26274:         return call_imacro((*cx->fp->regs->pc == JSOP_INITELEM)
26274:                            ? initelem_imacros.initelem
26274:                            : setelem_imacros.setelem);
26274:     }
26274: 
26274:     // Make sure the array is actually dense.
26274:     if (!guardDenseArray(obj, obj_ins, BRANCH_EXIT))
26274:         return false;
26274: 
26274:     // Fast path for dense arrays accessed with a non-negative integer index. In case the trace
26274:     // calculated the index using the FPU, force it to be an integer.
26274:     idx_ins = makeNumberInt32(idx_ins);
26274: 
26274:     // Box the value so we can use one builtin instead of having to add one builtin for every
26274:     // storage type.
26012:     LIns* boxed_v_ins = v_ins;
26012:     box_jsval(v, boxed_v_ins);
26012: 
24489:     LIns* args[] = { boxed_v_ins, idx_ins, obj_ins, cx_ins };
24489:     LIns* res_ins = lir->insCall(&js_Array_dense_setelem_ci, args);
19993:     guard(false, lir->ins_eq0(res_ins), MISMATCH_EXIT);
18300: 
18300:     jsbytecode* pc = cx->fp->regs->pc;
18300:     if (*pc == JSOP_SETELEM && pc[JSOP_SETELEM_LENGTH] != JSOP_POP)
19983:         set(&lval, v_ins);
19993: 
18300:     return true;
18300: }
18300: 
22652: JS_REQUIRES_STACK bool
18300: TraceRecorder::record_JSOP_CALLNAME()
18300: {
18300:     JSObject* obj = cx->fp->scopeChain;
18300:     if (obj != globalObj) {
18300:         jsval* vp;
18300:         if (!activeCallOrGlobalSlot(obj, vp))
18300:             return false;
18300:         stack(0, get(vp));
24282:         stack(1, globalObj_ins);
18300:         return true;
18300:     }
18300: 
18300:     LIns* obj_ins = scopeChain();
18300:     JSObject* obj2;
18300:     jsuword pcval;
25633: 
18300:     if (!test_property_cache(obj, obj_ins, obj2, pcval))
18300:         return false;
18300: 
18300:     if (PCVAL_IS_NULL(pcval) || !PCVAL_IS_OBJECT(pcval))
18300:         ABORT_TRACE("callee is not an object");
25633: 
18300:     JS_ASSERT(HAS_FUNCTION_CLASS(PCVAL_TO_OBJECT(pcval)));
18300: 
18712:     stack(0, INS_CONSTPTR(PCVAL_TO_OBJECT(pcval)));
18300:     stack(1, obj_ins);
18300:     return true;
18300: }
18300: 
22652: JS_REQUIRES_STACK bool
18308: TraceRecorder::record_JSOP_GETUPVAR()
18308: {
23075:     return false;
18308: }
18308: 
22652: JS_REQUIRES_STACK bool
18308: TraceRecorder::record_JSOP_CALLUPVAR()
18308: {
23075:     return false;
18308: }
18308: 
22652: JS_REQUIRES_STACK bool
22634: TraceRecorder::guardCallee(jsval& callee)
21526: {
23712:     JS_ASSERT(VALUE_IS_FUNCTION(cx, callee));
23712: 
22623:     LIns* exit = snapshot(BRANCH_EXIT);
22623:     JSObject* callee_obj = JSVAL_TO_OBJECT(callee);
22623:     LIns* callee_ins = get(&callee);
23074: 
23074:     /*
23074:      * NB: The following guard guards at runtime that the callee is a
23074:      * function. Even if the given value is an object that doesn't have
23074:      * a private slot, the value we're matching against is not forgeable.
23074:      */
18300:     guard(true,
22623:           lir->ins2(LIR_eq,
22623:                     lir->ins2(LIR_piand,
22623:                               stobj_get_fslot(callee_ins, JSSLOT_PRIVATE),
22623:                               INS_CONSTPTR((void*)(~JSVAL_INT))),
22623:                     INS_CONSTPTR(OBJ_GET_PRIVATE(cx, callee_obj))),
22623:           exit);
22623:     guard(true,
22623:           lir->ins2(LIR_eq,
22623:                     stobj_get_fslot(callee_ins, JSSLOT_PARENT),
22623:                     INS_CONSTPTR(OBJ_GET_PARENT(cx, callee_obj))),
22623:           exit);
18300:     return true;
18300: }
18300: 
22652: JS_REQUIRES_STACK bool
19577: TraceRecorder::interpretedFunctionCall(jsval& fval, JSFunction* fun, uintN argc, bool constructing)
18300: {
19149:     if (JS_GetGlobalForObject(cx, JSVAL_TO_OBJECT(fval)) != globalObj)
19149:         ABORT_TRACE("JSOP_CALL or JSOP_NEW crosses global scopes");
19149: 
18300:     JSStackFrame* fp = cx->fp;
18300: 
18300:     // TODO: track the copying via the tracker...
18300:     if (argc < fun->nargs &&
18300:         jsuword(fp->regs->sp + (fun->nargs - argc)) > cx->stackPool.current->limit) {
18300:         ABORT_TRACE("can't trace calls with too few args requiring argv move");
18300:     }
18300: 
19085:     // Generate a type map for the outgoing frame and stash it in the LIR
19085:     unsigned stackSlots = js_NativeStackSlots(cx, 0/*callDepth*/);
23918:     if (sizeof(FrameInfo) + stackSlots * sizeof(uint8) > MAX_SKIP_BYTES)
23918:         ABORT_TRACE("interpreted function call requires saving too much stack");
24850:     LIns* data = lir->skip(sizeof(FrameInfo) + stackSlots * sizeof(uint8));
23262:     FrameInfo* fi = (FrameInfo*)data->payload();
23262:     uint8* typemap = (uint8 *)(fi + 1);
19085:     uint8* m = typemap;
19085:     /* Determine the type of a store by looking at the current type of the actual value the
19085:        interpreter is using. For numbers we have to check what kind of store we used last
19085:        (integer or double) to figure out what the side exit show reflect in its typemap. */
19085:     FORALL_SLOTS_IN_PENDING_FRAMES(cx, 0/*callDepth*/,
19085:         *m++ = determineSlotType(vp);
19085:     );
19085: 
19577:     if (argc >= 0x8000)
19577:         ABORT_TRACE("too many arguments");
19577: 
23262:     fi->callee = JSVAL_TO_OBJECT(fval);
23262:     fi->block = fp->blockChain;
25111:     fi->pc = fp->regs->pc;
25111:     fi->imacpc = fp->imacpc;
23262:     fi->s.spdist = fp->regs->sp - fp->slots;
23262:     fi->s.argc = argc | (constructing ? 0x8000 : 0);
18300: 
18300:     unsigned callDepth = getCallDepth();
18300:     if (callDepth >= treeInfo->maxCallDepth)
18300:         treeInfo->maxCallDepth = callDepth + 1;
18300: 
23262:     lir->insStorei(INS_CONSTPTR(fi), lirbuf->rp, callDepth * sizeof(FrameInfo*));
18300: 
18300:     atoms = fun->u.i.script->atomMap.vector;
18300:     return true;
18300: }
18300: 
22652: JS_REQUIRES_STACK bool
18300: TraceRecorder::record_JSOP_CALL()
18300: {
22634:     return functionCall(false, GET_ARGC(cx->fp->regs->pc));
19582: }
19582: 
23097: static jsbytecode* apply_imacro_table[] = {
23097:     apply_imacros.apply0,
23097:     apply_imacros.apply1,
23097:     apply_imacros.apply2,
23097:     apply_imacros.apply3,
23097:     apply_imacros.apply4,
23097:     apply_imacros.apply5,
23097:     apply_imacros.apply6,
23097:     apply_imacros.apply7,
23097:     apply_imacros.apply8
23097: };
23097: 
23097: static jsbytecode* call_imacro_table[] = {
23097:     apply_imacros.call0,
23097:     apply_imacros.call1,
23097:     apply_imacros.call2,
23097:     apply_imacros.call3,
23097:     apply_imacros.call4,
23097:     apply_imacros.call5,
23097:     apply_imacros.call6,
23097:     apply_imacros.call7,
23097:     apply_imacros.call8
23097: };
23097: 
22652: JS_REQUIRES_STACK bool
21452: TraceRecorder::record_JSOP_APPLY()
21452: {
22634:     JSStackFrame* fp = cx->fp;
22634:     jsbytecode *pc = fp->regs->pc;
22634:     uintN argc = GET_ARGC(pc);
22634:     jsval* vp = fp->regs->sp - (argc + 2);
22634:     JS_ASSERT(vp >= StackBase(fp));
22634:     jsuint length = 0;
22634:     JSObject* aobj = NULL;
22634:     LIns* aobj_ins = NULL;
23097: 
23097:     JS_ASSERT(!fp->imacpc);
22634: 
22634:     if (!VALUE_IS_FUNCTION(cx, vp[0]))
22634:         return record_JSOP_CALL();
22634: 
22634:     JSObject* obj = JSVAL_TO_OBJECT(vp[0]);
22634:     JSFunction* fun = GET_FUNCTION_PRIVATE(cx, obj);
22634:     if (FUN_INTERPRETED(fun))
22634:         return record_JSOP_CALL();
22634: 
22634:     bool apply = (JSFastNative)fun->u.n.native == js_fun_apply;
22634:     if (!apply && (JSFastNative)fun->u.n.native != js_fun_call)
22634:         return record_JSOP_CALL();
22634: 
22634:     /*
23097:      * We don't trace apply and call with a primitive 'this', which is the
23097:      * first positional parameter.
22634:      */
23097:     if (argc > 0 && JSVAL_IS_PRIMITIVE(vp[2]))
23097:         return record_JSOP_CALL();
23097: 
23097:     /*
23097:      * Guard on the identity of this, which is the function we are applying.
23097:      */
23712:     if (!VALUE_IS_FUNCTION(cx, vp[1]))
23712:         ABORT_TRACE("callee is not a function");
23097:     if (!guardCallee(vp[1]))
23097:         return false;
23097: 
22634:     if (apply && argc >= 2) {
23097:         if (argc != 2)
23097:             ABORT_TRACE("apply with excess arguments");
22634:         if (JSVAL_IS_PRIMITIVE(vp[3]))
22634:             ABORT_TRACE("arguments parameter of apply is primitive");
22634:         aobj = JSVAL_TO_OBJECT(vp[3]);
22634:         aobj_ins = get(&vp[3]);
22634: 
22634:         /*
22634:          * We expect a dense array for the arguments (the other
22634:          * frequent case is the arguments object, but that we
22634:          * don't trace at the moment).
22634:          */
22634:         if (!guardDenseArray(aobj, aobj_ins))
22634:             ABORT_TRACE("arguments parameter of apply is not a dense array");
22634: 
22634:         /*
23097:          * We trace only apply calls with a certain number of arguments.
23097:          */
23097:         length = jsuint(aobj->fslots[JSSLOT_ARRAY_LENGTH]);
23097:         if (length >= JS_ARRAY_LENGTH(apply_imacro_table))
23097:             ABORT_TRACE("too many arguments to apply");
23097: 
23097:         /*
22634:          * Make sure the array has the same length at runtime.
22634:          */
23097:         guard(true,
23097:               lir->ins2i(LIR_eq,
22634:                          stobj_get_fslot(aobj_ins, JSSLOT_ARRAY_LENGTH),
22634:                          length),
22634:               BRANCH_EXIT);
22634: 
23097:         return call_imacro(apply_imacro_table[length]);
23097:     }
23097: 
23097:     if (argc >= JS_ARRAY_LENGTH(call_imacro_table))
23097:         ABORT_TRACE("too many arguments to call");
23097: 
23097:     return call_imacro(call_imacro_table[argc]);
21452: }
21452: 
22652: JS_REQUIRES_STACK bool
20405: TraceRecorder::record_FastNativeCallComplete()
20405: {
20405:     JS_ASSERT(pendingTraceableNative);
20405: 
26552:     JS_ASSERT(*cx->fp->regs->pc == JSOP_CALL ||
26552:               *cx->fp->regs->pc == JSOP_APPLY);
26552: 
26552:     jsval& v = stackval(-1);
26552:     LIns* v_ins = get(&v);
26552: 
20405:     /* At this point the generated code has already called the native function
20405:        and we can no longer fail back to the original pc location (JSOP_CALL)
20405:        because that would cause the interpreter to re-execute the native
20969:        function, which might have side effects.
20969: 
26286:        Instead, snapshot(), which is invoked from unbox_jsval() below, will see
26286:        that we are currently parked on a traceable native's JSOP_CALL
26286:        instruction, and it will advance the pc to restore by the length of the
26286:        current opcode.  If the native's return type is jsval, snapshot() will
26286:        also indicate in the type map that the element on top of the stack is a
26286:        boxed value which doesn't need to be boxed if the type guard generated
26286:        by unbox_jsval() fails. */
24612: 
24612:     if (JSTN_ERRTYPE(pendingTraceableNative) == FAIL_STATUS) {
24612: #ifdef DEBUG
24612:         // Keep cx->bailExit null when it's invalid.
24612:         lir->insStorei(INS_CONSTPTR(NULL), cx_ins, (int) offsetof(JSContext, bailExit));
24612: #endif
26552:         LIns* status = lir->insLoad(LIR_ld, cx_ins, (int) offsetof(JSContext, builtinStatus));
26552:         if (pendingTraceableNative == generatedTraceableNative) {
26552:             LIns* ok_ins = v_ins;
26552: 
26552:             /*
26552:              * If we run a generic traceable native, the return value is in the argument
26552:              * vector. The actual return value of the fast native is a JSBool indicated
26552:              * the error status.
26552:              */
26552:             v_ins = lir->insLoad(LIR_ld, invokevp_ins, 0);
26552:             set(&v, v_ins);
26552: 
26552:             /*
26552:              * If this is a generic traceable native invocation, propagate the boolean return
26552:              * value of the fast native into builtinStatus. If the return value (v_ins)
26552:              * is true, status' == status. Otherwise status' = status | JSBUILTIN_ERROR.
26552:              * We calculate (rval&1)^1, which is 1 if rval is JS_FALSE (error), and then
26552:              * shift that by 1 which is JSBUILTIN_ERROR.
26552:              */
26552:             JS_STATIC_ASSERT((1 - JS_TRUE) << 1 == 0);
26552:             JS_STATIC_ASSERT((1 - JS_FALSE) << 1 == JSBUILTIN_ERROR);
26552:             status = lir->ins2(LIR_or,
26552:                                status,
26552:                                lir->ins2i(LIR_lsh,
26552:                                           lir->ins2i(LIR_xor,
26552:                                                      lir->ins2i(LIR_and, ok_ins, 1),
26552:                                                      1),
26552:                                           1));
26552:             lir->insStorei(status, cx_ins, (int) offsetof(JSContext, builtinStatus));
26552:         }
24612:         guard(true,
26552:               lir->ins_eq0(status),
26286:               STATUS_EXIT);
24612:     }
24612: 
20405:     bool ok = true;
24612:     if (pendingTraceableNative->flags & JSTN_UNBOX_AFTER) {
26286:         unbox_jsval(v, v_ins);
20405:         set(&v, v_ins);
24612:     } else if (JSTN_ERRTYPE(pendingTraceableNative) == FAIL_NEG) {
20966:         /* Already added i2f in functionCall. */
20966:         JS_ASSERT(JSVAL_IS_NUMBER(v));
24612:     } else {
20966:         /* Convert the result to double if the builtin returns int32. */
20966:         if (JSVAL_IS_NUMBER(v) &&
20966:             (pendingTraceableNative->builtin->_argtypes & 3) == nanojit::ARGSIZE_LO) {
20966:             set(&v, lir->ins1(LIR_i2f, v_ins));
20966:         }
20405:     }
20405: 
23111:     // We'll null pendingTraceableNative in monitorRecording, on the next op cycle.
20969:     // There must be a next op since the stack is non-empty.
20405:     return ok;
20405: }
20405: 
22652: JS_REQUIRES_STACK bool
18096: TraceRecorder::name(jsval*& vp)
18096: {
18096:     JSObject* obj = cx->fp->scopeChain;
18096:     if (obj != globalObj)
18286:         return activeCallOrGlobalSlot(obj, vp);
18115: 
18115:     /* Can't use prop here, because we don't want unboxing from global slots. */
18286:     LIns* obj_ins = scopeChain();
18096:     uint32 slot;
18096:     if (!test_property_cache_direct_slot(obj, obj_ins, slot))
18096:         return false;
18096: 
18096:     if (slot == SPROP_INVALID_SLOT)
25633:         ABORT_TRACE("named property not found");
18096: 
18096:     if (!lazilyImportGlobalSlot(slot))
18096:         ABORT_TRACE("lazy import of global slot failed");
18096: 
18096:     vp = &STOBJ_GET_SLOT(obj, slot);
18096:     return true;
18096: }
18096: 
22652: JS_REQUIRES_STACK bool
17761: TraceRecorder::prop(JSObject* obj, LIns* obj_ins, uint32& slot, LIns*& v_ins)
17758: {
17758:     /*
17758:      * Can't specialize to assert obj != global, must guard to avoid aliasing
17758:      * stale homes of stacked global variables.
17758:      */
26274:     if (!guardNotGlobalObject(obj, obj_ins))
26274:         return false;
17758: 
18115:     /*
18115:      * Property cache ensures that we are dealing with an existing property,
18115:      * and guards the shape for us.
18115:      */
18115:     JSObject* obj2;
18115:     jsuword pcval;
18115:     if (!test_property_cache(obj, obj_ins, obj2, pcval))
17665:         return false;
17665: 
17998:     /* Check for non-existent property reference, which results in undefined. */
17998:     const JSCodeSpec& cs = js_CodeSpec[*cx->fp->regs->pc];
18115:     if (PCVAL_IS_NULL(pcval)) {
25633:         /*
25633:          * This trace will be valid as long as neither the object nor any object
25633:          * on its prototype chain change shape.
25633:          */
25886:         LIns* exit = snapshot(BRANCH_EXIT);
25633:         for (;;) {
25633:             LIns* map_ins = lir->insLoad(LIR_ldp, obj_ins, (int)offsetof(JSObject, map));
25633:             LIns* ops_ins;
25636:             if (map_is_native(obj->map, map_ins, ops_ins)) {
25633:                 LIns* shape_ins = addName(lir->insLoad(LIR_ld, map_ins, offsetof(JSScope, shape)),
25633:                                           "shape");
25633:                 guard(true,
25633:                       addName(lir->ins2i(LIR_eq, shape_ins, OBJ_SHAPE(obj)), "guard(shape)"),
25886:                       exit);
25636:             } else if (!guardDenseArray(obj, obj_ins, BRANCH_EXIT))
25636:                 ABORT_TRACE("non-native object involved in undefined property access");
25633: 
25633:             obj = JSVAL_TO_OBJECT(obj->fslots[JSSLOT_PROTO]);
25633:             if (!obj)
25633:                 break;
25633:             obj_ins = stobj_get_fslot(obj_ins, JSSLOT_PROTO);
25633:         }
25633: 
24846:         v_ins = INS_CONST(JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID));
17998:         JS_ASSERT(cs.ndefs == 1);
17998:         stack(-cs.nuses, v_ins);
18666:         slot = SPROP_INVALID_SLOT;
17998:         return true;
17998:     }
17998: 
18115:     /* Insist if setting on obj being the directly addressed object. */
23085:     uint32 setflags = (cs.format & (JOF_SET | JOF_INCDEC | JOF_FOR));
18143:     LIns* dslots_ins = NULL;
18143: 
18115:     /* Don't trace getter or setter calls, our caller wants a direct slot. */
18115:     if (PCVAL_IS_SPROP(pcval)) {
18115:         JSScopeProperty* sprop = PCVAL_TO_SPROP(pcval);
18115: 
18115:         if (setflags && !SPROP_HAS_STUB_SETTER(sprop))
18115:             ABORT_TRACE("non-stub setter");
23085:         if (setflags && (sprop->attrs & JSPROP_READONLY))
23085:             ABORT_TRACE("writing to a readonly property");
18115:         if (setflags != JOF_SET && !SPROP_HAS_STUB_GETTER(sprop)) {
18115:             // FIXME 450335: generalize this away from regexp built-in getters.
18115:             if (setflags == 0 &&
18115:                 sprop->getter == js_RegExpClass.getProperty &&
18115:                 sprop->shortid < 0) {
19986:                 if (sprop->shortid == REGEXP_LAST_INDEX)
19986:                     ABORT_TRACE("can't trace regexp.lastIndex yet");
18115:                 LIns* args[] = { INS_CONSTPTR(sprop), obj_ins, cx_ins };
20915:                 v_ins = lir->insCall(&js_CallGetter_ci, args);
18766:                 guard(false, lir->ins2(LIR_eq, v_ins, INS_CONST(JSVAL_ERROR_COOKIE)), OOM_EXIT);
23710:                 unbox_jsval((sprop->shortid == REGEXP_SOURCE) ? JSVAL_STRING : JSVAL_BOOLEAN,
26286:                              v_ins);
18115:                 JS_ASSERT(cs.ndefs == 1);
18115:                 stack(-cs.nuses, v_ins);
18115:                 return true;
18115:             }
18115:             ABORT_TRACE("non-stub getter");
18115:         }
18115:         if (!SPROP_HAS_VALID_SLOT(sprop, OBJ_SCOPE(obj)))
18115:             ABORT_TRACE("no valid slot");
18115:         slot = sprop->slot;
18115:     } else {
18115:         if (!PCVAL_IS_SLOT(pcval))
18115:             ABORT_TRACE("PCE is not a slot");
18115:         slot = PCVAL_TO_SLOT(pcval);
18115:     }
18115: 
25092:     if (obj2 != obj) {
25092:         if (setflags)
25092:             ABORT_TRACE("JOF_SET opcode hit prototype chain");
25092: 
25092:         /*
25092:          * We're getting a proto-property. Walk up the prototype chain emitting
25092:          * proto slot loads, updating obj as we go, leaving obj set to obj2 with
25092:          * obj_ins the last proto-load.
25092:          */
25092:         while (obj != obj2) {
25092:             obj_ins = stobj_get_slot(obj_ins, JSSLOT_PROTO, dslots_ins);
25092:             obj = STOBJ_GET_PROTO(obj);
25092:         }
25092:     }
25092: 
17758:     v_ins = stobj_get_slot(obj_ins, slot, dslots_ins);
26286:     unbox_jsval(STOBJ_GET_SLOT(obj, slot), v_ins);
17758:     return true;
17758: }
17758: 
22652: JS_REQUIRES_STACK bool
19983: TraceRecorder::elem(jsval& oval, jsval& idx, jsval*& vp, LIns*& v_ins, LIns*& addr_ins)
17758: {
17758:     /* no guards for type checks, trace specialized this already */
19983:     if (JSVAL_IS_PRIMITIVE(oval) || !JSVAL_IS_INT(idx))
19983:         return false;
19983: 
19983:     JSObject* obj = JSVAL_TO_OBJECT(oval);
19983:     LIns* obj_ins = get(&oval);
17758: 
17758:     /* make sure the object is actually a dense array */
17899:     if (!guardDenseArray(obj, obj_ins))
17758:         return false;
17758: 
17758:     /* check that the index is within bounds */
19979:     jsint i = JSVAL_TO_INT(idx);
19979:     LIns* idx_ins = makeNumberInt32(get(&idx));
17899: 
18230:     LIns* dslots_ins = lir->insLoad(LIR_ldp, obj_ins, offsetof(JSObject, dslots));
20972:     if (!guardDenseArrayIndex(obj, i, obj_ins, dslots_ins, idx_ins, BRANCH_EXIT)) {
25883:         /*
25883:          * If we read a hole, make sure at recording time and at runtime that nothing along
25883:          * the prototype has numeric properties.
25883:          */
25886:         if (js_PrototypeHasIndexedProperties(cx, obj))
25886:             return false;
25886: 
25886:         LIns* exit = snapshot(BRANCH_EXIT);
25886:         while ((obj = JSVAL_TO_OBJECT(obj->fslots[JSSLOT_PROTO])) != NULL) {
25886:             obj_ins = stobj_get_fslot(obj_ins, JSSLOT_PROTO);
25886:             LIns* map_ins = lir->insLoad(LIR_ldp, obj_ins, (int)offsetof(JSObject, map));
25886:             LIns* ops_ins;
25886:             if (!map_is_native(obj->map, map_ins, ops_ins))
25886:                 ABORT_TRACE("non-native object involved along prototype chain");
25886: 
25886:             LIns* shape_ins = addName(lir->insLoad(LIR_ld, map_ins, offsetof(JSScope, shape)),
25886:                                       "shape");
25886:             guard(true,
25886:                   addName(lir->ins2i(LIR_eq, shape_ins, OBJ_SHAPE(obj)), "guard(shape)"),
25886:                   exit);
25886:         }
25883: 
20972:         // Return undefined and indicate that we didn't actually read this (addr_ins).
24846:         v_ins = lir->insImm(JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID));
20972:         addr_ins = NULL;
20972:         return true;
20972:     }
20404: 
20404:     // We can't "see through" a hole to a possible Array.prototype property, so
20404:     // we abort here and guard below (after unboxing).
19983:     vp = &obj->dslots[i];
20404:     if (*vp == JSVAL_HOLE)
20404:         ABORT_TRACE("can't see through hole in dense array");
17758: 
18232:     addr_ins = lir->ins2(LIR_piadd, dslots_ins,
18232:                          lir->ins2i(LIR_pilsh, idx_ins, (sizeof(jsval) == 4) ? 2 : 3));
17899: 
20404:     /* Load the value and guard on its type to unbox it. */
18230:     v_ins = lir->insLoad(LIR_ldp, addr_ins, 0);
26286:     unbox_jsval(*vp, v_ins);
20404: 
19053:     if (JSVAL_TAG(*vp) == JSVAL_BOOLEAN) {
20404:         // Optimize to guard for a hole only after untagging, so we know that
20404:         // we have a boolean, to avoid an extra guard for non-boolean values.
24846:         guard(false, lir->ins2(LIR_eq, v_ins, INS_CONST(JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_HOLE))),
20404:               MISMATCH_EXIT);
19053:     }
20972:     return true;
17758: }
17758: 
22652: JS_REQUIRES_STACK bool
17758: TraceRecorder::getProp(JSObject* obj, LIns* obj_ins)
17758: {
17761:     uint32 slot;
17758:     LIns* v_ins;
17761:     if (!prop(obj, obj_ins, slot, v_ins))
17758:         return false;
17758: 
17758:     const JSCodeSpec& cs = js_CodeSpec[*cx->fp->regs->pc];
17758:     JS_ASSERT(cs.ndefs == 1);
17758:     stack(-cs.nuses, v_ins);
17665:     return true;
17665: }
17665: 
22652: JS_REQUIRES_STACK bool
17688: TraceRecorder::getProp(jsval& v)
17665: {
17665:     if (JSVAL_IS_PRIMITIVE(v))
17665:         ABORT_TRACE("primitive lhs");
17665: 
17688:     return getProp(JSVAL_TO_OBJECT(v), get(&v));
17665: }
17665: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_NAME()
17409: {
18096:     jsval* vp;
18096:     if (!name(vp))
17908:         return false;
18096:     stack(0, get(vp));
17459:     return true;
17409: }
17541: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_DOUBLE()
17409: {
17611:     jsval v = jsval(atoms[GET_INDEX(cx->fp->regs->pc)]);
26265:     stack(0, lir->insImmf(*JSVAL_TO_DOUBLE(v)));
17453:     return true;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_STRING()
17409: {
17902:     JSAtom* atom = atoms[GET_INDEX(cx->fp->regs->pc)];
17902:     JS_ASSERT(ATOM_IS_STRING(atom));
18712:     stack(0, INS_CONSTPTR(ATOM_TO_STRING(atom)));
17902:     return true;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_ZERO()
17409: {
26265:     stack(0, lir->insImmq(0));
17418:     return true;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_ONE()
17409: {
26265:     stack(0, lir->insImmf(1));
17418:     return true;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_NULL()
17409: {
18712:     stack(0, INS_CONSTPTR(NULL));
17418:     return true;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_THIS()
17409: {
17688:     LIns* this_ins;
17688:     if (!getThis(this_ins))
17688:         return false;
17688:     stack(0, this_ins);
17682:     return true;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_FALSE()
17409: {
17418:     stack(0, lir->insImm(0));
17418:     return true;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_TRUE()
17409: {
17418:     stack(0, lir->insImm(1));
17418:     return true;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_OR()
17409: {
20416:     return ifop();
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_AND()
17409: {
20416:     return ifop();
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_TABLESWITCH()
17409: {
25099: #ifdef NANOJIT_IA32
26557:     /* Handle tableswitches specially -- prepare a jump table if needed. */
26557:     LIns* guardIns = tableswitch();
26557:     if (guardIns) {
26557:         fragment->lastIns = guardIns;
26557:         compile(&JS_TRACE_MONITOR(cx));
26557:     }
26557:     return false;
25099: #else
18687:     return switchop();
25099: #endif
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_LOOKUPSWITCH()
17409: {
18687:     return switchop();
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_STRICTEQ()
17409: {
23093:     strictEquality(true, false);
22705:     return true;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_STRICTNE()
17409: {
23093:     strictEquality(false, false);
22705:     return true;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_OBJECT()
17409: {
18027:     JSStackFrame* fp = cx->fp;
18027:     JSScript* script = fp->script;
18027:     unsigned index = atoms - script->atomMap.vector + GET_INDEX(fp->regs->pc);
18027: 
18027:     JSObject* obj;
18027:     JS_GET_SCRIPT_OBJECT(script, index, obj);
18712:     stack(0, INS_CONSTPTR(obj));
18027:     return true;
17409: }
17899: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_POP()
17409: {
17456:     return true;
17409: }
17899: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_TRAP()
17899: {
17409:     return false;
17409: }
17899: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_GETARG()
17409: {
17412:     stack(0, arg(GET_ARGNO(cx->fp->regs->pc)));
17412:     return true;
17409: }
17899: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_SETARG()
17409: {
17415:     arg(GET_ARGNO(cx->fp->regs->pc), stack(-1));
17415:     return true;
17409: }
17899: 
22652: JS_REQUIRES_STACK bool
18005: TraceRecorder::record_JSOP_GETLOCAL()
17409: {
17807:     stack(0, var(GET_SLOTNO(cx->fp->regs->pc)));
17412:     return true;
17409: }
17899: 
22652: JS_REQUIRES_STACK bool
18005: TraceRecorder::record_JSOP_SETLOCAL()
17409: {
17807:     var(GET_SLOTNO(cx->fp->regs->pc), stack(-1));
17415:     return true;
17409: }
17899: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_UINT16()
17409: {
26265:     stack(0, lir->insImmf(GET_UINT16(cx->fp->regs->pc)));
17412:     return true;
17409: }
17899: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_NEWINIT()
17409: {
18036:     JSProtoKey key = JSProtoKey(GET_INT8(cx->fp->regs->pc));
18712:     JSObject* obj;
20408:     const CallInfo *ci;
26372: 
18712:     if (!js_GetClassPrototype(cx, globalObj, INT_TO_JSID(key), &obj))
17409:         return false;
26372:     ci = (key == JSProto_Array) ? &js_FastNewArray_ci : &js_Object_tn_ci;
18712:     LIns* args[] = { INS_CONSTPTR(obj), cx_ins };
20408:     LIns* v_ins = lir->insCall(ci, args);
18036:     guard(false, lir->ins_eq0(v_ins), OOM_EXIT);
18036:     stack(0, v_ins);
18036:     return true;
17409: }
17899: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_ENDINIT()
17409: {
24198: #ifdef DEBUG
18712:     jsval& v = stackval(-1);
18712:     JS_ASSERT(!JSVAL_IS_PRIMITIVE(v));
24198: #endif
18036:     return true;
17409: }
17899: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_INITPROP()
17409: {
19093:     // All the action is in record_SetPropHit.
19093:     return true;
17409: }
17899: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_INITELEM()
17409: {
18036:     return record_JSOP_SETELEM();
17409: }
17899: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_DEFSHARP()
17409: {
17409:     return false;
17409: }
17899: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_USESHARP()
17409: {
17409:     return false;
17409: }
17899: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_INCARG()
17409: {
17544:     return inc(argval(GET_ARGNO(cx->fp->regs->pc)), 1);
17409: }
17899: 
22652: JS_REQUIRES_STACK bool
18005: TraceRecorder::record_JSOP_INCLOCAL()
17409: {
17807:     return inc(varval(GET_SLOTNO(cx->fp->regs->pc)), 1);
17409: }
17899: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_DECARG()
17409: {
17544:     return inc(argval(GET_ARGNO(cx->fp->regs->pc)), -1);
17409: }
17899: 
22652: JS_REQUIRES_STACK bool
18005: TraceRecorder::record_JSOP_DECLOCAL()
17409: {
17807:     return inc(varval(GET_SLOTNO(cx->fp->regs->pc)), -1);
17409: }
17899: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_ARGINC()
17409: {
17412:     return inc(argval(GET_ARGNO(cx->fp->regs->pc)), 1, false);
17409: }
17899: 
22652: JS_REQUIRES_STACK bool
18005: TraceRecorder::record_JSOP_LOCALINC()
17409: {
17807:     return inc(varval(GET_SLOTNO(cx->fp->regs->pc)), 1, false);
17409: }
17899: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_ARGDEC()
17409: {
17412:     return inc(argval(GET_ARGNO(cx->fp->regs->pc)), -1, false);
17409: }
17899: 
22652: JS_REQUIRES_STACK bool
18005: TraceRecorder::record_JSOP_LOCALDEC()
17409: {
17807:     return inc(varval(GET_SLOTNO(cx->fp->regs->pc)), -1, false);
17409: }
17899: 
22652: JS_REQUIRES_STACK bool
21685: TraceRecorder::record_JSOP_IMACOP()
21685: {
21685:     JS_ASSERT(cx->fp->imacpc);
21685:     return true;
21685: }
21685: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_ITER()
17409: {
17958:     jsval& v = stackval(-1);
24384:     if (JSVAL_IS_PRIMITIVE(v))
24384:         ABORT_TRACE("for-in on a primitive value");
24384: 
18136:     jsuint flags = cx->fp->regs->pc[1];
21685: 
24384:     if (hasIteratorMethod(JSVAL_TO_OBJECT(v))) {
21685:         if (flags == JSITER_ENUMERATE)
21685:             return call_imacro(iter_imacros.for_in);
21686:         if (flags == (JSITER_ENUMERATE | JSITER_FOREACH))
21685:             return call_imacro(iter_imacros.for_each);
24384:     } else {
24384:         if (flags == JSITER_ENUMERATE)
24384:             return call_imacro(iter_imacros.for_in_native);
24384:         if (flags == (JSITER_ENUMERATE | JSITER_FOREACH))
24384:             return call_imacro(iter_imacros.for_each_native);
24384:     }
21685:     ABORT_TRACE("unimplemented JSITER_* flags");
21685: }
21685: 
24384: JS_REQUIRES_STACK bool
24384: TraceRecorder::record_JSOP_NEXTITER()
24384: {
24384:     jsval& iterobj_val = stackval(-2);
24384:     if (JSVAL_IS_PRIMITIVE(iterobj_val))
24310:         ABORT_TRACE("for-in on a primitive value");
24384: 
24310:     LIns* iterobj_ins = get(&iterobj_val);
25879:     if (guardClass(JSVAL_TO_OBJECT(iterobj_val), iterobj_ins, &js_IteratorClass,
25879:                    snapshot(BRANCH_EXIT))) {
24384:         return call_imacro(nextiter_imacros.native_iter_next);
25879:     }
24384:     return call_imacro(nextiter_imacros.custom_iter_next);
21441: }
21441: 
22652: JS_REQUIRES_STACK bool
18085: TraceRecorder::record_JSOP_ENDITER()
18085: {
21441:     LIns* args[] = { stack(-2), cx_ins };
20915:     LIns* ok_ins = lir->insCall(&js_CloseIterator_ci, args);
18085:     guard(false, lir->ins_eq0(ok_ins), MISMATCH_EXIT);
18085:     return true;
18085: }
18085: 
22652: JS_REQUIRES_STACK bool
18085: TraceRecorder::record_JSOP_FORNAME()
18085: {
18136:     jsval* vp;
21441:     if (name(vp)) {
21441:         set(vp, stack(-1));
21441:         return true;
21441:     }
21441:     return false;
18136: }
18136: 
22652: JS_REQUIRES_STACK bool
18136: TraceRecorder::record_JSOP_FORPROP()
18136: {
18085:     return false;
18136: }
18136: 
22652: JS_REQUIRES_STACK bool
18136: TraceRecorder::record_JSOP_FORELEM()
18136: {
21441:     return record_JSOP_DUP();
18085: }
18085: 
22652: JS_REQUIRES_STACK bool
18085: TraceRecorder::record_JSOP_FORARG()
18085: {
21441:     return record_JSOP_SETARG();
18085: }
18085: 
22652: JS_REQUIRES_STACK bool
18085: TraceRecorder::record_JSOP_FORLOCAL()
18085: {
21441:     return record_JSOP_SETLOCAL();
17899: }
17899: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_POPN()
17409: {
17456:     return true;
17409: }
17899: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_BINDNAME()
17409: {
17544:     JSObject* obj = cx->fp->scopeChain;
17657:     if (obj != globalObj)
17998:         ABORT_TRACE("JSOP_BINDNAME crosses global scopes");
17544: 
18286:     LIns* obj_ins = scopeChain();
17544:     JSObject* obj2;
17746:     jsuword pcval;
17746:     if (!test_property_cache(obj, obj_ins, obj2, pcval))
17544:         return false;
25633: 
25633:     if (PCVAL_IS_NULL(pcval))
25633:         ABORT_TRACE("JSOP_BINDNAME is trying to add a new property");
25633: 
18669:     if (obj2 != obj)
18669:         ABORT_TRACE("JSOP_BINDNAME found a non-direct property on the global object");
17544: 
17544:     stack(0, obj_ins);
17417:     return true;
17409: }
17541: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_SETNAME()
17409: {
17545:     jsval& l = stackval(-2);
18085:     JS_ASSERT(!JSVAL_IS_PRIMITIVE(l));
17519: 
17541:     /*
17541:      * Trace cases that are global code or in lightweight functions scoped by
17541:      * the global object only.
17541:      */
17541:     JSObject* obj = JSVAL_TO_OBJECT(l);
17657:     if (obj != cx->fp->scopeChain || obj != globalObj)
19093:         ABORT_TRACE("JSOP_SETNAME left operand is not the global object");
19093: 
19093:     // The rest of the work is in record_SetPropHit.
17519:     return true;
17409: }
17519: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_THROW()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_IN()
17409: {
18495:     jsval& rval = stackval(-1);
21784:     jsval& lval = stackval(-2);
21784: 
18495:     if (JSVAL_IS_PRIMITIVE(rval))
18495:         ABORT_TRACE("JSOP_IN on non-object right operand");
21784:     JSObject* obj = JSVAL_TO_OBJECT(rval);
21784:     LIns* obj_ins = get(&rval);
18495: 
18495:     jsid id;
21784:     LIns* x;
18495:     if (JSVAL_IS_INT(lval)) {
18495:         id = INT_JSVAL_TO_JSID(lval);
21784:         LIns* args[] = { makeNumberInt32(get(&lval)), obj_ins, cx_ins };
21784:         x = lir->insCall(&js_HasNamedPropertyInt32_ci, args);
21784:     } else if (JSVAL_IS_STRING(lval)) {
21784:         if (!js_ValueToStringId(cx, lval, &id))
21784:             ABORT_TRACE("left operand of JSOP_IN didn't convert to a string-id");
21784:         LIns* args[] = { get(&lval), obj_ins, cx_ins };
21784:         x = lir->insCall(&js_HasNamedProperty_ci, args);
18495:     } else {
21784:         ABORT_TRACE("string or integer expected");
21784:     }
21784: 
24846:     guard(false, lir->ins2i(LIR_eq, x, JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID)), OOM_EXIT);
21784:     x = lir->ins2i(LIR_eq, x, 1);
18495: 
18495:     JSObject* obj2;
18495:     JSProperty* prop;
18495:     if (!OBJ_LOOKUP_PROPERTY(cx, obj, id, &obj2, &prop))
18495:         ABORT_TRACE("OBJ_LOOKUP_PROPERTY failed in JSOP_IN");
21784:     bool cond = prop != NULL;
18495:     if (prop)
18495:         OBJ_DROP_PROPERTY(cx, obj2, prop);
18495: 
18495:     /* The interpreter fuses comparisons and the following branch,
18495:        so we have to do that here as well. */
18694:     fuseIf(cx->fp->regs->pc + 1, cond, x);
18495: 
18495:     /* We update the stack after the guard. This is safe since
18495:        the guard bails out at the comparison and the interpreter
18680:        will therefore re-execute the comparison. This way the
18495:        value of the condition doesn't have to be calculated and
18495:        saved on the stack in most cases. */
18495:     set(&lval, x);
18495:     return true;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_INSTANCEOF()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_DEBUGGER()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_GOSUB()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_RETSUB()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_EXCEPTION()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_LINENO()
17409: {
17456:     return true;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_CONDSWITCH()
17409: {
17456:     return true;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_CASE()
17409: {
23093:     strictEquality(true, true);
23093:     return true;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_DEFAULT()
17409: {
18687:     return true;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_EVAL()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_ENUMELEM()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_GETTER()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_SETTER()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_DEFFUN()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_DEFCONST()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_DEFVAR()
17409: {
17409:     return false;
17409: }
17926: 
17763: /*
17763:  * XXX could hoist out to jsinterp.h and share with jsinterp.cpp, but
17763:  * XXX jsopcode.cpp has different definitions of same-named macros.
17763:  */
17763: #define GET_FULL_INDEX(PCOFF)                                                 \
17763:     (atoms - script->atomMap.vector + GET_INDEX(regs.pc + PCOFF))
17763: 
17763: #define LOAD_FUNCTION(PCOFF)                                                  \
17763:     JS_GET_SCRIPT_FUNCTION(script, GET_FULL_INDEX(PCOFF), fun)
17764: 
22652: JS_REQUIRES_STACK bool
18020: TraceRecorder::record_JSOP_ANONFUNOBJ()
17409: {
17763:     JSFunction* fun;
17763:     JSFrameRegs& regs = *cx->fp->regs;
17763:     JSScript* script = cx->fp->script;
18020:     LOAD_FUNCTION(0); // needs script, regs, fun
18020: 
18020:     JSObject* obj = FUN_OBJECT(fun);
18020:     if (OBJ_GET_PARENT(cx, obj) != cx->fp->scopeChain)
18020:         ABORT_TRACE("can't trace with activation object on scopeChain");
18020: 
18712:     stack(0, INS_CONSTPTR(obj));
18020:     return true;
18020: }
18020: 
22652: JS_REQUIRES_STACK bool
18020: TraceRecorder::record_JSOP_NAMEDFUNOBJ()
18020: {
18020:     return false;
18020: }
18020: 
22652: JS_REQUIRES_STACK bool
18020: TraceRecorder::record_JSOP_SETLOCALPOP()
18020: {
20394:     var(GET_SLOTNO(cx->fp->regs->pc), stack(-1));
20394:     return true;
18020: }
18020: 
22652: JS_REQUIRES_STACK bool
23075: TraceRecorder::record_JSOP_IFPRIMTOP()
23075: {
23075:     // Traces are type-specialized, including null vs. object, so we need do
23075:     // nothing here. The upstream unbox_jsval called after valueOf or toString
23075:     // from an imacro (e.g.) will fork the trace for us, allowing us to just
23075:     // follow along mindlessly :-).
23075:     return true;
23075: }
23075: 
23075: JS_REQUIRES_STACK bool
18020: TraceRecorder::record_JSOP_SETCALL()
18020: {
18020:     return false;
18020: }
18020: 
22652: JS_REQUIRES_STACK bool
18020: TraceRecorder::record_JSOP_TRY()
18020: {
18020:     return true;
18020: }
18020: 
22652: JS_REQUIRES_STACK bool
18020: TraceRecorder::record_JSOP_FINALLY()
18020: {
18020:     return true;
18020: }
18020: 
22652: JS_REQUIRES_STACK bool
18020: TraceRecorder::record_JSOP_NOP()
18020: {
18020:     return true;
18020: }
18020: 
22652: JS_REQUIRES_STACK bool
18020: TraceRecorder::record_JSOP_ARGSUB()
18020: {
19068:     JSStackFrame* fp = cx->fp;
19068:     if (!(fp->fun->flags & JSFUN_HEAVYWEIGHT)) {
19068:         uintN slot = GET_ARGNO(fp->regs->pc);
19579:         if (slot < fp->fun->nargs && slot < fp->argc && !fp->argsobj) {
19068:             stack(0, get(&cx->fp->argv[slot]));
19068:             return true;
19068:         }
19068:     }
19068:     ABORT_TRACE("can't trace JSOP_ARGSUB hard case");
18020: }
18020: 
22652: JS_REQUIRES_STACK bool
18020: TraceRecorder::record_JSOP_ARGCNT()
18020: {
19068:     if (!(cx->fp->fun->flags & JSFUN_HEAVYWEIGHT)) {
26265:         stack(0, lir->insImmf(cx->fp->argc));
19068:         return true;
19068:     }
19068:     ABORT_TRACE("can't trace heavyweight JSOP_ARGCNT");
18020: }
18020: 
22652: JS_REQUIRES_STACK bool
19970: TraceRecorder::record_DefLocalFunSetSlot(uint32 slot, JSObject* obj)
19970: {
19970:     var(slot, INS_CONSTPTR(obj));
19970:     return true;
19970: }
19970: 
22652: JS_REQUIRES_STACK bool
18020: TraceRecorder::record_JSOP_DEFLOCALFUN()
18020: {
17763:     return true;
17409: }
17764: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_GOTOX()
17409: {
26557:     return record_JSOP_GOTO();
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_IFEQX()
17409: {
17611:     return record_JSOP_IFEQ();
17611: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_IFNEX()
17409: {
17611:     return record_JSOP_IFNE();
17611: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_ORX()
17409: {
17611:     return record_JSOP_OR();
17611: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_ANDX()
17409: {
17611:     return record_JSOP_AND();
17611: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_GOSUBX()
17409: {
17611:     return record_JSOP_GOSUB();
17611: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_CASEX()
17409: {
23093:     strictEquality(true, true);
23093:     return true;
17611: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_DEFAULTX()
17409: {
18687:     return true;
17611: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_TABLESWITCHX()
17409: {
26557:     return record_JSOP_TABLESWITCH();
17611: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_LOOKUPSWITCHX()
17409: {
18687:     return switchop();
17611: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_BACKPATCH()
17409: {
17456:     return true;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_BACKPATCH_POP()
17409: {
17456:     return true;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_THROWING()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_SETRVAL()
17409: {
17844:     // If we implement this, we need to update JSOP_STOP.
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_RETRVAL()
17409: {
17409:     return false;
17409: }
17899: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_GETGVAR()
17409: {
17807:     jsval slotval = cx->fp->slots[GET_SLOTNO(cx->fp->regs->pc)];
17468:     if (JSVAL_IS_NULL(slotval))
17468:         return true; // We will see JSOP_NAME from the interpreter's jump, so no-op here.
17545: 
17468:     uint32 slot = JSVAL_TO_INT(slotval);
17891: 
17892:     if (!lazilyImportGlobalSlot(slot))
17891:          ABORT_TRACE("lazy import of global slot failed");
17891: 
19973:     stack(0, get(&STOBJ_GET_SLOT(globalObj, slot)));
17468:     return true;
17409: }
17545: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_SETGVAR()
17409: {
17807:     jsval slotval = cx->fp->slots[GET_SLOTNO(cx->fp->regs->pc)];
17494:     if (JSVAL_IS_NULL(slotval))
17494:         return true; // We will see JSOP_NAME from the interpreter's jump, so no-op here.
17545: 
17494:     uint32 slot = JSVAL_TO_INT(slotval);
17891: 
17892:     if (!lazilyImportGlobalSlot(slot))
17891:          ABORT_TRACE("lazy import of global slot failed");
17891: 
19973:     set(&STOBJ_GET_SLOT(globalObj, slot), stack(-1));
17494:     return true;
17409: }
17545: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_INCGVAR()
17409: {
17807:     jsval slotval = cx->fp->slots[GET_SLOTNO(cx->fp->regs->pc)];
17468:     if (JSVAL_IS_NULL(slotval))
17468:         return true; // We will see JSOP_INCNAME from the interpreter's jump, so no-op here.
17545: 
17468:     uint32 slot = JSVAL_TO_INT(slotval);
17891: 
17892:     if (!lazilyImportGlobalSlot(slot))
17891:          ABORT_TRACE("lazy import of global slot failed");
17891: 
19973:     return inc(STOBJ_GET_SLOT(globalObj, slot), 1);
17409: }
17545: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_DECGVAR()
17409: {
17807:     jsval slotval = cx->fp->slots[GET_SLOTNO(cx->fp->regs->pc)];
17494:     if (JSVAL_IS_NULL(slotval))
17494:         return true; // We will see JSOP_INCNAME from the interpreter's jump, so no-op here.
17545: 
17494:     uint32 slot = JSVAL_TO_INT(slotval);
17891: 
17892:     if (!lazilyImportGlobalSlot(slot))
17891:          ABORT_TRACE("lazy import of global slot failed");
17891: 
19973:     return inc(STOBJ_GET_SLOT(globalObj, slot), -1);
17409: }
17545: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_GVARINC()
17409: {
17807:     jsval slotval = cx->fp->slots[GET_SLOTNO(cx->fp->regs->pc)];
17494:     if (JSVAL_IS_NULL(slotval))
17494:         return true; // We will see JSOP_INCNAME from the interpreter's jump, so no-op here.
17545: 
17494:     uint32 slot = JSVAL_TO_INT(slotval);
17891: 
17892:     if (!lazilyImportGlobalSlot(slot))
17891:          ABORT_TRACE("lazy import of global slot failed");
17891: 
19973:     return inc(STOBJ_GET_SLOT(globalObj, slot), 1, false);
17409: }
17545: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_GVARDEC()
17409: {
17807:     jsval slotval = cx->fp->slots[GET_SLOTNO(cx->fp->regs->pc)];
17494:     if (JSVAL_IS_NULL(slotval))
17494:         return true; // We will see JSOP_INCNAME from the interpreter's jump, so no-op here.
17545: 
17494:     uint32 slot = JSVAL_TO_INT(slotval);
17891: 
17892:     if (!lazilyImportGlobalSlot(slot))
17891:          ABORT_TRACE("lazy import of global slot failed");
17891: 
19973:     return inc(STOBJ_GET_SLOT(globalObj, slot), -1, false);
17409: }
17545: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_REGEXP()
17409: {
17409:     return false;
17409: }
17926: 
17926: // begin JS_HAS_XML_SUPPORT
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_DEFXMLNS()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_ANYNAME()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_QNAMEPART()
17409: {
24625:     return record_JSOP_STRING();
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_QNAMECONST()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_QNAME()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_TOATTRNAME()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_TOATTRVAL()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_ADDATTRNAME()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_ADDATTRVAL()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_BINDXMLNAME()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_SETXMLNAME()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_XMLNAME()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_DESCENDANTS()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_FILTER()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_ENDFILTER()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_TOXML()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_TOXMLLIST()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_XMLTAGEXPR()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_XMLELTEXPR()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_XMLOBJECT()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_XMLCDATA()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_XMLCOMMENT()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_XMLPI()
17409: {
17409:     return false;
17409: }
17642: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_GETFUNNS()
17926: {
17926:     return false;
17926: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_STARTXML()
17926: {
17926:     return false;
17926: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_STARTXMLEXPR()
17926: {
17926:     return false;
17926: }
17926: 
17926: // end JS_HAS_XML_SUPPORT
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_CALLPROP()
17409: {
17630:     jsval& l = stackval(-1);
17870:     JSObject* obj;
17870:     LIns* obj_ins;
20427:     LIns* this_ins;
17870:     if (!JSVAL_IS_PRIMITIVE(l)) {
17870:         obj = JSVAL_TO_OBJECT(l);
17870:         obj_ins = get(&l);
20427:         this_ins = obj_ins; // |this| for subsequent call
21840:         if (JSVAL_IS_NULL(l))
21840:             ABORT_TRACE("callprop on null");
21840:         if (!this_ins->isconstp())
21840:             guard(false, lir->ins_eq0(this_ins), MISMATCH_EXIT);
17870:     } else {
17870:         jsint i;
17998:         debug_only(const char* protoname = NULL;)
17870:         if (JSVAL_IS_STRING(l)) {
17870:             i = JSProto_String;
17998:             debug_only(protoname = "String.prototype";)
17870:         } else if (JSVAL_IS_NUMBER(l)) {
17870:             i = JSProto_Number;
17998:             debug_only(protoname = "Number.prototype";)
19995:         } else if (JSVAL_TAG(l) == JSVAL_BOOLEAN) {
19995:             if (l == JSVAL_VOID)
19995:                 ABORT_TRACE("callprop on void");
24846:             guard(false, lir->ins2i(LIR_eq, get(&l), JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID)), MISMATCH_EXIT);
17870:             i = JSProto_Boolean;
17998:             debug_only(protoname = "Boolean.prototype";)
17870:         } else {
17870:             JS_ASSERT(JSVAL_IS_NULL(l) || JSVAL_IS_VOID(l));
17870:             ABORT_TRACE("callprop on null or void");
17870:         }
17870: 
17870:         if (!js_GetClassPrototype(cx, NULL, INT_TO_JSID(i), &obj))
17870:             ABORT_TRACE("GetClassPrototype failed!");
17870: 
18712:         obj_ins = INS_CONSTPTR(obj);
17998:         debug_only(obj_ins = addName(obj_ins, protoname);)
20427:         this_ins = get(&l); // use primitive as |this|
17870:     }
17870: 
17632:     JSObject* obj2;
17746:     jsuword pcval;
17746:     if (!test_property_cache(obj, obj_ins, obj2, pcval))
17998:         return false;
17998: 
17998:     if (PCVAL_IS_NULL(pcval) || !PCVAL_IS_OBJECT(pcval))
17998:         ABORT_TRACE("callee is not an object");
17998:     JS_ASSERT(HAS_FUNCTION_CLASS(PCVAL_TO_OBJECT(pcval)));
17630: 
19054:     if (JSVAL_IS_PRIMITIVE(l)) {
19054:         JSFunction* fun = GET_FUNCTION_PRIVATE(cx, PCVAL_TO_OBJECT(pcval));
19054:         if (!PRIMITIVE_THIS_TEST(fun, l))
19054:             ABORT_TRACE("callee does not accept primitive |this|");
19054:     }
19054: 
20427:     stack(0, this_ins);
18712:     stack(-1, INS_CONSTPTR(PCVAL_TO_OBJECT(pcval)));
17630:     return true;
17409: }
17642: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_DELDESC()
17409: {
17409:     return false;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_UINT24()
17409: {
26265:     stack(0, lir->insImmf(GET_UINT24(cx->fp->regs->pc)));
17412:     return true;
17409: }
17926: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_INDEXBASE()
17611: {
17611:     atoms += GET_INDEXBASE(cx->fp->regs->pc);
17611:     return true;
17611: }
17611: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_RESETBASE()
17611: {
17611:     atoms = cx->fp->script->atomMap.vector;
17611:     return true;
17611: }
17611: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_RESETBASE0()
17611: {
17611:     atoms = cx->fp->script->atomMap.vector;
17611:     return true;
17611: }
17611: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_CALLELEM()
17409: {
26551:     return record_JSOP_GETELEM();
17409: }
17611: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_STOP()
17611: {
21685:     JSStackFrame *fp = cx->fp;
21685: 
21685:     if (fp->imacpc) {
21685:         // End of imacro, so return true to the interpreter immediately. The
21685:         // interpreter's JSOP_STOP case will return from the imacro, back to
21685:         // the pc after the calling op, still in the same JSStackFrame.
21721:         atoms = fp->script->atomMap.vector;
21685:         return true;
21685:     }
21685: 
18001:     /*
18001:      * We know falling off the end of a constructor returns the new object that
18001:      * was passed in via fp->argv[-1], while falling off the end of a function
18001:      * returns undefined.
18001:      *
18001:      * NB: we do not support script rval (eval, API users who want the result
18001:      * of the last expression-statement, debugger API calls).
18001:      */
18001:     if (fp->flags & JSFRAME_CONSTRUCTING) {
18001:         JS_ASSERT(OBJECT_TO_JSVAL(fp->thisp) == fp->argv[-1]);
18001:         rval_ins = get(&fp->argv[-1]);
18001:     } else {
24846:         rval_ins = INS_CONST(JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID));
18001:     }
17818:     clearFrameSlotsFromCache();
17818:     return true;
17611: }
17611: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_GETXPROP()
17409: {
17686:     jsval& l = stackval(-1);
17686:     if (JSVAL_IS_PRIMITIVE(l))
17686:         ABORT_TRACE("primitive-this for GETXPROP?");
17686: 
17686:     JSObject* obj = JSVAL_TO_OBJECT(l);
18096:     if (obj != cx->fp->scopeChain || obj != globalObj)
17686:         return false;
18096: 
18096:     jsval* vp;
18096:     if (!name(vp))
18096:         return false;
18096:     stack(-1, get(vp));
17686:     return true;
17409: }
17611: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_CALLXMLNAME()
17409: {
17409:     return false;
17409: }
17611: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_TYPEOFEXPR()
17409: {
18019:     return record_JSOP_TYPEOF();
17409: }
17611: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_ENTERBLOCK()
17409: {
22907:     JSScript* script = cx->fp->script;
22907:     JSFrameRegs& regs = *cx->fp->regs;
22907:     JSObject* obj;
22907:     JS_GET_SCRIPT_OBJECT(script, GET_FULL_INDEX(0), obj);
22907: 
24846:     LIns* void_ins = INS_CONST(JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID));
22907:     for (int i = 0, n = OBJ_BLOCK_COUNT(cx, obj); i < n; i++)
22907:         stack(i, void_ins);
22907:     return true;
17409: }
17611: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_LEAVEBLOCK()
17409: {
26238:     /* We mustn't exit the lexical block we began recording in.  */
26238:     return cx->fp->blockChain != lexicalBlock;
17409: }
17611: 
22652: JS_REQUIRES_STACK bool
18005: TraceRecorder::record_JSOP_GENERATOR()
17409: {
17409:     return false;
21685: #if 0
21685:     JSStackFrame* fp = cx->fp;
21685:     if (fp->callobj || fp->argsobj || fp->varobj)
21685:         ABORT_TRACE("can't trace hard-case generator");
21685: 
21685:     // Generate a type map for the outgoing frame and stash it in the LIR
21685:     unsigned stackSlots = js_NativeStackSlots(cx, 0/*callDepth*/);
23918:     if (stackSlots > MAX_SKIP_BYTES)
23918:         ABORT_TRACE("generator requires saving too much stack");
24850:     LIns* data = lir->skip(stackSlots * sizeof(uint8));
21685:     uint8* typemap = (uint8 *)data->payload();
21685:     uint8* m = typemap;
21685:     /* Determine the type of a store by looking at the current type of the actual value the
21685:        interpreter is using. For numbers we have to check what kind of store we used last
21685:        (integer or double) to figure out what the side exit show reflect in its typemap. */
21685:     FORALL_SLOTS_IN_PENDING_FRAMES(cx, 0/*callDepth*/,
21685:         *m++ = determineSlotType(vp);
21685:     );
21685:     FlushNativeStackFrame(cx, 0, typemap, state.???, NULL);
21685: 
21685:     LIns* args[] = { INS_CONST(fp->argc), INS_CONSTPTR(fp->callee), cx_ins };
21685:     LIns* g_ins = lir->insCall(&js_FastNewGenerator_ci, args);
21685:     guard(false, lir->ins_eq0(g_ins), OOM_EXIT);
21685:     return true;
21685: #endif
17409: }
17611: 
22652: JS_REQUIRES_STACK bool
18005: TraceRecorder::record_JSOP_YIELD()
17409: {
17409:     return false;
17409: }
17611: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_ARRAYPUSH()
17409: {
24861:     uint32_t slot = GET_UINT16(cx->fp->regs->pc);
24861:     JS_ASSERT(cx->fp->script->nfixed <= slot);
24861:     JS_ASSERT(cx->fp->slots + slot < cx->fp->regs->sp - 1);
24861:     jsval &arrayval = cx->fp->slots[slot];
24861:     JS_ASSERT(JSVAL_IS_OBJECT(arrayval));
24861:     JS_ASSERT(OBJ_IS_DENSE_ARRAY(cx, JSVAL_TO_OBJECT(arrayval)));
24861:     LIns *array_ins = get(&arrayval);
24861:     jsval &elt = stackval(-1);
24861:     LIns *elt_ins = get(&elt);
24861:     box_jsval(elt, elt_ins);
24861: 
24861:     LIns *args[] = { elt_ins, array_ins, cx_ins };
24861:     LIns *ok_ins = lir->insCall(&js_ArrayCompPush_ci, args);
24861:     guard(false, lir->ins_eq0(ok_ins), OOM_EXIT);
24861:     return true;
17409: }
17611: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_ENUMCONSTELEM()
17409: {
17409:     return false;
17409: }
17611: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_LEAVEBLOCKEXPR()
17409: {
22907:     LIns* v_ins = stack(-1);
22907:     int n = -1 - GET_UINT16(cx->fp->regs->pc);
22907:     stack(n, v_ins);
22907:     return true;
17409: }
17611: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_GETTHISPROP()
17409: {
17688:     LIns* this_ins;
17758: 
17688:     /* its safe to just use cx->fp->thisp here because getThis() returns false if thisp
17688:        is not available */
17688:     return getThis(this_ins) && getProp(cx->fp->thisp, this_ins);
17409: }
17611: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_GETARGPROP()
17409: {
17688:     return getProp(argval(GET_ARGNO(cx->fp->regs->pc)));
17409: }
17611: 
22652: JS_REQUIRES_STACK bool
18005: TraceRecorder::record_JSOP_GETLOCALPROP()
17409: {
17807:     return getProp(varval(GET_SLOTNO(cx->fp->regs->pc)));
17409: }
17611: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_INDEXBASE1()
17611: {
17611:     atoms += 1 << 16;
17611:     return true;
17611: }
17611: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_INDEXBASE2()
17611: {
17611:     atoms += 2 << 16;
17611:     return true;
17611: }
17611: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_INDEXBASE3()
17611: {
17611:     atoms += 3 << 16;
17611:     return true;
17611: }
17611: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_CALLGVAR()
17409: {
18003:     jsval slotval = cx->fp->slots[GET_SLOTNO(cx->fp->regs->pc)];
18003:     if (JSVAL_IS_NULL(slotval))
18003:         return true; // We will see JSOP_CALLNAME from the interpreter's jump, so no-op here.
18003: 
18003:     uint32 slot = JSVAL_TO_INT(slotval);
18003: 
18003:     if (!lazilyImportGlobalSlot(slot))
18003:          ABORT_TRACE("lazy import of global slot failed");
18003: 
23731:     jsval& v = STOBJ_GET_SLOT(globalObj, slot);
18031:     stack(0, get(&v));
18712:     stack(1, INS_CONSTPTR(NULL));
18280:     return true;
17409: }
17611: 
22652: JS_REQUIRES_STACK bool
18005: TraceRecorder::record_JSOP_CALLLOCAL()
17409: {
18003:     uintN slot = GET_SLOTNO(cx->fp->regs->pc);
18003:     stack(0, var(slot));
18712:     stack(1, INS_CONSTPTR(NULL));
18280:     return true;
17409: }
17611: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_CALLARG()
17409: {
18003:     uintN slot = GET_ARGNO(cx->fp->regs->pc);
18003:     stack(0, arg(slot));
18712:     stack(1, INS_CONSTPTR(NULL));
18280:     return true;
17409: }
17611: 
24384: /* Functions for use with JSOP_CALLBUILTIN. */
24384: 
24384: static JSBool
24384: ObjectToIterator(JSContext *cx, uintN argc, jsval *vp)
24384: {
24384:     jsval *argv = JS_ARGV(cx, vp);
24384:     JS_ASSERT(JSVAL_IS_INT(argv[0]));
24384:     JS_SET_RVAL(cx, vp, JS_THIS(cx, vp));
24384:     return js_ValueToIterator(cx, JSVAL_TO_INT(argv[0]), &JS_RVAL(cx, vp));
24384: }
24384: 
24384: static JSObject* FASTCALL
24612: ObjectToIterator_tn(JSContext* cx, jsbytecode* pc, JSObject *obj, int32 flags)
24384: {
24384:     jsval v = OBJECT_TO_JSVAL(obj);
25628:     JSBool ok = js_ValueToIterator(cx, flags, &v);
24612: 
24612:     if (!ok) {
24612:         cx->builtinStatus |= JSBUILTIN_ERROR;
24384:         return NULL;
24612:     }
24384:     return JSVAL_TO_OBJECT(v);
24384: }
24384: 
24384: static JSBool
24384: CallIteratorNext(JSContext *cx, uintN argc, jsval *vp)
24384: {
24384:     return js_CallIteratorNext(cx, JS_THIS_OBJECT(cx, vp), &JS_RVAL(cx, vp));
24384: }
24384: 
24384: static jsval FASTCALL
24612: CallIteratorNext_tn(JSContext* cx, jsbytecode* pc, JSObject* iterobj)
24384: {
25094:     JSAutoTempValueRooter tvr(cx);
25628:     JSBool ok = js_CallIteratorNext(cx, iterobj, tvr.addr());
24612: 
24612:     if (!ok) {
24612:         cx->builtinStatus |= JSBUILTIN_ERROR;
24384:         return JSVAL_ERROR_COOKIE;
24612:     }
25094:     return tvr.value();
24384: }
24384: 
24384: JS_DEFINE_TRCINFO_1(ObjectToIterator,
24612:     (4, (static, OBJECT_FAIL, ObjectToIterator_tn, CONTEXT, PC, THIS, INT32, 0, 0)))
24384: JS_DEFINE_TRCINFO_1(CallIteratorNext,
24612:     (3, (static, JSVAL_FAIL,  CallIteratorNext_tn, CONTEXT, PC, THIS,        0, 0)))
24384: 
24384: static const struct BuiltinFunctionInfo {
24384:     JSTraceableNative *tn;
24384:     int nargs;
24384: } builtinFunctionInfo[JSBUILTIN_LIMIT] = {
24384:     {ObjectToIterator_trcinfo,   1},
24489:     {CallIteratorNext_trcinfo,   0},
24489:     {GetProperty_trcinfo,        1},
24489:     {GetElement_trcinfo,         1},
24489:     {SetProperty_trcinfo,        2},
24489:     {SetElement_trcinfo,         2}
24384: };
24384: 
24384: JSObject *
24384: js_GetBuiltinFunction(JSContext *cx, uintN index)
24384: {
24384:     JSRuntime *rt = cx->runtime;
24384:     JSObject *funobj = rt->builtinFunctions[index];
24384: 
24384:     if (!funobj) {
24384:         /* Use NULL parent and atom. Builtin functions never escape to scripts. */
25218:         JS_ASSERT(index < JS_ARRAY_LENGTH(builtinFunctionInfo));
25218:         const BuiltinFunctionInfo *bfi = &builtinFunctionInfo[index];
24384:         JSFunction *fun = js_NewFunction(cx,
24384:                                          NULL,
25218:                                          JS_DATA_TO_FUNC_PTR(JSNative, bfi->tn),
25218:                                          bfi->nargs,
24384:                                          JSFUN_FAST_NATIVE | JSFUN_TRACEABLE,
24384:                                          NULL,
24384:                                          NULL);
24384:         if (fun) {
24384:             funobj = FUN_OBJECT(fun);
24384:             STOBJ_CLEAR_PROTO(funobj);
24384:             STOBJ_CLEAR_PARENT(funobj);
24384: 
24384:             JS_LOCK_GC(rt);
24384:             if (!rt->builtinFunctions[index])  /* retest now that the lock is held */
24384:                 rt->builtinFunctions[index] = funobj;
24384:             else
24384:                 funobj = rt->builtinFunctions[index];
24384:             JS_UNLOCK_GC(rt);
24384:         }
24384:     }
24384:     return funobj;
24384: }
24384: 
24384: JS_REQUIRES_STACK bool
24384: TraceRecorder::record_JSOP_CALLBUILTIN()
24384: {
24384:     JSObject *obj = js_GetBuiltinFunction(cx, GET_INDEX(cx->fp->regs->pc));
24384:     if (!obj)
24384:         return false;
24384: 
24384:     stack(0, get(&stackval(-1)));
24384:     stack(-1, INS_CONSTPTR(obj));
24384:     return true;
24384: }
24384: 
22652: JS_REQUIRES_STACK bool
18031: TraceRecorder::record_JSOP_NULLTHIS()
18031: {
18712:     stack(0, INS_CONSTPTR(NULL));
18280:     return true;
18031: }
18031: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_INT8()
17409: {
26265:     stack(0, lir->insImmf(GET_INT8(cx->fp->regs->pc)));
17412:     return true;
17409: }
17611: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_INT32()
17409: {
26265:     stack(0, lir->insImmf(GET_INT32(cx->fp->regs->pc)));
17412:     return true;
17409: }
17611: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_LENGTH()
17409: {
17715:     jsval& l = stackval(-1);
17869:     if (JSVAL_IS_PRIMITIVE(l)) {
17869:         if (!JSVAL_IS_STRING(l))
24625:             ABORT_TRACE("non-string primitive JSOP_LENGTH unsupported");
17869:         LIns* str_ins = get(&l);
18254:         LIns* len_ins = lir->insLoad(LIR_ldp, str_ins, (int)offsetof(JSString, length));
18254: 
18261:         LIns* masked_len_ins = lir->ins2(LIR_piand,
18070:                                          len_ins,
26397:                                          INS_CONSTPTR(reinterpret_cast<void *>(JSSTRING_LENGTH_MASK)));
18070: 
18070:         LIns* choose_len_ins =
18232:             lir->ins_choose(lir->ins_eq0(lir->ins2(LIR_piand,
18070:                                                    len_ins,
26397:                                                    INS_CONSTPTR(reinterpret_cast<void *>(JSSTRFLAG_DEPENDENT)))),
18070:                             masked_len_ins,
18232:                             lir->ins_choose(lir->ins_eq0(lir->ins2(LIR_piand,
18070:                                                                    len_ins,
26397:                                                                    INS_CONSTPTR(reinterpret_cast<void *>(JSSTRFLAG_PREFIX)))),
18232:                                             lir->ins2(LIR_piand,
18070:                                                       len_ins,
26397:                                                       INS_CONSTPTR(reinterpret_cast<void *>(JSSTRDEP_LENGTH_MASK))),
19997:                                             masked_len_ins));
18070: 
18070:         set(&l, lir->ins1(LIR_i2f, choose_len_ins));
17869:         return true;
17869:     }
17869: 
17869:     JSObject* obj = JSVAL_TO_OBJECT(l);
26285:     LIns* obj_ins = get(&l);
26285:     LIns* v_ins;
26285:     if (OBJ_IS_ARRAY(cx, obj)) {
26285:         if (OBJ_IS_DENSE_ARRAY(cx, obj)) {
26565:             if (!guardDenseArray(obj, obj_ins, BRANCH_EXIT)) {
26285:                 JS_NOT_REACHED("OBJ_IS_DENSE_ARRAY but not?!?");
26565:                 return false;
26565:             }
26285:         } else {
26285:             if (!guardClass(obj, obj_ins, &js_SlowArrayClass, snapshot(BRANCH_EXIT)))
26285:                 ABORT_TRACE("can't trace length property access on non-array");
26285:         }
26285:         v_ins = lir->ins1(LIR_i2f, stobj_get_fslot(obj_ins, JSSLOT_ARRAY_LENGTH));
26285:     } else {
26285:         if (!OBJ_IS_NATIVE(obj))
26285:             ABORT_TRACE("can't trace length property access on non-array, non-native object");
26285:         return getProp(obj, obj_ins);
26285:     }
17715:     set(&l, v_ins);
17715:     return true;
17409: }
17611: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_NEWARRAY()
17409: {
23708:     JSObject* proto;
23708:     const CallInfo* ci = &js_NewUninitializedArray_ci;
23708:     if (!js_GetClassPrototype(cx, globalObj, INT_TO_JSID(JSProto_Array), &proto))
23708:         return false;
23708: 
23708:     uint32 len = GET_UINT24(cx->fp->regs->pc);
23708:     LIns* args[] = { lir->insImm(len), INS_CONSTPTR(proto), cx_ins };
23708:     LIns* v_ins = lir->insCall(ci, args);
23708:     guard(false, lir->ins_eq0(v_ins), OOM_EXIT);
23708: 
23708:     LIns* dslots_ins = NULL;
23708:     for (uint32 i = 0; i < len; i++) {
25628:         jsval& v = stackval(int(i) - int(len));
23708:         LIns* elt_ins = get(&v);
23708:         box_jsval(v, elt_ins);
23708:         stobj_set_dslot(v_ins, i, dslots_ins, elt_ins, "set_array_elt");
23708:     }
23708: 
25628:     stack(-int(len), v_ins);
23708:     return true;
17409: }
17611: 
22652: JS_REQUIRES_STACK bool
17926: TraceRecorder::record_JSOP_HOLE()
17409: {
24846:     stack(0, INS_CONST(JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_HOLE)));
17456:     return true;
17409: }
18005: 
25627: bool
25627: TraceRecorder::record_JSOP_LOOP()
25627: {
25627:     return true;
25627: }
25627: 
21459: #ifdef JS_JIT_SPEW
21433: /* Prints information about entry typemaps and unstable exits for all peers at a PC */
21433: void
24491: js_DumpPeerStability(JSTraceMonitor* tm, const void* ip, uint32 globalShape)
21433: {
21433:     Fragment* f;
21433:     TreeInfo* ti;
21456:     bool looped = false;
21685:     unsigned length = 0;
21433: 
24491:     for (f = getLoop(tm, ip, globalShape); f != NULL; f = f->peer) {
21433:         if (!f->vmprivate)
21433:             continue;
25469:         printf("fragment %p:\nENTRY: ", (void*)f);
21433:         ti = (TreeInfo*)f->vmprivate;
21456:         if (looped)
24491:             JS_ASSERT(ti->nStackTypes == length);
24491:         for (unsigned i = 0; i < ti->nStackTypes; i++)
24246:             printf("S%d ", ti->stackTypeMap()[i]);
24491:         for (unsigned i = 0; i < ti->nGlobalTypes(); i++)
24246:             printf("G%d ", ti->globalTypeMap()[i]);
21433:         printf("\n");
21433:         UnstableExit* uexit = ti->unstableExits;
21433:         while (uexit != NULL) {
21433:             printf("EXIT:  ");
24246:             uint8* m = getFullTypeMap(uexit->exit);
21433:             for (unsigned i = 0; i < uexit->exit->numStackSlots; i++)
24246:                 printf("S%d ", m[i]);
24246:             for (unsigned i = 0; i < uexit->exit->numGlobalSlots; i++)
24246:                 printf("G%d ", m[uexit->exit->numStackSlots + i]);
21433:             printf("\n");
21433:             uexit = uexit->next;
21433:         }
24491:         length = ti->nStackTypes;
21456:         looped = true;
21433:     }
21433: }
21433: #endif
21433: 
23106: #define UNUSED(n)                                                             \
23106:     JS_REQUIRES_STACK bool                                                    \
23106:     TraceRecorder::record_JSOP_UNUSED##n() {                                  \
23106:         JS_NOT_REACHED("JSOP_UNUSED" # n);                                    \
23106:         return false;                                                         \
23106:     }
23106: 
20969: UNUSED(203)
20969: UNUSED(204)
20969: UNUSED(205)
20969: UNUSED(206)
20969: UNUSED(207)
21441: UNUSED(208)
21441: UNUSED(209)
20969: UNUSED(219)
