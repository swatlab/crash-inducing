29366: /* -*- Mode: C++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*-
29900:  * vim: set ts=4 sw=4 et tw=99 ft=cpp:
17186:  *
17186:  * ***** BEGIN LICENSE BLOCK *****
17186:  * Version: MPL 1.1/GPL 2.0/LGPL 2.1
17186:  *
17186:  * The contents of this file are subject to the Mozilla Public License Version
17186:  * 1.1 (the "License"); you may not use this file except in compliance with
17186:  * the License. You may obtain a copy of the License at
17186:  * http://www.mozilla.org/MPL/
17186:  *
17186:  * Software distributed under the License is distributed on an "AS IS" basis,
17186:  * WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
17186:  * for the specific language governing rights and limitations under the
17186:  * License.
17186:  *
17186:  * The Original Code is Mozilla SpiderMonkey JavaScript 1.9 code, released
17186:  * May 28, 2008.
17186:  *
17186:  * The Initial Developer of the Original Code is
17339:  *   Brendan Eich <brendan@mozilla.org>
17186:  *
17186:  * Contributor(s):
17339:  *   Andreas Gal <gal@mozilla.com>
17671:  *   Mike Shaver <shaver@mozilla.org>
17671:  *   David Anderson <danderson@mozilla.com>
17186:  *
17186:  * Alternatively, the contents of this file may be used under the terms of
17186:  * either of the GNU General Public License Version 2 or later (the "GPL"),
17186:  * or the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
17186:  * in which case the provisions of the GPL or the LGPL are applicable instead
17186:  * of those above. If you wish to allow use of your version of this file only
17186:  * under the terms of either the GPL or the LGPL, and not to allow others to
17186:  * use your version of this file under the terms of the MPL, indicate your
17186:  * decision by deleting the provisions above and replace them with the notice
17186:  * and other provisions required by the GPL or the LGPL. If you do not delete
17186:  * the provisions above, a recipient may use your version of this file under
17186:  * the terms of any one of the MPL, the GPL or the LGPL.
17186:  *
17186:  * ***** END LICENSE BLOCK ***** */
17186: 
17196: #ifndef jstracer_h___
17196: #define jstracer_h___
17186: 
19171: #ifdef JS_TRACER
18091: 
32777: #include "jstypes.h"
32777: #include "jsbuiltins.h"
21521: #include "jscntxt.h"
33560: #include "jsdhash.h"
32777: #include "jsinterp.h"
17186: #include "jslock.h"
17393: #include "jsnum.h"
33930: #include "jsvector.h"
59733: #include "jscompartment.h"
56750: #include "Writer.h"
17434: 
37741: namespace js {
37741: 
17886: template <typename T>
31920: class Queue {
17886:     T* _data;
17886:     unsigned _len;
17886:     unsigned _max;
31920:     nanojit::Allocator* alloc;
17891: 
31913: public:
17891:     void ensure(unsigned size) {
36398:         if (_max > size)
36398:             return;
35361:         if (!_max)
36398:             _max = 8;
36398:         _max = JS_MAX(_max * 2, size);
31920:         if (alloc) {
31920:             T* tmp = new (*alloc) T[_max];
31920:             memcpy(tmp, _data, _len * sizeof(T));
31920:             _data = tmp;
31920:         } else {
64560:             _data = (T*) js::OffTheBooks::realloc_(_data, _max * sizeof(T));
31920:         }
24246: #if defined(DEBUG)
24246:         memset(&_data[_len], 0xcd, _max - _len);
24246: #endif
17891:     }
31913: 
32616:     Queue(nanojit::Allocator* alloc)
31920:         : alloc(alloc)
31920:     {
35361:         this->_max =
17886:         this->_len = 0;
35361:         this->_data = NULL;
17886:     }
17886: 
17886:     ~Queue() {
35361:         if (!alloc)
64560:             js::UnwantedForeground::free_(_data);
17886:     }
17886: 
17973:     bool contains(T a) {
23709:         for (unsigned n = 0; n < _len; ++n) {
17973:             if (_data[n] == a)
17973:                 return true;
23709:         }
17973:         return false;
17973:     }
17973: 
17886:     void add(T a) {
18621:         ensure(_len + 1);
17886:         JS_ASSERT(_len <= _max);
17891:         _data[_len++] = a;
17886:     }
17891: 
18621:     void add(T* chunk, unsigned size) {
18621:         ensure(_len + size);
18621:         JS_ASSERT(_len <= _max);
18621:         memcpy(&_data[_len], chunk, size * sizeof(T));
18621:         _len += size;
18621:     }
18621: 
18051:     void addUnique(T a) {
18051:         if (!contains(a))
18051:             add(a);
18051:     }
18051: 
17891:     void setLength(unsigned len) {
17891:         ensure(len + 1);
17891:         _len = len;
17886:     }
17886: 
17886:     void clear() {
17886:         _len = 0;
17886:     }
17886: 
31473:     T & get(unsigned i) {
31473:         JS_ASSERT(i < length());
31473:         return _data[i];
31473:     }
31473: 
22609:     const T & get(unsigned i) const {
29900:         JS_ASSERT(i < length());
22609:         return _data[i];
22609:     }
22609: 
31473:     T & operator [](unsigned i) {
31473:         return get(i);
31473:     }
31473: 
29900:     const T & operator [](unsigned i) const {
29900:         return get(i);
29900:     }
29900: 
17886:     unsigned length() const {
17886:         return _len;
17886:     }
17886: 
17886:     T* data() const {
17886:         return _data;
17886:     }
36401: 
36401:     int offsetOf(T slot) {
36401:         T* p = _data;
36401:         unsigned n = 0;
36401:         for (n = 0; n < _len; ++n)
36401:             if (*p++ == slot)
36401:                 return n;
36401:         return -1;
36401:     }
36401: 
17886: };
17886: 
17434: /*
17487:  * Tracker is used to keep track of values being manipulated by the interpreter
33550:  * during trace recording.  It maps opaque, 4-byte aligned address to LIns pointers.
33550:  * pointers. To do this efficiently, we observe that the addresses of jsvals
33550:  * living in the interpreter tend to be aggregated close to each other -
33550:  * usually on the same page (where a tracker page doesn't have to be the same
33550:  * size as the OS page size, but it's typically similar).  The Tracker
33550:  * consists of a linked-list of structures representing a memory page, which
33550:  * are created on-demand as memory locations are used.
33550:  *
33550:  * For every address, first we split it into two parts: upper bits which
33550:  * represent the "base", and lower bits which represent an offset against the
33550:  * base.  For the offset, we then right-shift it by two because the bottom two
33550:  * bits of a 4-byte aligned address are always zero.  The mapping then
33550:  * becomes:
33550:  *
33550:  *   page = page in pagelist such that Base(address) == page->base,
33550:  *   page->map[Offset(address)]
17293:  */
17487: class Tracker {
33550:     #define TRACKER_PAGE_SZB        4096
33550:     #define TRACKER_PAGE_ENTRIES    (TRACKER_PAGE_SZB >> 2)    // each slot is 4 bytes
33550:     #define TRACKER_PAGE_MASK       jsuword(TRACKER_PAGE_SZB - 1)
33550: 
33177:     struct TrackerPage {
33177:         struct TrackerPage* next;
17424:         jsuword             base;
33550:         nanojit::LIns*      map[TRACKER_PAGE_ENTRIES];
17293:     };
33177:     struct TrackerPage* pagelist;
17293: 
64559:     /* Keep track of memory allocation. */
64559:     JSContext* cx;
64559: 
33177:     jsuword             getTrackerPageBase(const void* v) const;
33550:     jsuword             getTrackerPageOffset(const void* v) const;
33177:     struct TrackerPage* findTrackerPage(const void* v) const;
33177:     struct TrackerPage* addTrackerPage(const void* v);
17293: public:
64559:     Tracker(JSContext* cx);
17293:     ~Tracker();
17293: 
17773:     bool            has(const void* v) const;
17596:     nanojit::LIns*  get(const void* v) const;
17596:     void            set(const void* v, nanojit::LIns* ins);
17293:     void            clear();
17293: };
17293: 
35044: class VMFragment : public nanojit::Fragment {
35044: public:
35044:     VMFragment(const void* _ip verbose_only(, uint32_t profFragID))
35044:       : Fragment(_ip verbose_only(, profFragID))
35044:     {}
35044: 
35044:     /*
35044:      * If this is anchored off a TreeFragment, this points to that tree fragment.
35044:      * Otherwise, it is |this|.
35044:      */
35044:     TreeFragment* root;
35083: 
35083:     TreeFragment* toTreeFragment();
35044: };
35044: 
68899: #if defined(JS_JIT_SPEW)
30342: 
64186: // Top level Nanojit config object.
64186: extern nanojit::Config NJConfig;
64186: 
29883: // Top level logging controller object.
37741: extern nanojit::LogControl LogController;
29883: 
32784: // Top level profiling hook, needed to harvest profile info from Fragments
32784: // whose logical lifetime is about to finish
37741: extern void FragProfiling_FragFinalizer(nanojit::Fragment* f, TraceMonitor*);
32784: 
29883: #define debug_only_stmt(stmt) \
29883:     stmt
30860: 
29883: #define debug_only_printf(mask, fmt, ...)                                      \
30860:     JS_BEGIN_MACRO                                                             \
37741:         if ((LogController.lcbits & (mask)) > 0) {                             \
37741:             LogController.printf(fmt, __VA_ARGS__);                            \
30860:             fflush(stdout);                                                    \
30860:         }                                                                      \
30860:     JS_END_MACRO
30860: 
29883: #define debug_only_print0(mask, str)                                           \
30860:     JS_BEGIN_MACRO                                                             \
37741:         if ((LogController.lcbits & (mask)) > 0) {                             \
37741:             LogController.printf("%s", str);                                   \
30860:             fflush(stdout);                                                    \
30860:         }                                                                      \
30860:     JS_END_MACRO
29883: 
23450: #else
29883: 
29883: #define debug_only_stmt(action)            /* */
59972: #define debug_only_printf(mask, fmt, ...)  JS_BEGIN_MACRO JS_END_MACRO
59972: #define debug_only_print0(mask, str)       JS_BEGIN_MACRO JS_END_MACRO
29883: 
23450: #endif
23450: 
17981: /*
24290:  * The oracle keeps track of hit counts for program counter locations, as
24290:  * well as slots that should not be demoted to int because we know them to
24290:  * overflow or they result in type-unstable traces. We are using simple
24290:  * hash tables.  Collisions lead to loss of optimization (demotable slots
24290:  * are not demoted, etc.) but have no correctness implications.
17981:  */
17981: #define ORACLE_SIZE 4096
17981: 
17981: class Oracle {
71334:     nanojit::BitSet _stackDontDemote;
71334:     nanojit::BitSet _globalDontDemote;
71334:     nanojit::BitSet _pcDontDemote;
71334:     nanojit::BitSet _pcSlowZeroTest;
17981: public:
71334:     Oracle(VMAllocator* allocator);
24290: 
23456:     JS_REQUIRES_STACK void markGlobalSlotUndemotable(JSContext* cx, unsigned slot);
23456:     JS_REQUIRES_STACK bool isGlobalSlotUndemotable(JSContext* cx, unsigned slot) const;
23456:     JS_REQUIRES_STACK void markStackSlotUndemotable(JSContext* cx, unsigned slot);
33564:     JS_REQUIRES_STACK void markStackSlotUndemotable(JSContext* cx, unsigned slot, const void* pc);
23456:     JS_REQUIRES_STACK bool isStackSlotUndemotable(JSContext* cx, unsigned slot) const;
33564:     JS_REQUIRES_STACK bool isStackSlotUndemotable(JSContext* cx, unsigned slot, const void* pc) const;
29354:     void markInstructionUndemotable(jsbytecode* pc);
29354:     bool isInstructionUndemotable(jsbytecode* pc) const;
56185:     void markInstructionSlowZeroTest(jsbytecode* pc);
56185:     bool isInstructionSlowZeroTest(jsbytecode* pc) const;
29354: 
24290:     void clearDemotability();
24290:     void clear() {
24290:         clearDemotability();
24290:     }
17981: };
17981: 
17992: typedef Queue<uint16> SlotList;
17992: 
48470: class TypeMap : public Queue<JSValueType> {
46181:     Oracle *oracle;
17981: public:
62032:     TypeMap(nanojit::Allocator* alloc, Oracle *oracle)
62032:       : Queue<JSValueType>(alloc),
62032:         oracle(oracle)
62032:     {}
36401:     void set(unsigned stackSlots, unsigned ngslots,
48470:              const JSValueType* stackTypeMap, const JSValueType* globalTypeMap);
46181:     JS_REQUIRES_STACK void captureTypes(JSContext* cx, JSObject* globalObj, SlotList& slots, unsigned callDepth,
46181:                                         bool speculate);
29880:     JS_REQUIRES_STACK void captureMissingGlobalTypes(JSContext* cx, JSObject* globalObj, SlotList& slots,
46181:                                                      unsigned stackSlots, bool speculate);
18239:     bool matches(TypeMap& other) const;
48470:     void fromRaw(JSValueType* other, unsigned numSlots);
17981: };
17981: 
29893: #define JS_TM_EXITCODES(_)    \
29893:     /*                                                                          \
29893:      * An exit at a possible branch-point in the trace at which to attach a     \
29893:      * future secondary trace. Therefore the recorder must generate different   \
29893:      * code to handle the other outcome of the branch condition from the        \
29893:      * primary trace's outcome.                                                 \
29893:      */                                                                         \
29893:     _(BRANCH)                                                                   \
29893:     _(LOOP)                                                                     \
29893:     _(NESTED)                                                                   \
29893:     /*                                                                          \
29893:      * An exit from a trace because a condition relied upon at recording time   \
29893:      * no longer holds, where the alternate path of execution is so rare or     \
29893:      * difficult to address in native code that it is not traced at all, e.g.   \
29893:      * negative array index accesses, which differ from positive indexes in     \
29893:      * that they require a string-based property lookup rather than a simple    \
29893:      * memory access.                                                           \
29893:      */                                                                         \
29893:     _(MISMATCH)                                                                 \
29893:     /*                                                                          \
29893:      * A specialization of MISMATCH_EXIT to handle allocation failures.         \
29893:      */                                                                         \
29893:     _(OOM)                                                                      \
29893:     _(OVERFLOW)                                                                 \
56185:     _(MUL_ZERO)                                                                 \
29893:     _(UNSTABLE_LOOP)                                                            \
29893:     _(TIMEOUT)                                                                  \
29893:     _(DEEP_BAIL)                                                                \
53524:     _(STATUS)
29893: 
21521: enum ExitType {
29893:     #define MAKE_EXIT_CODE(x) x##_EXIT,
29893:     JS_TM_EXITCODES(MAKE_EXIT_CODE)
29893:     #undef MAKE_EXIT_CODE
29895:     TOTAL_EXIT_TYPES
21521: };
21521: 
33564: struct FrameInfo;
33564: 
21521: struct VMSideExit : public nanojit::SideExit
21521: {
25111:     jsbytecode* pc;
25111:     jsbytecode* imacpc;
21521:     intptr_t sp_adj;
21521:     intptr_t rp_adj;
21521:     int32_t calldepth;
21521:     uint32 numGlobalSlots;
21521:     uint32 numStackSlots;
21521:     uint32 numStackSlotsBelowCurrentFrame;
21521:     ExitType exitType;
30034:     uintN lookupFlags;
33564:     unsigned hitcount;
28086: 
48470:     inline JSValueType* stackTypeMap() {
48470:         return (JSValueType*)(this + 1);
31495:     }
31495: 
48470:     inline JSValueType& stackType(unsigned i) {
33564:         JS_ASSERT(i < numStackSlots);
33564:         return stackTypeMap()[i];
33564:     }
33564: 
48470:     inline JSValueType* globalTypeMap() {
48470:         return (JSValueType*)(this + 1) + this->numStackSlots;
31495:     }
31495: 
48470:     inline JSValueType* fullTypeMap() {
31495:         return stackTypeMap();
31495:     }
31495: 
35044:     inline VMFragment* fromFrag() {
35044:         return (VMFragment*)from;
35044:     }
35044: 
35044:     inline TreeFragment* root() {
35044:         return fromFrag()->root;
31495:     }
21521: };
21521: 
31918: class VMAllocator : public nanojit::Allocator
31475: {
31475: public:
64559:     VMAllocator(JSRuntime *rt, char* reserve, size_t reserveSize)
64559:       : mOutOfMemory(false)
64559:       , mSize(0)
64559:       , mReserve(reserve)
69289:       , mReserveSize(reserveSize)
64559:       , mReserveCurr(uintptr_t(reserve))
64559:       , mReserveLimit(uintptr_t(reserve + reserveSize))
64559:       , mRt(rt)
31475:     {}
31475: 
61053:     ~VMAllocator() {
64560:         js::UnwantedForeground::free_(mReserve);
61053:     }
61053: 
31475:     size_t size() {
31475:         return mSize;
31475:     }
31475: 
31475:     bool outOfMemory() {
31475:         return mOutOfMemory;
31475:     }
31475: 
33545:     struct Mark
33545:     {
33545:         VMAllocator& vma;
33545:         bool committed;
33545:         nanojit::Allocator::Chunk* saved_chunk;
33545:         char* saved_top;
33545:         char* saved_limit;
33545:         size_t saved_size;
33545: 
33545:         Mark(VMAllocator& vma) :
33545:             vma(vma),
33545:             committed(false),
33545:             saved_chunk(vma.current_chunk),
33545:             saved_top(vma.current_top),
33545:             saved_limit(vma.current_limit),
33545:             saved_size(vma.mSize)
33545:         {}
33545: 
33545:         ~Mark()
33545:         {
33545:             if (!committed)
33545:                 vma.rewind(*this);
33545:         }
33545: 
33545:         void commit() { committed = true; }
33545:     };
33545: 
33545:     void rewind(const Mark& m) {
33545:         while (current_chunk != m.saved_chunk) {
33545:             Chunk *prev = current_chunk->prev;
33545:             freeChunk(current_chunk);
33545:             current_chunk = prev;
33545:         }
33545:         current_top = m.saved_top;
33545:         current_limit = m.saved_limit;
33545:         mSize = m.saved_size;
33545:         memset(current_top, 0, current_limit - current_top);
33545:     }
33545: 
31475:     bool mOutOfMemory;
31475:     size_t mSize;
31920: 
61053:     /* See nanojit::Allocator::allocChunk() for details on these. */
61053:     char* mReserve;
69289:     size_t mReserveSize;
61053:     uintptr_t mReserveCurr;
61053:     uintptr_t mReserveLimit;
64559: 
64559:     /* To keep track of allocation. */
64559:     JSRuntime* mRt;
31918: };
31918: 
24612: struct FrameInfo {
24612:     JSObject*       block;      // caller block chain head
25111:     jsbytecode*     pc;         // caller fp->regs->pc
25111:     jsbytecode*     imacpc;     // caller fp->imacpc
31924:     uint32          spdist;     // distance from fp->slots to fp->regs->sp at JSOP_CALL
28949: 
28949:     /*
28949:      * Bit  15 (0x8000) is a flag that is set if constructing (called through new).
28949:      * Bits 0-14 are the actual argument count. This may be less than fun->nargs.
31924:      * NB: This is argc for the callee, not the caller.
28949:      */
31924:     uint32          argc;
28887: 
28840:     /*
31924:      * Number of stack slots in the caller, not counting slots pushed when
31924:      * invoking the callee. That is, slots after JSOP_CALL completes but
31924:      * without the return value. This is also equal to the number of slots
53840:      * between fp->prev->argv[-2] (calleR fp->callee) and fp->argv[-2]
31924:      * (calleE fp->callee).
28840:      */
31924:     uint32          callerHeight;
31924: 
31924:     /* argc of the caller */
31924:     uint32          callerArgc;
28949: 
28949:     // Safer accessors for argc.
31924:     enum { CONSTRUCTING_FLAG = 0x10000 };
28949:     void   set_argc(uint16 argc, bool constructing) {
31924:         this->argc = uint32(argc) | (constructing ? CONSTRUCTING_FLAG: 0);
28949:     }
33166:     uint16 get_argc() const { return uint16(argc & ~CONSTRUCTING_FLAG); }
31924:     bool   is_constructing() const { return (argc & CONSTRUCTING_FLAG) != 0; }
28993: 
28993:     // The typemap just before the callee is called.
48470:     JSValueType* get_typemap() { return (JSValueType*) (this+1); }
48470:     const JSValueType* get_typemap() const { return (JSValueType*) (this+1); }
21521: };
21521: 
21433: struct UnstableExit
21433: {
35044:     VMFragment* fragment;
21521:     VMSideExit* exit;
21433:     UnstableExit* next;
21433: };
21433: 
36361: struct LinkableFragment : public VMFragment
36361: {
62032:     LinkableFragment(const void* _ip, nanojit::Allocator* alloc, Oracle *oracle
36361:                      verbose_only(, uint32_t profFragID))
62032:         : VMFragment(_ip verbose_only(, profFragID)), typeMap(alloc, oracle), nStackTypes(0)
36361:     { }
36361: 
36361:     uint32                  branchCount;
24246:     TypeMap                 typeMap;
24491:     unsigned                nStackTypes;
42707:     unsigned                spOffsetAtEntry;
24491:     SlotList*               globalSlots;
36361: };
36361: 
36361: /*
36361:  * argc is cx->fp->argc at the trace loop header, i.e., the number of arguments
36361:  * pushed for the innermost JS frame. This is required as part of the fragment
36361:  * key because the fragment will write those arguments back to the interpreter
36361:  * stack when it exits, using its typemap, which implicitly incorporates a
36361:  * given value of argc. Without this feature, a fragment could be called as an
36361:  * inner tree with two different values of argc, and entry type checking or
36361:  * exit frame synthesis could crash.
36361:  */
36361: struct TreeFragment : public LinkableFragment
36361: {
62032:     TreeFragment(const void* _ip, nanojit::Allocator* alloc, Oracle *oracle, JSObject* _globalObj,
36361:                  uint32 _globalShape, uint32 _argc verbose_only(, uint32_t profFragID)):
62032:         LinkableFragment(_ip, alloc, oracle verbose_only(, profFragID)),
36361:         first(NULL),
36361:         next(NULL),
36361:         peer(NULL),
36361:         globalObj(_globalObj),
36361:         globalShape(_globalShape),
36361:         argc(_argc),
36361:         dependentTrees(alloc),
36361:         linkedTrees(alloc),
36361:         sideExits(alloc),
36361:         gcthings(alloc),
52503:         shapes(alloc)
36361:     { }
36361: 
36361:     TreeFragment* first;
36361:     TreeFragment* next;
36361:     TreeFragment* peer;
36361:     JSObject* globalObj;
36361:     uint32 globalShape;
36361:     uint32 argc;
25491:     /* Dependent trees must be trashed if this tree dies, and updated on missing global types */
35044:     Queue<TreeFragment*>    dependentTrees;
25491:     /* Linked trees must be updated on missing global types, but are not dependent */
35044:     Queue<TreeFragment*>    linkedTrees;
25627: #ifdef DEBUG
25627:     const char*             treeFileName;
25627:     uintN                   treeLineNumber;
25627:     uintN                   treePCOffset;
25627: #endif
36361:     JSScript*               script;
36361:     UnstableExit*           unstableExits;
36361:     Queue<VMSideExit*>      sideExits;
36361:     ptrdiff_t               nativeStackBase;
36361:     unsigned                maxCallDepth;
36361:     /* All embedded GC things are registered here so the GC can scan them. */
48470:     Queue<Value>            gcthings;
52503:     Queue<const js::Shape*> shapes;
36361:     unsigned                maxNativeStackSlots;
56551:     /* Gives the number of times we have entered this trace. */
53324:     uintN                   execs;
56551:     /* Gives the total number of iterations executed by the trace (up to a limit). */
56551:     uintN                   iters;
24246: 
24491:     inline unsigned nGlobalTypes() {
24491:         return typeMap.length() - nStackTypes;
24246:     }
48470:     inline JSValueType* globalTypeMap() {
24491:         return typeMap.data() + nStackTypes;
24246:     }
48470:     inline JSValueType* stackTypeMap() {
24246:         return typeMap.data();
24246:     }
31495: 
46181:     JS_REQUIRES_STACK void initialize(JSContext* cx, SlotList *globalSlots, bool speculate);
31495:     UnstableExit* removeUnstableExit(VMSideExit* exit);
17413: };
17413: 
36361: inline TreeFragment*
36361: VMFragment::toTreeFragment()
36361: {
36361:     JS_ASSERT(root == this);
36361:     return static_cast<TreeFragment*>(this);
36361: }
36361: 
56551: enum MonitorResult {
56551:     MONITOR_RECORDING,
56551:     MONITOR_NOT_RECORDING,
56551:     MONITOR_ERROR
56551: };
56551: 
56551: const uintN PROFILE_MAX_INNER_LOOPS = 8;
56603: const uintN PROFILE_MAX_STACK = 6;
56551: 
56551: /*
56551:  * A loop profile keeps track of the instruction mix of a hot loop. We use this
56551:  * information to predict whether tracing would be beneficial for the loop.
56551:  */
56551: class LoopProfile
56551: {
56551: public:
56551:     /* Instructions are divided into a few categories. */
56551:     enum OpKind {
56551:         OP_FLOAT, // Floating point arithmetic
56551:         OP_INT, // Integer arithmetic
56551:         OP_BIT, // Bit operations
56551:         OP_EQ, // == and !=
56551:         OP_EVAL, // Calls to eval()
56551:         OP_CALL, // JSOP_CALL instructions
56551:         OP_FWDJUMP, // Jumps with positive delta
56551:         OP_NEW, // JSOP_NEW instructions
56551:         OP_RECURSIVE, // Recursive calls
60543:         OP_ARRAY_READ, // Reads from dense arrays
56562:         OP_TYPED_ARRAY, // Accesses to typed arrays
64279:         OP_SCRIPTED_GETTER, // Getters defined in JS
56551:         OP_LIMIT
56551:     };
56551: 
62032:     /* The TraceMonitor for which we're profiling. */
62032:     TraceMonitor *traceMonitor;
62032:     
56551:     /* The script in which the loop header lives. */
60158:     JSScript *entryScript;
60158: 
60158:     /* The stack frame where we started profiling. Only valid while profiling! */
69223:     StackFrame *entryfp;
56551: 
56551:     /* The bytecode locations of the loop header and the back edge. */
56551:     jsbytecode *top, *bottom;
56551: 
56551:     /* Number of times we have seen this loop executed; used to decide when to profile. */
56551:     uintN hits;
56551: 
56551:     /* Whether we have run a complete profile of the loop. */
56551:     bool profiled;
56551: 
60534:     /* Sometimes we can't decide in one profile run whether to trace, so we set undecided. */
60534:     bool undecided;
60534: 
56551:     /* If we have profiled the loop, this saves the decision of whether to trace it. */
56551:     bool traceOK;
56551: 
60542:     /* Memoized value of isCompilationUnprofitable. */
60542:     bool unprofitable;
60542: 
56551:     /*
56551:      * Sometimes loops are not good tracing opportunities, but they are nested inside
56551:      * loops that we want to trace. In that case, we set their traceOK flag to true,
56551:      * but we set execOK to false. That way, the loop is traced so that it can be
56551:      * integrated into the outer trace. But we never execute the trace on its only.
56551:      */
56551:     bool execOK;
56551: 
56551:     /* Instruction mix for the loop and total number of instructions. */
56551:     uintN allOps[OP_LIMIT];
56551:     uintN numAllOps;
56551: 
56551:     /* Instruction mix and total for the loop, excluding nested inner loops. */
56551:     uintN selfOps[OP_LIMIT];
56551:     uintN numSelfOps;
56551: 
56551:     /*
56551:      * A prediction of the number of instructions we would have to compile
56551:      * for the loop. This takes into account the fact that a branch may cause us to
56551:      * compile every instruction after it twice. Polymorphic calls are
56551:      * treated as n-way branches.
56551:      */
56551:     double numSelfOpsMult;
56551: 
56551:     /*
56551:      * This keeps track of the number of times that every succeeding instruction
56551:      * in the trace will have to be compiled. Every time we hit a branch, we
56551:      * double this number. Polymorphic calls multiply it by n (for n-way
56551:      * polymorphism).
56551:      */
56551:     double branchMultiplier;
56551: 
56551:     /* Set to true if the loop is short (i.e., has fewer than 8 iterations). */
56551:     bool shortLoop;
56551: 
56551:     /* Set to true if the loop may be short (has few iterations at profiling time). */
56551:     bool maybeShortLoop;
56551: 
56551:     /*
56551:      * When we hit a nested loop while profiling, we record where it occurs
56551:      * and how many iterations we execute it.
56551:      */
56551:     struct InnerLoop {
69223:         StackFrame *entryfp;
56551:         jsbytecode *top, *bottom;
56551:         uintN iters;
56551: 
56551:         InnerLoop() {}
69223:         InnerLoop(StackFrame *entryfp, jsbytecode *top, jsbytecode *bottom)
60158:             : entryfp(entryfp), top(top), bottom(bottom), iters(0) {}
56551:     };
56551: 
56551:     /* These two variables track all the inner loops seen while profiling (up to a limit). */
56551:     InnerLoop innerLoops[PROFILE_MAX_INNER_LOOPS];
56551:     uintN numInnerLoops;
56551: 
56551:     /*
56551:      * These two variables track the loops that we are currently nested
56551:      * inside while profiling. Loops get popped off here when they exit.
56551:      */
56551:     InnerLoop loopStack[PROFILE_MAX_INNER_LOOPS];
56551:     uintN loopStackDepth;
56551: 
56603:     /*
56603:      * These fields keep track of values on the JS stack. If the stack grows larger
56603:      * than PROFILE_MAX_STACK, we continue to track sp, but we return conservative results
56603:      * for stackTop().
56603:      */
56603:     struct StackValue {
56603:         bool isConst;
56603:         bool hasValue;
56603:         int value;
56603: 
56603:         StackValue() : isConst(false), hasValue(false) {}
56603:         StackValue(bool isConst) : isConst(isConst), hasValue(false) {}
56603:         StackValue(bool isConst, int value) : isConst(isConst), hasValue(true), value(value) {}
56603:     };
56603:     StackValue stack[PROFILE_MAX_STACK];
56603:     uintN sp;
56603: 
56603:     inline void stackClear() { sp = 0; }
56603:     
56603:     inline void stackPush(const StackValue &v) {
56603:         if (sp < PROFILE_MAX_STACK)
56603:             stack[sp++] = v;
56603:         else
56603:             stackClear();
56603:     }
56603: 
56603:     inline void stackPop() { if (sp > 0) sp--; }
56603: 
56603:     inline StackValue stackAt(int pos) {
56603:         pos += sp;
56603:         if (pos >= 0 && uintN(pos) < PROFILE_MAX_STACK)
56603:             return stack[pos];
56603:         else
56603:             return StackValue(false);
56603:     }
56603:     
69223:     LoopProfile(TraceMonitor *tm, StackFrame *entryfp, jsbytecode *top, jsbytecode *bottom);
57798: 
60157:     void reset();
60157: 
56551:     enum ProfileAction {
56551:         ProfContinue,
56551:         ProfComplete
56551:     };
56551: 
56551:     /* These two functions track the instruction mix. */
56551:     inline void increment(OpKind kind)
56551:     {
56551:         allOps[kind]++;
56551:         if (loopStackDepth == 0)
56551:             selfOps[kind]++;
56551:     }
56551: 
56551:     inline uintN count(OpKind kind) { return allOps[kind]; }
56551: 
56551:     /* Called for every back edge being profiled. */
56551:     MonitorResult profileLoopEdge(JSContext* cx, uintN& inlineCallCount);
56551:     
56551:     /* Called for every instruction being profiled. */
56551:     ProfileAction profileOperation(JSContext *cx, JSOp op);
56551: 
56551:     /* Once a loop's profile is done, these decide whether it should be traced. */
57802:     bool isCompilationExpensive(JSContext *cx, uintN depth);
60542:     bool isCompilationUnprofitable(JSContext *cx, uintN goodOps);
56551:     void decide(JSContext *cx);
62032: 
62032:     void stopProfiling(JSContext *cx);
56551: };
56551: 
53545: /*
53545:  * BUILTIN_NO_FIXUP_NEEDED indicates that after the initial LeaveTree of a deep
53545:  * bail, the builtin call needs no further fixup when the trace exits and calls
53545:  * LeaveTree the second time.
53545:  */
37741: typedef enum BuiltinStatus {
37741:     BUILTIN_BAILED = 1,
59879:     BUILTIN_ERROR = 2
37741: } BuiltinStatus;
27166: 
40237: static JS_INLINE void
62032: SetBuiltinError(TraceMonitor *tm)
40237: {
62032:     tm->tracerState->builtinStatus |= BUILTIN_ERROR;
60574: }
60574: 
60574: static JS_INLINE bool
62032: WasBuiltinSuccessful(TraceMonitor *tm)
60574: {
62032:     return tm->tracerState->builtinStatus == 0;
40237: }
27166: 
33542: #ifdef DEBUG_RECORDING_STATUS_NOT_BOOL
33542: /* #define DEBUG_RECORDING_STATUS_NOT_BOOL to detect misuses of RecordingStatus */
33542: struct RecordingStatus {
27933:     int code;
33542:     bool operator==(RecordingStatus &s) { return this->code == s.code; };
33542:     bool operator!=(RecordingStatus &s) { return this->code != s.code; };
23111: };
33542: enum RecordingStatusCodes {
33542:     RECORD_ERROR_code     = 0,
33542:     RECORD_STOP_code      = 1,
33542: 
33542:     RECORD_CONTINUE_code  = 3,
33542:     RECORD_IMACRO_code    = 4
27933: };
33542: RecordingStatus RECORD_CONTINUE = { RECORD_CONTINUE_code };
33542: RecordingStatus RECORD_STOP     = { RECORD_STOP_code };
33542: RecordingStatus RECORD_IMACRO   = { RECORD_IMACRO_code };
33542: RecordingStatus RECORD_ERROR    = { RECORD_ERROR_code };
33542: 
33542: struct AbortableRecordingStatus {
33542:     int code;
33542:     bool operator==(AbortableRecordingStatus &s) { return this->code == s.code; };
33542:     bool operator!=(AbortableRecordingStatus &s) { return this->code != s.code; };
33542: };
33542: enum AbortableRecordingStatusCodes {
33542:     ARECORD_ERROR_code     = 0,
33542:     ARECORD_STOP_code      = 1,
33542:     ARECORD_ABORTED_code   = 2,
33542:     ARECORD_CONTINUE_code  = 3,
35083:     ARECORD_IMACRO_code    = 4,
43801:     ARECORD_IMACRO_ABORTED_code = 5,
43801:     ARECORD_COMPLETED_code = 6
33542: };
33542: AbortableRecordingStatus ARECORD_ERROR    = { ARECORD_ERROR_code };
33542: AbortableRecordingStatus ARECORD_STOP     = { ARECORD_STOP_code };
33542: AbortableRecordingStatus ARECORD_CONTINUE = { ARECORD_CONTINUE_code };
33542: AbortableRecordingStatus ARECORD_IMACRO   = { ARECORD_IMACRO_code };
43801: AbortableRecordingStatus ARECORD_IMACRO_ABORTED   = { ARECORD_IMACRO_ABORTED_code };
33542: AbortableRecordingStatus ARECORD_ABORTED =  { ARECORD_ABORTED_code };
35083: AbortableRecordingStatus ARECORD_COMPLETED =  { ARECORD_COMPLETED_code };
33542: 
33542: static inline AbortableRecordingStatus
33542: InjectStatus(RecordingStatus rs)
33542: {
33542:     AbortableRecordingStatus ars = { rs.code };
33542:     return ars;
33542: }
33542: static inline AbortableRecordingStatus
33542: InjectStatus(AbortableRecordingStatus ars)
33542: {
33542:     return ars;
33542: }
33542: 
33542: static inline bool
41777: StatusAbortsRecorderIfActive(AbortableRecordingStatus ars)
33542: {
35083:     return ars == ARECORD_ERROR || ars == ARECORD_STOP;
33542: }
27933: #else
33542: 
33542: /*
33542:  * Normally, during recording, when the recorder cannot continue, it returns
33542:  * ARECORD_STOP to indicate that recording should be aborted by the top-level
33542:  * recording function. However, if the recorder reenters the interpreter (e.g.,
33542:  * when executing an inner loop), there will be an immediate abort. This
33542:  * condition must be carefully detected and propagated out of all nested
33542:  * recorder calls lest the now-invalid TraceRecorder object be accessed
33542:  * accidentally. This condition is indicated by the ARECORD_ABORTED value.
33542:  *
33542:  * The AbortableRecordingStatus enumeration represents the general set of
33542:  * possible results of calling a recorder function. Functions that cannot
33542:  * possibly return ARECORD_ABORTED may statically guarantee this to the caller
33542:  * using the RecordingStatus enumeration. Ideally, C++ would allow subtyping
33542:  * of enumerations, but it doesn't. To simulate subtype conversion manually,
33542:  * code should call InjectStatus to inject a value of the restricted set into a
33542:  * value of the general set.
33542:  */
33542: 
33542: enum RecordingStatus {
41777:     RECORD_STOP       = 0,  // Recording should be aborted at the top-level
33542:                             // call to the recorder.
41777:     RECORD_ERROR      = 1,  // Recording should be aborted at the top-level
41777:                             // call to the recorder and the interpreter should
41777:                             // goto error
41777:     RECORD_CONTINUE   = 2,  // Continue recording.
41777:     RECORD_IMACRO     = 3   // Entered imacro; continue recording.
27933:                             // Only JSOP_IS_IMACOP opcodes may return this.
27933: };
33542: 
33542: enum AbortableRecordingStatus {
41777:     ARECORD_STOP      = 0,  // see RECORD_STOP
41777:     ARECORD_ERROR     = 1,  // Recording may or may not have been aborted.
41777:                             // Recording should be aborted at the top-level
41777:                             // if it has not already been and the interpreter
41777:                             // should goto error
41777:     ARECORD_CONTINUE  = 2,  // see RECORD_CONTINUE
41777:     ARECORD_IMACRO    = 3,  // see RECORD_IMACRO
43801:     ARECORD_IMACRO_ABORTED = 4, // see comment in TR::monitorRecording.
43801:     ARECORD_ABORTED   = 5,  // Recording has already been aborted; the
41777:                             // interpreter should continue executing
43801:     ARECORD_COMPLETED = 6   // Recording completed successfully, the
41777:                             // trace recorder has been deleted
33542: };
33542: 
33542: static JS_ALWAYS_INLINE AbortableRecordingStatus
33542: InjectStatus(RecordingStatus rs)
33542: {
33542:     return static_cast<AbortableRecordingStatus>(rs);
33542: }
33542: 
33542: static JS_ALWAYS_INLINE AbortableRecordingStatus
33542: InjectStatus(AbortableRecordingStatus ars)
33542: {
33542:     return ars;
33542: }
33542: 
35083: /*
35083:  * Return whether the recording status requires the current recording session
41777:  * to be deleted. ERROR means the recording session should be deleted if it
41777:  * hasn't already. ABORTED and COMPLETED indicate the recording session is
35083:  * already deleted, so they return 'false'.
35083:  */
33542: static JS_ALWAYS_INLINE bool
41777: StatusAbortsRecorderIfActive(AbortableRecordingStatus ars)
33542: {
41777:     return ars <= ARECORD_ERROR;
33542: }
27933: #endif
27933: 
31473: class SlotMap;
33564: class SlurpInfo;
27933: 
31473: /* Results of trying to compare two typemaps together */
31473: enum TypeConsensus
31473: {
31473:     TypeConsensus_Okay,         /* Two typemaps are compatible */
31473:     TypeConsensus_Undemotes,    /* Not compatible now, but would be with pending undemotes. */
31473:     TypeConsensus_Bad           /* Typemaps are not compatible */
31473: };
23111: 
53133: enum TracePointAction {
53133:     TPA_Nothing,
53133:     TPA_RanStuff,
53133:     TPA_Recorded,
53133:     TPA_Error
53133: };
53133: 
38568: typedef HashMap<nanojit::LIns*, JSObject*> GuardedShapeTable;
38568: 
35083: #ifdef DEBUG
37741: # define AbortRecording(cx, reason) AbortRecordingImpl(cx, reason)
35083: #else
37741: # define AbortRecording(cx, reason) AbortRecordingImpl(cx)
35083: #endif
35083: 
56551: void
56551: AbortProfiling(JSContext *cx);
56551: 
34351: class TraceRecorder
34351: {
64559:     JS_DECLARE_ALLOCATION_FRIENDS_FOR_PRIVATE_CONSTRUCTOR;
64559: 
34351:     /*************************************************************** Recording session constants */
34351: 
34351:     /* The context in which recording started. */
34351:     JSContext* const                cx;
34351: 
34351:     /* Cached value of JS_TRACE_MONITOR(cx). */
37741:     TraceMonitor* const             traceMonitor;
34351: 
41802:     /* Cached oracle keeps track of hit counts for program counter locations */
41802:     Oracle*                         oracle;
41802: 
34351:     /* The Fragment being recorded by this recording session. */
35044:     VMFragment* const               fragment;
34351: 
36361:     /* The root fragment representing the tree. */
36361:     TreeFragment* const             tree;
34351: 
34351:     /* The global object from the start of recording until now. */
34351:     JSObject* const                 globalObj;
34351: 
56217:     /* If non-null, the script of outer loop aborted to start recording this loop. */
56217:     JSScript* const                 outerScript;
34351:     
56217:     /* If non-null, the pc of the outer loop aborted to start recording this loop. */
56217:     jsbytecode* const               outerPC;
56217: 
56217:     /* If |outerPC|, the argc to use when looking up |outerPC| in the fragments table. */
34351:     uint32 const                    outerArgc;
34351: 
34351:     /* If non-null, the side exit from which we are growing. */
34351:     VMSideExit* const               anchor;
34351: 
41276:     /* Instructions yielding the corresponding trace-const members of TracerState. */
34351:     nanojit::LIns* const            cx_ins;
34351:     nanojit::LIns* const            eos_ins;
34351:     nanojit::LIns* const            eor_ins;
34351:     nanojit::LIns* const            loopLabel;
34351: 
36401:     /* Lazy slot import state. */
36401:     unsigned                        importStackSlots;
36401:     unsigned                        importGlobalSlots;
36401:     TypeMap                         importTypeMap;
36401: 
34351:     /*
34351:      * The LirBuffer used to supply memory to our LirWriter pipeline. Also contains the most recent
34351:      * instruction for {sp, rp, state}. Also contains names for debug JIT spew. Should be split.
34351:      */
34351:     nanojit::LirBuffer* const       lirbuf;
34351: 
34351:     /*
34351:      * Remembers traceAlloc state before recording started; automatically rewinds when mark is
34351:      * destroyed on a failed compilation.
34351:      */
33545:     VMAllocator::Mark               mark;
34351: 
35085:     /* Remembers the number of sideExits in treeInfo before recording started. */
35085:     const unsigned                  numSideExitsBefore;
35085: 
34351:     /*********************************************************** Recording session mutable state */
34351: 
34351:     /* Maps interpreter stack values to the instruction generating that value. */
17596:     Tracker                         tracker;
34351: 
34351:     /* Maps interpreter stack values to the instruction writing back to the native stack. */
17815:     Tracker                         nativeFrameTracker;
34351: 
56180:     /* The start of the global object's slots we assume for the trackers. */
56180:     Value*                          global_slots;
34351: 
34351:     /* The number of interpreted calls entered (and not yet left) since recording began. */
17789:     unsigned                        callDepth;
34351: 
34351:     /* The current atom table, mirroring the interpreter loop's variable of the same name. */
17611:     JSAtom**                        atoms;
48470:     Value*                          consts;
34351: 
54169:     /* An instruction yielding the current script's strict mode code flag.  */
54169:     nanojit::LIns*                  strictModeCode_ins;
54169: 
34351:     /* FIXME: Dead, but soon to be used for something or other. */
34351:     Queue<jsbytecode*>              cfgMerges;
34351: 
34351:     /* Indicates whether the current tree should be trashed when the recording session ends. */
34351:     bool                            trashSelf;
34351: 
34351:     /* A list of trees to trash at the end of the recording session. */
35044:     Queue<TreeFragment*>            whichTreesToTrash;
34351: 
38568:     /* The set of objects whose shapes already have been guarded. */
38568:     GuardedShapeTable               guardedShapeTable;
38568: 
58056:     /* Current initializer depth, and whether any of the initializers are unoptimized NEWINIT. */
58056:     int                             initDepth;
58056:     bool                            hadNewInit;
58056: 
60780: #ifdef DEBUG
60780:     /*
60780:      * If we are expecting a record_AddProperty callback for this instruction,
60780:      * the shape of the object before adding the data property. Else NULL.
60780:      */
60780:     const js::Shape* addPropShapeBefore;
60780: #endif
60780: 
34351:     /***************************************** Temporal state hoisted into the recording session */
34351: 
34351:     /* Carry the return value from a STOP/RETURN to the subsequent record_LeaveFrame. */
17818:     nanojit::LIns*                  rval_ins;
34351: 
34351:     /* Carry the return value from a native call to the record_NativeCallComplete. */
28086:     nanojit::LIns*                  native_rval_ins;
34351: 
55503:     /* Carry the return value of js_CreateThis to record_NativeCallComplete. */
28086:     nanojit::LIns*                  newobj_ins;
34351: 
34351:     /* Carry the JSSpecializedNative used to generate a call to record_NativeCallComplete. */
34351:     JSSpecializedNative*            pendingSpecializedNative;
34351: 
34351:     /* Carry whether this is a jsval on the native stack from finishGetProp to monitorRecording. */
48470:     Value*                          pendingUnboxSlot;
34351: 
34351:     /* Carry a guard condition to the beginning of the next monitorRecording. */
34351:     nanojit::LIns*                  pendingGuardCondition;
34351: 
59878:     /* See AbortRecordingIfUnexpectedGlobalWrite. */
60559:     js::Vector<unsigned>            pendingGlobalSlotsToSet;
59878: 
34351:     /* Carry whether we have an always-exit from emitIf to checkTraceEnd. */
34351:     bool                            pendingLoop;
34351: 
34351:     /* Temporary JSSpecializedNative used to describe non-specialized fast natives. */
32669:     JSSpecializedNative             generatedSpecializedNative;
34351: 
48470:     /* Temporary JSValueType array used to construct temporary typemaps. */
48470:     js::Vector<JSValueType, 256>    tempTypeMap;
17323: 
56750:     /* Used to generate LIR.  Has a short name because it's used a lot. */
56750:     tjit::Writer w;
56750: 
34351:     /************************************************************* 10 bajillion member functions */
34351: 
56180:     /*
56750:      * These would be in Writer if they didn't modify TraceRecorder state.
56750:      * They are invoked the via macros below that make them look like they are
56750:      * part of Writer (hence the "w_" prefix, which looks like "w.").
56180:      */
56750:     nanojit::LIns* w_immpObjGC(JSObject* obj);
56750:     nanojit::LIns* w_immpFunGC(JSFunction* fun);
56750:     nanojit::LIns* w_immpStrGC(JSString* str);
56750:     nanojit::LIns* w_immpShapeGC(const js::Shape* shape);
56750:     nanojit::LIns* w_immpIdGC(jsid id);
56180: 
56750:     #define immpObjGC(obj)        name(w_immpObjGC(obj), #obj)
56750:     #define immpFunGC(fun)        name(w_immpFunGC(fun), #fun)
56750:     #define immpStrGC(str)        name(w_immpStrGC(str), #str)
64345:     #define immpAtomGC(atom)      name(w_immpStrGC(atom), "ATOM(" #atom ")")
56750:     #define immpShapeGC(shape)    name(w_immpShapeGC(shape), #shape)
56750:     #define immpIdGC(id)          name(w_immpIdGC(id), #id)
31843: 
34351:     /*
34351:      * Examines current interpreter state to record information suitable for returning to the
34351:      * interpreter through a side exit of the given type.
34351:      */
34351:     JS_REQUIRES_STACK VMSideExit* snapshot(ExitType exitType);
34351: 
34351:     /*
34351:      * Creates a separate but identical copy of the given side exit, allowing the guards associated
34351:      * with each to be entirely separate even after subsequent patching.
34351:      */
34351:     JS_REQUIRES_STACK VMSideExit* copy(VMSideExit* exit);
34351: 
34351:     /*
34351:      * Creates an instruction whose payload is a GuardRecord for the given exit.  The instruction
34351:      * is suitable for use as the final argument of a single call to LirBuffer::insGuard; do not
34351:      * reuse the returned value.
34351:      */
34351:     JS_REQUIRES_STACK nanojit::GuardRecord* createGuardRecord(VMSideExit* exit);
34351: 
41802:     JS_REQUIRES_STACK JS_INLINE void markSlotUndemotable(LinkableFragment* f, unsigned slot);
41802: 
41802:     JS_REQUIRES_STACK JS_INLINE void markSlotUndemotable(LinkableFragment* f, unsigned slot, const void* pc);
41802: 
41802:     JS_REQUIRES_STACK unsigned findUndemotesInTypemaps(const TypeMap& typeMap, LinkableFragment* f,
41802:                             Queue<unsigned>& undemotes);
41802: 
41802:     JS_REQUIRES_STACK void assertDownFrameIsConsistent(VMSideExit* anchor, FrameInfo* fi);
41802: 
48470:     JS_REQUIRES_STACK void captureStackTypes(unsigned callDepth, JSValueType* typeMap);
41802: 
48470:     bool isVoidPtrGlobal(const void* p) const;
48470:     bool isGlobal(const Value* p) const;
48470:     ptrdiff_t nativeGlobalSlot(const Value *p) const;
48470:     ptrdiff_t nativeGlobalOffset(const Value* p) const;
48470:     JS_REQUIRES_STACK ptrdiff_t nativeStackOffsetImpl(const void* p) const;
48470:     JS_REQUIRES_STACK ptrdiff_t nativeStackOffset(const Value* p) const;
48470:     JS_REQUIRES_STACK ptrdiff_t nativeStackSlotImpl(const void* p) const;
48470:     JS_REQUIRES_STACK ptrdiff_t nativeStackSlot(const Value* p) const;
48470:     JS_REQUIRES_STACK ptrdiff_t nativespOffsetImpl(const void* p) const;
48470:     JS_REQUIRES_STACK ptrdiff_t nativespOffset(const Value* p) const;
56750:     JS_REQUIRES_STACK void importImpl(tjit::Address addr, const void* p, JSValueType t,
69223:                                       const char *prefix, uintN index, StackFrame *fp);
56750:     JS_REQUIRES_STACK void import(tjit::Address addr, const Value* p, JSValueType t,
69223:                                   const char *prefix, uintN index, StackFrame *fp);
36361:     JS_REQUIRES_STACK void import(TreeFragment* tree, nanojit::LIns* sp, unsigned stackSlots,
48470:                                   unsigned callDepth, unsigned ngslots, JSValueType* typeMap);
17815:     void trackNativeStackUse(unsigned slots);
17381: 
52503:     JS_REQUIRES_STACK bool isValidSlot(JSObject *obj, const js::Shape* shape);
22652:     JS_REQUIRES_STACK bool lazilyImportGlobalSlot(unsigned slot);
36401:     JS_REQUIRES_STACK void importGlobalSlot(unsigned slot);
17891: 
56222:     void ensureCond(nanojit::LIns** ins, bool* cond);
56222: 
55556:     JS_REQUIRES_STACK RecordingStatus guard(bool expected, nanojit::LIns* cond, ExitType exitType,
55556:                                             bool abortIfAlwaysExits = false);
55556:     JS_REQUIRES_STACK RecordingStatus guard(bool expected, nanojit::LIns* cond, VMSideExit* exit,
55556:                                             bool abortIfAlwaysExits = false);
23111: 
32700:     nanojit::LIns* writeBack(nanojit::LIns* i, nanojit::LIns* base, ptrdiff_t offset,
59993:                              bool shouldDemoteToInt32);
31444: 
48470: #ifdef DEBUG
53840:     bool isValidFrameObjPtr(void *obj);
48470: #endif
57807:     void assertInsideLoop();
48470: 
59993:     JS_REQUIRES_STACK void setImpl(void* p, nanojit::LIns* l, bool shouldDemoteToInt32 = true);
59993:     JS_REQUIRES_STACK void set(Value* p, nanojit::LIns* l, bool shouldDemoteToInt32 = true);
59993:     JS_REQUIRES_STACK void setFrameObjPtr(void* p, nanojit::LIns* l,
59993:                                           bool shouldDemoteToInt32 = true);
48470:     nanojit::LIns* getFromTrackerImpl(const void *p);
48470:     nanojit::LIns* getFromTracker(const Value* p);
48470:     JS_REQUIRES_STACK nanojit::LIns* getImpl(const void* p);
48470:     JS_REQUIRES_STACK nanojit::LIns* get(const Value* p);
53840:     JS_REQUIRES_STACK nanojit::LIns* getFrameObjPtr(void* p);
48470:     JS_REQUIRES_STACK nanojit::LIns* attemptImport(const Value* p);
48470:     JS_REQUIRES_STACK nanojit::LIns* addr(Value* p);
48470: 
48470:     JS_REQUIRES_STACK bool knownImpl(const void* p);
48470:     JS_REQUIRES_STACK bool known(const Value* p);
48470:     JS_REQUIRES_STACK bool known(JSObject** p);
53559:     /*
56180:      * The slots of the global object are sometimes reallocated by the
53559:      * interpreter.  This function checks for that condition and re-maps the
53559:      * entries of the tracker accordingly.
53559:      */
53559:     JS_REQUIRES_STACK void checkForGlobalObjectReallocation() {
56180:         if (global_slots != globalObj->getSlots())
53559:             checkForGlobalObjectReallocationHelper();
53559:     }
53559:     JS_REQUIRES_STACK void checkForGlobalObjectReallocationHelper();
17320: 
31473:     JS_REQUIRES_STACK TypeConsensus selfTypeStability(SlotMap& smap);
33564:     JS_REQUIRES_STACK TypeConsensus peerTypeStability(SlotMap& smap, const void* ip,
35044:                                                       TreeFragment** peer);
17410: 
48470:     JS_REQUIRES_STACK Value& argval(unsigned n) const;
48470:     JS_REQUIRES_STACK Value& varval(unsigned n) const;
48470:     JS_REQUIRES_STACK Value& stackval(int n) const;
48470: 
48470:     JS_REQUIRES_STACK void updateAtoms();
48470:     JS_REQUIRES_STACK void updateAtoms(JSScript *script);
17412: 
31075:     struct NameResult {
31075:         // |tracked| is true iff the result of the name lookup is a variable that
31075:         // is already in the tracker. The rest of the fields are set only if
31075:         // |tracked| is false.
31075:         bool             tracked;
48470:         Value            v;              // current property value
31075:         JSObject         *obj;           // Call object where name was found
32593:         nanojit::LIns    *obj_ins;       // LIR value for obj
52503:         js::Shape        *shape;         // shape name was resolved to
31075:     };
31075: 
37694:     JS_REQUIRES_STACK nanojit::LIns* scopeChain();
37694:     JS_REQUIRES_STACK nanojit::LIns* entryScopeChain() const;
51446:     JS_REQUIRES_STACK nanojit::LIns* entryFrameIns() const;
69223:     JS_REQUIRES_STACK StackFrame* frameIfInRange(JSObject* obj, unsigned* depthp = NULL) const;
33542:     JS_REQUIRES_STACK RecordingStatus traverseScopeChain(JSObject *obj, nanojit::LIns *obj_ins, JSObject *obj2, nanojit::LIns *&obj2_ins);
63257:     JS_REQUIRES_STACK AbortableRecordingStatus scopeChainProp(JSObject* obj, Value*& vp, nanojit::LIns*& ins, NameResult& nr, JSObject **scopeObjp = NULL);
52503:     JS_REQUIRES_STACK RecordingStatus callProp(JSObject* obj, JSProperty* shape, jsid id, Value*& vp, nanojit::LIns*& ins, NameResult& nr);
18286: 
22652:     JS_REQUIRES_STACK nanojit::LIns* arg(unsigned n);
22652:     JS_REQUIRES_STACK void arg(unsigned n, nanojit::LIns* i);
22652:     JS_REQUIRES_STACK nanojit::LIns* var(unsigned n);
22652:     JS_REQUIRES_STACK void var(unsigned n, nanojit::LIns* i);
48470:     JS_REQUIRES_STACK nanojit::LIns* upvar(JSScript* script, JSUpvarArray* uva, uintN index, Value& v);
56750:     nanojit::LIns* stackLoad(tjit::Address addr, uint8 type);
22652:     JS_REQUIRES_STACK nanojit::LIns* stack(int n);
22652:     JS_REQUIRES_STACK void stack(int n, nanojit::LIns* i);
17412: 
56185:     JS_REQUIRES_STACK void guardNonNeg(nanojit::LIns* d0, nanojit::LIns* d1, VMSideExit* exit);
64315:     JS_REQUIRES_STACK nanojit::LIns* tryToDemote(nanojit::LOpcode op, jsdouble v0, jsdouble v1,
21799:                                                  nanojit::LIns* s0, nanojit::LIns* s1);
40307: 
41987:     nanojit::LIns* d2i(nanojit::LIns* f, bool resultCanBeImpreciseIfFractional = false);
56750:     nanojit::LIns* d2u(nanojit::LIns* d);
55556:     JS_REQUIRES_STACK RecordingStatus makeNumberInt32(nanojit::LIns* d, nanojit::LIns** num_ins);
59913:     JS_REQUIRES_STACK RecordingStatus makeNumberUint32(nanojit::LIns* d, nanojit::LIns** num_ins);
48470:     JS_REQUIRES_STACK nanojit::LIns* stringify(const Value& v);
21685: 
62943:     JS_REQUIRES_STACK nanojit::LIns* newArguments(nanojit::LIns* callee_ins);
32709: 
41290:     JS_REQUIRES_STACK bool canCallImacro() const;
41290:     JS_REQUIRES_STACK RecordingStatus callImacro(jsbytecode* imacro);
41290:     JS_REQUIRES_STACK RecordingStatus callImacroInfallibly(jsbytecode* imacro);
17469: 
33542:     JS_REQUIRES_STACK AbortableRecordingStatus ifop();
33542:     JS_REQUIRES_STACK RecordingStatus switchop();
48470:     JS_REQUIRES_STACK RecordingStatus inc(Value& v, jsint incr, bool pre = true);
59903:     JS_REQUIRES_STACK RecordingStatus inc(const Value &v, nanojit::LIns*& v_ins,
59903:                                           Value &v_out, jsint incr,
27933:                                           bool pre = true);
58282:     JS_REQUIRES_STACK RecordingStatus incHelper(const Value &v, nanojit::LIns*& v_ins,
59903:                                                 Value &v_after,
59903:                                                 nanojit::LIns*& v_ins_after,
59903:                                                 jsint incr);
33542:     JS_REQUIRES_STACK AbortableRecordingStatus incProp(jsint incr, bool pre = true);
33542:     JS_REQUIRES_STACK RecordingStatus incElem(jsint incr, bool pre = true);
33542:     JS_REQUIRES_STACK AbortableRecordingStatus incName(jsint incr, bool pre = true);
18687: 
59890:     JS_REQUIRES_STACK RecordingStatus strictEquality(bool equal, bool cmpCase);
33542:     JS_REQUIRES_STACK AbortableRecordingStatus equality(bool negate, bool tryBranchAfterCond);
48470:     JS_REQUIRES_STACK AbortableRecordingStatus equalityHelper(Value& l, Value& r,
23223:                                                                 nanojit::LIns* l_ins, nanojit::LIns* r_ins,
23223:                                                                 bool negate, bool tryBranchAfterCond,
48470:                                                                 Value& rval);
33542:     JS_REQUIRES_STACK AbortableRecordingStatus relational(nanojit::LOpcode op, bool tryBranchAfterCond);
17467: 
64315:     JS_REQUIRES_STACK RecordingStatus unaryIntOp(nanojit::LOpcode op);
33542:     JS_REQUIRES_STACK RecordingStatus binary(nanojit::LOpcode op);
17467: 
33560:     JS_REQUIRES_STACK RecordingStatus guardShape(nanojit::LIns* obj_ins, JSObject* obj,
40393:                                                  uint32 shape, const char* name, VMSideExit* exit);
17417: 
33560: #if defined DEBUG_notme && defined XP_UNIX
33560:     void dumpGuardedShapes(const char* prefix);
33560: #endif
33560: 
33560:     void forgetGuardedShapes();
33560: 
33542:     JS_REQUIRES_STACK AbortableRecordingStatus test_property_cache(JSObject* obj, nanojit::LIns* obj_ins,
40374:                                                                      JSObject*& obj2, PCVal& pcval);
33542:     JS_REQUIRES_STACK RecordingStatus guardPropertyCacheHit(nanojit::LIns* obj_ins,
30847:                                                             JSObject* aobj,
30847:                                                             JSObject* obj2,
40362:                                                             PropertyCacheEntry* entry,
40374:                                                             PCVal& pcval);
30847: 
48470:     void stobj_set_fslot(nanojit::LIns *obj_ins, unsigned slot, const Value &v,
31849:                          nanojit::LIns* v_ins);
48470:     void stobj_set_dslot(nanojit::LIns *obj_ins, unsigned slot,
56180:                          nanojit::LIns*& slots_ins, const Value &v, nanojit::LIns* v_ins);
55746:     void stobj_set_slot(JSObject *obj, nanojit::LIns* obj_ins, unsigned slot,
56180:                         nanojit::LIns*& slots_ins, const Value &v, nanojit::LIns* v_ins);
22626: 
48470:     nanojit::LIns* unbox_slot(JSObject *obj, nanojit::LIns *obj_ins, uint32 slot,
48470:                               VMSideExit *exit);
55746: 
48470:     JS_REQUIRES_STACK AbortableRecordingStatus name(Value*& vp, nanojit::LIns*& ins, NameResult& nr);
37685:     JS_REQUIRES_STACK AbortableRecordingStatus prop(JSObject* obj, nanojit::LIns* obj_ins,
37685:                                                     uint32 *slotp, nanojit::LIns** v_insp,
48470:                                                     Value* outp);
41290:     JS_REQUIRES_STACK RecordingStatus propTail(JSObject* obj, nanojit::LIns* obj_ins,
40374:                                                JSObject* obj2, PCVal pcval,
37685:                                                uint32 *slotp, nanojit::LIns** v_insp,
48470:                                                Value* outp);
48470:     JS_REQUIRES_STACK RecordingStatus denseArrayElement(Value& oval, Value& idx, Value*& vp,
28411:                                                         nanojit::LIns*& v_ins,
53615:                                                         nanojit::LIns*& addr_ins,
53615:                                                         VMSideExit* exit);
48822:     JS_REQUIRES_STACK nanojit::LIns *canonicalizeNaNs(nanojit::LIns *dval_ins);
48470:     JS_REQUIRES_STACK AbortableRecordingStatus typedArrayElement(Value& oval, Value& idx, Value*& vp,
56750:                                                                  nanojit::LIns*& v_ins);
33542:     JS_REQUIRES_STACK AbortableRecordingStatus getProp(JSObject* obj, nanojit::LIns* obj_ins);
48470:     JS_REQUIRES_STACK AbortableRecordingStatus getProp(Value& v);
33542:     JS_REQUIRES_STACK RecordingStatus getThis(nanojit::LIns*& this_ins);
17758: 
56750:     JS_REQUIRES_STACK void storeMagic(JSWhyMagic why, tjit::Address addr);
42641:     JS_REQUIRES_STACK AbortableRecordingStatus unboxNextValue(nanojit::LIns* &v_ins);
42641: 
32761:     JS_REQUIRES_STACK VMSideExit* enterDeepBailCall();
31444:     JS_REQUIRES_STACK void leaveDeepBailCall();
31444: 
48470:     JS_REQUIRES_STACK RecordingStatus primitiveToStringInPlace(Value* vp);
31444:     JS_REQUIRES_STACK void finishGetProp(nanojit::LIns* obj_ins, nanojit::LIns* vp_ins,
48470:                                          nanojit::LIns* ok_ins, Value* outp);
48470:     JS_REQUIRES_STACK RecordingStatus getPropertyByName(nanojit::LIns* obj_ins, Value* idvalp,
48470:                                                         Value* outp);
33542:     JS_REQUIRES_STACK RecordingStatus getPropertyByIndex(nanojit::LIns* obj_ins,
48470:                                                          nanojit::LIns* index_ins, Value* outp);
48470:     JS_REQUIRES_STACK RecordingStatus getPropertyById(nanojit::LIns* obj_ins, Value* outp);
33542:     JS_REQUIRES_STACK RecordingStatus getPropertyWithNativeGetter(nanojit::LIns* obj_ins,
52503:                                                                   const js::Shape* shape,
48470:                                                                   Value* outp);
41290:     JS_REQUIRES_STACK RecordingStatus getPropertyWithScriptGetter(JSObject *obj,
41290:                                                                   nanojit::LIns* obj_ins,
52503:                                                                   const js::Shape* shape);
31444: 
55556:     JS_REQUIRES_STACK RecordingStatus getCharCodeAt(JSString *str,
55556:                                                     nanojit::LIns* str_ins, nanojit::LIns* idx_ins,
55556:                                                     nanojit::LIns** out_ins);
55505:     JS_REQUIRES_STACK nanojit::LIns* getUnitString(nanojit::LIns* str_ins, nanojit::LIns* idx_ins);
55556:     JS_REQUIRES_STACK RecordingStatus getCharAt(JSString *str,
52500:                                                 nanojit::LIns* str_ins, nanojit::LIns* idx_ins,
55556:                                                 JSOp mode, nanojit::LIns** out_ins);
49109: 
33542:     JS_REQUIRES_STACK RecordingStatus initOrSetPropertyByName(nanojit::LIns* obj_ins,
48470:                                                               Value* idvalp, Value* rvalp,
31829:                                                               bool init);
33542:     JS_REQUIRES_STACK RecordingStatus initOrSetPropertyByIndex(nanojit::LIns* obj_ins,
31829:                                                                nanojit::LIns* index_ins,
48470:                                                                Value* rvalp, bool init);
35466:     JS_REQUIRES_STACK AbortableRecordingStatus setElem(int lval_spindex, int idx_spindex,
35466:                                                        int v_spindex);
30847: 
60780:     JS_REQUIRES_STACK RecordingStatus lookupForSetPropertyOp(JSObject* obj, nanojit::LIns* obj_ins,
60780:                                                              jsid id, bool* safep,
60780:                                                              JSObject** pobjp,
60780:                                                              const js::Shape** shapep);
60780:     JS_REQUIRES_STACK RecordingStatus nativeSet(JSObject* obj, nanojit::LIns* obj_ins,
60780:                                                 const js::Shape* shape,
60780:                                                 const Value& v, nanojit::LIns* v_ins);
60780:     JS_REQUIRES_STACK RecordingStatus addDataProperty(JSObject* obj);
60780:     JS_REQUIRES_STACK RecordingStatus setCallProp(JSObject* callobj, nanojit::LIns* callobj_ins,
60780:                                                   const js::Shape* shape, nanojit::LIns* v_ins,
60780:                                                   const Value& v);
60780:     JS_REQUIRES_STACK RecordingStatus setProperty(JSObject* obj, nanojit::LIns* obj_ins,
60780:                                                   const Value& v, nanojit::LIns* v_ins,
60780:                                                   bool* deferredp);
60780:     JS_REQUIRES_STACK RecordingStatus recordSetPropertyOp();
60780:     JS_REQUIRES_STACK RecordingStatus recordInitPropertyOp(jsbytecode op);
60780: 
56750:     void box_undefined_into(tjit::Address addr);
48470: #if JS_BITS_PER_WORD == 32
56750:     void box_null_into(tjit::Address addr);
56750:     nanojit::LIns* unbox_number_as_double(tjit::Address addr, nanojit::LIns* tag_ins,
56750:                                           VMSideExit* exit);
56750:     nanojit::LIns* unbox_object(tjit::Address addr, nanojit::LIns* tag_ins, JSValueType type,
56750:                                 VMSideExit* exit);
56750:     nanojit::LIns* unbox_non_double_object(tjit::Address addr, nanojit::LIns* tag_ins,
56750:                                            JSValueType type, VMSideExit* exit);
48470: #elif JS_BITS_PER_WORD == 64
48470:     nanojit::LIns* non_double_object_value_has_type(nanojit::LIns* v_ins, JSValueType type);
48470:     nanojit::LIns* unpack_ptr(nanojit::LIns* v_ins);
48470:     nanojit::LIns* unbox_number_as_double(nanojit::LIns* v_ins, VMSideExit* exit);
48470:     nanojit::LIns* unbox_object(nanojit::LIns* v_ins, JSValueType type, VMSideExit* exit);
48470:     nanojit::LIns* unbox_non_double_object(nanojit::LIns* v_ins, JSValueType type, VMSideExit* exit);
48470: #endif
48470: 
56750:     nanojit::LIns* unbox_value(const Value& v, tjit::Address addr, VMSideExit* exit,
48470:                                bool force_double=false);
56750:     void unbox_any_object(tjit::Address addr, nanojit::LIns** obj_ins, nanojit::LIns** is_obj_ins);
56750:     nanojit::LIns* is_boxed_true(tjit::Address addr);
56750:     nanojit::LIns* is_boxed_magic(tjit::Address addr, JSWhyMagic why);
48470: 
48470:     nanojit::LIns* is_string_id(nanojit::LIns* id_ins);
48470:     nanojit::LIns* unbox_string_id(nanojit::LIns* id_ins);
48470:     nanojit::LIns* unbox_int_id(nanojit::LIns* id_ins);
48470: 
48470:     /* Box a slot on trace into the given address at the given offset. */
56750:     void box_value_into(const Value& v, nanojit::LIns* v_ins, tjit::Address addr);
48470: 
48470:     /*
48470:      * Box a slot so that it may be passed with value semantics to a native. On
48470:      * 32-bit, this currently means boxing the value into insAlloc'd memory and
48470:      * returning the address which is passed as a Value*. On 64-bit, this
48470:      * currently means returning the boxed value which is passed as a jsval.
48470:      */
48470:     nanojit::LIns* box_value_for_native_call(const Value& v, nanojit::LIns* v_ins);
48470: 
48470:     /* Box a slot into insAlloc'd memory. */
48470:     nanojit::LIns* box_value_into_alloc(const Value& v, nanojit::LIns* v_ins);
48470: 
48470:     JS_REQUIRES_STACK void guardClassHelper(bool cond, nanojit::LIns* obj_ins, Class* clasp,
48613:                                             VMSideExit* exit, nanojit::LoadQual loadQual);
48470:     JS_REQUIRES_STACK void guardClass(nanojit::LIns* obj_ins, Class* clasp,
48613:                                       VMSideExit* exit, nanojit::LoadQual loadQual);
48470:     JS_REQUIRES_STACK void guardNotClass(nanojit::LIns* obj_ins, Class* clasp,
48613:                                          VMSideExit* exit, nanojit::LoadQual loadQual);
42749:     JS_REQUIRES_STACK void guardDenseArray(nanojit::LIns* obj_ins, ExitType exitType);
42749:     JS_REQUIRES_STACK void guardDenseArray(nanojit::LIns* obj_ins, VMSideExit* exit);
29513:     JS_REQUIRES_STACK bool guardHasPrototype(JSObject* obj, nanojit::LIns* obj_ins,
29513:                                              JSObject** pobj, nanojit::LIns** pobj_ins,
29513:                                              VMSideExit* exit);
33542:     JS_REQUIRES_STACK RecordingStatus guardPrototypeHasNoIndexedProperties(JSObject* obj,
27933:                                                                            nanojit::LIns* obj_ins,
53614:                                                                            VMSideExit* exit);
48470:     JS_REQUIRES_STACK RecordingStatus guardNativeConversion(Value& v);
60559:     JS_REQUIRES_STACK void clearReturningFrameFromNativeTracker();
62933:     JS_REQUIRES_STACK AbortableRecordingStatus putActivationObjects();
62590:     JS_REQUIRES_STACK RecordingStatus createThis(JSObject& ctor, nanojit::LIns* ctor_ins,
62590:                                                  nanojit::LIns** thisobj_insp);
48470:     JS_REQUIRES_STACK RecordingStatus guardCallee(Value& callee);
69223:     JS_REQUIRES_STACK StackFrame *guardArguments(JSObject *obj, nanojit::LIns* obj_ins,
31460:                                                  unsigned *depthp);
37214:     JS_REQUIRES_STACK nanojit::LIns* guardArgsLengthNotAssigned(nanojit::LIns* argsobj_ins);
55525:     JS_REQUIRES_STACK void guardNotHole(nanojit::LIns* argsobj_ins, nanojit::LIns* ids_ins);
33542:     JS_REQUIRES_STACK RecordingStatus getClassPrototype(JSObject* ctor,
27933:                                                           nanojit::LIns*& proto_ins);
33542:     JS_REQUIRES_STACK RecordingStatus getClassPrototype(JSProtoKey key,
27933:                                                           nanojit::LIns*& proto_ins);
48470:     JS_REQUIRES_STACK RecordingStatus newArray(JSObject* ctor, uint32 argc, Value* argv,
48470:                                                  Value* rval);
48470:     JS_REQUIRES_STACK RecordingStatus newString(JSObject* ctor, uint32 argc, Value* argv,
48470:                                                   Value* rval);
48470:     JS_REQUIRES_STACK RecordingStatus interpretedFunctionCall(Value& fval, JSFunction* fun,
27933:                                                                 uintN argc, bool constructing);
30847:     JS_REQUIRES_STACK void propagateFailureToBuiltinStatus(nanojit::LIns *ok_ins,
30847:                                                            nanojit::LIns *&status_ins);
33542:     JS_REQUIRES_STACK RecordingStatus emitNativeCall(JSSpecializedNative* sn, uintN argc,
32678:                                                        nanojit::LIns* args[], bool rooted);
52503:     JS_REQUIRES_STACK void emitNativePropertyOp(const js::Shape* shape,
30847:                                                 nanojit::LIns* obj_ins,
30847:                                                 bool setflag,
48470:                                                 nanojit::LIns* addr_boxed_val_ins);
33542:     JS_REQUIRES_STACK RecordingStatus callSpecializedNative(JSNativeTraceInfo* trcinfo, uintN argc,
22652:                                                               bool constructing);
33542:     JS_REQUIRES_STACK RecordingStatus callNative(uintN argc, JSOp mode);
56184:     JS_REQUIRES_STACK RecordingStatus callFloatReturningInt(uintN argc,
56184:                                                             const nanojit::CallInfo *ci);
33542:     JS_REQUIRES_STACK RecordingStatus functionCall(uintN argc, JSOp mode);
17921: 
22652:     JS_REQUIRES_STACK void trackCfgMerges(jsbytecode* pc);
26557:     JS_REQUIRES_STACK void emitIf(jsbytecode* pc, bool cond, nanojit::LIns* x);
22652:     JS_REQUIRES_STACK void fuseIf(jsbytecode* pc, bool cond, nanojit::LIns* x);
33542:     JS_REQUIRES_STACK AbortableRecordingStatus checkTraceEnd(jsbytecode* pc);
19068: 
41777:     AbortableRecordingStatus hasMethod(JSObject* obj, jsid id, bool& found);
41777:     JS_REQUIRES_STACK AbortableRecordingStatus hasIteratorMethod(JSObject* obj, bool& found);
21685: 
27014:     JS_REQUIRES_STACK jsatomid getFullIndex(ptrdiff_t pcoff = 0);
27012: 
48470:     JS_REQUIRES_STACK JSValueType determineSlotType(Value* vp);
27540: 
48470:     JS_REQUIRES_STACK RecordingStatus setUpwardTrackedVar(Value* stackVp, const Value& v,
41076:                                                           nanojit::LIns* v_ins);
41076: 
35083:     JS_REQUIRES_STACK AbortableRecordingStatus compile();
33564:     JS_REQUIRES_STACK AbortableRecordingStatus closeLoop();
33542:     JS_REQUIRES_STACK AbortableRecordingStatus endLoop();
33542:     JS_REQUIRES_STACK AbortableRecordingStatus endLoop(VMSideExit* exit);
64278:     JS_REQUIRES_STACK bool joinEdgesToEntry(TreeFragment* peer_root);
35044:     JS_REQUIRES_STACK void adjustCallerTypes(TreeFragment* f);
39937:     JS_REQUIRES_STACK void prepareTreeCall(TreeFragment* inner);
39937:     JS_REQUIRES_STACK void emitTreeCall(TreeFragment* inner, VMSideExit* exit);
48470:     JS_REQUIRES_STACK void determineGlobalTypes(JSValueType* typeMap);
34351:     JS_REQUIRES_STACK VMSideExit* downSnapshot(FrameInfo* downFrame);
35044:     JS_REQUIRES_STACK TreeFragment* findNestedCompatiblePeer(TreeFragment* f);
35044:     JS_REQUIRES_STACK AbortableRecordingStatus attemptTreeCall(TreeFragment* inner,
33564:                                                                uintN& inlineCallCount);
17409: 
41777:     static JS_REQUIRES_STACK MonitorResult recordLoopEdge(JSContext* cx, TraceRecorder* r,
34351:                                                           uintN& inlineCallCount);
33564: 
34351:     /* Allocators associated with this recording session. */
34351:     VMAllocator& tempAlloc() const { return *traceMonitor->tempAlloc; }
34351:     VMAllocator& traceAlloc() const { return *traceMonitor->traceAlloc; }
34351:     VMAllocator& dataAlloc() const { return *traceMonitor->dataAlloc; }
34351: 
34351:     /* Member declarations for each opcode, to be called before interpreting the opcode. */
34351: #define OPDEF(op,val,name,token,length,nuses,ndefs,prec,format)               \
34351:     JS_REQUIRES_STACK AbortableRecordingStatus record_##op();
34351: # include "jsopcode.tbl"
34351: #undef OPDEF
34351: 
35083:     JS_REQUIRES_STACK
62032:     TraceRecorder(JSContext* cx, TraceMonitor *tm, VMSideExit*, VMFragment*,
48470:                   unsigned stackSlots, unsigned ngslots, JSValueType* typeMap,
56217:                   VMSideExit* expectedInnerExit, JSScript* outerScript, jsbytecode* outerPC,
53524:                   uint32 outerArgc, bool speculate);
35083: 
35083:     /* The destructor should only be called through finish*, not directly. */
35083:     ~TraceRecorder();
35083:     JS_REQUIRES_STACK AbortableRecordingStatus finishSuccessfully();
54835: 
54835:     enum AbortResult { NORMAL_ABORT, JIT_RESET };
54835:     JS_REQUIRES_STACK AbortResult finishAbort(const char* reason);
35083: 
64215: #ifdef DEBUG
64215:     /* Debug printing functionality to emit printf() on trace. */
64215:     JS_REQUIRES_STACK void tprint(const char *format, int count, nanojit::LIns *insa[]);
64215:     JS_REQUIRES_STACK void tprint(const char *format);
64215:     JS_REQUIRES_STACK void tprint(const char *format, nanojit::LIns *ins);
64215:     JS_REQUIRES_STACK void tprint(const char *format, nanojit::LIns *ins1,
64215:                                   nanojit::LIns *ins2);
64215:     JS_REQUIRES_STACK void tprint(const char *format, nanojit::LIns *ins1,
64215:                                   nanojit::LIns *ins2, nanojit::LIns *ins3);
64215:     JS_REQUIRES_STACK void tprint(const char *format, nanojit::LIns *ins1,
64215:                                   nanojit::LIns *ins2, nanojit::LIns *ins3,
64215:                                   nanojit::LIns *ins4);
64215:     JS_REQUIRES_STACK void tprint(const char *format, nanojit::LIns *ins1,
64215:                                   nanojit::LIns *ins2, nanojit::LIns *ins3,
64215:                                   nanojit::LIns *ins4, nanojit::LIns *ins5);
64215:     JS_REQUIRES_STACK void tprint(const char *format, nanojit::LIns *ins1,
64215:                                   nanojit::LIns *ins2, nanojit::LIns *ins3,
64215:                                   nanojit::LIns *ins4, nanojit::LIns *ins5,
64215:                                   nanojit::LIns *ins6);
64215: #endif
64215: 
34351:     friend class ImportBoxedStackSlotVisitor;
34351:     friend class AdjustCallerGlobalTypesVisitor;
34351:     friend class AdjustCallerStackTypesVisitor;
34351:     friend class TypeCompatibilityVisitor;
34351:     friend class SlotMap;
34351:     friend class DefaultSlotMap;
34351:     friend class DetermineTypesVisitor;
62032:     friend MonitorResult RecordLoopEdge(JSContext*, TraceMonitor*, uintN&);
62032:     friend TracePointAction RecordTracePoint(JSContext*, TraceMonitor*, uintN &inlineCallCount,
56551:                                              bool *blacklist);
54835:     friend AbortResult AbortRecording(JSContext*, const char*);
53840:     friend class BoxArg;
60258:     friend void TraceMonitor::sweep(JSContext *cx);
34351: 
34351:   public:
35083:     static bool JS_REQUIRES_STACK
62032:     startRecorder(JSContext*, TraceMonitor *, VMSideExit*, VMFragment*,
48470:                   unsigned stackSlots, unsigned ngslots, JSValueType* typeMap,
56217:                   VMSideExit* expectedInnerExit, JSScript* outerScript, jsbytecode* outerPC,
53524:                   uint32 outerArgc, bool speculate);
34351: 
34351:     /* Accessors. */
35044:     VMFragment*         getFragment() const { return fragment; }
36361:     TreeFragment*       getTree() const { return tree; }
35083:     bool                outOfMemory() const { return traceMonitor->outOfMemory(); }
46181:     Oracle*             getOracle() const { return oracle; }
60251:     JSObject*           getGlobal() const { return globalObj; }
34351: 
34351:     /* Entry points / callbacks from the interpreter. */
35083:     JS_REQUIRES_STACK AbortableRecordingStatus monitorRecording(JSOp op);
53524:     JS_REQUIRES_STACK AbortableRecordingStatus record_EnterFrame();
33542:     JS_REQUIRES_STACK AbortableRecordingStatus record_LeaveFrame();
60780:     JS_REQUIRES_STACK AbortableRecordingStatus record_AddProperty(JSObject *obj);
52503:     JS_REQUIRES_STACK AbortableRecordingStatus record_DefLocalFunSetSlot(uint32 slot,
52503:                                                                          JSObject* obj);
33542:     JS_REQUIRES_STACK AbortableRecordingStatus record_NativeCallComplete();
33560:     void forgetGuardedShapesForObject(JSObject* obj);
18706: 
59878:     bool globalSetExpected(unsigned slot) {
60559:         unsigned *pi = Find(pendingGlobalSlotsToSet, slot);
60559:         if (pi == pendingGlobalSlotsToSet.end()) {
60167:             /*
60167:              * Do slot arithmetic manually to avoid getSlotRef assertions which
60167:              * do not need to be satisfied for this purpose.
60167:              */
60247:             Value *vp = globalObj->getSlots() + slot;
60247: 
60247:             /* If this global is definitely being tracked, then the write is unexpected. */
60247:             if (tracker.has(vp))
60247:                 return false;
60247:             
60247:             /*
60247:              * Otherwise, only abort if the global is not present in the
60247:              * import typemap. Just deep aborting false here is not acceptable,
60247:              * because the recorder does not guard on every operation that
60247:              * could lazily resolve. Since resolving adds properties to
60247:              * reserved slots, the tracer will never have imported them.
60247:              */
60527:             return tree->globalSlots->offsetOf((uint16)nativeGlobalSlot(vp)) == -1;
60167:         }
60559:         pendingGlobalSlotsToSet.erase(pi);
59878:         return true;
59878:     }
34351: };
33157: 
54175: #define TRACING_ENABLED(cx)       ((cx)->traceJitEnabled)
54175: #define REGEX_JIT_ENABLED(cx)     ((cx)->traceJitEnabled || (cx)->methodJitEnabled)
17186: 
27012: #define JSOP_IN_RANGE(op,lo,hi)   (uintN((op) - (lo)) <= uintN((hi) - (lo)))
27012: #define JSOP_IS_BINARY(op)        JSOP_IN_RANGE(op, JSOP_BITOR, JSOP_MOD)
27012: #define JSOP_IS_UNARY(op)         JSOP_IN_RANGE(op, JSOP_NEG, JSOP_POS)
27012: #define JSOP_IS_EQUALITY(op)      JSOP_IN_RANGE(op, JSOP_EQ, JSOP_NE)
21685: 
23111: #define TRACE_ARGS_(x,args)                                                   \
23110:     JS_BEGIN_MACRO                                                            \
33171:         if (TraceRecorder* tr_ = TRACE_RECORDER(cx)) {                        \
33542:             AbortableRecordingStatus status = tr_->record_##x args;           \
41777:             if (StatusAbortsRecorderIfActive(status)) {                       \
41777:                 if (TRACE_RECORDER(cx)) {                                     \
41777:                     JS_ASSERT(TRACE_RECORDER(cx) == tr_);                     \
37741:                     AbortRecording(cx, #x);                                   \
41777:                 }                                                             \
33542:                 if (status == ARECORD_ERROR)                                  \
27933:                     goto error;                                               \
27933:             }                                                                 \
33542:             JS_ASSERT(status != ARECORD_IMACRO);                              \
27933:         }                                                                     \
23110:     JS_END_MACRO
23110: 
23111: #define TRACE_ARGS(x,args)      TRACE_ARGS_(x, args)
19582: #define TRACE_0(x)              TRACE_ARGS(x, ())
19093: #define TRACE_1(x,a)            TRACE_ARGS(x, (a))
19093: #define TRACE_2(x,a,b)          TRACE_ARGS(x, (a, b))
19093: 
41777: extern JS_REQUIRES_STACK MonitorResult
69223: MonitorLoopEdge(JSContext* cx, uintN& inlineCallCount, InterpMode interpMode);
56551: 
53133: extern JS_REQUIRES_STACK TracePointAction
57802: RecordTracePoint(JSContext*, uintN& inlineCallCount, bool* blacklist);
56551: 
56551: extern JS_REQUIRES_STACK TracePointAction
56551: MonitorTracePoint(JSContext*, uintN& inlineCallCount, bool* blacklist,
60534:                   void** traceData, uintN *traceEpoch, uint32 *loopCounter, uint32 hits);
18683: 
54835: extern JS_REQUIRES_STACK TraceRecorder::AbortResult
37741: AbortRecording(JSContext* cx, const char* reason);
17350: 
69633: extern void
69633: InitJIT();
17442: 
17726: extern void
69633: FinishJIT();
17726: 
17976: extern void
58041: PurgeScriptFragments(TraceMonitor* tm, JSScript* script);
24879: 
26826: extern bool
59733: OverfullJITCache(JSContext *cx, TraceMonitor* tm);
32767: 
32767: extern void
62032: FlushJITCache(JSContext* cx, TraceMonitor* tm);
26826: 
24384: extern JSObject *
37741: GetBuiltinFunction(JSContext *cx, uintN index);
24384: 
27884: extern void
37741: SetMaxCodeCacheBytes(JSContext* cx, uint32 bytes);
27884: 
48470: extern void
48470: ExternNativeToValue(JSContext* cx, Value& v, JSValueType type, double* slot);
32709: 
29368: #ifdef MOZ_TRACEVIS
29368: 
29368: extern JS_FRIEND_API(bool)
38585: StartTraceVis(const char* filename);
29368: 
29368: extern JS_FRIEND_API(JSBool)
54568: StartTraceVisNative(JSContext *cx, uintN argc, jsval *vp);
29368: 
29368: extern JS_FRIEND_API(bool)
38585: StopTraceVis();
29368: 
29368: extern JS_FRIEND_API(JSBool)
54568: StopTraceVisNative(JSContext *cx, uintN argc, jsval *vp);
29368: 
29368: /* Must contain no more than 16 items. */
29368: enum TraceVisState {
32748:     // Special: means we returned from current activity to last
29368:     S_EXITLAST,
32748:     // Activities
29368:     S_INTERP,
29368:     S_MONITOR,
29368:     S_RECORD,
29368:     S_COMPILE,
29368:     S_EXECUTE,
32748:     S_NATIVE,
32748:     // Events: these all have (bit 3) == 1.
32748:     S_RESET = 8
29368: };
29368: 
29368: /* Reason for an exit to the interpreter. */
29368: enum TraceVisExitReason {
29368:     R_NONE,
29368:     R_ABORT,
37741:     /* Reasons in MonitorLoopEdge */
29368:     R_INNER_SIDE_EXIT,
29368:     R_DOUBLES,
29368:     R_CALLBACK_PENDING,
29368:     R_OOM_GETANCHOR,
29368:     R_BACKED_OFF,
29368:     R_COLD,
29368:     R_FAIL_RECORD_TREE,
29368:     R_MAX_PEERS,
29368:     R_FAIL_EXECUTE_TREE,
29368:     R_FAIL_STABILIZE,
29368:     R_FAIL_EXTEND_FLUSH,
29368:     R_FAIL_EXTEND_MAX_BRANCHES,
29368:     R_FAIL_EXTEND_START,
29368:     R_FAIL_EXTEND_COLD,
41111:     R_FAIL_SCOPE_CHAIN_CHECK,
29368:     R_NO_EXTEND_OUTER,
29368:     R_MISMATCH_EXIT,
29368:     R_OOM_EXIT,
29368:     R_TIMEOUT_EXIT,
29368:     R_DEEP_BAIL_EXIT,
29368:     R_STATUS_EXIT,
29368:     R_OTHER_EXIT
29368: };
29368: 
32748: enum TraceVisFlushReason {
32748:     FR_DEEP_BAIL,
32748:     FR_OOM,
32748:     FR_GLOBAL_SHAPE_MISMATCH,
32748:     FR_GLOBALS_FULL
32748: };
32748: 
31063: const unsigned long long MS64_MASK = 0xfull << 60;
31063: const unsigned long long MR64_MASK = 0x1full << 55;
29368: const unsigned long long MT64_MASK = ~(MS64_MASK | MR64_MASK);
29368: 
29368: extern FILE* traceVisLogFile;
31063: extern JSHashTable *traceVisScriptTable;
31063: 
31063: extern JS_FRIEND_API(void)
37741: StoreTraceVisState(JSContext *cx, TraceVisState s, TraceVisExitReason r);
29368: 
29368: static inline void
37741: LogTraceVisState(JSContext *cx, TraceVisState s, TraceVisExitReason r)
29368: {
29368:     if (traceVisLogFile) {
29368:         unsigned long long sllu = s;
29368:         unsigned long long rllu = r;
29368:         unsigned long long d = (sllu << 60) | (rllu << 55) | (rdtsc() & MT64_MASK);
29368:         fwrite(&d, sizeof(d), 1, traceVisLogFile);
29368:     }
31063:     if (traceVisScriptTable) {
37741:         StoreTraceVisState(cx, s, r);
31063:     }
29368: }
29368: 
32748: /*
37741:  * Although this runs the same code as LogTraceVisState, it is a separate
32748:  * function because the meaning of the log entry is different. Also, the entry
32748:  * formats may diverge someday.
32748:  */
32748: static inline void
37741: LogTraceVisEvent(JSContext *cx, TraceVisState s, TraceVisFlushReason r)
32748: {
37741:     LogTraceVisState(cx, s, (TraceVisExitReason) r);
32748: }
32748: 
29368: static inline void
37741: EnterTraceVisState(JSContext *cx, TraceVisState s, TraceVisExitReason r)
29368: {
37741:     LogTraceVisState(cx, s, r);
29368: }
29368: 
29368: static inline void
37741: ExitTraceVisState(JSContext *cx, TraceVisExitReason r)
29368: {
37741:     LogTraceVisState(cx, S_EXITLAST, r);
29368: }
29368: 
29368: struct TraceVisStateObj {
29368:     TraceVisExitReason r;
31063:     JSContext *mCx;
29368: 
31063:     inline TraceVisStateObj(JSContext *cx, TraceVisState s) : r(R_NONE)
29368:     {
37741:         EnterTraceVisState(cx, s, R_NONE);
31063:         mCx = cx;
29368:     }
29368:     inline ~TraceVisStateObj()
29368:     {
37741:         ExitTraceVisState(mCx, r);
29368:     }
29368: };
29368: 
29368: #endif /* MOZ_TRACEVIS */
29368: 
37741: }      /* namespace js */
32581: 
19171: #else  /* !JS_TRACER */
19171: 
19599: #define TRACE_0(x)              ((void)0)
19171: #define TRACE_1(x,a)            ((void)0)
19171: #define TRACE_2(x,a,b)          ((void)0)
19171: 
19171: #endif /* !JS_TRACER */
18091: 
59878: namespace js {
59878: 
59878: /*
59878:  * While recording, the slots of the global object may change payload or type.
59878:  * This is fine as long as the recorder expects this change (and therefore has
59878:  * generated the corresponding LIR, snapshots, etc). The recorder indicates
60559:  * that it expects a write to a global slot by setting pendingGlobalSlotsToSet
59878:  * in the recorder, before the write is made by the interpreter, and clearing
60559:  * pendingGlobalSlotsToSet before recording the next op. Any global slot write
59878:  * that has not been whitelisted in this manner is therefore unexpected and, if
59878:  * the global slot is actually being tracked, recording must be aborted.
59878:  */
59878: static JS_INLINE void
59878: AbortRecordingIfUnexpectedGlobalWrite(JSContext *cx, JSObject *obj, unsigned slot)
59878: {
59878: #ifdef JS_TRACER
59878:     if (TraceRecorder *tr = TRACE_RECORDER(cx)) {
60251:         if (obj == tr->getGlobal() && !tr->globalSetExpected(slot))
59878:             AbortRecording(cx, "Global slot written outside tracer supervision");
59878:     }
59878: #endif
59878: }
59878: 
59878: }  /* namespace js */
59878: 
17196: #endif /* jstracer_h___ */
