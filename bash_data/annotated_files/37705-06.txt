31927: /* -*- Mode: C++; c-basic-offset: 4; indent-tabs-mode: nil; tab-width: 4 -*- */
31927: /* vi: set ts=4 sw=4 expandtab: (add to ~/.vimrc: set modeline modelines=5) */
31927: /* ***** BEGIN LICENSE BLOCK *****
31927:  * Version: MPL 1.1/GPL 2.0/LGPL 2.1
31927:  *
31927:  * The contents of this file are subject to the Mozilla Public License Version
31927:  * 1.1 (the "License"); you may not use this file except in compliance with
31927:  * the License. You may obtain a copy of the License at
31927:  * http://www.mozilla.org/MPL/
31927:  *
31927:  * Software distributed under the License is distributed on an "AS IS" basis,
31927:  * WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
31927:  * for the specific language governing rights and limitations under the
31927:  * License.
31927:  *
31927:  * The Original Code is [Open Source Virtual Machine].
31927:  *
31927:  * The Initial Developer of the Original Code is
31927:  * Adobe System Incorporated.
31927:  * Portions created by the Initial Developer are Copyright (C) 2009
31927:  * the Initial Developer. All Rights Reserved.
31927:  *
31927:  * Contributor(s):
31927:  *   Adobe AS3 Team
31927:  *
31927:  * Alternatively, the contents of this file may be used under the terms of
31927:  * either the GNU General Public License Version 2 or later (the "GPL"), or
31927:  * the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
31927:  * in which case the provisions of the GPL or the LGPL are applicable instead
31927:  * of those above. If you wish to allow use of your version of this file only
31927:  * under the terms of either the GPL or the LGPL, and not to allow others to
31927:  * use your version of this file under the terms of the MPL, indicate your
31927:  * decision by deleting the provisions above and replace them with the notice
31927:  * and other provisions required by the GPL or the LGPL. If you do not delete
31927:  * the provisions above, a recipient may use your version of this file under
31927:  * the terms of any one of the MPL, the GPL or the LGPL.
31927:  *
31927:  * ***** END LICENSE BLOCK ***** */
31927: 
31927: #include "nanojit.h"
31927: 
31927: // uncomment this to enable _vprof/_nvprof macros
31927: //#define DOPROF
31927: #include "../vprof/vprof.h"
31927: 
31927: #if defined FEATURE_NANOJIT && defined NANOJIT_X64
31927: 
31927: /*
31927: completion
31927: - 64bit branch offsets
31927: - finish cmov/qcmov with other conditions
31927: - validate asm_cond with other conditions
31927: 
31927: better code
31927: - put R12 back in play as a base register
31927: - no-disp addr modes (except RBP/R13)
31927: - disp64 branch/call
31927: - spill gp values to xmm registers?
31927: - prefer xmm registers for copies since gprs are in higher demand?
31927: - stack arg doubles
31927: - stack based LIR_param
31927: 
31927: tracing
31927: - asm_qjoin
31927: - asm_qhi
31927: - nFragExit
31927: 
31927: */
31927: 
31927: namespace nanojit
31927: {
31927:     const Register Assembler::retRegs[] = { RAX };
33539: #ifdef _WIN64
31927:     const Register Assembler::argRegs[] = { RCX, RDX, R8, R9 };
31927:     const Register Assembler::savedRegs[] = { RBX, RSI, RDI, R12, R13, R14, R15 };
31927: #else
31927:     const Register Assembler::argRegs[] = { RDI, RSI, RDX, RCX, R8, R9 };
31927:     const Register Assembler::savedRegs[] = { RBX, R12, R13, R14, R15 };
31927: #endif
31927: 
31927:     const char *regNames[] = {
31927:         "rax",  "rcx",  "rdx",   "rbx",   "rsp",   "rbp",   "rsi",   "rdi",
31927:         "r8",   "r9",   "r10",   "r11",   "r12",   "r13",   "r14",   "r15",
31927:         "xmm0", "xmm1", "xmm2",  "xmm3",  "xmm4",  "xmm5",  "xmm6",  "xmm7",
31927:         "xmm8", "xmm9", "xmm10", "xmm11", "xmm12", "xmm13", "xmm14", "xmm15"
31927:     };
31927: 
35101:     const char *gpRegNames32[] = {
35101:         "eax", "ecx", "edx",  "ebx",  "esp",  "ebp",  "esi",  "edi",
35101:         "r8d", "r9d", "r10d", "r11d", "r12d", "r13d", "r14d", "r15d"
35101:     };
35101: 
35101:     const char *gpRegNames8[] = {
35101:         "al",  "cl",  "dl",   "bl",   "spl",  "bpl",  "sil",  "dil",
35101:         "r8l", "r9l", "r10l", "r11l", "r12l", "r13l", "r14l", "r15l"
35101:     };
35101: 
35101:     const char *gpRegNames8hi[] = {
35101:         "ah", "ch", "dh", "bh"
35101:     };
35101: 
36414:     const char *gpRegNames16[] = {
36414:         "ax",  "cx",  "dx",   "bx",   "spx",  "bpx",  "six",  "dix",
36414:         "r8x", "r9x", "r10x", "r11x", "r12x", "r13x", "r14x", "r15x"
36414:     };
36414: 
31927: #ifdef _DEBUG
31927:     #define TODO(x) todo(#x)
31927:     static void todo(const char *s) {
31927:         verbose_only( avmplus::AvmLog("%s",s); )
31927:         NanoAssertMsgf(false, "%s", s);
31927:     }
31927: #else
31927:     #define TODO(x)
31927: #endif
31927: 
35087:     // MODRM and SIB restrictions:
31927:     // memory access modes != 11 require SIB if base&7 == 4 (RSP or R12)
35087:     // mode 00 with base == x101 means RIP+disp32 (RBP or R13), use mode 01 disp8=0 instead
35087:     // mode 01 or 11 with base = x101 means disp32 + EBP or R13, not RIP relative
35087:     // base == x100 means SIB byte is present, so using ESP|R12 as base requires SIB
31927:     // rex prefix required to use RSP-R15 as 8bit registers in mod/rm8 modes.
31927: 
35087:     // take R12 out of play as a base register because using ESP or R12 as base requires the SIB byte
31927:     const RegisterMask BaseRegs = GpRegs & ~rmask(R12);
31927: 
31927:     static inline int oplen(uint64_t op) {
31927:         return op & 255;
31927:     }
31927: 
31927:     // encode 2-register rex prefix.  dropped if none of its bits are set.
31927:     static inline uint64_t rexrb(uint64_t op, Register r, Register b) {
31927:         int shift = 64 - 8*oplen(op);
31927:         uint64_t rex = ((op >> shift) & 255) | ((r&8)>>1) | ((b&8)>>3);
31927:         return rex != 0x40 ? op | rex << shift : op - 1;
31927:     }
31927: 
35087:     // encode 3-register rex prefix.  dropped if none of its bits are set.
35087:     static inline uint64_t rexrxb(uint64_t op, Register r, Register x, Register b) {
35087:         int shift = 64 - 8*oplen(op);
35087:         uint64_t rex = ((op >> shift) & 255) | ((r&8)>>1) | ((x&8)>>2) | ((b&8)>>3);
35087:         return rex != 0x40 ? op | rex << shift : op - 1;
35087:     }
35087: 
31927:     // encode 2-register rex prefix.  dropped if none of its bits are set, but
31927:     // keep REX if b >= rsp, to allow uniform use of all 16 8bit registers
31927:     static inline uint64_t rexrb8(uint64_t op, Register r, Register b) {
31927:         int shift = 64 - 8*oplen(op);
31927:         uint64_t rex = ((op >> shift) & 255) | ((r&8)>>1) | ((b&8)>>3);
31927:         return ((rex | (b & ~3)) != 0x40) ? (op | (rex << shift)) : op - 1;
31927:     }
31927: 
31927:     // encode 2-register rex prefix that follows a manditory prefix (66,F2,F3)
31927:     // [prefix][rex][opcode]
31927:     static inline uint64_t rexprb(uint64_t op, Register r, Register b) {
31927:         int shift = 64 - 8*oplen(op) + 8;
31927:         uint64_t rex = ((op >> shift) & 255) | ((r&8)>>1) | ((b&8)>>3);
31927:         // to drop rex, we replace rex with manditory prefix, and decrement length
31927:         return rex != 0x40 ? op | rex << shift :
31927:             ((op & ~(255LL<<shift)) | (op>>(shift-8)&255) << shift) - 1;
31927:     }
31927: 
31927:     // [rex][opcode][mod-rr]
31927:     static inline uint64_t mod_rr(uint64_t op, Register r, Register b) {
31927:         return op | uint64_t((r&7)<<3 | (b&7))<<56;
31927:     }
31927: 
35087:     // [rex][opcode][modrm=r][sib=xb]
35087:     static inline uint64_t mod_rxb(uint64_t op, Register r, Register x, Register b) {
35087:         return op | /*modrm*/uint64_t((r&7)<<3)<<48 | /*sib*/uint64_t((x&7)<<3|(b&7))<<56;
35087:     }
35087: 
31927:     static inline uint64_t mod_disp32(uint64_t op, Register r, Register b, int32_t d) {
31927:         NanoAssert(IsGpReg(r) && IsGpReg(b));
31927:         NanoAssert((b & 7) != 4); // using RSP or R12 as base requires SIB
31927:         if (isS8(d)) {
31927:             // op is:  0x[disp32=0][mod=2:r:b][op][rex][len]
31927:             NanoAssert((((op>>24)&255)>>6) == 2); // disp32 mode
31927:             int len = oplen(op);
31927:             op = (op & ~0xff000000LL) | (0x40 | (r&7)<<3 | (b&7))<<24; // replace mod
31927:             return op<<24 | int64_t(d)<<56 | (len-3); // shrink disp, add disp8
31927:         } else {
31927:             // op is: 0x[disp32][mod][op][rex][len]
31927:             return op | int64_t(d)<<32 | uint64_t((r&7)<<3 | (b&7))<<24;
31927:         }
31927:     }
31927: 
35101:     // All the emit() functions should only be called from within codegen
35101:     // functions PUSHR(), SHR(), etc.
31927: 
31927:     void Assembler::emit(uint64_t op) {
31927:         int len = oplen(op);
31927:         // we will only move nIns by -len bytes, but we write 8
31927:         // bytes.  so need to protect 8 so we dont stomp the page
31927:         // header or the end of the preceding page (might segf)
31927:         underrunProtect(8);
31927:         ((int64_t*)_nIns)[-1] = op;
31927:         _nIns -= len; // move pointer by length encoded in opcode
31927:         _nvprof("x64-bytes", len);
31927:     }
31927: 
31927:     void Assembler::emit8(uint64_t op, int64_t v) {
31927:         NanoAssert(isS8(v));
31927:         emit(op | uint64_t(v)<<56);
31927:     }
31927: 
35101:     void Assembler::emit_target8(size_t underrun, uint64_t op, NIns* target) {
35101:         underrunProtect(underrun); // must do this before calculating offset
35101:         // Nb: see emit_target32() for why we use _nIns here.
35101:         int64_t offset = target - _nIns;
35101:         NanoAssert(isS8(offset));
35101:         emit(op | uint64_t(offset)<<56);
35101:     }
35101: 
35101:     void Assembler::emit_target32(size_t underrun, uint64_t op, NIns* target) {
35101:         underrunProtect(underrun); // must do this before calculating offset
35101:         // Nb: at this point in time, _nIns points to the most recently
35101:         // written instruction, ie. the jump's successor.  So why do we use it
35101:         // to compute the offset, rather than the jump's address?  Because in
35101:         // x86/x64-64 the offset in a relative jump is not from the jmp itself
35101:         // but from the following instruction.  Eg. 'jmp $0' will jump to the
35101:         // next instruction.
35101:         int64_t offset = target ? target - _nIns : 0;
35101:         NanoAssert(isS32(offset));
35101:         emit(op | uint64_t(uint32_t(offset))<<32);
31927:     }
31927: 
35087:     // 3-register modrm32+sib form
35087:     void Assembler::emitrxb(uint64_t op, Register r, Register x, Register b) {
35087:         emit(rexrxb(mod_rxb(op, r, x, b), r, x, b));
35087:     }
35087: 
31927:     // 2-register modrm32 form
31927:     void Assembler::emitrr(uint64_t op, Register r, Register b) {
31927:         emit(rexrb(mod_rr(op, r, b), r, b));
31927:     }
31927: 
31927:     // 2-register modrm8 form (8 bit operand size)
31927:     void Assembler::emitrr8(uint64_t op, Register r, Register b) {
31927:         emit(rexrb8(mod_rr(op, r, b), r, b));
31927:     }
31927: 
31927:     // same as emitrr, but with a prefix byte
31927:     void Assembler::emitprr(uint64_t op, Register r, Register b) {
31927:         emit(rexprb(mod_rr(op, r, b), r, b));
31927:     }
31927: 
36414:     // disp32 modrm8 form, when the disp fits in the instruction (opcode is 1-3 bytes)
36414:     void Assembler::emitrm8(uint64_t op, Register r, int32_t d, Register b) {
36414:         emit(rexrb8(mod_disp32(op, r, b, d), r, b));
36414:     }
36414: 
31927:     // disp32 modrm form, when the disp fits in the instruction (opcode is 1-3 bytes)
31927:     void Assembler::emitrm(uint64_t op, Register r, int32_t d, Register b) {
31927:         emit(rexrb(mod_disp32(op, r, b, d), r, b));
31927:     }
31927: 
31927:     // disp32 modrm form when the disp must be written separately (opcode is 4+ bytes)
32706:     uint64_t Assembler::emit_disp32(uint64_t op, int32_t d) {
31927:         if (isS8(d)) {
31927:             NanoAssert(((op>>56)&0xC0) == 0x80); // make sure mod bits == 2 == disp32 mode
31927:             underrunProtect(1+8);
31927:             *(--_nIns) = (NIns) d;
31927:             _nvprof("x64-bytes", 1);
31927:             op ^= 0xC000000000000000LL; // change mod bits to 1 == disp8 mode
31927:         } else {
31927:             underrunProtect(4+8); // room for displ plus fullsize op
31927:             *((int32_t*)(_nIns -= 4)) = d;
31927:             _nvprof("x64-bytes", 4);
31927:         }
32706:         return op;
32706:     }
32706: 
32706:     // disp32 modrm form when the disp must be written separately (opcode is 4+ bytes)
32706:     void Assembler::emitrm_wide(uint64_t op, Register r, int32_t d, Register b) {
32706:         op = emit_disp32(op, d);
32706:         emitrr(op, r, b);
32706:     }
32706: 
32706:     // disp32 modrm form when the disp must be written separately (opcode is 4+ bytes)
32706:     // p = prefix -- opcode must have a 66, F2, or F3 prefix
32706:     void Assembler::emitprm(uint64_t op, Register r, int32_t d, Register b) {
32706:         op = emit_disp32(op, d);
31927:         emitprr(op, r, b);
31927:     }
31927: 
31927:     void Assembler::emitrr_imm(uint64_t op, Register r, Register b, int32_t imm) {
31927:         NanoAssert(IsGpReg(r) && IsGpReg(b));
31927:         underrunProtect(4+8); // room for imm plus fullsize op
31927:         *((int32_t*)(_nIns -= 4)) = imm;
31927:         _nvprof("x86-bytes", 4);
31927:         emitrr(op, r, b);
31927:     }
31927: 
35101:     void Assembler::emitr_imm64(uint64_t op, Register r, uint64_t imm64) {
35101:         underrunProtect(8+8); // imm64 + worst case instr len
35101:         *((uint64_t*)(_nIns -= 8)) = imm64;
35101:         _nvprof("x64-bytes", 8);
35101:         emitr(op, r);
35101:     }
35101: 
35087:     void Assembler::emitrxb_imm(uint64_t op, Register r, Register x, Register b, int32_t imm) {
35087:         NanoAssert(IsGpReg(r) && IsGpReg(x) && IsGpReg(b));
35087:         underrunProtect(4+8); // room for imm plus fullsize op
35087:         *((int32_t*)(_nIns -= 4)) = imm;
35087:         _nvprof("x86-bytes", 4);
35087:         emitrxb(op, r, x, b);
35087:     }
35087: 
31927:     // op = [rex][opcode][modrm][imm8]
31927:     void Assembler::emitr_imm8(uint64_t op, Register b, int32_t imm8) {
31927:         NanoAssert(IsGpReg(b) && isS8(imm8));
31927:         op |= uint64_t(imm8)<<56 | uint64_t(b&7)<<48;  // modrm is 2nd to last byte
31927:         emit(rexrb(op, (Register)0, b));
31927:     }
31927: 
35101:     void Assembler::emitxm_abs(uint64_t op, Register r, int32_t addr32)
35101:     {
35101:         underrunProtect(4+8);
35101:         *((int32_t*)(_nIns -= 4)) = addr32;
35101:         _nvprof("x64-bytes", 4);
35101:         op = op | uint64_t((r&7)<<3)<<48; // put rr[0:2] into mod/rm byte
35101:         op = rexrb(op, r, (Register)0);   // put rr[3] into rex byte
35101:         emit(op);
35101:     }
35101: 
35101:     void Assembler::emitxm_rel(uint64_t op, Register r, NIns* addr64)
35101:     {
35101:         underrunProtect(4+8);
35101:         int32_t d = (int32_t)(addr64 - _nIns);
35101:         *((int32_t*)(_nIns -= 4)) = d;
35101:         _nvprof("x64-bytes", 4);
35101:         emitrr(op, r, (Register)0);
35101:     }
35101: 
35312:     // Succeeds if 'target' is within a signed 8-bit offset from the current
35312:     // instruction's address.
35312:     bool Assembler::isTargetWithinS8(NIns* target)
35312:     {
35312:         NanoAssert(target);
35312:         // First call underrunProtect().  Without it, we might compute the
35312:         // difference just before starting a new code chunk.
35312:         underrunProtect(8);
35312:         return isS8(target - _nIns);
35312:     }
35312: 
35312:     // Like isTargetWithinS8(), but for signed 32-bit offsets.
35312:     bool Assembler::isTargetWithinS32(NIns* target)
35312:     {
35312:         NanoAssert(target);
35312:         underrunProtect(8);
35312:         return isS32(target - _nIns);
35312:     }
35312: 
35101: #define RB(r)       gpRegNames8[(r)]
36414: #define RS(r)       gpRegNames16[(r)]
35101: #define RBhi(r)     gpRegNames8hi[(r)]
35101: #define RL(r)       gpRegNames32[(r)]
35101: #define RQ(r)       gpn(r)
35101: 
35101: #define R           Register
35101: #define I           int
35101: #define I32         int32_t
35101: #define U64         uint64_t
35101: #define S           size_t
35101: 
35101:     void Assembler::PUSHR(R r)  { emitr(X64_pushr,r); asm_output("push %s", RQ(r)); }
35101:     void Assembler::POPR( R r)  { emitr(X64_popr, r); asm_output("pop %s",  RQ(r)); }
35101:     void Assembler::NOT(  R r)  { emitr(X64_not,  r); asm_output("notl %s", RL(r)); }
35101:     void Assembler::NEG(  R r)  { emitr(X64_neg,  r); asm_output("negl %s", RL(r)); }
35101:     void Assembler::IDIV( R r)  { emitr(X64_idiv, r); asm_output("idivl edx:eax, %s",RL(r)); }
35101: 
35101:     void Assembler::SHR( R r)   { emitr(X64_shr,  r); asm_output("shrl %s, ecx", RL(r)); }
35101:     void Assembler::SAR( R r)   { emitr(X64_sar,  r); asm_output("sarl %s, ecx", RL(r)); }
35101:     void Assembler::SHL( R r)   { emitr(X64_shl,  r); asm_output("shll %s, ecx", RL(r)); }
35101:     void Assembler::SHRQ(R r)   { emitr(X64_shrq, r); asm_output("shrq %s, ecx", RQ(r)); }
35101:     void Assembler::SARQ(R r)   { emitr(X64_sarq, r); asm_output("sarq %s, ecx", RQ(r)); }
35101:     void Assembler::SHLQ(R r)   { emitr(X64_shlq, r); asm_output("shlq %s, ecx", RQ(r)); }
35101: 
35101:     void Assembler::SHRI( R r, I i)   { emit8(rexrb(X64_shri  | U64(r&7)<<48, (R)0, r), i); asm_output("shrl %s, %d", RL(r), i); }
35101:     void Assembler::SARI( R r, I i)   { emit8(rexrb(X64_sari  | U64(r&7)<<48, (R)0, r), i); asm_output("sarl %s, %d", RL(r), i); }
35101:     void Assembler::SHLI( R r, I i)   { emit8(rexrb(X64_shli  | U64(r&7)<<48, (R)0, r), i); asm_output("shll %s, %d", RL(r), i); }
35101:     void Assembler::SHRQI(R r, I i)   { emit8(rexrb(X64_shrqi | U64(r&7)<<48, (R)0, r), i); asm_output("shrq %s, %d", RQ(r), i); }
35101:     void Assembler::SARQI(R r, I i)   { emit8(rexrb(X64_sarqi | U64(r&7)<<48, (R)0, r), i); asm_output("sarq %s, %d", RQ(r), i); }
35101:     void Assembler::SHLQI(R r, I i)   { emit8(rexrb(X64_shlqi | U64(r&7)<<48, (R)0, r), i); asm_output("shlq %s, %d", RQ(r), i); }
35101: 
35101:     void Assembler::SETE( R r)  { emitr8(X64_sete, r); asm_output("sete %s", RB(r)); }
35101:     void Assembler::SETL( R r)  { emitr8(X64_setl, r); asm_output("setl %s", RB(r)); }
35101:     void Assembler::SETLE(R r)  { emitr8(X64_setle,r); asm_output("setle %s",RB(r)); }
35101:     void Assembler::SETG( R r)  { emitr8(X64_setg, r); asm_output("setg %s", RB(r)); }
35101:     void Assembler::SETGE(R r)  { emitr8(X64_setge,r); asm_output("setge %s",RB(r)); }
35101:     void Assembler::SETB( R r)  { emitr8(X64_setb, r); asm_output("setb %s", RB(r)); }
35101:     void Assembler::SETBE(R r)  { emitr8(X64_setbe,r); asm_output("setbe %s",RB(r)); }
35101:     void Assembler::SETA( R r)  { emitr8(X64_seta, r); asm_output("seta %s", RB(r)); }
35101:     void Assembler::SETAE(R r)  { emitr8(X64_setae,r); asm_output("setae %s",RB(r)); }
35101:     void Assembler::SETO( R r)  { emitr8(X64_seto, r); asm_output("seto %s", RB(r)); }
35101: 
35101:     void Assembler::ADDRR(R l, R r)     { emitrr(X64_addrr,l,r); asm_output("addl %s, %s", RL(l),RL(r)); }
35101:     void Assembler::SUBRR(R l, R r)     { emitrr(X64_subrr,l,r); asm_output("subl %s, %s", RL(l),RL(r)); }
35101:     void Assembler::ANDRR(R l, R r)     { emitrr(X64_andrr,l,r); asm_output("andl %s, %s", RL(l),RL(r)); }
35101:     void Assembler::ORLRR(R l, R r)     { emitrr(X64_orlrr,l,r); asm_output("orl %s, %s",  RL(l),RL(r)); }
35101:     void Assembler::XORRR(R l, R r)     { emitrr(X64_xorrr,l,r); asm_output("xorl %s, %s", RL(l),RL(r)); }
35101:     void Assembler::IMUL( R l, R r)     { emitrr(X64_imul, l,r); asm_output("imull %s, %s",RL(l),RL(r)); }
35101:     void Assembler::CMPLR(R l, R r)     { emitrr(X64_cmplr,l,r); asm_output("cmpl %s, %s", RL(l),RL(r)); }
35101:     void Assembler::MOVLR(R l, R r)     { emitrr(X64_movlr,l,r); asm_output("movl %s, %s", RL(l),RL(r)); }
35101: 
35101:     void Assembler::ADDQRR( R l, R r)   { emitrr(X64_addqrr, l,r); asm_output("addq %s, %s",  RQ(l),RQ(r)); }
35101:     void Assembler::SUBQRR( R l, R r)   { emitrr(X64_subqrr, l,r); asm_output("subq %s, %s",  RQ(l),RQ(r)); }
35101:     void Assembler::ANDQRR( R l, R r)   { emitrr(X64_andqrr, l,r); asm_output("andq %s, %s",  RQ(l),RQ(r)); }
35101:     void Assembler::ORQRR(  R l, R r)   { emitrr(X64_orqrr,  l,r); asm_output("orq %s, %s",   RQ(l),RQ(r)); }
35101:     void Assembler::XORQRR( R l, R r)   { emitrr(X64_xorqrr, l,r); asm_output("xorq %s, %s",  RQ(l),RQ(r)); }
35101:     void Assembler::CMPQR(  R l, R r)   { emitrr(X64_cmpqr,  l,r); asm_output("cmpq %s, %s",  RQ(l),RQ(r)); }
35101:     void Assembler::MOVQR(  R l, R r)   { emitrr(X64_movqr,  l,r); asm_output("movq %s, %s",  RQ(l),RQ(r)); }
35101:     void Assembler::MOVAPSR(R l, R r)   { emitrr(X64_movapsr,l,r); asm_output("movaps %s, %s",RQ(l),RQ(r)); }
35101: 
35101:     void Assembler::CMOVNO( R l, R r)   { emitrr(X64_cmovno, l,r); asm_output("cmovlno %s, %s",  RL(l),RL(r)); }
35101:     void Assembler::CMOVNE( R l, R r)   { emitrr(X64_cmovne, l,r); asm_output("cmovlne %s, %s",  RL(l),RL(r)); }
35101:     void Assembler::CMOVNL( R l, R r)   { emitrr(X64_cmovnl, l,r); asm_output("cmovlnl %s, %s",  RL(l),RL(r)); }
35101:     void Assembler::CMOVNLE(R l, R r)   { emitrr(X64_cmovnle,l,r); asm_output("cmovlnle %s, %s", RL(l),RL(r)); }
35101:     void Assembler::CMOVNG( R l, R r)   { emitrr(X64_cmovng, l,r); asm_output("cmovlng %s, %s",  RL(l),RL(r)); }
35101:     void Assembler::CMOVNGE(R l, R r)   { emitrr(X64_cmovnge,l,r); asm_output("cmovlnge %s, %s", RL(l),RL(r)); }
35101:     void Assembler::CMOVNB( R l, R r)   { emitrr(X64_cmovnb, l,r); asm_output("cmovlnb %s, %s",  RL(l),RL(r)); }
35101:     void Assembler::CMOVNBE(R l, R r)   { emitrr(X64_cmovnbe,l,r); asm_output("cmovlnbe %s, %s", RL(l),RL(r)); }
35101:     void Assembler::CMOVNA( R l, R r)   { emitrr(X64_cmovna, l,r); asm_output("cmovlna %s, %s",  RL(l),RL(r)); }
35101:     void Assembler::CMOVNAE(R l, R r)   { emitrr(X64_cmovnae,l,r); asm_output("cmovlnae %s, %s", RL(l),RL(r)); }
35101: 
35101:     void Assembler::CMOVQNO( R l, R r)  { emitrr(X64_cmovqno, l,r); asm_output("cmovqno %s, %s",  RQ(l),RQ(r)); }
35101:     void Assembler::CMOVQNE( R l, R r)  { emitrr(X64_cmovqne, l,r); asm_output("cmovqne %s, %s",  RQ(l),RQ(r)); }
35101:     void Assembler::CMOVQNL( R l, R r)  { emitrr(X64_cmovqnl, l,r); asm_output("cmovqnl %s, %s",  RQ(l),RQ(r)); }
35101:     void Assembler::CMOVQNLE(R l, R r)  { emitrr(X64_cmovqnle,l,r); asm_output("cmovqnle %s, %s", RQ(l),RQ(r)); }
35101:     void Assembler::CMOVQNG( R l, R r)  { emitrr(X64_cmovqng, l,r); asm_output("cmovqng %s, %s",  RQ(l),RQ(r)); }
35101:     void Assembler::CMOVQNGE(R l, R r)  { emitrr(X64_cmovqnge,l,r); asm_output("cmovqnge %s, %s", RQ(l),RQ(r)); }
35101:     void Assembler::CMOVQNB( R l, R r)  { emitrr(X64_cmovqnb, l,r); asm_output("cmovqnb %s, %s",  RQ(l),RQ(r)); }
35101:     void Assembler::CMOVQNBE(R l, R r)  { emitrr(X64_cmovqnbe,l,r); asm_output("cmovqnbe %s, %s", RQ(l),RQ(r)); }
35101:     void Assembler::CMOVQNA( R l, R r)  { emitrr(X64_cmovqna, l,r); asm_output("cmovqna %s, %s",  RQ(l),RQ(r)); }
35101:     void Assembler::CMOVQNAE(R l, R r)  { emitrr(X64_cmovqnae,l,r); asm_output("cmovqnae %s, %s", RQ(l),RQ(r)); }
35101: 
35101:     void Assembler::MOVSXDR(R l, R r)   { emitrr(X64_movsxdr,l,r); asm_output("movsxd %s, %s",RQ(l),RL(r)); }
35101: 
35101:     void Assembler::MOVZX8(R l, R r)    { emitrr8(X64_movzx8,l,r); asm_output("movzx %s, %s",RQ(l),RB(r)); }
35101: 
35101: // XORPS is a 4x32f vector operation, we use it instead of the more obvious
35101: // XORPD because it's one byte shorter.  This is ok because it's only used for
35101: // zeroing an XMM register;  hence the single argument.
36413: // Also note that (unlike most SSE2 instructions) XORPS does not have a prefix, thus emitrr() should be used.
36413:     void Assembler::XORPS(        R r)  { emitrr(X64_xorps,   r,r); asm_output("xorps %s, %s",   RQ(r),RQ(r)); }
35101:     void Assembler::DIVSD(   R l, R r)  { emitprr(X64_divsd,   l,r); asm_output("divsd %s, %s",   RQ(l),RQ(r)); }
35101:     void Assembler::MULSD(   R l, R r)  { emitprr(X64_mulsd,   l,r); asm_output("mulsd %s, %s",   RQ(l),RQ(r)); }
35101:     void Assembler::ADDSD(   R l, R r)  { emitprr(X64_addsd,   l,r); asm_output("addsd %s, %s",   RQ(l),RQ(r)); }
35101:     void Assembler::SUBSD(   R l, R r)  { emitprr(X64_subsd,   l,r); asm_output("subsd %s, %s",   RQ(l),RQ(r)); }
35101:     void Assembler::CVTSQ2SD(R l, R r)  { emitprr(X64_cvtsq2sd,l,r); asm_output("cvtsq2sd %s, %s",RQ(l),RQ(r)); }
35101:     void Assembler::CVTSI2SD(R l, R r)  { emitprr(X64_cvtsi2sd,l,r); asm_output("cvtsi2sd %s, %s",RQ(l),RL(r)); }
36414:     void Assembler::CVTSS2SD(R l, R r)  { emitprr(X64_cvtss2sd,l,r); asm_output("cvtss2sd %s, %s",RQ(l),RL(r)); }
36414:     void Assembler::CVTSD2SS(R l, R r)  { emitprr(X64_cvtsd2ss,l,r); asm_output("cvtsd2ss %s, %s",RL(l),RQ(r)); }
37700:     void Assembler::CVTSD2SI(R l, R r)  { emitprr(X64_cvtsd2si,l,r); asm_output("cvtsd2si %s, %s",RL(l),RQ(r)); }
35101:     void Assembler::UCOMISD( R l, R r)  { emitprr(X64_ucomisd, l,r); asm_output("ucomisd %s, %s", RQ(l),RQ(r)); }
35101:     void Assembler::MOVQRX(  R l, R r)  { emitprr(X64_movqrx,  r,l); asm_output("movq %s, %s",    RQ(l),RQ(r)); } // Nb: r and l are deliberately reversed within the emitprr() call.
35101:     void Assembler::MOVQXR(  R l, R r)  { emitprr(X64_movqxr,  l,r); asm_output("movq %s, %s",    RQ(l),RQ(r)); }
35101: 
35101: // MOVI must not affect condition codes!
35101:     void Assembler::MOVI(  R r, I32 i32)    { emitr_imm(X64_movi,  r,i32); asm_output("movl %s, %d",RL(r),i32); }
35101:     void Assembler::ADDLRI(R r, I32 i32)    { emitr_imm(X64_addlri,r,i32); asm_output("addl %s, %d",RL(r),i32); }
35101:     void Assembler::SUBLRI(R r, I32 i32)    { emitr_imm(X64_sublri,r,i32); asm_output("subl %s, %d",RL(r),i32); }
35101:     void Assembler::ANDLRI(R r, I32 i32)    { emitr_imm(X64_andlri,r,i32); asm_output("andl %s, %d",RL(r),i32); }
35101:     void Assembler::ORLRI( R r, I32 i32)    { emitr_imm(X64_orlri, r,i32); asm_output("orl %s, %d", RL(r),i32); }
35101:     void Assembler::XORLRI(R r, I32 i32)    { emitr_imm(X64_xorlri,r,i32); asm_output("xorl %s, %d",RL(r),i32); }
35101:     void Assembler::CMPLRI(R r, I32 i32)    { emitr_imm(X64_cmplri,r,i32); asm_output("cmpl %s, %d",RL(r),i32); }
35101: 
35101:     void Assembler::ADDQRI( R r, I32 i32)   { emitr_imm(X64_addqri, r,i32); asm_output("addq %s, %d",   RQ(r),i32); }
35101:     void Assembler::SUBQRI( R r, I32 i32)   { emitr_imm(X64_subqri, r,i32); asm_output("subq %s, %d",   RQ(r),i32); }
35101:     void Assembler::ANDQRI( R r, I32 i32)   { emitr_imm(X64_andqri, r,i32); asm_output("andq %s, %d",   RQ(r),i32); }
35101:     void Assembler::ORQRI(  R r, I32 i32)   { emitr_imm(X64_orqri,  r,i32); asm_output("orq %s, %d",    RQ(r),i32); }
35101:     void Assembler::XORQRI( R r, I32 i32)   { emitr_imm(X64_xorqri, r,i32); asm_output("xorq %s, %d",   RQ(r),i32); }
35101:     void Assembler::CMPQRI( R r, I32 i32)   { emitr_imm(X64_cmpqri, r,i32); asm_output("cmpq %s, %d",   RQ(r),i32); }
35101:     void Assembler::MOVQI32(R r, I32 i32)   { emitr_imm(X64_movqi32,r,i32); asm_output("movqi32 %s, %d",RQ(r),i32); }
35101: 
35101:     void Assembler::ADDLR8(R r, I32 i8)     { emitr_imm8(X64_addlr8,r,i8); asm_output("addl %s, %d", RL(r),i8); }
35101:     void Assembler::SUBLR8(R r, I32 i8)     { emitr_imm8(X64_sublr8,r,i8); asm_output("subl %s, %d", RL(r),i8); }
35101:     void Assembler::ANDLR8(R r, I32 i8)     { emitr_imm8(X64_andlr8,r,i8); asm_output("andl %s, %d", RL(r),i8); }
35101:     void Assembler::ORLR8( R r, I32 i8)     { emitr_imm8(X64_orlr8, r,i8); asm_output("orl %s, %d",  RL(r),i8); }
35101:     void Assembler::XORLR8(R r, I32 i8)     { emitr_imm8(X64_xorlr8,r,i8); asm_output("xorl %s, %d", RL(r),i8); }
35101:     void Assembler::CMPLR8(R r, I32 i8)     { emitr_imm8(X64_cmplr8,r,i8); asm_output("cmpl %s, %d", RL(r),i8); }
35101: 
35101:     void Assembler::ADDQR8(R r, I32 i8)     { emitr_imm8(X64_addqr8,r,i8); asm_output("addq %s, %d",RQ(r),i8); }
35101:     void Assembler::SUBQR8(R r, I32 i8)     { emitr_imm8(X64_subqr8,r,i8); asm_output("subq %s, %d",RQ(r),i8); }
35101:     void Assembler::ANDQR8(R r, I32 i8)     { emitr_imm8(X64_andqr8,r,i8); asm_output("andq %s, %d",RQ(r),i8); }
35101:     void Assembler::ORQR8( R r, I32 i8)     { emitr_imm8(X64_orqr8, r,i8); asm_output("orq %s, %d", RQ(r),i8); }
35101:     void Assembler::XORQR8(R r, I32 i8)     { emitr_imm8(X64_xorqr8,r,i8); asm_output("xorq %s, %d",RQ(r),i8); }
35101:     void Assembler::CMPQR8(R r, I32 i8)     { emitr_imm8(X64_cmpqr8,r,i8); asm_output("cmpq %s, %d",RQ(r),i8); }
35101: 
35101:     void Assembler::IMULI(R l, R r, I32 i32)    { emitrr_imm(X64_imuli,l,r,i32); asm_output("imuli %s, %s, %d",RL(l),RL(r),i32); }
35101: 
35313:     void Assembler::MOVQI(R r, U64 u64)         { emitr_imm64(X64_movqi,r,u64); asm_output("movq %s, %p",RQ(r),(void*)u64); }
35101: 
35101:     void Assembler::LEARIP(R r, I32 d)          { emitrm(X64_learip,r,d,(Register)0); asm_output("lea %s, %d(rip)",RQ(r),d); }
35101: 
37016:     void Assembler::LEAQRM(R r, I d, R b)       { emitrm(X64_leaqrm,r,d,b); asm_output("leaq %s, %d(%s)",RQ(r),d,RQ(b)); }
37016:     void Assembler::MOVLRM(R r, I d, R b)       { emitrm(X64_movlrm,r,d,b); asm_output("movl %s, %d(%s)",RL(r),d,RQ(b)); }
37016:     void Assembler::MOVQRM(R r, I d, R b)       { emitrm(X64_movqrm,r,d,b); asm_output("movq %s, %d(%s)",RQ(r),d,RQ(b)); }
37016:     void Assembler::MOVBMR(R r, I d, R b)       { emitrm8(X64_movbmr,r,d,b); asm_output("movb %d(%s), %s",d,RQ(b),RB(r)); }
37016:     void Assembler::MOVSMR(R r, I d, R b)       { emitprm(X64_movsmr,r,d,b); asm_output("movs %d(%s), %s",d,RQ(b),RS(r)); }
37016:     void Assembler::MOVLMR(R r, I d, R b)       { emitrm(X64_movlmr,r,d,b); asm_output("movl %d(%s), %s",d,RQ(b),RL(r)); }
37016:     void Assembler::MOVQMR(R r, I d, R b)       { emitrm(X64_movqmr,r,d,b); asm_output("movq %d(%s), %s",d,RQ(b),RQ(r)); }
35101: 
37016:     void Assembler::MOVZX8M( R r, I d, R b)     { emitrm_wide(X64_movzx8m, r,d,b); asm_output("movzxb %s, %d(%s)",RQ(r),d,RQ(b)); }
37016:     void Assembler::MOVZX16M(R r, I d, R b)     { emitrm_wide(X64_movzx16m,r,d,b); asm_output("movzxs %s, %d(%s)",RQ(r),d,RQ(b)); }
35101: 
37016:     void Assembler::MOVSX8M( R r, I d, R b)     { emitrm_wide(X64_movsx8m, r,d,b); asm_output("movsxb %s, %d(%s)",RQ(r),d,RQ(b)); }
37016:     void Assembler::MOVSX16M(R r, I d, R b)     { emitrm_wide(X64_movsx16m,r,d,b); asm_output("movsxs %s, %d(%s)",RQ(r),d,RQ(b)); }
36414: 
37016:     void Assembler::MOVSDRM(R r, I d, R b)      { emitprm(X64_movsdrm,r,d,b); asm_output("movsd %s, %d(%s)",RQ(r),d,RQ(b)); }
37016:     void Assembler::MOVSDMR(R r, I d, R b)      { emitprm(X64_movsdmr,r,d,b); asm_output("movsd %d(%s), %s",d,RQ(b),RQ(r)); }
37016:     void Assembler::MOVSSRM(R r, I d, R b)      { emitprm(X64_movssrm,r,d,b); asm_output("movss %s, %d(%s)",RQ(r),d,RQ(b)); }
37016:     void Assembler::MOVSSMR(R r, I d, R b)      { emitprm(X64_movssmr,r,d,b); asm_output("movss %d(%s), %s",d,RQ(b),RQ(r)); }
35101: 
35101:     void Assembler::JMP8( S n, NIns* t)    { emit_target8(n, X64_jmp8,t); asm_output("jmp %p", t); }
35101: 
35101:     void Assembler::JMP32(S n, NIns* t)    { emit_target32(n,X64_jmp, t); asm_output("jmp %p", t); }
35101: 
35364:     void Assembler::JMPX(R indexreg, NIns** table)  { emitrxb_imm(X64_jmpx, (R)0, indexreg, (Register)5, (int32_t)(uintptr_t)table); asm_output("jmpq [%s*8 + %p]", RQ(indexreg), (void*)table); }
35364: 
35364:     void Assembler::JMPXB(R indexreg, R tablereg)   { emitxb(X64_jmpxb, indexreg, tablereg); asm_output("jmp [%s*8 + %s]", RQ(indexreg), RQ(tablereg)); }
35364: 
35101:     void Assembler::JO( S n, NIns* t)      { emit_target32(n,X64_jo,  t); asm_output("jo %p", t); }
35101:     void Assembler::JE( S n, NIns* t)      { emit_target32(n,X64_je,  t); asm_output("je %p", t); }
35101:     void Assembler::JL( S n, NIns* t)      { emit_target32(n,X64_jl,  t); asm_output("jl %p", t); }
35101:     void Assembler::JLE(S n, NIns* t)      { emit_target32(n,X64_jle, t); asm_output("jle %p",t); }
35101:     void Assembler::JG( S n, NIns* t)      { emit_target32(n,X64_jg,  t); asm_output("jg %p", t); }
35101:     void Assembler::JGE(S n, NIns* t)      { emit_target32(n,X64_jge, t); asm_output("jge %p",t); }
35101:     void Assembler::JB( S n, NIns* t)      { emit_target32(n,X64_jb,  t); asm_output("jb %p", t); }
35101:     void Assembler::JBE(S n, NIns* t)      { emit_target32(n,X64_jbe, t); asm_output("jbe %p",t); }
35101:     void Assembler::JA( S n, NIns* t)      { emit_target32(n,X64_ja,  t); asm_output("ja %p", t); }
35101:     void Assembler::JAE(S n, NIns* t)      { emit_target32(n,X64_jae, t); asm_output("jae %p",t); }
35101:     void Assembler::JP( S n, NIns* t)      { emit_target32(n,X64_jp,  t); asm_output("jp  %p",t); }
35101: 
35101:     void Assembler::JNO( S n, NIns* t)     { emit_target32(n,X64_jo ^X64_jneg, t); asm_output("jno %p", t); }
35101:     void Assembler::JNE( S n, NIns* t)     { emit_target32(n,X64_je ^X64_jneg, t); asm_output("jne %p", t); }
35101:     void Assembler::JNL( S n, NIns* t)     { emit_target32(n,X64_jl ^X64_jneg, t); asm_output("jnl %p", t); }
35101:     void Assembler::JNLE(S n, NIns* t)     { emit_target32(n,X64_jle^X64_jneg, t); asm_output("jnle %p",t); }
35101:     void Assembler::JNG( S n, NIns* t)     { emit_target32(n,X64_jg ^X64_jneg, t); asm_output("jng %p", t); }
35101:     void Assembler::JNGE(S n, NIns* t)     { emit_target32(n,X64_jge^X64_jneg, t); asm_output("jnge %p",t); }
35101:     void Assembler::JNB( S n, NIns* t)     { emit_target32(n,X64_jb ^X64_jneg, t); asm_output("jnb %p", t); }
35101:     void Assembler::JNBE(S n, NIns* t)     { emit_target32(n,X64_jbe^X64_jneg, t); asm_output("jnbe %p",t); }
35101:     void Assembler::JNA( S n, NIns* t)     { emit_target32(n,X64_ja ^X64_jneg, t); asm_output("jna %p", t); }
35101:     void Assembler::JNAE(S n, NIns* t)     { emit_target32(n,X64_jae^X64_jneg, t); asm_output("jnae %p",t); }
35101: 
35101:     void Assembler::JO8( S n, NIns* t)     { emit_target8(n,X64_jo8,  t); asm_output("jo %p", t); }
35101:     void Assembler::JE8( S n, NIns* t)     { emit_target8(n,X64_je8,  t); asm_output("je %p", t); }
35101:     void Assembler::JL8( S n, NIns* t)     { emit_target8(n,X64_jl8,  t); asm_output("jl %p", t); }
35101:     void Assembler::JLE8(S n, NIns* t)     { emit_target8(n,X64_jle8, t); asm_output("jle %p",t); }
35101:     void Assembler::JG8( S n, NIns* t)     { emit_target8(n,X64_jg8,  t); asm_output("jg %p", t); }
35101:     void Assembler::JGE8(S n, NIns* t)     { emit_target8(n,X64_jge8, t); asm_output("jge %p",t); }
35101:     void Assembler::JB8( S n, NIns* t)     { emit_target8(n,X64_jb8,  t); asm_output("jb %p", t); }
35101:     void Assembler::JBE8(S n, NIns* t)     { emit_target8(n,X64_jbe8, t); asm_output("jbe %p",t); }
35101:     void Assembler::JA8( S n, NIns* t)     { emit_target8(n,X64_ja8,  t); asm_output("ja %p", t); }
35101:     void Assembler::JAE8(S n, NIns* t)     { emit_target8(n,X64_jae8, t); asm_output("jae %p",t); }
35101:     void Assembler::JP8( S n, NIns* t)     { emit_target8(n,X64_jp8,  t); asm_output("jp  %p",t); }
35101: 
35101:     void Assembler::JNO8( S n, NIns* t)    { emit_target8(n,X64_jo8 ^X64_jneg8, t); asm_output("jno %p", t); }
35101:     void Assembler::JNE8( S n, NIns* t)    { emit_target8(n,X64_je8 ^X64_jneg8, t); asm_output("jne %p", t); }
35101:     void Assembler::JNL8( S n, NIns* t)    { emit_target8(n,X64_jl8 ^X64_jneg8, t); asm_output("jnl %p", t); }
35101:     void Assembler::JNLE8(S n, NIns* t)    { emit_target8(n,X64_jle8^X64_jneg8, t); asm_output("jnle %p",t); }
35101:     void Assembler::JNG8( S n, NIns* t)    { emit_target8(n,X64_jg8 ^X64_jneg8, t); asm_output("jng %p", t); }
35101:     void Assembler::JNGE8(S n, NIns* t)    { emit_target8(n,X64_jge8^X64_jneg8, t); asm_output("jnge %p",t); }
35101:     void Assembler::JNB8( S n, NIns* t)    { emit_target8(n,X64_jb8 ^X64_jneg8, t); asm_output("jnb %p", t); }
35101:     void Assembler::JNBE8(S n, NIns* t)    { emit_target8(n,X64_jbe8^X64_jneg8, t); asm_output("jnbe %p",t); }
35101:     void Assembler::JNA8( S n, NIns* t)    { emit_target8(n,X64_ja8 ^X64_jneg8, t); asm_output("jna %p", t); }
35101:     void Assembler::JNAE8(S n, NIns* t)    { emit_target8(n,X64_jae8^X64_jneg8, t); asm_output("jnae %p",t); }
35101: 
35101:     void Assembler::CALL( S n, NIns* t)    { emit_target32(n,X64_call,t); asm_output("call %p",t); }
35101: 
35101:     void Assembler::CALLRAX()       { emit(X64_callrax); asm_output("call (rax)"); }
35101:     void Assembler::RET()           { emit(X64_ret);     asm_output("ret");        }
35101: 
35101:     void Assembler::MOVQSPR(I d, R r)   { emit(X64_movqspr | U64(d) << 56 | U64((r&7)<<3) << 40 | U64((r&8)>>1) << 24); asm_output("movq %d(rsp), %s", d, RQ(r)); }    // insert r into mod/rm and rex bytes
35101: 
35101:     void Assembler::XORPSA(R r, I32 i32)    { emitxm_abs(X64_xorpsa, r, i32); asm_output("xorps %s, (0x%x)",RQ(r), i32); }
35101:     void Assembler::XORPSM(R r, NIns* a64)  { emitxm_rel(X64_xorpsm, r, a64); asm_output("xorps %s, (%p)",  RQ(r), a64); }
35101: 
35101:     void Assembler::X86_AND8R(R r)  { emit(X86_and8r | U64(r<<3|(r|4))<<56); asm_output("andb %s, %s", RB(r), RBhi(r)); }
35101:     void Assembler::X86_SETNP(R r)  { emit(X86_setnp | U64(r|4)<<56); asm_output("setnp %s", RBhi(r)); }
35101:     void Assembler::X86_SETE(R r)   { emit(X86_sete  | U64(r)<<56);   asm_output("sete %s", RB(r)); }
35101: 
35101: #undef R
35101: #undef I
35101: #undef I32
35101: #undef U64
35101: #undef S
35101: 
31927:     void Assembler::MR(Register d, Register s) {
31927:         NanoAssert(IsGpReg(d) && IsGpReg(s));
35101:         MOVQR(d, s);
31927:     }
31927: 
35101:     // This is needed for guards;  we must be able to patch the jmp later and
35101:     // we cannot do that if an 8-bit relative jump is used, so we can't use
35101:     // JMP().
32665:     void Assembler::JMPl(NIns* target) {
35101:         JMP32(8, target);
32665:     }
32665: 
31927:     void Assembler::JMP(NIns *target) {
35312:         if (!target || isTargetWithinS32(target)) {
35357:             if (target && isTargetWithinS8(target)) {
35101:                 JMP8(8, target);
31927:             } else {
35101:                 JMP32(8, target);
31927:             }
31927:         } else {
31927:             TODO(jmp64);
31927:         }
31927:     }
31927: 
31927:     // register allocation for 2-address style ops of the form R = R (op) B
31927:     void Assembler::regalloc_binary(LIns *ins, RegisterMask allow, Register &rr, Register &ra, Register &rb) {
32765: #ifdef _DEBUG
32765:         RegisterMask originalAllow = allow;
32765: #endif
31927:         rb = UnknownReg;
31927:         LIns *a = ins->oprnd1();
31927:         LIns *b = ins->oprnd2();
31927:         if (a != b) {
31927:             rb = findRegFor(b, allow);
31927:             allow &= ~rmask(rb);
31927:         }
31927:         rr = prepResultReg(ins, allow);
31927:         // if this is last use of a in reg, we can re-use result reg
35060:         if (a->isUnusedOrHasUnknownReg()) {
35089:             ra = findSpecificRegForUnallocated(a, rr);
35060:         } else if (!(allow & rmask(a->getReg()))) {
35060:             // 'a' already has a register assigned, but it's not valid.
33923:             // To make sure floating point operations stay in FPU registers
32765:             // as much as possible, make sure that only a few opcodes are
32765:             // reserving GPRs.
37020:             NanoAssert(a->isop(LIR_quad) || a->isop(LIR_float) ||
37020:                        a->isop(LIR_ldf) || a->isop(LIR_ldfc) ||
37020:                        a->isop(LIR_ldq) || a->isop(LIR_ldqc) ||
37020:                        a->isop(LIR_ld32f) || a->isop(LIR_ldc32f) ||
37020:                        a->isop(LIR_u2f) || a->isop(LIR_fcall));
32765:             allow &= ~rmask(rr);
32765:             ra = findRegFor(a, allow);
35060:         } else {
35060:             ra = a->getReg();
31927:         }
31927:         if (a == b) {
31927:             rb = ra;
31927:         }
32765:         NanoAssert(originalAllow & rmask(rr));
32765:         NanoAssert(originalAllow & rmask(ra));
32765:         NanoAssert(originalAllow & rmask(rb));
31927:     }
31927: 
31927:     void Assembler::asm_qbinop(LIns *ins) {
31927:         asm_arith(ins);
31927:     }
31927: 
31927:     void Assembler::asm_shift(LIns *ins) {
31927:         // shift require rcx for shift count
31927:         LIns *b = ins->oprnd2();
31927:         if (b->isconst()) {
31927:             asm_shift_imm(ins);
31927:             return;
31927:         }
31927:         Register rr, ra;
31927:         if (b != ins->oprnd1()) {
31927:             findSpecificRegFor(b, RCX);
31927:             regalloc_unary(ins, GpRegs & ~rmask(RCX), rr, ra);
31927:         } else {
31927:             // a == b means both must be in RCX
31927:             regalloc_unary(ins, rmask(RCX), rr, ra);
31927:         }
31927:         switch (ins->opcode()) {
31927:         default:
31927:             TODO(asm_shift);
35101:         case LIR_qursh: SHRQ(rr);   break;
35101:         case LIR_qirsh: SARQ(rr);   break;
35101:         case LIR_qilsh: SHLQ(rr);   break;
35101:         case LIR_ush:   SHR( rr);   break;
35101:         case LIR_rsh:   SAR( rr);   break;
35101:         case LIR_lsh:   SHL( rr);   break;
31927:         }
31927:         if (rr != ra)
31927:             MR(rr, ra);
31927:     }
31927: 
31927:     void Assembler::asm_shift_imm(LIns *ins) {
31927:         Register rr, ra;
31927:         regalloc_unary(ins, GpRegs, rr, ra);
35101:         int shift = ins->oprnd2()->imm32() & 63;
31927:         switch (ins->opcode()) {
31927:         default: TODO(shiftimm);
35101:         case LIR_qursh: SHRQI(rr, shift);   break;
35101:         case LIR_qirsh: SARQI(rr, shift);   break;
35101:         case LIR_qilsh: SHLQI(rr, shift);   break;
35101:         case LIR_ush:   SHRI( rr, shift);   break;
35101:         case LIR_rsh:   SARI( rr, shift);   break;
35101:         case LIR_lsh:   SHLI( rr, shift);   break;
31927:         }
31927:         if (rr != ra)
31927:             MR(rr, ra);
31927:     }
31927: 
31927:     static bool isImm32(LIns *ins) {
31927:         return ins->isconst() || (ins->isconstq() && isS32(ins->imm64()));
31927:     }
31927:     static int32_t getImm32(LIns *ins) {
31927:         return ins->isconst() ? ins->imm32() : int32_t(ins->imm64());
31927:     }
31927: 
31927:     // binary op, integer regs, rhs is int32 const
31927:     void Assembler::asm_arith_imm(LIns *ins) {
31927:         LIns *b = ins->oprnd2();
31927:         int32_t imm = getImm32(b);
31927:         LOpcode op = ins->opcode();
31927:         Register rr, ra;
31927:         if (op == LIR_mul) {
31927:             // imul has true 3-addr form, it doesn't clobber ra
31927:             rr = prepResultReg(ins, GpRegs);
31927:             LIns *a = ins->oprnd1();
31927:             ra = findRegFor(a, GpRegs);
35101:             IMULI(rr, ra, imm);
31927:             return;
31927:         }
31927:         regalloc_unary(ins, GpRegs, rr, ra);
31927:         if (isS8(imm)) {
31927:             switch (ins->opcode()) {
31927:             default: TODO(arith_imm8);
31927:             case LIR_iaddp:
35101:             case LIR_add:   ADDLR8(rr, imm);   break;
35101:             case LIR_and:   ANDLR8(rr, imm);   break;
35101:             case LIR_or:    ORLR8( rr, imm);   break;
35101:             case LIR_sub:   SUBLR8(rr, imm);   break;
35101:             case LIR_xor:   XORLR8(rr, imm);   break;
31927:             case LIR_qiadd:
35101:             case LIR_qaddp: ADDQR8(rr, imm);   break;
35101:             case LIR_qiand: ANDQR8(rr, imm);   break;
35101:             case LIR_qior:  ORQR8( rr, imm);   break;
35101:             case LIR_qxor:  XORQR8(rr, imm);   break;
31927:             }
31927:         } else {
31927:             switch (ins->opcode()) {
31927:             default: TODO(arith_imm);
31927:             case LIR_iaddp:
35101:             case LIR_add:   ADDLRI(rr, imm);   break;
35101:             case LIR_and:   ANDLRI(rr, imm);   break;
35101:             case LIR_or:    ORLRI( rr, imm);   break;
35101:             case LIR_sub:   SUBLRI(rr, imm);   break;
35101:             case LIR_xor:   XORLRI(rr, imm);   break;
31927:             case LIR_qiadd:
35101:             case LIR_qaddp: ADDQRI(rr, imm);   break;
35101:             case LIR_qiand: ANDQRI(rr, imm);   break;
35101:             case LIR_qior:  ORQRI( rr, imm);   break;
35101:             case LIR_qxor:  XORQRI(rr, imm);   break;
31927:             }
31927:         }
31927:         if (rr != ra)
31927:             MR(rr, ra);
31927:     }
31927: 
33095:     void Assembler::asm_div_mod(LIns *ins) {
33095:         LIns *div;
33095:         if (ins->opcode() == LIR_mod) {
33095:             // LIR_mod expects the LIR_div to be near
33095:             div = ins->oprnd1();
33095:             prepResultReg(ins, rmask(RDX));
33095:         } else {
33095:             div = ins;
33095:             evictIfActive(RDX);
33095:         }
33095: 
33095:         NanoAssert(div->isop(LIR_div));
33095: 
33095:         LIns *lhs = div->oprnd1();
33095:         LIns *rhs = div->oprnd2();
33095: 
33095:         prepResultReg(div, rmask(RAX));
33095: 
37672:         Register rhsReg = findRegFor(rhs, GpRegs & ~(rmask(RAX)|rmask(RDX)));
33095:         Register lhsReg = lhs->isUnusedOrHasUnknownReg()
35089:                           ? findSpecificRegForUnallocated(lhs, RAX)
33095:                           : lhs->getReg();
35101:         IDIV(rhsReg);
35101:         SARI(RDX, 31);
33095:         MR(RDX, RAX);
33095:         if (RAX != lhsReg)
33095:             MR(RAX, lhsReg);
33095:     }
33095: 
31927:     // binary op with integer registers
31927:     void Assembler::asm_arith(LIns *ins) {
31927:         Register rr, ra, rb;
33095: 
36390:         switch (ins->opcode()) {
36390:         case LIR_lsh: case LIR_qilsh:
36390:         case LIR_rsh: case LIR_qirsh:
36390:         case LIR_ush: case LIR_qursh:
31927:             asm_shift(ins);
31927:             return;
33095:         case LIR_mod:
33095:         case LIR_div:
33095:             asm_div_mod(ins);
33095:             return;
33095:         default:
33095:             break;
31927:         }
33095: 
31927:         LIns *b = ins->oprnd2();
31927:         if (isImm32(b)) {
31927:             asm_arith_imm(ins);
31927:             return;
31927:         }
31927:         regalloc_binary(ins, GpRegs, rr, ra, rb);
31927:         switch (ins->opcode()) {
35101:         default:        TODO(asm_arith);
35101:         case LIR_or:    ORLRR(rr, rb);  break;
35101:         case LIR_sub:   SUBRR(rr, rb);  break;
31927:         case LIR_iaddp:
35101:         case LIR_add:   ADDRR(rr, rb);  break;
35101:         case LIR_and:   ANDRR(rr, rb);  break;
35101:         case LIR_xor:   XORRR(rr, rb);  break;
35101:         case LIR_mul:   IMUL(rr, rb);   break;
35101:         case LIR_qxor:  XORQRR(rr, rb); break;
35101:         case LIR_qior:  ORQRR(rr, rb);  break;
35101:         case LIR_qiand: ANDQRR(rr, rb); break;
31927:         case LIR_qiadd:
35101:         case LIR_qaddp: ADDQRR(rr, rb); break;
31927:         }
31927:         if (rr != ra)
31927:             MR(rr,ra);
31927:     }
31927: 
31927:     // binary op with fp registers
31927:     void Assembler::asm_fop(LIns *ins) {
31927:         Register rr, ra, rb;
31927:         regalloc_binary(ins, FpRegs, rr, ra, rb);
31927:         switch (ins->opcode()) {
35101:         default:       TODO(asm_fop);
35101:         case LIR_fdiv: DIVSD(rr, rb); break;
35101:         case LIR_fmul: MULSD(rr, rb); break;
35101:         case LIR_fadd: ADDSD(rr, rb); break;
35101:         case LIR_fsub: SUBSD(rr, rb); break;
31927:         }
31927:         if (rr != ra) {
31927:             asm_nongp_copy(rr, ra);
31927:         }
31927:     }
31927: 
31927:     void Assembler::asm_neg_not(LIns *ins) {
31927:         Register rr, ra;
31927:         regalloc_unary(ins, GpRegs, rr, ra);
31927:         NanoAssert(IsGpReg(ra));
35101:         if (ins->isop(LIR_not))
35101:             NOT(rr);
35101:         else
35101:             NEG(rr);
31927:         if (rr != ra)
31927:             MR(rr, ra);
31927:     }
31927: 
31927:     void Assembler::asm_call(LIns *ins) {
36374:         Register retReg = ( ins->isop(LIR_fcall) ? XMM0 : retRegs[0] );
36374:         prepResultReg(ins, rmask(retReg));
36374: 
36374:         // Do this after we've handled the call result, so we don't
36374:         // force the call result to be spilled unnecessarily.
36374: 
36374:         evictScratchRegs();
36374: 
31927:         const CallInfo *call = ins->callInfo();
31927:         ArgSize sizes[MAXARGS];
31927:         int argc = call->get_sizes(sizes);
31927: 
31927:         bool indirect = call->isIndirect();
31927:         if (!indirect) {
31927:             verbose_only(if (_logc->lcbits & LC_Assembly)
31927:                 outputf("        %p:", _nIns);
31927:             )
31927:             NIns *target = (NIns*)call->_address;
35312:             if (isTargetWithinS32(target)) {
35101:                 CALL(8, target);
31927:             } else {
31927:                 // can't reach target from here, load imm64 and do an indirect jump
35101:                 CALLRAX();
35101:                 asm_quad(RAX, (uint64_t)target);
31927:             }
31927:         } else {
31927:             // Indirect call: we assign the address arg to RAX since it's not
31927:             // used for regular arguments, and is otherwise scratch since it's
31927:             // clobberred by the call.
31927:             asm_regarg(ARGSIZE_P, ins->arg(--argc), RAX);
35101:             CALLRAX();
31927:         }
31927: 
33539:     #ifdef _WIN64
31927:         int stk_used = 32; // always reserve 32byte shadow area
31927:     #else
31927:         int stk_used = 0;
31927:         Register fr = XMM0;
31927:     #endif
31927:         int arg_index = 0;
31927:         for (int i = 0; i < argc; i++) {
31927:             int j = argc - i - 1;
31927:             ArgSize sz = sizes[j];
31927:             LIns* arg = ins->arg(j);
31927:             if ((sz & ARGSIZE_MASK_INT) && arg_index < NumArgRegs) {
31927:                 // gp arg
31927:                 asm_regarg(sz, arg, argRegs[arg_index]);
31927:                 arg_index++;
31927:             }
33539:         #ifdef _WIN64
31927:             else if (sz == ARGSIZE_F && arg_index < NumArgRegs) {
31927:                 // double goes in XMM reg # based on overall arg_index
31927:                 asm_regarg(sz, arg, Register(XMM0+arg_index));
31927:                 arg_index++;
31927:             }
31927:         #else
31927:             else if (sz == ARGSIZE_F && fr < XMM8) {
31927:                 // double goes in next available XMM register
31927:                 asm_regarg(sz, arg, fr);
31927:                 fr = nextreg(fr);
31927:             }
31927:         #endif
31927:             else {
31927:                 asm_stkarg(sz, arg, stk_used);
31927:                 stk_used += sizeof(void*);
31927:             }
31927:         }
31927: 
31927:         if (stk_used > max_stk_used)
31927:             max_stk_used = stk_used;
31927:     }
31927: 
31927:     void Assembler::asm_regarg(ArgSize sz, LIns *p, Register r) {
31927:         if (sz == ARGSIZE_I) {
31927:             NanoAssert(!p->isQuad());
31927:             if (p->isconst()) {
35101:                 asm_quad(r, int64_t(p->imm32()));
31927:                 return;
31927:             }
31927:             // sign extend int32 to int64
35101:             MOVSXDR(r, r);
31927:         } else if (sz == ARGSIZE_U) {
31927:             NanoAssert(!p->isQuad());
31927:             if (p->isconst()) {
35101:                 asm_quad(r, uint64_t(uint32_t(p->imm32())));
31927:                 return;
31927:             }
31927:             // zero extend with 32bit mov, auto-zeros upper 32bits
35101:             MOVLR(r, r);
31927:         }
31927:         /* there is no point in folding an immediate here, because
31927:          * the argument register must be a scratch register and we're
31927:          * just before a call.  Just reserving the register will cause
31927:          * the constant to be rematerialized nearby in asm_restore(),
31927:          * which is the same instruction we would otherwise emit right
31927:          * here, and moving it earlier in the stream provides more scheduling
31927:          * freedom to the cpu. */
31927:         findSpecificRegFor(p, r);
31927:     }
31927: 
31927:     void Assembler::asm_stkarg(ArgSize sz, LIns *p, int stk_off) {
31927:         NanoAssert(isS8(stk_off));
31927:         if (sz & ARGSIZE_MASK_INT) {
31927:             Register r = findRegFor(p, GpRegs);
35101:             MOVQSPR(stk_off, r);    // movq [rsp+d8], r
31927:             if (sz == ARGSIZE_I) {
31927:                 // extend int32 to int64
31927:                 NanoAssert(!p->isQuad());
35101:                 MOVSXDR(r, r);
31927:             } else if (sz == ARGSIZE_U) {
31927:                 // extend uint32 to uint64
31927:                 NanoAssert(!p->isQuad());
35101:                 MOVLR(r, r);
31927:             }
31927:         } else {
31927:             TODO(asm_stkarg_non_int);
31927:         }
31927:     }
31927: 
31927:     void Assembler::asm_promote(LIns *ins) {
31927:         Register rr, ra;
31927:         regalloc_unary(ins, GpRegs, rr, ra);
31927:         NanoAssert(IsGpReg(ra));
31927:         if (ins->isop(LIR_u2q)) {
35101:             MOVLR(rr, ra);      // 32bit mov zeros the upper 32bits of the target
31927:         } else {
31927:             NanoAssert(ins->isop(LIR_i2q));
35101:             MOVSXDR(rr, ra);    // sign extend 32->64
31927:         }
31927:     }
31927: 
31927:     // the CVTSI2SD instruction only writes to the low 64bits of the target
31927:     // XMM register, which hinders register renaming and makes dependence
31927:     // chains longer.  So we precede with XORPS to clear the target register.
31927: 
31927:     void Assembler::asm_i2f(LIns *ins) {
31927:         Register r = prepResultReg(ins, FpRegs);
31927:         Register b = findRegFor(ins->oprnd1(), GpRegs);
35101:         CVTSI2SD(r, b);     // cvtsi2sd xmmr, b  only writes xmm:0:64
35101:         XORPS(r);           // xorps xmmr,xmmr to break dependency chains
31927:     }
31927: 
31927:     void Assembler::asm_u2f(LIns *ins) {
31927:         Register r = prepResultReg(ins, FpRegs);
31927:         Register b = findRegFor(ins->oprnd1(), GpRegs);
31927:         NanoAssert(!ins->oprnd1()->isQuad());
31927:         // since oprnd1 value is 32bit, its okay to zero-extend the value without worrying about clobbering.
35101:         CVTSQ2SD(r, b);     // convert int64 to double
35101:         XORPS(r);           // xorps xmmr,xmmr to break dependency chains
35101:         MOVLR(b, b);        // zero extend u32 to int64
31927:     }
31927: 
37700:     void Assembler::asm_f2i(LIns *ins) {
37700:         LIns *lhs = ins->oprnd1();
37700: 
37700:         NanoAssert(!ins->isQuad() && lhs->isQuad());
37700:         Register r = prepareResultReg(ins, GpRegs);
37700:         Register b = findRegFor(lhs, FpRegs);
37700: 
37700:         CVTSD2SI(r, b);
37700: 
37700:         freeResourcesOf(ins);
37700:     }
37700: 
31927:     void Assembler::asm_cmov(LIns *ins) {
31927:         LIns* cond    = ins->oprnd1();
31927:         LIns* iftrue  = ins->oprnd2();
31927:         LIns* iffalse = ins->oprnd3();
31927:         NanoAssert(cond->isCmp());
31927:         NanoAssert((ins->isop(LIR_qcmov) && iftrue->isQuad() && iffalse->isQuad()) ||
31927:                    (ins->isop(LIR_cmov) && !iftrue->isQuad() && !iffalse->isQuad()));
31927: 
31927:         // this code assumes that neither LD nor MR nor MRcc set any of the condition flags.
31927:         // (This is true on Intel, is it true on all architectures?)
31927:         const Register rr = prepResultReg(ins, GpRegs);
31927:         const Register rf = findRegFor(iffalse, GpRegs & ~rmask(rr));
32708: 
35101:         LOpcode condop = cond->opcode();
35101:         if (ins->opcode() == LIR_cmov) {
36390:             switch (condop) {
35101:             case LIR_ov:                    CMOVNO( rr, rf);  break;
36390:             case LIR_eq:  case LIR_qeq:     CMOVNE( rr, rf);  break;
36390:             case LIR_lt:  case LIR_qlt:     CMOVNL( rr, rf);  break;
36390:             case LIR_gt:  case LIR_qgt:     CMOVNG( rr, rf);  break;
36390:             case LIR_le:  case LIR_qle:     CMOVNLE(rr, rf);  break;
36390:             case LIR_ge:  case LIR_qge:     CMOVNGE(rr, rf);  break;
36390:             case LIR_ult: case LIR_qult:    CMOVNB( rr, rf);  break;
36390:             case LIR_ugt: case LIR_qugt:    CMOVNA( rr, rf);  break;
36390:             case LIR_ule: case LIR_qule:    CMOVNBE(rr, rf);  break;
36390:             case LIR_uge: case LIR_quge:    CMOVNAE(rr, rf);  break;
35101:             default:                        NanoAssert(0);    break;
35101:             }
35101:         } else {
36390:             switch (condop) {
35101:             case LIR_ov:                    CMOVQNO( rr, rf); break;
36390:             case LIR_eq:  case LIR_qeq:     CMOVQNE( rr, rf); break;
36390:             case LIR_lt:  case LIR_qlt:     CMOVQNL( rr, rf); break;
36390:             case LIR_gt:  case LIR_qgt:     CMOVQNG( rr, rf); break;
36390:             case LIR_le:  case LIR_qle:     CMOVQNLE(rr, rf); break;
36390:             case LIR_ge:  case LIR_qge:     CMOVQNGE(rr, rf); break;
36390:             case LIR_ult: case LIR_qult:    CMOVQNB( rr, rf); break;
36390:             case LIR_ugt: case LIR_qugt:    CMOVQNA( rr, rf); break;
36390:             case LIR_ule: case LIR_qule:    CMOVQNBE(rr, rf); break;
36390:             case LIR_uge: case LIR_quge:    CMOVQNAE(rr, rf); break;
35101:             default:                        NanoAssert(0);    break;
35101:             }
35101:         }
31927:         /*const Register rt =*/ findSpecificRegFor(iftrue, rr);
31927:         asm_cmp(cond);
31927:     }
31927: 
31927:     NIns* Assembler::asm_branch(bool onFalse, LIns *cond, NIns *target) {
36390:         NanoAssert(cond->isCond());
31927:         LOpcode condop = cond->opcode();
31927:         if (condop >= LIR_feq && condop <= LIR_fge)
31927:             return asm_fbranch(onFalse, cond, target);
31927: 
36390:         // We must ensure there's room for the instr before calculating
36390:         // the offset.  And the offset determines the opcode (8bit or 32bit).
35312:         if (target && isTargetWithinS8(target)) {
35101:             if (onFalse) {
36390:                 switch (condop) {
35101:                 case LIR_ov:                    JNO8( 8, target); break;
36390:                 case LIR_eq:  case LIR_qeq:     JNE8( 8, target); break;
36390:                 case LIR_lt:  case LIR_qlt:     JNL8( 8, target); break;
36390:                 case LIR_gt:  case LIR_qgt:     JNG8( 8, target); break;
36390:                 case LIR_le:  case LIR_qle:     JNLE8(8, target); break;
36390:                 case LIR_ge:  case LIR_qge:     JNGE8(8, target); break;
36390:                 case LIR_ult: case LIR_qult:    JNB8( 8, target); break;
36390:                 case LIR_ugt: case LIR_qugt:    JNA8( 8, target); break;
36390:                 case LIR_ule: case LIR_qule:    JNBE8(8, target); break;
36390:                 case LIR_uge: case LIR_quge:    JNAE8(8, target); break;
35101:                 default:                        NanoAssert(0);    break;
35101:                 }
31927:             } else {
36390:                 switch (condop) {
35101:                 case LIR_ov:                    JO8( 8, target);  break;
36390:                 case LIR_eq:  case LIR_qeq:     JE8( 8, target);  break;
36390:                 case LIR_lt:  case LIR_qlt:     JL8( 8, target);  break;
36390:                 case LIR_gt:  case LIR_qgt:     JG8( 8, target);  break;
36390:                 case LIR_le:  case LIR_qle:     JLE8(8, target);  break;
36390:                 case LIR_ge:  case LIR_qge:     JGE8(8, target);  break;
36390:                 case LIR_ult: case LIR_qult:    JB8( 8, target);  break;
36390:                 case LIR_ugt: case LIR_qugt:    JA8( 8, target);  break;
36390:                 case LIR_ule: case LIR_qule:    JBE8(8, target);  break;
36390:                 case LIR_uge: case LIR_quge:    JAE8(8, target);  break;
35101:                 default:                        NanoAssert(0);    break;
35101:                 }
35101:             }
35101:         } else {
35101:             if (onFalse) {
36390:                 switch (condop) {
35101:                 case LIR_ov:                    JNO( 8, target);  break;
36390:                 case LIR_eq:  case LIR_qeq:     JNE( 8, target);  break;
36390:                 case LIR_lt:  case LIR_qlt:     JNL( 8, target);  break;
36390:                 case LIR_gt:  case LIR_qgt:     JNG( 8, target);  break;
36390:                 case LIR_le:  case LIR_qle:     JNLE(8, target);  break;
36390:                 case LIR_ge:  case LIR_qge:     JNGE(8, target);  break;
36390:                 case LIR_ult: case LIR_qult:    JNB( 8, target);  break;
36390:                 case LIR_ugt: case LIR_qugt:    JNA( 8, target);  break;
36390:                 case LIR_ule: case LIR_qule:    JNBE(8, target);  break;
36390:                 case LIR_uge: case LIR_quge:    JNAE(8, target);  break;
35101:                 default:                        NanoAssert(0);    break;
35101:                 }
35101:             } else {
36390:                 switch (condop) {
35101:                 case LIR_ov:                    JO( 8, target);   break;
36390:                 case LIR_eq:  case LIR_qeq:     JE( 8, target);   break;
36390:                 case LIR_lt:  case LIR_qlt:     JL( 8, target);   break;
36390:                 case LIR_gt:  case LIR_qgt:     JG( 8, target);   break;
36390:                 case LIR_le:  case LIR_qle:     JLE(8, target);   break;
36390:                 case LIR_ge:  case LIR_qge:     JGE(8, target);   break;
36390:                 case LIR_ult: case LIR_qult:    JB( 8, target);   break;
36390:                 case LIR_ugt: case LIR_qugt:    JA( 8, target);   break;
36390:                 case LIR_ule: case LIR_qule:    JBE(8, target);   break;
36390:                 case LIR_uge: case LIR_quge:    JAE(8, target);   break;
35101:                 default:                        NanoAssert(0);    break;
35101:                 }
35101:             }
31927:         }
31927:         NIns *patch = _nIns;            // addr of instr to patch
31927:         asm_cmp(cond);
31927:         return patch;
31927:     }
31927: 
31927:     void Assembler::asm_cmp(LIns *cond) {
32708:         // LIR_ov recycles the flags set by arithmetic ops
32708:         if (cond->opcode() == LIR_ov)
32708:             return;
31927:         LIns *b = cond->oprnd2();
31927:         if (isImm32(b)) {
31927:             asm_cmp_imm(cond);
31927:             return;
31927:         }
31927:         LIns *a = cond->oprnd1();
31927:         Register ra, rb;
31927:         if (a != b) {
37705:             findRegFor2(GpRegs, a, ra, GpRegs, b, rb);
31927:         } else {
31927:             // optimize-me: this will produce a const result!
31927:             ra = rb = findRegFor(a, GpRegs);
31927:         }
31927: 
31927:         LOpcode condop = cond->opcode();
36390:         if (LIR_qeq <= condop && condop <= LIR_quge) {
35101:             CMPQR(ra, rb);
36390:         } else {
36390:             NanoAssert(LIR_eq <= condop && condop <= LIR_uge);
35101:             CMPLR(ra, rb);
31927:         }
36390:     }
31927: 
31927:     void Assembler::asm_cmp_imm(LIns *cond) {
36390:         LOpcode condop = cond->opcode();
31927:         LIns *a = cond->oprnd1();
31927:         LIns *b = cond->oprnd2();
31927:         Register ra = findRegFor(a, GpRegs);
31927:         int32_t imm = getImm32(b);
36390:         if (LIR_qeq <= condop && condop <= LIR_quge) {
36390:             if (isS8(imm))
35101:                 CMPQR8(ra, imm);
35101:             else
36390:                 CMPQRI(ra, imm);
36390:         } else {
36390:             NanoAssert(LIR_eq <= condop && condop <= LIR_uge);
36390:             if (isS8(imm))
35101:                 CMPLR8(ra, imm);
35101:             else
35101:                 CMPLRI(ra, imm);
31927:         }
31927:     }
31927: 
31927:     // compiling floating point branches
31927:     // discussion in https://bugzilla.mozilla.org/show_bug.cgi?id=443886
31927:     //
31927:     //  fucom/p/pp: c3 c2 c0   jae ja    jbe jb je jne
31927:     //  ucomisd:     Z  P  C   !C  !C&!Z C|Z C  Z  !Z
31927:     //              -- -- --   --  ----- --- -- -- --
31927:     //  unordered    1  1  1             T   T  T
31927:     //  greater >    0  0  0   T   T               T
31927:     //  less    <    0  0  1             T   T     T
31927:     //  equal   =    1  0  0   T         T      T
31927:     //
31927:     //  here's the cases, using conditionals:
31927:     //
31927:     //  branch  >=  >   <=       <        =
31927:     //  ------  --- --- ---      ---      ---
31927:     //  LIR_jt  jae ja  swap+jae swap+ja  jp over je
31927:     //  LIR_jf  jb  jbe swap+jb  swap+jbe jne+jp
31927: 
31927:     NIns* Assembler::asm_fbranch(bool onFalse, LIns *cond, NIns *target) {
31927:         LOpcode condop = cond->opcode();
31927:         NIns *patch;
31927:         LIns *a = cond->oprnd1();
31927:         LIns *b = cond->oprnd2();
31927:         if (condop == LIR_feq) {
31927:             if (onFalse) {
31927:                 // branch if unordered or !=
35101:                 JP(16, target);     // underrun of 12 needed, round up for overhang --> 16
35101:                 JNE(0, target);     // no underrun needed, previous was enough
31927:                 patch = _nIns;
31927:             } else {
31927:                 // jp skip (2byte)
31927:                 // jeq target
31927:                 // skip: ...
35357:                 underrunProtect(16); // underrun of 7 needed but we write 2 instr --> 16
31927:                 NIns *skip = _nIns;
35357:                 JE(0, target);      // no underrun needed, previous was enough
31927:                 patch = _nIns;
35357:                 JP8(0, skip);       // ditto
31927:             }
31927:         }
31927:         else {
31927:             if (condop == LIR_flt) {
31927:                 condop = LIR_fgt;
31927:                 LIns *t = a; a = b; b = t;
31927:             } else if (condop == LIR_fle) {
31927:                 condop = LIR_fge;
31927:                 LIns *t = a; a = b; b = t;
31927:             }
35101:             if (condop == LIR_fgt) {
35101:                 if (onFalse)
35101:                     JBE(8, target);
35101:                 else
35101:                     JA(8, target);
35101:             } else { // LIR_fge
35101:                 if (onFalse)
35101:                     JB(8, target);
35101:                 else
35101:                     JAE(8, target);
35101:             }
31927:             patch = _nIns;
31927:         }
31927:         fcmp(a, b);
31927:         return patch;
31927:     }
31927: 
31927:     void Assembler::asm_fcond(LIns *ins) {
31927:         LOpcode op = ins->opcode();
31927:         LIns *a = ins->oprnd1();
31927:         LIns *b = ins->oprnd2();
31927:         if (op == LIR_feq) {
31927:             // result = ZF & !PF, must do logic on flags
31927:             // r = al|bl|cl|dl, can only use rh without rex prefix
31927:             Register r = prepResultReg(ins, 1<<RAX|1<<RCX|1<<RDX|1<<RBX);
35101:             MOVZX8(r, r);       // movzx8   r,rl     r[8:63] = 0
35101:             X86_AND8R(r);       // and      rl,rh    rl &= rh
35101:             X86_SETNP(r);       // setnp    rh       rh = !PF
35101:             X86_SETE(r);        // sete     rl       rl = ZF
31927:         } else {
31927:             if (op == LIR_flt) {
31927:                 op = LIR_fgt;
31927:                 LIns *t = a; a = b; b = t;
31927:             } else if (op == LIR_fle) {
31927:                 op = LIR_fge;
31927:                 LIns *t = a; a = b; b = t;
31927:             }
31927:             Register r = prepResultReg(ins, GpRegs); // x64 can use any GPR as setcc target
35101:             MOVZX8(r, r);
35101:             if (op == LIR_fgt)
35101:                 SETA(r);
35101:             else
35101:                 SETAE(r);
31927:         }
31927:         fcmp(a, b);
31927:     }
31927: 
31927:     void Assembler::fcmp(LIns *a, LIns *b) {
35060:         Register ra, rb;
37705:         findRegFor2(FpRegs, a, ra, FpRegs, b, rb);
35101:         UCOMISD(ra, rb);
31927:     }
31927: 
35365:     void Assembler::asm_restore(LIns *ins, Register r) {
31927:         if (ins->isop(LIR_alloc)) {
35060:             int d = disp(ins);
35101:             LEAQRM(r, d, FP);
31927:         }
31927:         else if (ins->isconst()) {
35060:             if (!ins->getArIndex()) {
35060:                 ins->markAsClear();
31927:             }
31927:             // unsafe to use xor r,r for zero because it changes cc's
35101:             MOVI(r, ins->imm32());
31927:         }
31927:         else if (ins->isconstq() && IsGpReg(r)) {
35060:             if (!ins->getArIndex()) {
35060:                 ins->markAsClear();
31927:             }
31927:             // unsafe to use xor r,r for zero because it changes cc's
35101:             asm_quad(r, ins->imm64());
31927:         }
31927:         else {
31927:             int d = findMemFor(ins);
31927:             if (IsFpReg(r)) {
31927:                 NanoAssert(ins->isQuad());
31927:                 // load 64bits into XMM.  don't know if double or int64, assume double.
35101:                 MOVSDRM(r, d, FP);
31927:             } else if (ins->isQuad()) {
35101:                 MOVQRM(r, d, FP);
31927:             } else {
35101:                 MOVLRM(r, d, FP);
31927:             }
31927:         }
31927:     }
31927: 
31927:     void Assembler::asm_cond(LIns *ins) {
31927:         LOpcode op = ins->opcode();
31927:         // unlike x86-32, with a rex prefix we can use any GP register as an 8bit target
31927:         Register r = prepResultReg(ins, GpRegs);
31927:         // SETcc only sets low 8 bits, so extend
35101:         MOVZX8(r, r);
31927:         switch (op) {
31927:         default:
31927:             TODO(cond);
31927:         case LIR_qeq:
35101:         case LIR_eq:    SETE(r);    break;
31927:         case LIR_qlt:
35101:         case LIR_lt:    SETL(r);    break;
31927:         case LIR_qle:
35101:         case LIR_le:    SETLE(r);   break;
31927:         case LIR_qgt:
35101:         case LIR_gt:    SETG(r);    break;
31927:         case LIR_qge:
35101:         case LIR_ge:    SETGE(r);   break;
31927:         case LIR_qult:
35101:         case LIR_ult:   SETB(r);    break;
31927:         case LIR_qule:
35101:         case LIR_ule:   SETBE(r);   break;
31927:         case LIR_qugt:
35101:         case LIR_ugt:   SETA(r);    break;
31927:         case LIR_quge:
35101:         case LIR_uge:   SETAE(r);   break;
35101:         case LIR_ov:    SETO(r);    break;
31927:         }
31927:         asm_cmp(ins);
31927:     }
31927: 
31927:     void Assembler::asm_ret(LIns *ins) {
32634:         genEpilogue();
32665: 
32665:         // Restore RSP from RBP, undoing SUB(RSP,amt) in the prologue
32665:         MR(RSP,FP);
32665: 
36672:         releaseRegisters();
31927:         assignSavedRegs();
31927:         LIns *value = ins->oprnd1();
31927:         Register r = ins->isop(LIR_ret) ? RAX : XMM0;
31927:         findSpecificRegFor(value, r);
31927:     }
31927: 
31927:     void Assembler::asm_nongp_copy(Register d, Register s) {
31927:         if (!IsFpReg(d) && IsFpReg(s)) {
31927:             // gpr <- xmm: use movq r/m64, xmm (66 REX.W 0F 7E /r)
35101:             MOVQRX(d, s);
31927:         } else if (IsFpReg(d) && IsFpReg(s)) {
31927:             // xmm <- xmm: use movaps. movsd r,r causes partial register stall
35101:             MOVAPSR(d, s);
31927:         } else {
36414:             NanoAssert(IsFpReg(d) && !IsFpReg(s));
31927:             // xmm <- gpr: use movq xmm, r/m64 (66 REX.W 0F 6E /r)
35101:             MOVQXR(d, s);
31927:         }
31927:     }
31927: 
36414:     void Assembler::regalloc_load(LIns *ins, RegisterMask allow, Register &rr, int32_t &dr, Register &rb) {
31927:         dr = ins->disp();
31927:         LIns *base = ins->oprnd1();
36552:         rb = getBaseReg(base, dr, BaseRegs);
36414:         if (ins->isUnusedOrHasUnknownReg() || !(allow & rmask(ins->getReg()))) {
36414:             rr = prepResultReg(ins, allow & ~rmask(rb));
35060:         } else {
35060:             // keep already assigned register
35060:             rr = ins->getReg();
36414:             NanoAssert(allow & rmask(rr));
35060:             freeRsrcOf(ins, false);
31927:         }
31927:     }
31927: 
31927:     void Assembler::asm_load64(LIns *ins) {
36414:         Register rr, rb;
36414:         int32_t dr;
36372:         switch (ins->opcode()) {
36372:             case LIR_ldq:
36372:             case LIR_ldqc:
37672:                 regalloc_load(ins, GpRegs, rr, dr, rb);
37672:                 NanoAssert(IsGpReg(rr));
37672:                 MOVQRM(rr, dr, rb);     // general 64bit load, 32bit const displacement
37672:                 break;
37020:             case LIR_ldf:
37020:             case LIR_ldfc:
37672:                 regalloc_load(ins, FpRegs, rr, dr, rb);
36414:                 NanoAssert(IsFpReg(rr));
37672:                 MOVSDRM(rr, dr, rb);    // load 64bits into XMM
36414:                 break;
36414:             case LIR_ld32f:
36414:             case LIR_ldc32f:
36414:                 regalloc_load(ins, FpRegs, rr, dr, rb);
36414:                 NanoAssert(IsFpReg(rr));
36414:                 CVTSS2SD(rr, rr);
36414:                 MOVSSRM(rr, dr, rb);
36414:                 break;
36414:             default:
36414:                 NanoAssertMsg(0, "asm_load64 should never receive this LIR opcode");
36414:                 break;
36414:         }
36414: 
31927:     }
31927: 
36372:     void Assembler::asm_load32(LIns *ins) {
31927:         NanoAssert(!ins->isQuad());
31927:         Register r, b;
31927:         int32_t d;
36414:         regalloc_load(ins, GpRegs, r, d, b);
32706:         LOpcode op = ins->opcode();
32706:         switch(op) {
36372:             case LIR_ldzb:
36372:             case LIR_ldcb:
36372:                 MOVZX8M( r, d, b);
36372:                 break;
36372:             case LIR_ldzs:
36372:             case LIR_ldcs:
36372:                 MOVZX16M(r, d, b);
36372:                 break;
36372:             case LIR_ld:
36372:             case LIR_ldc:
36372:                 MOVLRM(  r, d, b);
36372:                 break;
36372:             case LIR_ldsb:
36414:             case LIR_ldcsb:
36414:                 MOVSX8M( r, d, b);
36414:                 break;
36372:             case LIR_ldss:
36372:             case LIR_ldcss:
36414:                 MOVSX16M( r, d, b);
36414:                 break;
36372:             default:
36372:                 NanoAssertMsg(0, "asm_load32 should never receive this LIR opcode");
36414:                 break;
32706:         }
31927:     }
31927: 
36372:     void Assembler::asm_store64(LOpcode op, LIns *value, int d, LIns *base) {
31927:         NanoAssert(value->isQuad());
36372: 
36414:         switch (op) {
37672:             case LIR_stqi: {
37705:                 Register r, b;
37705:                 getBaseReg2(GpRegs, value, r, BaseRegs, base, b, d);
37672:                 MOVQMR(r, d, b);    // gpr store
36414:                 break;
36414:             }
37672:             case LIR_stfi: {
37705:                 Register b = getBaseReg(base, d, BaseRegs);
37672:                 Register r = findRegFor(value, FpRegs);
37672:                 MOVSDMR(r, d, b);   // xmm store
37672:                 break;
37672:             }
37672:             case LIR_st32f: {
37705:                 Register b = getBaseReg(base, d, BaseRegs);
37672:                 Register r = findRegFor(value, FpRegs);
36414:                 Register t = registerAllocTmp(FpRegs & ~rmask(r));
36414: 
37672:                 MOVSSMR(t, d, b);   // store
37672:                 CVTSD2SS(t, r);     // cvt to single-precision
36414:                 XORPS(t);           // break dependency chains
36414:                 break;
36414:             }
36414:             default:
36414:                 NanoAssertMsg(0, "asm_store64 should never receive this LIR opcode");
36414:                 break;
36414:         }
31927:     }
31927: 
36372:     void Assembler::asm_store32(LOpcode op, LIns *value, int d, LIns *base) {
36372: 
37672:         // Quirk of x86-64: reg cannot appear to be ah/bh/ch/dh for
37672:         // single-byte stores with REX prefix.
37672:         const RegisterMask SrcRegs = (op == LIR_stb) ? SingleByteStoreRegs : GpRegs;
36372: 
31927:         NanoAssert(!value->isQuad());
36552:         Register b = getBaseReg(base, d, BaseRegs);
36414:         Register r = findRegFor(value, SrcRegs & ~rmask(b));
31927: 
36414:         switch (op) {
36414:             case LIR_stb:
36414:                 MOVBMR(r, d, b);
36414:                 break;
36414:             case LIR_sts:
36414:                 MOVSMR(r, d, b);
36414:                 break;
36414:             case LIR_sti:
35101:                 MOVLMR(r, d, b);
36414:                 break;
36414:             default:
36414:                 NanoAssertMsg(0, "asm_store32 should never receive this LIR opcode");
36414:                 break;
36414:         }
36414: 
36414: 
31927:     }
31927: 
31927:     // generate a 64bit constant, must not affect condition codes!
35101:     void Assembler::asm_quad(Register r, uint64_t v) {
31927:         NanoAssert(IsGpReg(r));
31927:         if (isU32(v)) {
35101:             MOVI(r, int32_t(v));
35101:         } else if (isS32(v)) {
31927:             // safe for sign-extension 32->64
35101:             MOVQI32(r, int32_t(v));
35312:         } else if (isTargetWithinS32((NIns*)v)) {
35087:             // value is with +/- 2GB from RIP, can use LEA with RIP-relative disp32
35087:             int32_t d = int32_t(int64_t(v)-int64_t(_nIns));
35101:             LEARIP(r, d);
35101:         } else {
35101:             MOVQI(r, v);
35087:         }
31927:     }
31927: 
31927:     void Assembler::asm_int(LIns *ins) {
31927:         Register r = prepResultReg(ins, GpRegs);
31927:         int32_t v = ins->imm32();
31927:         if (v == 0) {
31927:             // special case for zero
35101:             XORRR(r, r);
35101:         } else {
35101:             MOVI(r, v);
31927:         }
31927:     }
31927: 
31927:     void Assembler::asm_quad(LIns *ins) {
31927:         uint64_t v = ins->imm64();
31927:         RegisterMask allow = v == 0 ? GpRegs|FpRegs : GpRegs;
31927:         Register r = prepResultReg(ins, allow);
31927:         if (v == 0) {
31927:             if (IsGpReg(r)) {
31927:                 // special case for zero
35101:                 XORRR(r, r);
31927:             } else {
31927:                 // xorps for xmm
35101:                 XORPS(r);
31927:             }
31927:         } else {
35101:             asm_quad(r, v);
31927:         }
31927:     }
31927: 
31927:     void Assembler::asm_qjoin(LIns*) {
31927:         TODO(asm_qjoin);
31927:     }
31927: 
31927:     void Assembler::asm_param(LIns *ins) {
31927:         uint32_t a = ins->paramArg();
31927:         uint32_t kind = ins->paramKind();
31927:         if (kind == 0) {
31927:             // ordinary param
33539:             // first four or six args always in registers for x86_64 ABI
33559:             if (a < (uint32_t)NumArgRegs) {
31927:                 // incoming arg in register
31927:                 prepResultReg(ins, rmask(argRegs[a]));
31927:             } else {
31927:                 // todo: support stack based args, arg 0 is at [FP+off] where off
31927:                 // is the # of regs to be pushed in genProlog()
31927:                 TODO(asm_param_stk);
31927:             }
31927:         }
31927:         else {
31927:             // saved param
31927:             prepResultReg(ins, rmask(savedRegs[a]));
31927:         }
31927:     }
31927: 
31927:     // register allocation for 2-address style unary ops of the form R = (op) R
31927:     void Assembler::regalloc_unary(LIns *ins, RegisterMask allow, Register &rr, Register &ra) {
31927:         LIns *a = ins->oprnd1();
31927:         rr = prepResultReg(ins, allow);
31927:         // if this is last use of a in reg, we can re-use result reg
35060:         if (a->isUnusedOrHasUnknownReg()) {
35089:             ra = findSpecificRegForUnallocated(a, rr);
31927:         } else {
35060:             // 'a' already has a register assigned.  Caller must emit a copy
31927:             // to rr once instr code is generated.  (ie  mov rr,ra ; op rr)
35060:             ra = a->getReg();
31927:         }
32765:         NanoAssert(allow & rmask(rr));
31927:     }
31927: 
31927:     static const AVMPLUS_ALIGN16(int64_t) negateMask[] = {0x8000000000000000LL,0};
31927: 
31927:     void Assembler::asm_fneg(LIns *ins) {
31927:         Register rr, ra;
35312:         if (isS32((uintptr_t)negateMask) || isTargetWithinS32((NIns*)negateMask)) {
31927:             regalloc_unary(ins, FpRegs, rr, ra);
31927:             if (isS32((uintptr_t)negateMask)) {
31927:                 // builtin code is in bottom or top 2GB addr space, use absolute addressing
35101:                 XORPSA(rr, (int32_t)(uintptr_t)negateMask);
31927:             } else {
31927:                 // jit code is within +/-2GB of builtin code, use rip-relative
35101:                 XORPSM(rr, (NIns*)negateMask);
31927:             }
31927:             if (ra != rr)
31927:                 asm_nongp_copy(rr,ra);
31927:         } else {
31927:             // this is just hideous - can't use RIP-relative load, can't use
31927:             // absolute-address load, and cant move imm64 const to XMM.
31927:             // so do it all in a GPR.  hrmph.
31927:             rr = prepResultReg(ins, GpRegs);
31927:             ra = findRegFor(ins->oprnd1(), GpRegs & ~rmask(rr));
35101:             XORQRR(rr, ra);                     // xor rr, ra
35101:             asm_quad(rr, negateMask[0]);        // mov rr, 0x8000000000000000
31927:         }
31927:     }
31927: 
31927:     void Assembler::asm_qhi(LIns*) {
31927:         TODO(asm_qhi);
31927:     }
31927: 
31927:     void Assembler::asm_qlo(LIns *ins) {
31927:         Register rr, ra;
31927:         regalloc_unary(ins, GpRegs, rr, ra);
31927:         NanoAssert(IsGpReg(ra));
35101:         MOVLR(rr, ra);  // 32bit mov zeros the upper 32bits of the target
31927:     }
31927: 
31927:     void Assembler::asm_spill(Register rr, int d, bool /*pop*/, bool quad) {
31927:         if (d) {
31927:             if (!IsFpReg(rr)) {
35101:                 if (quad)
35101:                     MOVQMR(rr, d, FP);
35101:                 else
35101:                     MOVLMR(rr, d, FP);
31927:             } else {
31927:                 // store 64bits from XMM to memory
31927:                 NanoAssert(quad);
35101:                 MOVSDMR(rr, d, FP);
31927:             }
31927:         }
31927:     }
31927: 
31927:     NIns* Assembler::genPrologue() {
31927:         // activation frame is 4 bytes per entry even on 64bit machines
36664:         uint32_t stackNeeded = max_stk_used + _activation.stackSlotsNeeded() * 4;
31927: 
31927:         uint32_t stackPushed =
31927:             sizeof(void*) + // returnaddr
31927:             sizeof(void*); // ebp
31927:         uint32_t aligned = alignUp(stackNeeded + stackPushed, NJ_ALIGN_STACK);
31927:         uint32_t amt = aligned - stackPushed;
31927: 
31927:         // Reserve stackNeeded bytes, padded
31927:         // to preserve NJ_ALIGN_STACK-byte alignment.
31927:         if (amt) {
31927:             if (isS8(amt))
35101:                 SUBQR8(RSP, amt);
31927:             else
35101:                 SUBQRI(RSP, amt);
31927:         }
31927: 
36388:         verbose_only( asm_output("[patch entry]"); )
31927:         NIns *patchEntry = _nIns;
31927:         MR(FP, RSP);    // Establish our own FP.
35101:         PUSHR(FP);      // Save caller's FP.
31927: 
31927:         return patchEntry;
31927:     }
31927: 
31927:     NIns* Assembler::genEpilogue() {
31927:         // pop rbp
31927:         // ret
35101:         RET();
35101:         POPR(RBP);
31927:         return _nIns;
31927:     }
31927: 
31927:     void Assembler::nRegisterResetAll(RegAlloc &a) {
31927:         // add scratch registers to our free list for the allocator
31927:         a.clear();
33539: #ifdef _WIN64
31927:         a.free = 0x001fffcf; // rax-rbx, rsi, rdi, r8-r15, xmm0-xmm5
31927: #else
31927:         a.free = 0xffffffff & ~(1<<RSP | 1<<RBP);
31927: #endif
31927:         debug_only( a.managed = a.free; )
31927:     }
31927: 
31927:     void Assembler::nPatchBranch(NIns *patch, NIns *target) {
31927:         NIns *next = 0;
31927:         if (patch[0] == 0xE9) {
31927:             // jmp disp32
31927:             next = patch+5;
31927:         } else if (patch[0] == 0x0F && (patch[1] & 0xF0) == 0x80) {
31927:             // jcc disp32
31927:             next = patch+6;
31927:         } else {
31927:             next = 0;
31927:             TODO(unknown_patch);
31927:         }
32746:         // Guards can result in a valid branch being patched again later, so don't assert
32746:         // that the old value is poison.
31927:         NanoAssert(isS32(target - next));
31927:         ((int32_t*)next)[-1] = int32_t(target - next);
31927:         if (next[0] == 0x0F && next[1] == 0x8A) {
31927:             // code is jne<target>,jp<target>, for LIR_jf(feq)
31927:             // we just patched the jne, now patch the jp.
31927:             next += 6;
31927:             NanoAssert(((int32_t*)next)[-1] == 0);
31927:             NanoAssert(isS32(target - next));
31927:             ((int32_t*)next)[-1] = int32_t(target - next);
31927:         }
31927:     }
31927: 
31927:     Register Assembler::nRegisterAllocFromSet(RegisterMask set) {
33539:     #if defined _MSC_VER
31927:         DWORD tr;
31927:         _BitScanForward(&tr, set);
31927:         _allocator.free &= ~rmask((Register)tr);
31927:         return (Register) tr;
31927:     #else
31927:         // gcc asm syntax
31927:         Register r;
31927:         asm("bsf    %1, %%eax\n\t"
31927:             "btr    %%eax, %2\n\t"
31927:             "movl   %%eax, %0\n\t"
31927:             : "=m"(r) : "m"(set), "m"(_allocator.free) : "%eax", "memory");
31927:         (void)set;
31927:         return r;
31927:     #endif
31927:     }
31927: 
32665:     void Assembler::nFragExit(LIns *guard) {
32665:         SideExit *exit = guard->record()->exit;
32665:         Fragment *frag = exit->target;
32665:         GuardRecord *lr = 0;
32665:         bool destKnown = (frag && frag->fragEntry);
32665:         // Generate jump to epilog and initialize lr.
32665:         // If the guard is LIR_xtbl, use a jump table with epilog in every entry
32665:         if (guard->isop(LIR_xtbl)) {
32665:             NanoAssert(!guard->isop(LIR_xtbl));
32665:         } else {
32665:             // If the guard already exists, use a simple jump.
32665:             if (destKnown) {
32665:                 JMP(frag->fragEntry);
32665:                 lr = 0;
32665:             } else {  // target doesn't exist. Use 0 jump offset and patch later
32665:                 if (!_epilogue)
32665:                     _epilogue = genEpilogue();
32665:                 lr = guard->record();
32665:                 JMPl(_epilogue);
32665:                 lr->jmp = _nIns;
32665:             }
32665:         }
32665: 
36550:         // profiling for the exit
36550:         verbose_only(
36550:            if (_logc->lcbits & LC_FragProfile) {
36550:               asm_inc_m32( &guard->record()->profCount );
36550:            }
36550:         )
36550: 
32665:         MR(RSP, RBP);
32665: 
32665:         // return value is GuardRecord*
35101:         asm_quad(RAX, uintptr_t(lr));
31927:     }
31927: 
32634:     void Assembler::nInit(AvmCore*) {
32634:     }
32634: 
32634:     void Assembler::nBeginAssembly() {
32634:         max_stk_used = 0;
32634:     }
31927: 
35101:     // This should only be called from within emit() et al.
31927:     void Assembler::underrunProtect(ptrdiff_t bytes) {
31927:         NanoAssertMsg(bytes<=LARGEST_UNDERRUN_PROT, "constant LARGEST_UNDERRUN_PROT is too small");
31927:         NIns *pc = _nIns;
35356:         NIns *top = codeStart;  // this may be in a normal code chunk or an exit code chunk
31927: 
31927:     #if PEDANTIC
31927:         // pedanticTop is based on the last call to underrunProtect; any time we call
31927:         // underrunProtect and would use more than what's already protected, then insert
31927:         // a page break jump.  Sometimes, this will be to a new page, usually it's just
31927:         // the next instruction
31927: 
31927:         NanoAssert(pedanticTop >= top);
31927:         if (pc - bytes < pedanticTop) {
31927:             // no page break required, but insert a far branch anyway just to be difficult
31927:             const int br_size = 8; // opcode + 32bit addr
31927:             if (pc - bytes - br_size < top) {
31927:                 // really do need a page break
31927:                 verbose_only(if (_logc->lcbits & LC_Assembly) outputf("newpage %p:", pc);)
35356:                 // This may be in a normal code chunk or an exit code chunk.
32791:                 codeAlloc(codeStart, codeEnd, _nIns verbose_only(, codeBytes));
31927:             }
31927:             // now emit the jump, but make sure we won't need another page break.
31927:             // we're pedantic, but not *that* pedantic.
31927:             pedanticTop = _nIns - br_size;
31927:             JMP(pc);
31927:             pedanticTop = _nIns - bytes;
31927:         }
31927:     #else
31927:         if (pc - bytes < top) {
31927:             verbose_only(if (_logc->lcbits & LC_Assembly) outputf("newpage %p:", pc);)
35356:             // This may be in a normal code chunk or an exit code chunk.
32791:             codeAlloc(codeStart, codeEnd, _nIns verbose_only(, codeBytes));
35356:             // This jump will call underrunProtect again, but since we're on a new
31927:             // page, nothing will happen.
31927:             JMP(pc);
31927:         }
31927:     #endif
31927:     }
31927: 
31927:     RegisterMask Assembler::hint(LIns *, RegisterMask allow) {
31927:         return allow;
31927:     }
31927: 
31927:     void Assembler::nativePageSetup() {
35356:         NanoAssert(!_inExit);
31927:         if (!_nIns) {
32791:             codeAlloc(codeStart, codeEnd, _nIns verbose_only(, codeBytes));
31927:             IF_PEDANTIC( pedanticTop = _nIns; )
31927:         }
31927:     }
31927: 
31927:     void Assembler::nativePageReset()
31927:     {}
31927: 
32791:     // Increment the 32-bit profiling counter at pCtr, without
32791:     // changing any registers.
32791:     verbose_only(
36550:     void Assembler::asm_inc_m32(uint32_t* pCtr)
32791:     {
36550:         // Not as simple as on x86.  We need to temporarily free up a
36550:         // register into which to generate the address, so just push
36550:         // it on the stack.  This assumes that the scratch area at
36550:         // -8(%rsp) .. -1(%esp) isn't being used for anything else
36550:         // at this point.
36550:         emitr(X64_popr, RAX);             // popq    %rax
36550:         emit(X64_inclmRAX);               // incl    (%rax)
36550:         asm_quad(RAX, (uint64_t)pCtr);    // movabsq $pCtr, %rax
36550:         emitr(X64_pushr, RAX);            // pushq   %rax
32791:     }
32791:     )
32791: 
35087:     void Assembler::asm_jtbl(LIns* ins, NIns** table)
35087:     {
35364:         // exclude R12 because ESP and R12 cannot be used as an index
35087:         // (index=100 in SIB means "none")
35087:         Register indexreg = findRegFor(ins->oprnd1(), GpRegs & ~rmask(R12));
35087:         if (isS32((intptr_t)table)) {
35087:             // table is in low 2GB or high 2GB, can use absolute addressing
35087:             // jmpq [indexreg*8 + table]
35364:             JMPX(indexreg, table);
35087:         } else {
35087:             // don't use R13 for base because we want to use mod=00, i.e. [index*8+base + 0]
35316:             Register tablereg = registerAllocTmp(GpRegs & ~(rmask(indexreg)|rmask(R13)));
35087:             // jmp [indexreg*8 + tablereg]
35364:             JMPXB(indexreg, tablereg);
35087:             // tablereg <- #table
35101:             asm_quad(tablereg, (uint64_t)table);
35087:         }
35087:     }
35087: 
35356:     void Assembler::swapCodeChunks() {
37698:         if (!_nExitIns) {
37698:             codeAlloc(exitStart, exitEnd, _nExitIns verbose_only(, exitBytes));
37698:         }
35356:         SWAP(NIns*, _nIns, _nExitIns);
35356:         SWAP(NIns*, codeStart, exitStart);
35356:         SWAP(NIns*, codeEnd, exitEnd);
35356:         verbose_only( SWAP(size_t, codeBytes, exitBytes); )
35356:     }
35356: 
31927: } // namespace nanojit
31927: 
31927: #endif // FEATURE_NANOJIT && NANOJIT_X64
