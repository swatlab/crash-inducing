30266: /* -*- Mode: C++; c-basic-offset: 4; indent-tabs-mode: nil; tab-width: 4 -*- */
30266: /* vi: set ts=4 sw=4 expandtab: (add to ~/.vimrc: set modeline modelines=5) */
17271: /* ***** BEGIN LICENSE BLOCK *****
17271:  * Version: MPL 1.1/GPL 2.0/LGPL 2.1
17271:  *
17271:  * The contents of this file are subject to the Mozilla Public License Version
17271:  * 1.1 (the "License"); you may not use this file except in compliance with
17271:  * the License. You may obtain a copy of the License at
17271:  * http://www.mozilla.org/MPL/
17271:  *
17271:  * Software distributed under the License is distributed on an "AS IS" basis,
17271:  * WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
17271:  * for the specific language governing rights and limitations under the
17271:  * License.
17271:  *
17271:  * The Original Code is [Open Source Virtual Machine].
17271:  *
17271:  * The Initial Developer of the Original Code is
17271:  * Adobe System Incorporated.
17271:  * Portions created by the Initial Developer are Copyright (C) 2004-2007
17271:  * the Initial Developer. All Rights Reserved.
17271:  *
17271:  * Contributor(s):
17271:  *   Adobe AS3 Team
17271:  *
17271:  * Alternatively, the contents of this file may be used under the terms of
17271:  * either the GNU General Public License Version 2 or later (the "GPL"), or
17271:  * the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
17271:  * in which case the provisions of the GPL or the LGPL are applicable instead
17271:  * of those above. If you wish to allow use of your version of this file only
17271:  * under the terms of either the GPL or the LGPL, and not to allow others to
17271:  * use your version of this file under the terms of the MPL, indicate your
17271:  * decision by deleting the provisions above and replace them with the notice
17271:  * and other provisions required by the GPL or the LGPL. If you do not delete
17271:  * the provisions above, a recipient may use your version of this file under
17271:  * the terms of any one of the MPL, the GPL or the LGPL.
17271:  *
17271:  * ***** END LICENSE BLOCK ***** */
17271: 
17271: 
17271: #ifndef __nanojit_Nativei386__
17271: #define __nanojit_Nativei386__
17271: 
22654: #ifdef PERFM
33109: #define DOPROF
22654: #include "../vprof/vprof.h"
22654: #define count_instr() _nvprof("x86",1)
22654: #define count_ret() _nvprof("x86-ret",1); count_instr();
22654: #define count_push() _nvprof("x86-push",1); count_instr();
22654: #define count_pop() _nvprof("x86-pop",1); count_instr();
22654: #define count_st() _nvprof("x86-st",1); count_instr();
22654: #define count_stq() _nvprof("x86-stq",1); count_instr();
22654: #define count_ld() _nvprof("x86-ld",1); count_instr();
22654: #define count_ldq() _nvprof("x86-ldq",1); count_instr();
22654: #define count_call() _nvprof("x86-call",1); count_instr();
22654: #define count_calli() _nvprof("x86-calli",1); count_instr();
22654: #define count_prolog() _nvprof("x86-prolog",1); count_instr();
22654: #define count_alu() _nvprof("x86-alu",1); count_instr();
22654: #define count_mov() _nvprof("x86-mov",1); count_instr();
22654: #define count_fpu() _nvprof("x86-fpu",1); count_instr();
22654: #define count_jmp() _nvprof("x86-jmp",1); count_instr();
22654: #define count_jcc() _nvprof("x86-jcc",1); count_instr();
22654: #define count_fpuld() _nvprof("x86-ldq",1); _nvprof("x86-fpu",1); count_instr()
22654: #define count_aluld() _nvprof("x86-ld",1); _nvprof("x86-alu",1); count_instr()
22654: #define count_alust() _nvprof("x86-ld",1); _nvprof("x86-alu",1); _nvprof("x86-st",1); count_instr()
22654: #define count_pushld() _nvprof("x86-ld",1); _nvprof("x86-push",1); count_instr()
22654: #define count_imt() _nvprof("x86-imt",1) count_instr()
22654: #else
22654: #define count_instr()
22654: #define count_ret()
22654: #define count_push()
22654: #define count_pop()
22654: #define count_st()
22654: #define count_stq()
22654: #define count_ld()
22654: #define count_ldq()
22654: #define count_call()
22654: #define count_calli()
22654: #define count_prolog()
22654: #define count_alu()
22654: #define count_mov()
22654: #define count_fpu()
22654: #define count_jmp()
22654: #define count_jcc()
22654: #define count_fpuld()
22654: #define count_aluld()
22654: #define count_alust()
22654: #define count_pushld()
22654: #define count_imt()
22654: #endif
17271: 
17271: namespace nanojit
17271: {
17271:     const int NJ_MAX_REGISTERS = 24; // gpregs, x87 regs, xmm regs
17271: 
36670:     #define NJ_MAX_STACK_ENTRY 4096
17271:     #define NJ_MAX_PARAMETERS 1
35087:     #define NJ_JTBL_SUPPORTED 1
36372:     #define NJ_EXPANDED_LOADSTORE_SUPPORTED 1
37665:     #define NJ_USES_QUAD_CONSTANTS 1
17271: 
17368:         // Preserve a 16-byte stack alignment, to support the use of
17368:         // SSE instructions like MOVDQA (if not by Tamarin itself,
17368:         // then by the C functions it calls).
17271:     const int NJ_ALIGN_STACK = 16;
17271: 
34322:     const int32_t LARGEST_UNDERRUN_PROT = 32;  // largest value passed to underrunProtect
22662: 
17271:     typedef uint8_t NIns;
17271: 
34321:     // Bytes of icache to flush after Assembler::patch
34321:     const size_t LARGEST_BRANCH_PATCH = 16 * sizeof(NIns);
34321: 
17271:     // These are used as register numbers in various parts of the code
17271:     typedef enum
17271:     {
17271:         // general purpose 32bit regs
17271:         EAX = 0, // return value, scratch
17271:         ECX = 1, // this/arg0, scratch
17271:         EDX = 2, // arg1, return-msw, scratch
17271:         EBX = 3,
17271:         ESP = 4, // stack pointer
17271:         EBP = 5, // frame pointer
17271:         ESI = 6,
17271:         EDI = 7,
17271: 
17271:         SP = ESP, // alias SP to ESP for convenience
17271:         FP = EBP, // alias FP to EBP for convenience
17271: 
17271:         // SSE regs come before X87 so we prefer them
17271:         XMM0 = 8,
17271:         XMM1 = 9,
17271:         XMM2 = 10,
17271:         XMM3 = 11,
17271:         XMM4 = 12,
17271:         XMM5 = 13,
17271:         XMM6 = 14,
17271:         XMM7 = 15,
17271: 
17271:         // X87 regs
17271:         FST0 = 16,
17271: 
17271:         FirstReg = 0,
32639:         LastReg = 16,
32639:         UnknownReg = 17
17271:     }
17271:     Register;
17271: 
17271:     typedef int RegisterMask;
17271: 
20893:     static const int NumSavedRegs = 3;
17271:     static const RegisterMask SavedRegs = 1<<EBX | 1<<EDI | 1<<ESI;
17271:     static const RegisterMask GpRegs = SavedRegs | 1<<EAX | 1<<ECX | 1<<EDX;
17271:     static const RegisterMask XmmRegs = 1<<XMM0|1<<XMM1|1<<XMM2|1<<XMM3|1<<XMM4|1<<XMM5|1<<XMM6|1<<XMM7;
17271:     static const RegisterMask x87Regs = 1<<FST0;
17271:     static const RegisterMask FpRegs = x87Regs | XmmRegs;
17271:     static const RegisterMask ScratchRegs = 1<<EAX | 1<<ECX | 1<<EDX | FpRegs;
17271: 
17271:     static const RegisterMask AllowableFlagRegs = 1<<EAX |1<<ECX | 1<<EDX | 1<<EBX;
17271: 
17271:     #define _rmask_(r)      (1<<(r))
17271:     #define _is_xmm_reg_(r) ((_rmask_(r)&XmmRegs)!=0)
17271:     #define _is_x87_reg_(r) ((_rmask_(r)&x87Regs)!=0)
17271:     #define _is_fp_reg_(r)  ((_rmask_(r)&FpRegs)!=0)
17271:     #define _is_gp_reg_(r)  ((_rmask_(r)&GpRegs)!=0)
17271: 
17271:     verbose_only( extern const char* regNames[]; )
17271: 
17271:     #define DECLARE_PLATFORM_STATS()
17271: 
17271:     #define DECLARE_PLATFORM_REGALLOC()
17271: 
17271:     #define DECLARE_PLATFORM_ASSEMBLER()    \
17271:         const static Register argRegs[2], retRegs[2]; \
33740:         int32_t max_stk_args;\
17271:         void nativePageReset();\
17687:         void nativePageSetup();\
20893:         void underrunProtect(int);\
36368:         void asm_int(Register r, int32_t val, bool canClobberCCs);\
33740:         void asm_stkarg(LInsp p, int32_t& stkd);\
33740:         void asm_farg(LInsp, int32_t& stkd);\
33740:         void asm_arg(ArgSize sz, LInsp p, Register r, int32_t& stkd);\
34340:         void asm_pusharg(LInsp);\
33170:         void asm_fcmp(LIns *cond);\
35314:         NIns* asm_fbranch(bool, LIns*, NIns*);\
32555:         void asm_cmp(LIns *cond); \
36372:         void asm_div_mod(LIns *cond); \
37665:         void asm_load(int d, Register r); \
37665:         void asm_quad(Register r, uint64_t q, double d, bool canClobberCCs);
36372: 
36372:  #define IMM8(i)    \
36372:      _nIns -= 1;     \
36372:      *((int8_t*)_nIns) = (int8_t)(i)
36372: 
36372:  #define IMM16(i)    \
36372:      _nIns -= 2;     \
36372:      *((int16_t*)_nIns) = (int16_t)(i)
17271: 
17271: #define IMM32(i)    \
17271:     _nIns -= 4;     \
17271:     *((int32_t*)_nIns) = (int32_t)(i)
17271: 
34303: // XXX rearrange NanoAssert() expression to workaround apparent gcc 4.3 bug:
34303: // XXX "error: logical && with non-zero constant will always evaluate as true"
36372: // underrunProtect(6) is necessary for worst-case
17271: #define MODRMs(r,d,b,l,i) \
33109:         NanoAssert(unsigned(i)<8 && unsigned(b)<8 && unsigned(r)<8); \
17271:         if ((d) == 0 && (b) != EBP) { \
17271:             _nIns -= 2; \
17271:             _nIns[0] = (uint8_t)     ( 0<<6 |   (r)<<3 | 4); \
17271:             _nIns[1] = (uint8_t) ((l)<<6 | (i)<<3 | (b)); \
17271:         } else if (isS8(d)) { \
17271:             _nIns -= 3; \
17271:             _nIns[0] = (uint8_t)     ( 1<<6 |   (r)<<3 | 4 ); \
17271:             _nIns[1] = (uint8_t) ( (l)<<6 | (i)<<3 | (b) ); \
17271:             _nIns[2] = (uint8_t) (d); \
17271:         } else { \
17271:             IMM32(d); \
17271:             *(--_nIns) = (uint8_t) ( (l)<<6 | (i)<<3 | (b) ); \
17271:             *(--_nIns) = (uint8_t)    ( 2<<6 |   (r)<<3 | 4 ); \
17271:         }
17271: 
36372: // underrunProtect(6) is necessary for worst-case
17271: #define MODRMm(r,d,b) \
20893:         NanoAssert(unsigned(r)<8 && ((b)==UnknownReg || unsigned(b)<8)); \
20893:         if ((b) == UnknownReg) {\
20893:             IMM32(d);\
20893:             *(--_nIns) = (uint8_t) (0<<6 | (r)<<3 | 5);\
20893:         } else if ((b) == ESP) { \
17271:             MODRMs(r, d, b, 0, (Register)4); \
17271:         } \
17271:         else if ( (d) == 0 && (b) != EBP) { \
17271:             *(--_nIns) = (uint8_t) ( 0<<6 | (r)<<3 | (b) ); \
17271:         } else if (isS8(d)) { \
17271:             *(--_nIns) = (uint8_t) (d); \
17271:             *(--_nIns) = (uint8_t) ( 1<<6 | (r)<<3 | (b) ); \
17271:         } else { \
17271:             IMM32(d); \
17271:             *(--_nIns) = (uint8_t) ( 2<<6 | (r)<<3 | (b) ); \
17271:         }
17271: 
21500: #define MODRMSIB(reg,base,index,scale,disp)                    \
21500:         if (disp != 0 || base == EBP) {                        \
21500:             if (isS8(disp)) {                                \
21500:                 *(--_nIns) = int8_t(disp);                    \
21500:             } else {                                        \
21500:                 IMM32(disp);                                \
21500:             }                                                \
21500:         }                                                    \
21500:         *(--_nIns) = uint8_t((scale)<<6|(index)<<3|(base));    \
21500:         if (disp == 0 && base != EBP) {                        \
21500:             *(--_nIns) = uint8_t(((reg)<<3)|4);                \
21500:         } else {                                            \
21500:             if (isS8(disp))                                    \
21500:                 *(--_nIns) = uint8_t((1<<6)|(reg<<3)|4);    \
21500:             else                                            \
21500:                 *(--_nIns) = uint8_t((2<<6)|(reg<<3)|4);    \
21500:         }
21500: 
21500: #define MODRMdm(r,addr)                    \
21500:         NanoAssert(unsigned(r)<8);        \
21500:         IMM32(addr);                    \
21500:         *(--_nIns) = (uint8_t)( (r)<<3 | 5 );
21500: 
17271: #define MODRM(d,s) \
17271:         NanoAssert(((unsigned)(d))<8 && ((unsigned)(s))<8); \
17271:         *(--_nIns) = (uint8_t) ( 3<<6|(d)<<3|(s) )
17271: 
17271: #define ALU0(o)             \
17271:         underrunProtect(1);\
17271:         *(--_nIns) = (uint8_t) (o)
17271: 
17271: #define ALUm(c,r,d,b)       \
17271:         underrunProtect(8); \
17271:         MODRMm(r,d,b);      \
17271:         *(--_nIns) = uint8_t(c)
17271: 
21500: #define ALUdm(c,r,addr)        \
21500:         underrunProtect(6);    \
21500:         MODRMdm(r,addr);    \
21500:         *(--_nIns) = uint8_t(c)
21500: 
21500: #define ALUsib(c,r,base,index,scale,disp)    \
21500:         underrunProtect(7);                    \
21500:         MODRMSIB(r,base,index,scale,disp);    \
21500:         *(--_nIns) = uint8_t(c)
21500: 
17271: #define ALUm16(c,r,d,b)     \
17271:         underrunProtect(9); \
17271:         MODRMm(r,d,b);      \
17271:         *(--_nIns) = uint8_t(c);\
17271:         *(--_nIns) = 0x66
17271: 
21500: #define ALU2dm(c,r,addr)    \
21500:         underrunProtect(7);    \
21500:         MODRMdm(r,addr);    \
21500:         *(--_nIns) = (uint8_t) (c);\
21500:         *(--_nIns) = (uint8_t) ((c)>>8)
21500: 
17271: #define ALU2m(c,r,d,b)      \
17271:         underrunProtect(9); \
17271:         MODRMm(r,d,b);      \
17271:         *(--_nIns) = (uint8_t) (c);\
17271:         *(--_nIns) = (uint8_t) ((c)>>8)
17271: 
21500: #define ALU2sib(c,r,base,index,scale,disp)    \
21500:         underrunProtect(8);                    \
21500:         MODRMSIB(r,base,index,scale,disp);    \
21500:         *(--_nIns) = (uint8_t) (c);            \
21500:         *(--_nIns) = (uint8_t) ((c)>>8)
21500: 
17271: #define ALU(c,d,s)  \
17271:         underrunProtect(2);\
17271:         MODRM(d,s); \
17271:         *(--_nIns) = (uint8_t) (c)
17271: 
17271: #define ALUi(c,r,i) \
17271:         underrunProtect(6); \
17271:         NanoAssert(unsigned(r)<8);\
17271:         if (isS8(i)) { \
17271:             *(--_nIns) = uint8_t(i); \
17271:             MODRM((c>>3),(r)); \
17271:             *(--_nIns) = uint8_t(0x83); \
17271:         } else { \
17271:             IMM32(i); \
17271:             if ( (r) == EAX) { \
17271:                 *(--_nIns) = (uint8_t) (c); \
17271:             } else { \
17271:                 MODRM((c>>3),(r)); \
17271:                 *(--_nIns) = uint8_t(0x81); \
17271:             } \
17271:         }
17271: 
17271: #define ALUmi(c,d,b,i) \
17271:         underrunProtect(10); \
17271:         NanoAssert(((unsigned)b)<8); \
17271:         if (isS8(i)) { \
17271:             *(--_nIns) = uint8_t(i); \
17271:             MODRMm((c>>3),(d),(b)); \
17271:             *(--_nIns) = uint8_t(0x83); \
17271:         } else { \
17271:             IMM32(i); \
17271:             MODRMm((c>>3),(d),(b)); \
17271:             *(--_nIns) = uint8_t(0x81); \
17271:         }
17271: 
17271: #define ALU2(c,d,s) \
17271:         underrunProtect(3); \
17271:         MODRM((d),(s)); \
17271:         _nIns -= 2; \
17271:         _nIns[0] = (uint8_t) ( ((c)>>8) ); \
17271:         _nIns[1] = (uint8_t) ( (c) )
17271: 
22654: #define LAHF()      do { count_alu(); ALU0(0x9F);                   asm_output("lahf"); } while(0)
22654: #define SAHF()      do { count_alu(); ALU0(0x9E);                   asm_output("sahf"); } while(0)
22654: #define OR(l,r)     do { count_alu(); ALU(0x0b, (l),(r));           asm_output("or %s,%s",gpn(l),gpn(r)); } while(0)
22654: #define AND(l,r)    do { count_alu(); ALU(0x23, (l),(r));           asm_output("and %s,%s",gpn(l),gpn(r)); } while(0)
22654: #define XOR(l,r)    do { count_alu(); ALU(0x33, (l),(r));           asm_output("xor %s,%s",gpn(l),gpn(r)); } while(0)
22654: #define ADD(l,r)    do { count_alu(); ALU(0x03, (l),(r));           asm_output("add %s,%s",gpn(l),gpn(r)); } while(0)
22654: #define SUB(l,r)    do { count_alu(); ALU(0x2b, (l),(r));           asm_output("sub %s,%s",gpn(l),gpn(r)); } while(0)
22654: #define MUL(l,r)    do { count_alu(); ALU2(0x0faf,(l),(r));         asm_output("mul %s,%s",gpn(l),gpn(r)); } while(0)
29354: #define DIV(r)      do { count_alu(); ALU(0xf7, (Register)7,(r));   asm_output("idiv  edx:eax, %s",gpn(r)); } while(0)
22654: #define NOT(r)      do { count_alu(); ALU(0xf7, (Register)2,(r));   asm_output("not %s",gpn(r)); } while(0)
22654: #define NEG(r)      do { count_alu(); ALU(0xf7, (Register)3,(r));   asm_output("neg %s",gpn(r)); } while(0)
22654: #define SHR(r,s)    do { count_alu(); ALU(0xd3, (Register)5,(r));   asm_output("shr %s,%s",gpn(r),gpn(s)); } while(0)
22654: #define SAR(r,s)    do { count_alu(); ALU(0xd3, (Register)7,(r));   asm_output("sar %s,%s",gpn(r),gpn(s)); } while(0)
22654: #define SHL(r,s)    do { count_alu(); ALU(0xd3, (Register)4,(r));   asm_output("shl %s,%s",gpn(r),gpn(s)); } while(0)
17271: 
17271: #define SHIFT(c,r,i) \
17271:         underrunProtect(3);\
17271:         *--_nIns = (uint8_t)(i);\
17271:         MODRM((Register)c,r);\
17271:         *--_nIns = 0xc1;
17271: 
22654: #define SHLi(r,i)   do { count_alu(); SHIFT(4,r,i); asm_output("shl %s,%d", gpn(r),i); } while(0)
22654: #define SHRi(r,i)   do { count_alu(); SHIFT(5,r,i); asm_output("shr %s,%d", gpn(r),i); } while(0)
22654: #define SARi(r,i)   do { count_alu(); SHIFT(7,r,i); asm_output("sar %s,%d", gpn(r),i); } while(0)
17271: 
22654: #define MOVZX8(d,s) do { count_alu(); ALU2(0x0fb6,d,s); asm_output("movzx %s,%s", gpn(d),gpn(s)); } while(0)
17271: 
22654: #define SUBi(r,i)   do { count_alu(); ALUi(0x2d,r,i);               asm_output("sub %s,%d",gpn(r),i); } while(0)
22654: #define ADDi(r,i)   do { count_alu(); ALUi(0x05,r,i);               asm_output("add %s,%d",gpn(r),i); } while(0)
22654: #define ANDi(r,i)   do { count_alu(); ALUi(0x25,r,i);               asm_output("and %s,%d",gpn(r),i); } while(0)
22654: #define ORi(r,i)    do { count_alu(); ALUi(0x0d,r,i);               asm_output("or %s,%d",gpn(r),i); } while(0)
22654: #define XORi(r,i)   do { count_alu(); ALUi(0x35,r,i);               asm_output("xor %s,%d",gpn(r),i); } while(0)
17271: 
22654: #define ADDmi(d,b,i) do { count_alust(); ALUmi(0x05, d, b, i); asm_output("add %d(%s), %d", d, gpn(b), i); } while(0)
17271: 
22654: #define TEST(d,s)   do { count_alu(); ALU(0x85,d,s);                asm_output("test %s,%s",gpn(d),gpn(s)); } while(0)
22654: #define CMP(l,r)    do { count_alu(); ALU(0x3b, (l),(r));           asm_output("cmp %s,%s",gpn(l),gpn(r)); } while(0)
22654: #define CMPi(r,i)   do { count_alu(); ALUi(0x3d,r,i);               asm_output("cmp %s,%d",gpn(r),i); } while(0)
17271: 
22654: #define MR(d,s)     do { count_mov(); ALU(0x8b,d,s);                asm_output("mov %s,%s",gpn(d),gpn(s)); } while(0)
22654: #define LEA(r,d,b)  do { count_alu(); ALUm(0x8d, r,d,b);            asm_output("lea %s,%d(%s)",gpn(r),d,gpn(b)); } while(0)
25099: // lea %r, d(%i*4)
25099: // This addressing mode is not supported by the MODRMSIB macro.
33109: #define LEAmi4(r,d,i) do { count_alu(); IMM32(d); *(--_nIns) = (2<<6)|((uint8_t)i<<3)|5; *(--_nIns) = (0<<6)|((uint8_t)r<<3)|4; *(--_nIns) = 0x8d;                    asm_output("lea %s, %p(%s*4)", gpn(r), (void*)d, gpn(i)); } while(0)
17271: 
29354: #define CDQ()       do { SARi(EDX, 31); MR(EDX, EAX); } while(0)
29354: 
32784: #define INCLi(p)    do { count_alu(); \
32784:                          underrunProtect(6); \
32784:                          IMM32((uint32_t)(ptrdiff_t)p); *(--_nIns) = 0x05; *(--_nIns) = 0xFF; \
32784:                          asm_output("incl  (%p)", (void*)p); } while (0)
32784: 
22654: #define SETE(r)     do { count_alu(); ALU2(0x0f94,(r),(r));         asm_output("sete %s",gpn(r)); } while(0)
22654: #define SETNP(r)    do { count_alu(); ALU2(0x0f9B,(r),(r));         asm_output("setnp %s",gpn(r)); } while(0)
22654: #define SETL(r)     do { count_alu(); ALU2(0x0f9C,(r),(r));         asm_output("setl %s",gpn(r)); } while(0)
22654: #define SETLE(r)    do { count_alu(); ALU2(0x0f9E,(r),(r));         asm_output("setle %s",gpn(r)); } while(0)
22654: #define SETG(r)     do { count_alu(); ALU2(0x0f9F,(r),(r));         asm_output("setg %s",gpn(r)); } while(0)
22654: #define SETGE(r)    do { count_alu(); ALU2(0x0f9D,(r),(r));         asm_output("setge %s",gpn(r)); } while(0)
22654: #define SETB(r)     do { count_alu(); ALU2(0x0f92,(r),(r));         asm_output("setb %s",gpn(r)); } while(0)
22654: #define SETBE(r)    do { count_alu(); ALU2(0x0f96,(r),(r));         asm_output("setbe %s",gpn(r)); } while(0)
22654: #define SETA(r)     do { count_alu(); ALU2(0x0f97,(r),(r));         asm_output("seta %s",gpn(r)); } while(0)
22654: #define SETAE(r)    do { count_alu(); ALU2(0x0f93,(r),(r));         asm_output("setae %s",gpn(r)); } while(0)
22654: #define SETO(r)     do { count_alu(); ALU2(0x0f92,(r),(r));         asm_output("seto %s",gpn(r)); } while(0)
17271: 
22654: #define MREQ(dr,sr) do { count_alu(); ALU2(0x0f44,dr,sr); asm_output("cmove %s,%s", gpn(dr),gpn(sr)); } while(0)
22654: #define MRNE(dr,sr) do { count_alu(); ALU2(0x0f45,dr,sr); asm_output("cmovne %s,%s", gpn(dr),gpn(sr)); } while(0)
22654: #define MRL(dr,sr)  do { count_alu(); ALU2(0x0f4C,dr,sr); asm_output("cmovl %s,%s", gpn(dr),gpn(sr)); } while(0)
22654: #define MRLE(dr,sr) do { count_alu(); ALU2(0x0f4E,dr,sr); asm_output("cmovle %s,%s", gpn(dr),gpn(sr)); } while(0)
22654: #define MRG(dr,sr)  do { count_alu(); ALU2(0x0f4F,dr,sr); asm_output("cmovg %s,%s", gpn(dr),gpn(sr)); } while(0)
22654: #define MRGE(dr,sr) do { count_alu(); ALU2(0x0f4D,dr,sr); asm_output("cmovge %s,%s", gpn(dr),gpn(sr)); } while(0)
22654: #define MRB(dr,sr)  do { count_alu(); ALU2(0x0f42,dr,sr); asm_output("cmovb %s,%s", gpn(dr),gpn(sr)); } while(0)
22654: #define MRBE(dr,sr) do { count_alu(); ALU2(0x0f46,dr,sr); asm_output("cmovbe %s,%s", gpn(dr),gpn(sr)); } while(0)
22654: #define MRA(dr,sr)  do { count_alu(); ALU2(0x0f47,dr,sr); asm_output("cmova %s,%s", gpn(dr),gpn(sr)); } while(0)
22654: #define MRAE(dr,sr) do { count_alu(); ALU2(0x0f43,dr,sr); asm_output("cmovae %s,%s", gpn(dr),gpn(sr)); } while(0)
22654: #define MRNO(dr,sr) do { count_alu(); ALU2(0x0f41,dr,sr); asm_output("cmovno %s,%s", gpn(dr),gpn(sr)); } while(0)
17271: 
17271: // these aren't currently used but left in for reference
22649: //#define LDEQ(r,d,b) do { ALU2m(0x0f44,r,d,b); asm_output("cmove %s,%d(%s)", gpn(r),d,gpn(b)); } while(0)
22649: //#define LDNEQ(r,d,b) do { ALU2m(0x0f45,r,d,b); asm_output("cmovne %s,%d(%s)", gpn(r),d,gpn(b)); } while(0)
17271: 
17271: #define LD(reg,disp,base)   do {    \
22654:     count_ld();\
17271:     ALUm(0x8b,reg,disp,base);   \
22649:     asm_output("mov %s,%d(%s)",gpn(reg),disp,gpn(base)); } while(0)
17271: 
21500: #define LDdm(reg,addr) do {        \
22654:     count_ld();                 \
21500:     ALUdm(0x8b,reg,addr);        \
25469:     asm_output("mov   %s,0(%lx)",gpn(reg),(unsigned long)addr); \
21500:     } while (0)
21500: 
21500: 
21500: #define SIBIDX(n)    "1248"[n]
21500: 
21500: #define LDsib(reg,disp,base,index,scale) do {    \
22654:     count_ld();                                 \
21500:     ALUsib(0x8b,reg,base,index,scale,disp);        \
22649:     asm_output("mov   %s,%d(%s+%s*%c)",gpn(reg),disp,gpn(base),gpn(index),SIBIDX(scale)); \
21500:     } while (0)
21500: 
36372: // note: movzx/movsx are being output with an 8/16 suffix to indicate the size
36372: // being loaded. this doesn't really match standard intel format (though is arguably
36372: // terser and more obvious in this case) and would probably be nice to fix.
36372: // (likewise, the 8/16 bit stores being output as "mov8" and "mov16" respectively.)
36372: 
17271: // load 16-bit, sign extend
36372: #define LD16S(r,d,b) do { count_ld(); ALU2m(0x0fbf,r,d,b); asm_output("movsx16 %s,%d(%s)", gpn(r),d,gpn(b)); } while(0)
36372: 
36372: #define LD16Sdm(r,addr) do { count_ld(); ALU2dm(0x0fbf,r,addr); asm_output("movsx16 %s,0(%lx)", gpn(r),(unsigned long)addr); } while (0)
36372: 
36372: #define LD16Ssib(r,disp,base,index,scale) do {    \
36372:     count_ld();                                 \
36372:     ALU2sib(0x0fbf,r,base,index,scale,disp);    \
36372:     asm_output("movsx16 %s,%d(%s+%s*%c)",gpn(r),disp,gpn(base),gpn(index),SIBIDX(scale)); \
36372:     } while (0)
17271: 
21469: // load 16-bit, zero extend
36372: #define LD16Z(r,d,b) do { count_ld(); ALU2m(0x0fb7,r,d,b); asm_output("movzx16 %s,%d(%s)", gpn(r),d,gpn(b)); } while(0)
21469: 
36372: #define LD16Zdm(r,addr) do { count_ld(); ALU2dm(0x0fb7,r,addr); asm_output("movzx16 %s,0(%lx)", gpn(r),(unsigned long)addr); } while (0)
21500: 
21500: #define LD16Zsib(r,disp,base,index,scale) do {    \
22654:     count_ld();                                 \
21500:     ALU2sib(0x0fb7,r,base,index,scale,disp);    \
36372:     asm_output("movzx16 %s,%d(%s+%s*%c)",gpn(r),disp,gpn(base),gpn(index),SIBIDX(scale)); \
21500:     } while (0)
21500: 
17271: // load 8-bit, zero extend
36372: #define LD8Z(r,d,b)    do { count_ld(); ALU2m(0x0fb6,r,d,b); asm_output("movzx8 %s,%d(%s)", gpn(r),d,gpn(b)); } while(0)
17271: 
21500: #define LD8Zdm(r,addr) do { \
22654:     count_ld(); \
21500:     ALU2dm(0x0fb6,r,addr); \
36372:     asm_output("movzx8 %s,0(%lx)", gpn(r),(long unsigned)addr); \
21500:     } while(0)
21500: 
21500: #define LD8Zsib(r,disp,base,index,scale) do {    \
22654:     count_ld();                                 \
21500:     ALU2sib(0x0fb6,r,base,index,scale,disp);    \
36372:     asm_output("movzx8 %s,%d(%s+%s*%c)",gpn(r),disp,gpn(base),gpn(index),SIBIDX(scale)); \
21500:     } while(0)
21500: 
36372: // load 8-bit, sign extend
36372: #define LD8S(r,d,b)    do { count_ld(); ALU2m(0x0fbe,r,d,b); asm_output("movsx8 %s,%d(%s)", gpn(r),d,gpn(b)); } while(0)
36372: 
36372: #define LD8Sdm(r,addr) do { \
36372:     count_ld(); \
36372:     ALU2dm(0x0fbe,r,addr); \
36372:     asm_output("movsx8 %s,0(%lx)", gpn(r),(long unsigned)addr); \
36372:     } while(0)
36372: 
36372: #define LD8Ssib(r,disp,base,index,scale) do {    \
36372:     count_ld();                                 \
36372:     ALU2sib(0x0fbe,r,base,index,scale,disp);    \
36372:     asm_output("movsx8 %s,%d(%s+%s*%c)",gpn(r),disp,gpn(base),gpn(index),SIBIDX(scale)); \
36372:     } while(0)
21500: 
17271: #define LDi(r,i) do { \
22654:     count_ld();\
17271:     underrunProtect(5);         \
17271:     IMM32(i);                   \
17271:     NanoAssert(((unsigned)r)<8); \
17271:     *(--_nIns) = (uint8_t) (0xb8 | (r) );       \
22649:     asm_output("mov %s,%d",gpn(r),i); } while(0)
17271: 
36372: // quirk of x86-32: reg must be a/b/c/d for byte stores here
36372: #define ST8(base,disp,reg) do {  \
36372:     count_st();\
36372:     NanoAssert(((unsigned)reg)<4); \
36372:     ALUm(0x88,reg,disp,base);   \
36372:     asm_output("mov8 %d(%s),%s",disp,base==UnknownReg?"0":gpn(base),gpn(reg)); } while(0)
36372: 
36372: #define ST16(base,disp,reg) do {  \
36372:     count_st();\
36372:     ALUm16(0x89,reg,disp,base);   \
36372:     asm_output("mov16 %d(%s),%s",disp,base==UnknownReg?"0":gpn(base),gpn(reg)); } while(0)
36372: 
17271: #define ST(base,disp,reg) do {  \
22654:     count_st();\
17271:     ALUm(0x89,reg,disp,base);   \
22649:     asm_output("mov %d(%s),%s",disp,base==UnknownReg?"0":gpn(base),gpn(reg)); } while(0)
17271: 
36372: #define ST8i(base,disp,imm)  do { \
36372:     count_st();\
36372:     underrunProtect(8);    \
36372:     IMM8(imm);             \
36372:     MODRMm(0, disp, base);  \
36372:     *(--_nIns) = 0xc6;      \
36372:     asm_output("mov8 %d(%s),%d",disp,gpn(base),imm); } while(0)
36372: 
36372: #define ST16i(base,disp,imm)  do { \
36372:     count_st();\
36372:     underrunProtect(10);    \
36372:     IMM16(imm);             \
36372:     MODRMm(0, disp, base);  \
36372:     *(--_nIns) = 0xc7;      \
36372:     *(--_nIns) = 0x66;      \
36372:     asm_output("mov16 %d(%s),%d",disp,gpn(base),imm); } while(0)
36372: 
17271: #define STi(base,disp,imm)  do { \
22654:     count_st();\
36372:     underrunProtect(11);    \
17271:     IMM32(imm);             \
17271:     MODRMm(0, disp, base);  \
17271:     *(--_nIns) = 0xc7;      \
22649:     asm_output("mov %d(%s),%d",disp,gpn(base),imm); } while(0)
17271: 
22654: #define RET()   do { count_ret(); ALU0(0xc3); asm_output("ret"); } while(0)
22654: #define NOP()   do { count_alu(); ALU0(0x90); asm_output("nop"); } while(0)
17271: #define INT3()  do { ALU0(0xcc); asm_output("int3"); } while(0)
17271: 
17271: #define PUSHi(i) do { \
22654:     count_push();\
17271:     if (isS8(i)) { \
17271:         underrunProtect(2);         \
17271:         _nIns-=2; _nIns[0] = 0x6a; _nIns[1] = (uint8_t)(i); \
22649:         asm_output("push %d",i); \
17271:     } else \
17271:         { PUSHi32(i); } } while(0)
17271: 
17271: #define PUSHi32(i)  do {    \
22654:     count_push();\
17271:     underrunProtect(5); \
17271:     IMM32(i);           \
17271:     *(--_nIns) = 0x68;  \
22649:     asm_output("push %d",i); } while(0)
17271: 
17271: #define PUSHr(r) do {  \
22654:     count_push();\
17271:     underrunProtect(1);         \
17271:     NanoAssert(((unsigned)r)<8); \
17271:     *(--_nIns) = (uint8_t) ( 0x50 | (r) );  \
22649:     asm_output("push %s",gpn(r)); } while(0)
17271: 
34340: #define PUSHm(d,b) do { \
34340:     count_pushld();\
34340:     ALUm(0xff, 6, d, b);        \
34340:     asm_output("push %d(%s)",d,gpn(b)); } while(0)
34340: 
17405: #define POPr(r) do { \
22654:     count_pop();\
17271:     underrunProtect(1);         \
17271:     NanoAssert(((unsigned)r)<8); \
17271:     *(--_nIns) = (uint8_t) ( 0x58 | (r) ); \
22649:     asm_output("pop %s",gpn(r)); } while(0)
17271: 
21489: #define JCC32 0x0f
21489: #define JMP8  0xeb
21489: #define JMP32 0xe9
21489: 
30730: #define JCC(o,t,n) do { \
22654:     count_jcc();\
17271:     underrunProtect(6); \
17271:     intptr_t tt = (intptr_t)t - (intptr_t)_nIns;    \
30730:     if (isS8(tt)) { \
17271:         verbose_only( NIns* next = _nIns; (void)next; ) \
17271:         _nIns -= 2; \
17271:         _nIns[0] = (uint8_t) ( 0x70 | (o) ); \
17271:         _nIns[1] = (uint8_t) (tt); \
29883:         asm_output("%-5s %p",(n),(next+tt)); \
17271:     } else { \
17271:         verbose_only( NIns* next = _nIns; ) \
17271:         IMM32(tt); \
17271:         _nIns -= 2; \
21489:         _nIns[0] = JCC32; \
17271:         _nIns[1] = (uint8_t) ( 0x80 | (o) ); \
29883:         asm_output("%-5s %p",(n),(next+tt)); \
17271:     } } while(0)
17271: 
17271: #define JMP_long(t) do { \
22654:     count_jmp();\
17271:     underrunProtect(5); \
17271:     intptr_t tt = (intptr_t)t - (intptr_t)_nIns;    \
17271:     JMP_long_nochk_offset(tt);  \
29883:     verbose_only( verbose_outputf("%010lx:", (unsigned long)_nIns); )    \
17271:     } while(0)
17271: 
17271: #define JMP(t)      do {    \
22654:     count_jmp();\
17271:     underrunProtect(5); \
17271:     intptr_t tt = (intptr_t)t - (intptr_t)_nIns;    \
17271:     if (isS8(tt)) { \
17271:         verbose_only( NIns* next = _nIns; (void)next; ) \
17271:         _nIns -= 2; \
21489:         _nIns[0] = JMP8; \
17271:         _nIns[1] = (uint8_t) ( (tt)&0xff ); \
22649:         asm_output("jmp %p",(next+tt)); \
17271:     } else { \
17271:         JMP_long_nochk_offset(tt);  \
17271:     } } while(0)
17271: 
17271: // this should only be used when you can guarantee there is enough room on the page
17271: #define JMP_long_nochk_offset(o) do {\
17271:         verbose_only( NIns* next = _nIns; (void)next; ) \
17271:         IMM32((o)); \
21489:         *(--_nIns) = JMP32; \
22649:         asm_output("jmp %p",(next+(o))); } while(0)
17271: 
25099: #define JMP_indirect(r) do { \
25099:         underrunProtect(2);  \
25099:         MODRMm(4, 0, r);     \
25099:         *(--_nIns) = 0xff;   \
25099:         asm_output("jmp   *(%s)", gpn(r)); } while (0)
25099: 
35087: #define JMP_indexed(x, ss, addr) do { \
35087:         underrunProtect(7);  \
35087:         IMM32(addr); \
35087:         _nIns -= 3;\
35087:         _nIns[0]   = (NIns) 0xff; /* jmp */ \
35087:         _nIns[1]   = (NIns) (0<<6 | 4<<3 | 4); /* modrm: base=sib + disp32 */ \
35087:         _nIns[2]   = (NIns) ((ss)<<6 | (x)<<3 | 5); /* sib: x<<ss + table */ \
35087:         asm_output("jmp   *(%s*%d+%p)", gpn(x), 1<<(ss), (void*)(addr)); } while (0)
35087: 
30730: #define JE(t)   JCC(0x04, t, "je")
30730: #define JNE(t)  JCC(0x05, t, "jne")
30730: #define JP(t)   JCC(0x0A, t, "jp")
30730: #define JNP(t)  JCC(0x0B, t, "jnp")
17271: 
30730: #define JB(t)   JCC(0x02, t, "jb")
30730: #define JNB(t)  JCC(0x03, t, "jnb")
30730: #define JBE(t)  JCC(0x06, t, "jbe")
30730: #define JNBE(t) JCC(0x07, t, "jnbe")
17271: 
30730: #define JA(t)   JCC(0x07, t, "ja")
30730: #define JNA(t)  JCC(0x06, t, "jna")
30730: #define JAE(t)  JCC(0x03, t, "jae")
30730: #define JNAE(t) JCC(0x02, t, "jnae")
17271: 
30730: #define JL(t)   JCC(0x0C, t, "jl")
30730: #define JNL(t)  JCC(0x0D, t, "jnl")
30730: #define JLE(t)  JCC(0x0E, t, "jle")
30730: #define JNLE(t) JCC(0x0F, t, "jnle")
17271: 
30730: #define JG(t)   JCC(0x0F, t, "jg")
30730: #define JNG(t)  JCC(0x0E, t, "jng")
30730: #define JGE(t)  JCC(0x0D, t, "jge")
30730: #define JNGE(t) JCC(0x0C, t, "jnge")
17271: 
30730: #define JO(t)   JCC(0x00, t, "jo")
30730: #define JNO(t)  JCC(0x01, t, "jno")
17271: 
17271: // sse instructions
17271: #define SSE(c,d,s)  \
17271:         underrunProtect(9); \
17271:         MODRM((d),(s)); \
17271:         _nIns -= 3; \
17271:         _nIns[0] = (uint8_t)(((c)>>16)&0xff); \
17271:         _nIns[1] = (uint8_t)(((c)>>8)&0xff); \
17271:         _nIns[2] = (uint8_t)((c)&0xff)
17271: 
17271: #define SSEm(c,r,d,b)   \
17271:         underrunProtect(9); \
17271:         MODRMm((r),(d),(b));    \
17271:         _nIns -= 3;     \
17271:         _nIns[0] = (uint8_t)(((c)>>16)&0xff); \
17271:         _nIns[1] = (uint8_t)(((c)>>8)&0xff); \
17271:         _nIns[2] = (uint8_t)((c)&0xff)
17271: 
17271: #define LDSD(r,d,b)do {     \
22654:     count_ldq();\
17271:     SSEm(0xf20f10, (r)&7, (d), (b)); \
22649:     asm_output("movsd %s,%d(%s)",gpn(r),(d),gpn(b)); \
17271:     } while(0)
17271: 
17271: #define LDSDm(r,addr)do {     \
22654:     count_ldq();\
17271:     underrunProtect(8); \
17271:     const double* daddr = addr; \
17271:     IMM32(int32_t(daddr));\
17271:     *(--_nIns) = uint8_t(((r)&7)<<3|5); \
17271:     *(--_nIns) = 0x10;\
17271:     *(--_nIns) = 0x0f;\
17271:     *(--_nIns) = 0xf2;\
37665:     asm_output("movsd %s,(%p) // =%f",gpn(r),(void*)daddr,*daddr); \
17271:     } while(0)
17271: 
17271: #define STSD(d,b,r)do {     \
22654:     count_stq();\
17271:     SSEm(0xf20f11, (r)&7, (d), (b)); \
22649:     asm_output("movsd %d(%s),%s",(d),gpn(b),gpn(r)); \
17271:     } while(0)
17271: 
17916: #define SSE_LDQ(r,d,b)do {  \
22654:     count_ldq();\
17271:     SSEm(0xf30f7e, (r)&7, (d), (b)); \
22649:     asm_output("movq %s,%d(%s)",gpn(r),d,gpn(b)); \
17271:     } while(0)
17271: 
17916: #define SSE_STQ(d,b,r)do {  \
22654:     count_stq();\
17271:     SSEm(0x660fd6, (r)&7, (d), (b)); \
22649:     asm_output("movq %d(%s),%s",(d),gpn(b),gpn(r)); \
17271:     } while(0)
17271: 
36372: #define SSE_LDSS(r,d,b)do {  \
36372:     count_ld();\
36372:     SSEm(0xf30f10, (r)&7, (d), (b)); \
36372:     asm_output("movss %s,%d(%s)",gpn(r),d,gpn(b)); \
36372:     } while(0)
36372: 
36372: #define SSE_STSS(d,b,r)do {  \
36372:     count_st();\
36372:     SSEm(0xf30f11, (r)&7, (d), (b)); \
36372:     asm_output("movss %d(%s),%s",(d),gpn(b),gpn(r)); \
36372:     } while(0)
36372: 
17916: #define SSE_CVTSI2SD(xr,gr) do{ \
22654:     count_fpu();\
17271:     SSE(0xf20f2a, (xr)&7, (gr)&7); \
22649:     asm_output("cvtsi2sd %s,%s",gpn(xr),gpn(gr)); \
17271:     } while(0)
17271: 
37700:  #define SSE_CVTSD2SI(gr,xr) do{ \
37700:     count_fpu();\
37700:     SSE(0xf20f2d, (gr)&7, (xr)&7); \
37700:     asm_output("cvtsd2si %s,%s",gpn(gr),gpn(xr)); \
37700:     } while(0)
37700: 
36372: #define SSE_CVTSD2SS(xr,gr) do{ \
36372:     count_fpu();\
36372:     SSE(0xf20f5a, (xr)&7, (gr)&7); \
36372:     asm_output("cvtsd2ss %s,%s",gpn(xr),gpn(gr)); \
36372:     } while(0)
36372: 
36372: #define SSE_CVTSS2SD(xr,gr) do{ \
36372:     count_fpu();\
36372:     SSE(0xf30f5a, (xr)&7, (gr)&7); \
36372:     asm_output("cvtss2sd %s,%s",gpn(xr),gpn(gr)); \
36372:     } while(0)
36372: 
17271: #define CVTDQ2PD(dstr,srcr) do{ \
22654:     count_fpu();\
17271:     SSE(0xf30fe6, (dstr)&7, (srcr)&7); \
22649:     asm_output("cvtdq2pd %s,%s",gpn(dstr),gpn(srcr)); \
17271:     } while(0)
17271: 
17271: // move and zero-extend gpreg to xmm reg
17916: #define SSE_MOVD(d,s) do{ \
22654:     count_mov();\
17271:     if (_is_xmm_reg_(s)) { \
17271:         NanoAssert(_is_gp_reg_(d)); \
17271:         SSE(0x660f7e, (s)&7, (d)&7); \
17271:     } else { \
17271:         NanoAssert(_is_gp_reg_(s)); \
17271:         NanoAssert(_is_xmm_reg_(d)); \
17271:         SSE(0x660f6e, (d)&7, (s)&7); \
17271:     } \
22649:     asm_output("movd %s,%s",gpn(d),gpn(s)); \
17271:     } while(0)
17271: 
17916: #define SSE_MOVSD(rd,rs) do{ \
22654:     count_mov();\
20893:     NanoAssert(_is_xmm_reg_(rd) && _is_xmm_reg_(rs));\
17271:     SSE(0xf20f10, (rd)&7, (rs)&7); \
22649:     asm_output("movsd %s,%s",gpn(rd),gpn(rs)); \
17271:     } while(0)
17271: 
17916: #define SSE_MOVDm(d,b,xrs) do {\
22654:     count_st();\
33923:     NanoAssert(_is_xmm_reg_(xrs) && (_is_gp_reg_(b) || b==FP));\
17271:     SSEm(0x660f7e, (xrs)&7, d, b);\
22649:     asm_output("movd %d(%s),%s", d, gpn(b), gpn(xrs));\
17271:     } while(0)
17271: 
17916: #define SSE_ADDSD(rd,rs) do{ \
22654:     count_fpu();\
20893:     NanoAssert(_is_xmm_reg_(rd) && _is_xmm_reg_(rs));\
17271:     SSE(0xf20f58, (rd)&7, (rs)&7); \
22649:     asm_output("addsd %s,%s",gpn(rd),gpn(rs)); \
17271:     } while(0)
17271: 
17916: #define SSE_ADDSDm(r,addr)do {     \
22654:     count_fpuld();\
17271:     underrunProtect(8); \
20893:     NanoAssert(_is_xmm_reg_(r));\
17271:     const double* daddr = addr; \
17271:     IMM32(int32_t(daddr));\
17271:     *(--_nIns) = uint8_t(((r)&7)<<3|5); \
17271:     *(--_nIns) = 0x58;\
17271:     *(--_nIns) = 0x0f;\
17271:     *(--_nIns) = 0xf2;\
22649:     asm_output("addsd %s,%p // =%f",gpn(r),(void*)daddr,*daddr); \
17271:     } while(0)
17271: 
17916: #define SSE_SUBSD(rd,rs) do{ \
22654:     count_fpu();\
20893:     NanoAssert(_is_xmm_reg_(rd) && _is_xmm_reg_(rs));\
17271:     SSE(0xf20f5c, (rd)&7, (rs)&7); \
22649:     asm_output("subsd %s,%s",gpn(rd),gpn(rs)); \
17271:     } while(0)
17916: #define SSE_MULSD(rd,rs) do{ \
22654:     count_fpu();\
20893:     NanoAssert(_is_xmm_reg_(rd) && _is_xmm_reg_(rs));\
17271:     SSE(0xf20f59, (rd)&7, (rs)&7); \
22649:     asm_output("mulsd %s,%s",gpn(rd),gpn(rs)); \
17271:     } while(0)
17916: #define SSE_DIVSD(rd,rs) do{ \
22654:     count_fpu();\
20893:     NanoAssert(_is_xmm_reg_(rd) && _is_xmm_reg_(rs));\
17271:     SSE(0xf20f5e, (rd)&7, (rs)&7); \
22649:     asm_output("divsd %s,%s",gpn(rd),gpn(rs)); \
17271:     } while(0)
17916: #define SSE_UCOMISD(rl,rr) do{ \
22654:     count_fpu();\
20893:     NanoAssert(_is_xmm_reg_(rl) && _is_xmm_reg_(rr));\
17271:     SSE(0x660f2e, (rl)&7, (rr)&7); \
22649:     asm_output("ucomisd %s,%s",gpn(rl),gpn(rr)); \
17271:     } while(0)
17271: 
17271: #define CVTSI2SDm(xr,d,b) do{ \
22654:     count_fpu();\
20893:     NanoAssert(_is_xmm_reg_(xr) && _is_gp_reg_(b));\
17271:     SSEm(0xf20f2a, (xr)&7, (d), (b)); \
22649:     asm_output("cvtsi2sd %s,%d(%s)",gpn(xr),(d),gpn(b)); \
17271:     } while(0)
17271: 
17916: #define SSE_XORPD(r, maskaddr) do {\
22654:     count_fpuld();\
17271:     underrunProtect(8); \
17271:     IMM32(maskaddr);\
17271:     *(--_nIns) = uint8_t(((r)&7)<<3|5); \
17271:     *(--_nIns) = 0x57;\
17271:     *(--_nIns) = 0x0f;\
17271:     *(--_nIns) = 0x66;\
22649:     asm_output("xorpd %s,[0x%p]",gpn(r),(void*)(maskaddr));\
17271:     } while(0)
17271: 
17916: #define SSE_XORPDr(rd,rs) do{ \
22654:     count_fpu();\
17271:     SSE(0x660f57, (rd)&7, (rs)&7); \
22649:     asm_output("xorpd %s,%s",gpn(rd),gpn(rs)); \
17271:     } while(0)
17271: 
17271: // floating point unit
17271: #define FPUc(o)                             \
17271:         underrunProtect(2);                 \
17271:         *(--_nIns) = ((uint8_t)(o)&0xff);       \
17271:         *(--_nIns) = (uint8_t)(((o)>>8)&0xff)
17271: 
17271: #define FPU(o,r)                            \
17271:         underrunProtect(2);                 \
25469:         *(--_nIns) = uint8_t(((uint8_t)(o)&0xff) | (r&7));\
17271:         *(--_nIns) = (uint8_t)(((o)>>8)&0xff)
17271: 
17271: #define FPUm(o,d,b)                         \
17271:         underrunProtect(7);                 \
17271:         MODRMm((uint8_t)(o), d, b);         \
17271:         *(--_nIns) = (uint8_t)((o)>>8)
17271: 
37665: #define FPUdm(o, m)                         \
37665:         underrunProtect(6);                 \
37665:         MODRMdm((uint8_t)(o), m);         \
37665:         *(--_nIns) = (uint8_t)((o)>>8)
37665: 
17271: #define TEST_AH(i) do {                             \
22654:         count_alu();\
17271:         underrunProtect(3);                 \
17271:         *(--_nIns) = ((uint8_t)(i));            \
17271:         *(--_nIns) = 0xc4;                  \
17271:         *(--_nIns) = 0xf6;                  \
22649:         asm_output("test ah, %d",i); } while(0)
17271: 
17271: #define TEST_AX(i) do {                             \
22654:         count_fpu();\
17271:         underrunProtect(5);                 \
17271:         *(--_nIns) = (0);       \
17271:         *(--_nIns) = ((uint8_t)(i));            \
17271:         *(--_nIns) = ((uint8_t)((i)>>8));       \
17271:         *(--_nIns) = (0);       \
17271:         *(--_nIns) = 0xa9;                  \
22649:         asm_output("test ax, %d",i); } while(0)
17271: 
22654: #define FNSTSW_AX() do { count_fpu(); FPUc(0xdfe0);             asm_output("fnstsw_ax"); } while(0)
22654: #define FCHS()      do { count_fpu(); FPUc(0xd9e0);             asm_output("fchs"); } while(0)
22654: #define FLD1()      do { count_fpu(); FPUc(0xd9e8);             asm_output("fld1"); fpu_push(); } while(0)
22654: #define FLDZ()      do { count_fpu(); FPUc(0xd9ee);             asm_output("fldz"); fpu_push(); } while(0)
22654: #define FFREE(r)    do { count_fpu(); FPU(0xddc0, r);           asm_output("ffree %s",fpn(r)); } while(0)
36372: #define FST32(p,d,b) do { count_stq(); FPUm(0xd902|(p), d, b);   asm_output("fst%s32 %d(%s)",((p)?"p":""),d,gpn(b)); if (p) fpu_pop(); } while(0)
22654: #define FSTQ(p,d,b) do { count_stq(); FPUm(0xdd02|(p), d, b);   asm_output("fst%sq %d(%s)",((p)?"p":""),d,gpn(b)); if (p) fpu_pop(); } while(0)
17271: #define FSTPQ(d,b)  FSTQ(1,d,b)
22654: #define FCOM(p,d,b) do { count_fpuld(); FPUm(0xdc02|(p), d, b); asm_output("fcom%s %d(%s)",((p)?"p":""),d,gpn(b)); if (p) fpu_pop(); } while(0)
37665: #define FCOMdm(p,m) do { const double* const dm = m; \
37705:                          count_fpuld(); FPUdm(0xdc02|(p), dm);   asm_output("fcom%s (%p)",((p)?"p":""),(void*)dm); if (p) fpu_pop(); } while(0)
36372: #define FLD32(d,b)  do { count_ldq(); FPUm(0xd900, d, b);       asm_output("fld32 %d(%s)",d,gpn(b)); fpu_push();} while(0)
22654: #define FLDQ(d,b)   do { count_ldq(); FPUm(0xdd00, d, b);       asm_output("fldq %d(%s)",d,gpn(b)); fpu_push();} while(0)
37665: #define FLDQdm(m)   do { const double* const dm = m; \
37705:                          count_ldq(); FPUdm(0xdd00, dm);        asm_output("fldq (%p)",(void*)dm); fpu_push();} while(0)
22654: #define FILDQ(d,b)  do { count_fpuld(); FPUm(0xdf05, d, b);     asm_output("fildq %d(%s)",d,gpn(b)); fpu_push(); } while(0)
22654: #define FILD(d,b)   do { count_fpuld(); FPUm(0xdb00, d, b);     asm_output("fild %d(%s)",d,gpn(b)); fpu_push(); } while(0)
37700: #define FIST(p,d,b) do { count_fpu(); FPUm(0xdb02|(p), d, b);   asm_output("fist%s %d(%s)",((p)?"p":""),d,gpn(b)); if(p) fpu_pop(); } while(0)
22654: #define FADD(d,b)   do { count_fpu(); FPUm(0xdc00, d, b);       asm_output("fadd %d(%s)",d,gpn(b)); } while(0)
37665: #define FADDdm(m)   do { const double* const dm = m; \
37705:                          count_ldq(); FPUdm(0xdc00, dm);        asm_output("fadd (%p)",(void*)dm); } while(0)
22654: #define FSUB(d,b)   do { count_fpu(); FPUm(0xdc04, d, b);       asm_output("fsub %d(%s)",d,gpn(b)); } while(0)
22654: #define FSUBR(d,b)  do { count_fpu(); FPUm(0xdc05, d, b);       asm_output("fsubr %d(%s)",d,gpn(b)); } while(0)
37665: #define FSUBRdm(m)  do { const double* const dm = m; \
37705:                          count_ldq(); FPUdm(0xdc05, dm);        asm_output("fsubr (%p)",(void*)dm); } while(0)
22654: #define FMUL(d,b)   do { count_fpu(); FPUm(0xdc01, d, b);       asm_output("fmul %d(%s)",d,gpn(b)); } while(0)
37665: #define FMULdm(m)   do { const double* const dm = m; \
37705:                          count_ldq(); FPUdm(0xdc01, dm);        asm_output("fmul (%p)",(void*)dm); } while(0)
22654: #define FDIV(d,b)   do { count_fpu(); FPUm(0xdc06, d, b);       asm_output("fdiv %d(%s)",d,gpn(b)); } while(0)
22654: #define FDIVR(d,b)  do { count_fpu(); FPUm(0xdc07, d, b);       asm_output("fdivr %d(%s)",d,gpn(b)); } while(0)
37665: #define FDIVRdm(m)  do { const double* const dm = m; \
37705:                          count_ldq(); FPUdm(0xdc07, dm);        asm_output("fdivr (%p)",(void*)dm); } while(0)
22654: #define FINCSTP()   do { count_fpu(); FPUc(0xd9f7);             asm_output("fincstp"); } while(0)
22654: #define FSTP(r)     do { count_fpu(); FPU(0xddd8, r&7);         asm_output("fstp %s",fpn(r)); fpu_pop();} while(0)
22654: #define FCOMP()     do { count_fpu(); FPUc(0xD8D9);             asm_output("fcomp"); fpu_pop();} while(0)
22654: #define FCOMPP()    do { count_fpu(); FPUc(0xDED9);             asm_output("fcompp"); fpu_pop();fpu_pop();} while(0)
22654: #define FLDr(r)     do { count_ldq(); FPU(0xd9c0,r);            asm_output("fld %s",fpn(r)); fpu_push(); } while(0)
22654: #define EMMS()      do { count_fpu(); FPUc(0x0f77);             asm_output("emms"); } while (0)
17271: 
20893: // standard direct call
17687: #define CALL(c) do { \
22654:   count_call();\
17271:   underrunProtect(5);                   \
17687:   int offset = (c->_address) - ((int)_nIns); \
17271:   IMM32( (uint32_t)offset );    \
17271:   *(--_nIns) = 0xE8;        \
22662:   verbose_only(asm_output("call %s",(c->_name));) \
31051:   debug_only(if ((c->_argtypes & ARGSIZE_MASK_ANY)==ARGSIZE_F) fpu_push();)\
17271: } while (0)
17687: 
20893: // indirect call thru register
20893: #define CALLr(c,r)  do { \
22654:   count_calli();\
20893:   underrunProtect(2);\
20893:   ALU(0xff, 2, (r));\
22662:   verbose_only(asm_output("call %s",gpn(r));) \
31051:   debug_only(if ((c->_argtypes & ARGSIZE_MASK_ANY)==ARGSIZE_F) fpu_push();)\
20893: } while (0)
20893: 
17271: }
17271: #endif // __nanojit_Nativei386__
