30262: /* -*- Mode: C++; c-basic-offset: 4; indent-tabs-mode: nil; tab-width: 4 -*- */
30266: /* vi: set ts=4 sw=4 expandtab: (add to ~/.vimrc: set modeline modelines=5) */
30262: /* ***** BEGIN LICENSE BLOCK *****
30262:  * Version: MPL 1.1/GPL 2.0/LGPL 2.1
30262:  *
30262:  * The contents of this file are subject to the Mozilla Public License Version
30262:  * 1.1 (the "License"); you may not use this file except in compliance with
30262:  * the License. You may obtain a copy of the License at
30262:  * http://www.mozilla.org/MPL/
30262:  *
30262:  * Software distributed under the License is distributed on an "AS IS" basis,
30262:  * WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
30262:  * for the specific language governing rights and limitations under the
30262:  * License.
30262:  *
30262:  * The Original Code is [Open Source Virtual Machine].
30262:  *
30262:  * The Initial Developer of the Original Code is
30262:  * Adobe System Incorporated.
30262:  * Portions created by the Initial Developer are Copyright (C) 2008
30262:  * the Initial Developer. All Rights Reserved.
30262:  *
30262:  * Contributor(s):
30262:  *   Adobe AS3 Team
30262:  *
30262:  * Alternatively, the contents of this file may be used under the terms of
30262:  * either the GNU General Public License Version 2 or later (the "GPL"), or
30262:  * the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
30262:  * in which case the provisions of the GPL or the LGPL are applicable instead
30262:  * of those above. If you wish to allow use of your version of this file only
30262:  * under the terms of either the GPL or the LGPL, and not to allow others to
30262:  * use your version of this file under the terms of the MPL, indicate your
30262:  * decision by deleting the provisions above and replace them with the notice
30262:  * and other provisions required by the GPL or the LGPL. If you do not delete
30262:  * the provisions above, a recipient may use your version of this file under
30262:  * the terms of any one of the MPL, the GPL or the LGPL.
30262:  *
30262:  * ***** END LICENSE BLOCK ***** */
30262: 
30262: #include "nanojit.h"
30262: 
30262: #if defined FEATURE_NANOJIT && defined NANOJIT_PPC
30262: 
30262: namespace nanojit
30262: {
30262:     const Register Assembler::retRegs[] = { R3, R4 }; // high=R3, low=R4
30262:     const Register Assembler::argRegs[] = { R3, R4, R5, R6, R7, R8, R9, R10 };
30262: 
30262:     const Register Assembler::savedRegs[] = {
30262:     #if !defined NANOJIT_64BIT
30262:         R13,
30262:     #endif
30262:         R14, R15, R16, R17, R18, R19, R20, R21, R22,
30262:         R23, R24, R25, R26, R27, R28, R29, R30
30262:     };
30262: 
30262:     const char *regNames[] = {
30262:         "r0",  "sp",  "r2",  "r3",  "r4",  "r5",  "r6",  "r7",
30262:         "r8",  "r9",  "r10", "r11", "r12", "r13", "r14", "r15",
30262:         "r16", "r17", "r18", "r19", "r20", "r21", "r22", "r23",
30262:         "r24", "r25", "r26", "r27", "r28", "r29", "r30", "r31",
30262:         "f0",  "f1",  "f2",  "f3",  "f4",  "f5",  "f6",  "f7",
30262:         "f8",  "f9",  "f10", "f11", "f12", "f13", "f14", "f15",
30262:         "f16", "f17", "f18", "f19", "f20", "f21", "f22", "f23",
30262:         "f24", "f25", "f26", "f27", "f28", "f29", "f30", "f31"
30262:     };
30262: 
30262:     const char *bitNames[] = { "lt", "gt", "eq", "so" };
30262: 
30262:     #define TODO(x) do{ avmplus::AvmLog(#x); NanoAssertMsgf(false, "%s", #x); } while(0)
30262: 
30262:     /*
30262:      * see http://developer.apple.com/documentation/developertools/Conceptual/LowLevelABI/index.html
30262:      * stack layout (higher address going down)
30262:      * sp ->    out linkage area
30262:      *          out parameter area
30262:      *          local variables
30262:      *          saved registers
30262:      * sp' ->   in linkage area
30262:      *          in parameter area
30262:      *
30262:      * linkage area layout:
30262:      * PPC32    PPC64
30262:      * sp+0     sp+0    saved sp
30262:      * sp+4     sp+8    saved cr
30262:      * sp+8     sp+16   saved lr
30262:      * sp+12    sp+24   reserved
30262:      */
30262: 
37699:     const int min_param_area_size = 8*sizeof(void*); // r3-r10
30262:     const int linkage_size = 6*sizeof(void*);
30262:     const int lr_offset = 2*sizeof(void*); // linkage.lr
30262:     const int cr_offset = 1*sizeof(void*); // linkage.cr
30262: 
30262:     NIns* Assembler::genPrologue() {
30262:         // mflr r0
30262:         // stw r0, lr_offset(sp)
30262:         // stwu sp, -framesize(sp)
30262: 
37699:         // param_area must be at least large enough for r3-r10 to be saved,
37699:         // regardless of whether we think the callee needs less: e.g., the callee
37699:         // might tail-call to a function that uses varargs, which could flush
37699:         // r3-r10 to the parameter area.
37699:         uint32_t param_area = (max_param_size > min_param_area_size) ? max_param_size : min_param_area_size;
30262:         // activation frame is 4 bytes per entry even on 64bit machines
37699:         uint32_t stackNeeded = param_area + linkage_size + _activation.stackSlotsNeeded() * 4;
30262:         uint32_t aligned = alignUp(stackNeeded, NJ_ALIGN_STACK);
30262: 
30262:         UNLESS_PEDANTIC( if (isS16(aligned)) {
30262:             STPU(SP, -aligned, SP); // *(sp-aligned) = sp; sp -= aligned
30262:         } else ) {
30262:             STPUX(SP, SP, R0);
30262:             asm_li(R0, -aligned);
30262:         }
30262: 
30262:         NIns *patchEntry = _nIns;
30262:         MR(FP,SP);              // save SP to use as a FP
30262:         STP(FP, cr_offset, SP); // cheat and save our FP in linkage.cr
30262:         STP(R0, lr_offset, SP); // save LR in linkage.lr
30262:         MFLR(R0);
30262: 
30262:         return patchEntry;
30262:     }
30262: 
30262:     NIns* Assembler::genEpilogue() {
30262:         BLR();
30262:         MTLR(R0);
30262:         LP(R0, lr_offset, SP);
30262:         LP(FP, cr_offset, SP); // restore FP from linkage.cr
30262:         MR(SP,FP);
30262:         return _nIns;
30262:     }
30262: 
30262:     void Assembler::asm_qjoin(LIns *ins) {
30262:         int d = findMemFor(ins);
30262:         NanoAssert(d && isS16(d));
30262:         LIns* lo = ins->oprnd1();
30262:         LIns* hi = ins->oprnd2();
30262: 
30262:         Register r = findRegFor(hi, GpRegs);
30262:         STW(r, d+4, FP);
30262: 
30262:         // okay if r gets recycled.
30262:         r = findRegFor(lo, GpRegs);
30262:         STW(r, d, FP);
30262:         freeRsrcOf(ins, false); // if we had a reg in use, emit a ST to flush it to mem
30262:     }
30262: 
36372:     void Assembler::asm_load32(LIns *ins) {
30262:         LIns* base = ins->oprnd1();
33125:         int d = ins->disp();
30262:         Register rr = prepResultReg(ins, GpRegs);
36552:         Register ra = getBaseReg(base, d, GpRegs);
30262: 
36372:         switch(ins->opcode()) {
36372:             case LIR_ldzb:
36372:             case LIR_ldcb:
30262:                 if (isS16(d)) {
30262:                     LBZ(rr, d, ra);
30262:                 } else {
36372:                     LBZX(rr, ra, R0); // rr = [ra+R0]
36372:                     asm_li(R0,d);
30262:                 }
30262:                 return;
36372:             case LIR_ldzs:
36372:             case LIR_ldcs:
36372:                 // these are expected to be 2 or 4-byte aligned
36372:                 if (isS16(d)) {
36372:                     LHZ(rr, d, ra);
36372:                 } else {
36372:                     LHZX(rr, ra, R0); // rr = [ra+R0]
36372:                     asm_li(R0,d);
30262:                 }
36372:                 return;
36372:             case LIR_ld:
36372:             case LIR_ldc:
36372:                 // these are expected to be 4-byte aligned
36372:                 if (isS16(d)) {
36372:                     LWZ(rr, d, ra);
36372:                 } else {
30262:                     LWZX(rr, ra, R0); // rr = [ra+R0]
30262:                     asm_li(R0,d);
30262:                 }
36372:                 return;
36372:             case LIR_ldsb:
36372:             case LIR_ldss:
36372:             case LIR_ldcsb:
36372:             case LIR_ldcss:
36372:                 NanoAssertMsg(0, "NJ_EXPANDED_LOADSTORE_SUPPORTED not yet supported for this architecture");
36372:                 return;
36372:             default:
36372:                 NanoAssertMsg(0, "asm_load32 should never receive this LIR opcode");
36372:                 return;
36372:         }
36372:     }
30262: 
36372:     void Assembler::asm_store32(LOpcode op, LIns *value, int32_t dr, LIns *base) {
36372: 
36372:         switch (op) {
36372:             case LIR_sti:
36372:                 // handled by mainline code below for now
36372:                 break;
36372:             case LIR_stb:
36372:             case LIR_sts:
36372:                 NanoAssertMsg(0, "NJ_EXPANDED_LOADSTORE_SUPPORTED not yet supported for this architecture");
36372:                 return;
36372:             default:
36372:                 NanoAssertMsg(0, "asm_store32 should never receive this LIR opcode");
36372:                 return;
36372:         }
36372: 
30262:         Register rs = findRegFor(value, GpRegs);
36552:         Register ra = value == base ? rs : getBaseReg(base, dr, GpRegs & ~rmask(rs));
30262: 
30262:     #if !PEDANTIC
30262:         if (isS16(dr)) {
30262:             STW(rs, dr, ra);
30262:             return;
30262:         }
30262:     #endif
30262: 
30262:         // general case store, any offset size
30262:         STWX(rs, ra, R0);
30262:         asm_li(R0, dr);
30262:     }
30262: 
30262:     void Assembler::asm_load64(LIns *ins) {
36372: 
36372:         switch (ins->opcode()) {
37020:             case LIR_ldf:
37020:             case LIR_ldfc:
36372:             case LIR_ldq:
36372:             case LIR_ldqc:
36372:                 // handled by mainline code below for now
36372:                 break;
36372:             case LIR_ld32f:
36372:             case LIR_ldc32f:
36372:                 NanoAssertMsg(0, "NJ_EXPANDED_LOADSTORE_SUPPORTED not yet supported for this architecture");
36372:                 return;
36372:             default:
36372:                 NanoAssertMsg(0, "asm_load64 should never receive this LIR opcode");
36372:                 return;
36372:         }
36372: 
30262:         LIns* base = ins->oprnd1();
30262:     #ifdef NANOJIT_64BIT
35320:         Register rr = ins->getReg();
35320:         if (isKnownReg(rr) && (rmask(rr) & FpRegs)) {
30262:             // FPR already assigned, fine, use it
30262:             freeRsrcOf(ins, false);
30262:         } else {
30262:             // use a GPR register; its okay to copy doubles with GPR's
30262:             // but *not* okay to copy non-doubles with FPR's
30262:             rr = prepResultReg(ins, GpRegs);
30262:         }
30262:     #else
30262:         Register rr = prepResultReg(ins, FpRegs);
30262:     #endif
30262: 
33125:         int dr = ins->disp();
36552:         Register ra = getBaseReg(base, dr, GpRegs);
30262: 
30262:     #ifdef NANOJIT_64BIT
30262:         if (rmask(rr) & GpRegs) {
30262:             #if !PEDANTIC
30262:                 if (isS16(dr)) {
30262:                     LD(rr, dr, ra);
30262:                     return;
30262:                 }
30262:             #endif
30262:             // general case 64bit GPR load
30262:             LDX(rr, ra, R0);
30262:             asm_li(R0, dr);
30262:             return;
30262:         }
30262:     #endif
30262: 
30262:         // FPR
30262:     #if !PEDANTIC
30262:         if (isS16(dr)) {
30262:             LFD(rr, dr, ra);
30262:             return;
30262:         }
30262:     #endif
30262: 
30262:         // general case FPR load
30262:         LFDX(rr, ra, R0);
30262:         asm_li(R0, dr);
30262:     }
30262: 
30262:     void Assembler::asm_li(Register r, int32_t imm) {
30262:     #if !PEDANTIC
30262:         if (isS16(imm)) {
30262:             LI(r, imm);
30262:             return;
30262:         }
30262:         if ((imm & 0xffff) == 0) {
30262:             imm = uint32_t(imm) >> 16;
30262:             LIS(r, imm);
30262:             return;
30262:         }
30262:     #endif
30262:         asm_li32(r, imm);
30262:     }
30262: 
30262:     void Assembler::asm_li32(Register r, int32_t imm) {
30262:         // general case
30262:         // TODO use ADDI instead of ORI if r != r0, impl might have 3way adder
30262:         ORI(r, r, imm);
30262:         LIS(r, imm>>16);  // on ppc64, this sign extends
30262:     }
30262: 
30262:     void Assembler::asm_li64(Register r, uint64_t imm) {
30262:         underrunProtect(5*sizeof(NIns)); // must be contiguous to be patchable
30262:         ORI(r,r,uint16_t(imm));        // r[0:15] = imm[0:15]
30262:         ORIS(r,r,uint16_t(imm>>16));   // r[16:31] = imm[16:31]
30262:         SLDI(r,r,32);                  // r[32:63] = r[0:31], r[0:31] = 0
30262:         asm_li32(r, int32_t(imm>>32)); // r[0:31] = imm[32:63]
30262:     }
30262: 
36372:     void Assembler::asm_store64(LOpcode op, LIns *value, int32_t dr, LIns *base) {
30262:         NanoAssert(value->isQuad());
36372: 
36372:         switch (op) {
37020:             case LIR_stfi:
36372:             case LIR_stqi:
36372:                 // handled by mainline code below for now
36372:                 break;
36372:             case LIR_st32f:
36372:                 NanoAssertMsg(0, "NJ_EXPANDED_LOADSTORE_SUPPORTED not yet supported for this architecture");
36372:                 return;
36372:             default:
36372:                 NanoAssertMsg(0, "asm_store64 should never receive this LIR opcode");
36372:                 return;
36372:         }
36372: 
36552:         Register ra = getBaseReg(base, dr, GpRegs);
30262: 
30262:     #if !PEDANTIC && !defined NANOJIT_64BIT
30262:         if (value->isop(LIR_quad) && isS16(dr) && isS16(dr+4)) {
30262:             // quad constant and short offset
33125:             uint64_t q = value->imm64();
30262:             STW(R0, dr, ra);   // hi
30262:             asm_li(R0, int32_t(q>>32)); // hi
30262:             STW(R0, dr+4, ra); // lo
30262:             asm_li(R0, int32_t(q));     // lo
30262:             return;
30262:         }
30262:         if (value->isop(LIR_qjoin) && isS16(dr) && isS16(dr+4)) {
30262:             // short offset and qjoin(lo,hi) - store lo & hi separately
30262:             RegisterMask allow = GpRegs & ~rmask(ra);
30262:             LIns *lo = value->oprnd1();
30262:             Register rlo = findRegFor(lo, allow);
30262:             LIns *hi = value->oprnd2();
30262:             Register rhi = hi == lo ? rlo : findRegFor(hi, allow & ~rmask(rlo));
30262:             STW(rhi, dr, ra); // hi
30262:             STW(rlo, dr+4, ra); // lo
30262:             return;
30262:         }
30262:     #endif // !PEDANTIC
30262: 
30262:         // general case for any value
30262:     #if !defined NANOJIT_64BIT
30262:         // on 32bit cpu's, we only use store64 for doubles
30262:         Register rs = findRegFor(value, FpRegs);
30262:     #else
30262:         // if we have to choose a register, use a GPR
35320:         Register rs = ( value->isUnusedOrHasUnknownReg()
35320:                       ? findRegFor(value, GpRegs & ~rmask(ra))
35320:                       : value->getReg() );
30262: 
30262:         if (rmask(rs) & GpRegs) {
30262:         #if !PEDANTIC
30262:             if (isS16(dr)) {
30262:                 // short offset
30262:                 STD(rs, dr, ra);
30262:                 return;
30262:             }
30262:         #endif
30262:             // general case store 64bit GPR
30262:             STDX(rs, ra, R0);
30262:             asm_li(R0, dr);
30262:             return;
30262:         }
30262:     #endif // NANOJIT_64BIT
30262: 
30262:     #if !PEDANTIC
30262:         if (isS16(dr)) {
30262:             // short offset
30262:             STFD(rs, dr, ra);
30262:             return;
30262:         }
30262:     #endif
30262: 
30262:         // general case for any offset
30262:         STFDX(rs, ra, R0);
30262:         asm_li(R0, dr);
30262:     }
30262: 
30262:     void Assembler::asm_cond(LIns *ins) {
30262:         LOpcode op = ins->opcode();
30262:         LIns *a = ins->oprnd1();
30262:         LIns *b = ins->oprnd2();
30262:         ConditionRegister cr = CR7;
30262:         Register r = prepResultReg(ins, GpRegs);
30262:         switch (op) {
30262:         case LIR_eq: case LIR_feq:
30262:         case LIR_qeq:
30262:             EXTRWI(r, r, 1, 4*cr+COND_eq); // extract CR7.eq
30262:             MFCR(r);
30262:             break;
30262:         case LIR_lt: case LIR_ult:
30262:         case LIR_flt: case LIR_fle:
30262:         case LIR_qlt: case LIR_qult:
30262:             EXTRWI(r, r, 1, 4*cr+COND_lt); // extract CR7.lt
30262:             MFCR(r);
30262:             break;
30262:         case LIR_gt: case LIR_ugt:
30262:         case LIR_fgt: case LIR_fge:
30262:         case LIR_qgt: case LIR_qugt:
30262:             EXTRWI(r, r, 1, 4*cr+COND_gt); // extract CR7.gt
30262:             MFCR(r);
30262:             break;
30262:         case LIR_le: case LIR_ule:
30262:         case LIR_qle: case LIR_qule:
30262:             EXTRWI(r, r, 1, 4*cr+COND_eq); // extract CR7.eq
30262:             MFCR(r);
30262:             CROR(CR7, eq, lt, eq);
30262:             break;
30262:         case LIR_ge: case LIR_uge:
30262:         case LIR_qge: case LIR_quge:
30262:             EXTRWI(r, r, 1, 4*cr+COND_eq); // select CR7.eq
30262:             MFCR(r);
30262:             CROR(CR7, eq, gt, eq);
30262:             break;
30262:         default:
30262:             debug_only(outputf("%s",lirNames[ins->opcode()]);)
30262:             TODO(asm_cond);
30262:             break;
30262:         }
30262:         asm_cmp(op, a, b, cr);
30262:     }
30262: 
30262:     void Assembler::asm_fcond(LIns *ins) {
30262:         asm_cond(ins);
30262:     }
30262: 
30262:     // cause 32bit sign extension to test bits
30262:     #define isS14(i) ((int32_t(bd<<18)>>18) == (i))
30262: 
30262:     NIns* Assembler::asm_branch(bool onfalse, LIns *cond, NIns * const targ) {
30262:         LOpcode condop = cond->opcode();
30262:         NanoAssert(cond->isCond());
30262: 
30262:         // powerpc offsets are based on the address of the branch instruction
30262:         NIns *patch;
30262:     #if !PEDANTIC
30262:         ptrdiff_t bd = targ - (_nIns-1);
30262:         if (targ && isS24(bd))
30262:             patch = asm_branch_near(onfalse, cond, targ);
30262:         else
30262:     #endif
30262:             patch = asm_branch_far(onfalse, cond, targ);
30262:         asm_cmp(condop, cond->oprnd1(), cond->oprnd2(), CR7);
30262:         return patch;
30262:     }
30262: 
30262:     NIns* Assembler::asm_branch_near(bool onfalse, LIns *cond, NIns * const targ) {
30262:         NanoAssert(targ != 0);
30262:         underrunProtect(4);
30262:         ptrdiff_t bd = targ - (_nIns-1);
30262:         NIns *patch = 0;
30262:         if (!isS14(bd)) {
30262:             underrunProtect(8);
30262:             bd = targ - (_nIns-1);
30262:             if (isS24(bd)) {
30262:                 // can't fit conditional branch offset into 14 bits, but
30262:                 // we can fit in 24, so invert the condition and branch
30262:                 // around an unconditional jump
30262:                 verbose_only(verbose_outputf("%p:", _nIns);)
30262:                 NIns *skip = _nIns;
30262:                 B(bd);
30262:                 patch = _nIns; // this is the patchable branch to the given target
30262:                 onfalse = !onfalse;
30262:                 bd = skip - (_nIns-1);
30262:                 NanoAssert(isS14(bd));
30262:                 verbose_only(verbose_outputf("branch24");)
30262:             }
30262:             else {
30262:                 // known far target
30262:                 return asm_branch_far(onfalse, cond, targ);
30262:             }
30262:         }
30262:         ConditionRegister cr = CR7;
30262:         switch (cond->opcode()) {
30262:         case LIR_eq:
30262:         case LIR_feq:
30262:         case LIR_qeq:
30262:             if (onfalse) BNE(cr,bd); else BEQ(cr,bd);
30262:             break;
30262:         case LIR_lt: case LIR_ult:
30262:         case LIR_flt: case LIR_fle:
30262:         case LIR_qlt: case LIR_qult:
30262:             if (onfalse) BNL(cr,bd); else BLT(cr,bd);
30262:             break;
30262:         case LIR_le: case LIR_ule:
30262:         case LIR_qle: case LIR_qule:
30262:             if (onfalse) BGT(cr,bd); else BLE(cr,bd);
30262:             break;
30262:         case LIR_gt: case LIR_ugt:
30262:         case LIR_fgt: case LIR_fge:
30262:         case LIR_qgt: case LIR_qugt:
30262:             if (onfalse) BNG(cr,bd); else BGT(cr,bd);
30262:             break;
30262:         case LIR_ge: case LIR_uge:
30262:         case LIR_qge: case LIR_quge:
30262:             if (onfalse) BLT(cr,bd); else BGE(cr,bd);
30262:             break;
30262:         default:
30262:             debug_only(outputf("%s",lirNames[cond->opcode()]);)
30262:             TODO(unknown_cond);
30262:         }
30262:         if (!patch)
30262:             patch = _nIns;
30262:         return patch;
30262:     }
30262: 
30262:     // general case branch to any address (using CTR)
30262:     NIns *Assembler::asm_branch_far(bool onfalse, LIns *cond, NIns * const targ) {
30262:         LOpcode condop = cond->opcode();
30262:         ConditionRegister cr = CR7;
30262:         underrunProtect(16);
30262:         switch (condop) {
30262:         case LIR_eq:
30262:         case LIR_feq:
30262:         case LIR_qeq:
30262:             if (onfalse) BNECTR(cr); else BEQCTR(cr);
30262:             break;
30262:         case LIR_lt: case LIR_ult:
30262:         case LIR_qlt: case LIR_qult:
30262:         case LIR_flt: case LIR_fle:
30262:             if (onfalse) BNLCTR(cr); else BLTCTR(cr);
30262:             break;
30262:         case LIR_le: case LIR_ule:
30262:         case LIR_qle: case LIR_qule:
30262:             if (onfalse) BGTCTR(cr); else BLECTR(cr);
30262:             break;
30262:         case LIR_gt: case LIR_ugt:
30262:         case LIR_qgt: case LIR_qugt:
30262:         case LIR_fgt: case LIR_fge:
30262:             if (onfalse) BNGCTR(cr); else BGTCTR(cr);
30262:             break;
30262:         case LIR_ge: case LIR_uge:
30262:         case LIR_qge: case LIR_quge:
30262:             if (onfalse) BLTCTR(cr); else BGECTR(cr);
30262:             break;
30262:         default:
30262:             debug_only(outputf("%s",lirNames[condop]);)
30262:             TODO(unknown_cond);
30262:         }
30262: 
30262:     #if !defined NANOJIT_64BIT
30262:         MTCTR(R0);
30262:         asm_li32(R0, (int)targ);
30262:     #else
30262:         MTCTR(R0);
30262:         if (!targ || !isU32(uintptr_t(targ))) {
30262:             asm_li64(R0, uint64_t(targ));
30262:         } else {
30262:             asm_li32(R0, uint32_t(uintptr_t(targ)));
30262:         }
30262:     #endif
30262:         return _nIns;
30262:     }
30262: 
30262:     void Assembler::asm_cmp(LOpcode condop, LIns *a, LIns *b, ConditionRegister cr) {
30262:         RegisterMask allow = condop >= LIR_feq && condop <= LIR_fge ? FpRegs : GpRegs;
30262:         Register ra = findRegFor(a, allow);
30262: 
30262:     #if !PEDANTIC
30262:         if (b->isconst()) {
33125:             int32_t d = b->imm32();
30262:             if (isS16(d)) {
30262:                 if (condop >= LIR_eq && condop <= LIR_ge) {
30262:                     CMPWI(cr, ra, d);
30262:                     return;
30262:                 }
30262:                 if (condop >= LIR_qeq && condop <= LIR_qge) {
30262:                     CMPDI(cr, ra, d);
30262:                     TODO(cmpdi);
30262:                     return;
30262:                 }
30262:             }
30262:             if (isU16(d)) {
30262:                 if ((condop == LIR_eq || condop >= LIR_ult && condop <= LIR_uge)) {
30262:                     CMPLWI(cr, ra, d);
30262:                     return;
30262:                 }
30262:                 if ((condop == LIR_qeq || condop >= LIR_qult && condop <= LIR_quge)) {
30262:                     CMPLDI(cr, ra, d);
30262:                     TODO(cmpldi);
30262:                     return;
30262:                 }
30262:             }
30262:         }
30262:     #endif
30262: 
30262:         // general case
30262:         Register rb = b==a ? ra : findRegFor(b, allow & ~rmask(ra));
30262:         if (condop >= LIR_eq && condop <= LIR_ge) {
30262:             CMPW(cr, ra, rb);
30262:         } else if (condop >= LIR_ult && condop <= LIR_uge) {
30262:             CMPLW(cr, ra, rb);
30262:         } else if (condop >= LIR_qeq && condop <= LIR_qge) {
30262:             CMPD(cr, ra, rb);
30262:         }
30262:         else if (condop >= LIR_qult && condop <= LIR_quge) {
30262:             CMPLD(cr, ra, rb);
30262:         }
30262:         else if (condop >= LIR_feq && condop <= LIR_fge) {
30262:             // set the lt/gt bit for fle/fge.  We don't do this for
30262:             // int/uint because in those cases we can invert the branch condition.
30262:             // for float, we can't because of unordered comparisons
30262:             if (condop == LIR_fle)
30262:                 CROR(cr, lt, lt, eq); // lt = lt|eq
30262:             else if (condop == LIR_fge)
30262:                 CROR(cr, gt, gt, eq); // gt = gt|eq
30262:             FCMPU(cr, ra, rb);
30262:         }
30262:         else {
30262:             TODO(asm_cmp);
30262:         }
30262:     }
30262: 
30262:     void Assembler::asm_ret(LIns *ins) {
32634:         genEpilogue();
36672:         releaseRegisters();
33125:         assignSavedRegs();
30262:         LIns *value = ins->oprnd1();
30262:         Register r = ins->isop(LIR_ret) ? R3 : F1;
30262:         findSpecificRegFor(value, r);
30262:     }
30262: 
30262:     void Assembler::asm_nongp_copy(Register r, Register s) {
30262:         // PPC doesn't support any GPR<->FPR moves
30262:         NanoAssert((rmask(r) & FpRegs) && (rmask(s) & FpRegs));
30262:         FMR(r, s);
30262:     }
30262: 
35365:     void Assembler::asm_restore(LIns *i, Register r) {
30262:         int d;
32582:         if (i->isop(LIR_alloc)) {
35320:             d = disp(i);
30262:             ADDI(r, FP, d);
30262:         }
30262:         else if (i->isconst()) {
35320:             if (!i->getArIndex()) {
35320:                 i->markAsClear();
30262:             }
33125:             asm_li(r, i->imm32());
30262:         }
30262:         else {
30262:             d = findMemFor(i);
30262:             if (IsFpReg(r)) {
30262:                 NanoAssert(i->isQuad());
30262:                 LFD(r, d, FP);
30262:             } else if (i->isQuad()) {
30262:                 LD(r, d, FP);
30262:             } else {
30262:                 LWZ(r, d, FP);
30262:             }
30262:         }
30262:     }
30262: 
30262:     void Assembler::asm_int(LIns *ins) {
30262:         Register rr = prepResultReg(ins, GpRegs);
33125:         asm_li(rr, ins->imm32());
30262:     }
30262: 
30262:     void Assembler::asm_fneg(LIns *ins) {
30262:         Register rr = prepResultReg(ins, FpRegs);
30262:         Register ra = findRegFor(ins->oprnd1(), FpRegs);
30262:         FNEG(rr,ra);
30262:     }
30262: 
30262:     void Assembler::asm_param(LIns *ins) {
33125:         uint32_t a = ins->paramArg();
33125:         uint32_t kind = ins->paramKind();
30262:         if (kind == 0) {
30262:             // ordinary param
30262:             // first eight args always in R3..R10 for PPC
30262:             if (a < 8) {
30262:                 // incoming arg in register
30262:                 prepResultReg(ins, rmask(argRegs[a]));
30262:             } else {
30262:                 // todo: support stack based args, arg 0 is at [FP+off] where off
30262:                 // is the # of regs to be pushed in genProlog()
30262:                 TODO(asm_param_stk);
30262:             }
30262:         }
30262:         else {
30262:             // saved param
30262:             prepResultReg(ins, rmask(savedRegs[a]));
30262:         }
30262:     }
30262: 
30262:     void Assembler::asm_call(LIns *ins) {
36374:         Register retReg = ( ins->isop(LIR_fcall) ? F1 : retRegs[0] );
36374:         prepResultReg(ins, rmask(retReg));
36374: 
36374:         // Do this after we've handled the call result, so we don't
36374:         // force the call result to be spilled unnecessarily.
36374: 
36374:         evictScratchRegs();
36374: 
30262:         const CallInfo* call = ins->callInfo();
30262:         ArgSize sizes[MAXARGS];
30262:         uint32_t argc = call->get_sizes(sizes);
30262: 
30262:         bool indirect;
30262:         if (!(indirect = call->isIndirect())) {
33125:             verbose_only(if (_logc->lcbits & LC_Assembly)
30262:                 outputf("        %p:", _nIns);
30262:             )
30262:             br((NIns*)call->_address, 1);
30262:         } else {
30262:             // Indirect call: we assign the address arg to R11 since it's not
30262:             // used for regular arguments, and is otherwise scratch since it's
30262:             // clobberred by the call.
30262:             underrunProtect(8); // underrunProtect might clobber CTR
30262:             BCTRL();
30262:             MTCTR(R11);
30262:             asm_regarg(ARGSIZE_P, ins->arg(--argc), R11);
30262:         }
30262: 
30262:         int param_size = 0;
30262: 
30262:         Register r = R3;
30262:         Register fr = F1;
30262:         for(uint32_t i = 0; i < argc; i++) {
30262:             uint32_t j = argc - i - 1;
30262:             ArgSize sz = sizes[j];
30262:             LInsp arg = ins->arg(j);
30262:             if (sz & ARGSIZE_MASK_INT) {
30262:                 // GP arg
30262:                 if (r <= R10) {
30262:                     asm_regarg(sz, arg, r);
30262:                     r = nextreg(r);
30262:                     param_size += sizeof(void*);
30262:                 } else {
30262:                     // put arg on stack
30262:                     TODO(stack_int32);
30262:                 }
30262:             } else if (sz == ARGSIZE_F) {
30262:                 // double
30262:                 if (fr <= F13) {
30262:                     asm_regarg(sz, arg, fr);
30262:                     fr = nextreg(fr);
30262:                 #ifdef NANOJIT_64BIT
30262:                     r = nextreg(r);
30262:                 #else
30262:                     r = nextreg(nextreg(r)); // skip 2 gpr's
30262:                 #endif
30262:                     param_size += sizeof(double);
30262:                 } else {
30262:                     // put arg on stack
30262:                     TODO(stack_double);
30262:                 }
30262:             } else {
30262:                 TODO(ARGSIZE_UNK);
30262:             }
30262:         }
30262:         if (param_size > max_param_size)
30262:             max_param_size = param_size;
30262:     }
30262: 
30262:     void Assembler::asm_regarg(ArgSize sz, LInsp p, Register r)
30262:     {
30262:         NanoAssert(r != UnknownReg);
30262:         if (sz & ARGSIZE_MASK_INT)
30262:         {
30262:         #ifdef NANOJIT_64BIT
30262:             if (sz == ARGSIZE_I) {
30262:                 // sign extend 32->64
30262:                 EXTSW(r, r);
30262:             } else if (sz == ARGSIZE_U) {
30262:                 // zero extend 32->64
30262:                 CLRLDI(r, r, 32);
30262:             }
30262:         #endif
30262:             // arg goes in specific register
30262:             if (p->isconst()) {
33125:                 asm_li(r, p->imm32());
30262:             } else {
35320:                 if (p->isUsed()) {
35320:                     if (!p->hasKnownReg()) {
30262:                         // load it into the arg reg
30262:                         int d = findMemFor(p);
32582:                         if (p->isop(LIR_alloc)) {
30262:                             NanoAssert(isS16(d));
30262:                             ADDI(r, FP, d);
30262:                         } else if (p->isQuad()) {
30262:                             LD(r, d, FP);
30262:                         } else {
30262:                             LWZ(r, d, FP);
30262:                         }
30262:                     } else {
30262:                         // it must be in a saved reg
35320:                         MR(r, p->getReg());
30262:                     }
30262:                 }
30262:                 else {
30262:                     // this is the last use, so fine to assign it
30262:                     // to the scratch reg, it's dead after this point.
30262:                     findSpecificRegFor(p, r);
30262:                 }
30262:             }
30262:         }
30262:         else if (sz == ARGSIZE_F) {
35320:             if (p->isUsed()) {
35320:                 Register rp = p->getReg();
35320:                 if (!isKnownReg(rp) || !IsFpReg(rp)) {
30262:                     // load it into the arg reg
30262:                     int d = findMemFor(p);
30262:                     LFD(r, d, FP);
30262:                 } else {
30262:                     // it must be in a saved reg
35320:                     NanoAssert(IsFpReg(r) && IsFpReg(rp));
35320:                     FMR(r, rp);
30262:                 }
30262:             }
30262:             else {
30262:                 // this is the last use, so fine to assign it
30262:                 // to the scratch reg, it's dead after this point.
30262:                 findSpecificRegFor(p, r);
30262:             }
30262:         }
30262:         else {
30262:             TODO(ARGSIZE_UNK);
30262:         }
30262:     }
30262: 
30262:     void Assembler::asm_spill(Register rr, int d, bool /* pop */, bool quad) {
30262:         (void)quad;
30262:         if (d) {
30262:             if (IsFpReg(rr)) {
30262:                 NanoAssert(quad);
30262:                 STFD(rr, d, FP);
30262:             }
30262:         #ifdef NANOJIT_64BIT
30262:             else if (quad) {
30262:                 STD(rr, d, FP);
30262:             }
30262:         #endif
30262:             else {
30262:                 NanoAssert(!quad);
30262:                 STW(rr, d, FP);
30262:             }
30262:         }
30262:     }
30262: 
30262:     void Assembler::asm_arith(LIns *ins) {
30262:         LOpcode op = ins->opcode();
30262:         LInsp lhs = ins->oprnd1();
30262:         LInsp rhs = ins->oprnd2();
30262:         RegisterMask allow = GpRegs;
30262:         Register rr = prepResultReg(ins, allow);
30262:         Register ra = findRegFor(lhs, GpRegs);
30262: 
30262:         if (rhs->isconst()) {
33125:             int32_t rhsc = rhs->imm32();
30262:             if (isS16(rhsc)) {
30262:                 // ppc arith immediate ops sign-exted the imm16 value
30262:                 switch (op) {
30262:                 case LIR_add:
30262:                 case LIR_iaddp:
30262:                 IF_64BIT(case LIR_qiadd:)
30262:                 IF_64BIT(case LIR_qaddp:)
30262:                     ADDI(rr, ra, rhsc);
30262:                     return;
30262:                 case LIR_sub:
30262:                     SUBI(rr, ra, rhsc);
30262:                     return;
30262:                 case LIR_mul:
30262:                     MULLI(rr, ra, rhsc);
30262:                     return;
30262:                 }
30262:             }
30262:             if (isU16(rhsc)) {
30262:                 // ppc logical immediate zero-extend the imm16 value
30262:                 switch (op) {
30262:                 IF_64BIT(case LIR_qior:)
30262:                 case LIR_or:
30262:                     ORI(rr, ra, rhsc);
30262:                     return;
30262:                 IF_64BIT(case LIR_qiand:)
30262:                 case LIR_and:
30262:                     ANDI(rr, ra, rhsc);
30262:                     return;
30262:                 IF_64BIT(case LIR_qxor:)
30262:                 case LIR_xor:
30262:                     XORI(rr, ra, rhsc);
30262:                     return;
30262:                 }
30262:             }
30262: 
30262:             // LIR shift ops only use last 5bits of shift const
30262:             switch (op) {
30262:             case LIR_lsh:
30262:                 SLWI(rr, ra, rhsc&31);
30262:                 return;
30262:             case LIR_ush:
30262:                 SRWI(rr, ra, rhsc&31);
30262:                 return;
30262:             case LIR_rsh:
30262:                 SRAWI(rr, ra, rhsc&31);
30262:                 return;
30262:             }
30262:         }
30262: 
30262:         // general case, put rhs in register
30262:         Register rb = rhs==lhs ? ra : findRegFor(rhs, GpRegs&~rmask(ra));
30262:         switch (op) {
30262:             IF_64BIT(case LIR_qiadd:)
30262:             IF_64BIT(case LIR_qaddp:)
30262:             case LIR_add:
30262:             case LIR_iaddp:
30262:                 ADD(rr, ra, rb);
30262:                 break;
30262:             IF_64BIT(case LIR_qiand:)
30262:             case LIR_and:
30262:                 AND(rr, ra, rb);
30262:                 break;
30262:             IF_64BIT(case LIR_qior:)
30262:             case LIR_or:
30262:                 OR(rr, ra, rb);
30262:                 break;
30262:             IF_64BIT(case LIR_qxor:)
30262:             case LIR_xor:
30262:                 XOR(rr, ra, rb);
30262:                 break;
30262:             case LIR_sub:  SUBF(rr, rb, ra);    break;
30262:             case LIR_lsh:  SLW(rr, ra, R0);     ANDI(R0, rb, 31);   break;
30262:             case LIR_rsh:  SRAW(rr, ra, R0);    ANDI(R0, rb, 31);   break;
30262:             case LIR_ush:  SRW(rr, ra, R0);     ANDI(R0, rb, 31);   break;
30262:             case LIR_mul:  MULLW(rr, ra, rb);   break;
30262:         #ifdef NANOJIT_64BIT
30262:             case LIR_qilsh:
30262:                 SLD(rr, ra, R0);
30262:                 ANDI(R0, rb, 63);
30262:                 break;
30262:             case LIR_qursh:
30262:                 SRD(rr, ra, R0);
30262:                 ANDI(R0, rb, 63);
30262:                 break;
30262:             case LIR_qirsh:
30262:                 SRAD(rr, ra, R0);
30262:                 ANDI(R0, rb, 63);
30262:                 TODO(qirsh);
30262:                 break;
30262:         #endif
30262:             default:
30262:                 debug_only(outputf("%s",lirNames[op]);)
30262:                 TODO(asm_arith);
30262:         }
30262:     }
30262: 
30262:     void Assembler::asm_fop(LIns *ins) {
30262:         LOpcode op = ins->opcode();
30262:         LInsp lhs = ins->oprnd1();
30262:         LInsp rhs = ins->oprnd2();
30262:         RegisterMask allow = FpRegs;
30262:         Register rr = prepResultReg(ins, allow);
35320:         Register ra, rb;
37705:         findRegFor2(allow, lhs, ra, allow, rhs, rb);
30262:         switch (op) {
30262:             case LIR_fadd: FADD(rr, ra, rb); break;
30262:             case LIR_fsub: FSUB(rr, ra, rb); break;
30262:             case LIR_fmul: FMUL(rr, ra, rb); break;
30262:             case LIR_fdiv: FDIV(rr, ra, rb); break;
30262:             default:
30262:                 debug_only(outputf("%s",lirNames[op]);)
30262:                 TODO(asm_fop);
30262:         }
30262:     }
30262: 
30262:     void Assembler::asm_i2f(LIns *ins) {
30262:         Register r = prepResultReg(ins, FpRegs);
30262:         Register v = findRegFor(ins->oprnd1(), GpRegs);
30262:         const int d = 16; // natural aligned
30262: 
30262:     #if defined NANOJIT_64BIT && !PEDANTIC
30262:         FCFID(r, r);    // convert to double
30262:         LFD(r, d, SP);  // load into fpu register
30262:         STD(v, d, SP);  // save int64
30262:         EXTSW(v, v);    // extend sign destructively, ok since oprnd1 only is 32bit
30262:     #else
30262:         FSUB(r, r, F0);
30262:         LFD(r, d, SP); // scratch area in outgoing linkage area
30262:         STW(R0, d+4, SP);
30262:         XORIS(R0, v, 0x8000);
30262:         LFD(F0, d, SP);
30262:         STW(R0, d+4, SP);
30262:         LIS(R0, 0x8000);
30262:         STW(R0, d, SP);
30262:         LIS(R0, 0x4330);
30262:     #endif
30262:     }
30262: 
30262:     void Assembler::asm_u2f(LIns *ins) {
30262:         Register r = prepResultReg(ins, FpRegs);
30262:         Register v = findRegFor(ins->oprnd1(), GpRegs);
30262:         const int d = 16;
30262: 
30262:     #if defined NANOJIT_64BIT && !PEDANTIC
30262:         FCFID(r, r);    // convert to double
30262:         LFD(r, d, SP);  // load into fpu register
30262:         STD(v, d, SP);  // save int64
30262:         CLRLDI(v, v, 32); // zero-extend destructively
30262:     #else
30262:         FSUB(r, r, F0);
30262:         LFD(F0, d, SP);
30262:         STW(R0, d+4, SP);
30262:         LI(R0, 0);
30262:         LFD(r, d, SP);
30262:         STW(v, d+4, SP);
30262:         STW(R0, d, SP);
30262:         LIS(R0, 0x4330);
30262:     #endif
30262:     }
30262: 
30262:     void Assembler::asm_promote(LIns *ins) {
30262:         LOpcode op = ins->opcode();
30262:         Register r = prepResultReg(ins, GpRegs);
30262:         Register v = findRegFor(ins->oprnd1(), GpRegs);
30262:         switch (op) {
30262:         default:
30262:             debug_only(outputf("%s",lirNames[op]));
30262:             TODO(asm_promote);
30262:         case LIR_u2q:
30262:             CLRLDI(r, v, 32); // clears the top 32 bits
30262:             break;
30262:         case LIR_i2q:
30262:             EXTSW(r, v);
30262:             break;
30262:         }
30262:     }
30262: 
30262:     void Assembler::asm_quad(LIns *ins) {
30262:     #ifdef NANOJIT_64BIT
35320:         Register r = ins->getReg();
35320:         if (isKnownReg(r) && (rmask(r) & FpRegs)) {
30262:             // FPR already assigned, fine, use it
30262:             freeRsrcOf(ins, false);
30262:         } else {
30262:             // use a GPR register; its okay to copy doubles with GPR's
30262:             // but *not* okay to copy non-doubles with FPR's
30262:             r = prepResultReg(ins, GpRegs);
30262:         }
30262:     #else
30262:         Register r = prepResultReg(ins, FpRegs);
30262:     #endif
30262: 
30262:         if (rmask(r) & FpRegs) {
30262:             union {
30262:                 double d;
30262:                 struct {
30262:                     int32_t hi, lo;
30262:                 } w;
30262:             };
33125:             d = ins->imm64f();
37017:             LFD(r, 8, SP);
30262:             STW(R0, 12, SP);
37017:             asm_li(R0, w.lo);
37017:             STW(R0, 8, SP);
30262:             asm_li(R0, w.hi);
30262:         }
30262:         else {
33125:             int64_t q = ins->imm64();
30262:             if (isS32(q)) {
30262:                 asm_li(r, int32_t(q));
30262:                 return;
30262:             }
30262:             RLDIMI(r,R0,32,0); // or 32,32?
30262:             asm_li(R0, int32_t(q>>32)); // hi bits into R0
30262:             asm_li(r, int32_t(q)); // lo bits into dest reg
30262:         }
30262:     }
30262: 
30262:     void Assembler::br(NIns* addr, int link) {
30262:         // destination unknown, then use maximum branch possible
30262:         if (!addr) {
30262:             br_far(addr,link);
30262:             return;
30262:         }
30262: 
30262:         // powerpc offsets are based on the address of the branch instruction
30262:         underrunProtect(4);       // ensure _nIns is addr of Bx
30262:         ptrdiff_t offset = addr - (_nIns-1); // we want ptr diff's implicit >>2 here
30262: 
30262:         #if !PEDANTIC
30262:         if (isS24(offset)) {
30262:             Bx(offset, 0, link); // b addr or bl addr
30262:             return;
30262:         }
30262:         ptrdiff_t absaddr = addr - (NIns*)0; // ptr diff implies >>2
30262:         if (isS24(absaddr)) {
30262:             Bx(absaddr, 1, link); // ba addr or bla addr
30262:             return;
30262:         }
30262:         #endif // !PEDANTIC
30262: 
30262:         br_far(addr,link);
30262:     }
30262: 
30262:     void Assembler::br_far(NIns* addr, int link) {
30262:         // far jump.
30262:         // can't have a page break in this sequence, because the break
30262:         // would also clobber ctr and r2.  We use R2 here because it's not available
30262:         // to the register allocator, and we use R0 everywhere else as scratch, so using
30262:         // R2 here avoids clobbering anything else besides CTR.
30262:     #ifdef NANOJIT_64BIT
30262:         if (addr==0 || !isU32(uintptr_t(addr))) {
30262:             // really far jump to 64bit abs addr
30262:             underrunProtect(28); // 7 instructions
30262:             BCTR(link);
30262:             MTCTR(R2);
30262:             asm_li64(R2, uintptr_t(addr)); // 5 instructions
30262:             return;
30262:         }
30262:     #endif
30262:         underrunProtect(16);
30262:         BCTR(link);
30262:         MTCTR(R2);
30262:         asm_li32(R2, uint32_t(uintptr_t(addr))); // 2 instructions
30262:     }
30262: 
30262:     void Assembler::underrunProtect(int bytes) {
30262:         NanoAssertMsg(bytes<=LARGEST_UNDERRUN_PROT, "constant LARGEST_UNDERRUN_PROT is too small");
30262:         int instr = (bytes + sizeof(NIns) - 1) / sizeof(NIns);
30262:         NIns *pc = _nIns;
35356:         NIns *top = codeStart;  // this may be in a normal code chunk or an exit code chunk
30262: 
30262:     #if PEDANTIC
30262:         // pedanticTop is based on the last call to underrunProtect; any time we call
30262:         // underrunProtect and would use more than what's already protected, then insert
30262:         // a page break jump.  Sometimes, this will be to a new page, usually it's just
30262:         // the next instruction and the only effect is to clobber R2 & CTR
30262: 
30262:         NanoAssert(pedanticTop >= top);
30262:         if (pc - instr < pedanticTop) {
30262:             // no page break required, but insert a far branch anyway just to be difficult
30262:         #ifdef NANOJIT_64BIT
30262:             const int br_size = 7;
30262:         #else
30262:             const int br_size = 4;
30262:         #endif
30262:             if (pc - instr - br_size < top) {
30262:                 // really do need a page break
33125:                 verbose_only(if (_logc->lcbits & LC_Assembly) outputf("newpage %p:", pc);)
30262:                 codeAlloc();
30262:             }
30262:             // now emit the jump, but make sure we won't need another page break.
30262:             // we're pedantic, but not *that* pedantic.
30262:             pedanticTop = _nIns - br_size;
30262:             br(pc, 0);
30262:             pedanticTop = _nIns - instr;
30262:         }
30262:     #else
30262:         if (pc - instr < top) {
33125:             verbose_only(if (_logc->lcbits & LC_Assembly) outputf("newpage %p:", pc);)
35356:             // This may be in a normal code chunk or an exit code chunk.
33939:             codeAlloc(codeStart, codeEnd, _nIns verbose_only(, codeBytes));
35356:             // This jump will call underrunProtect again, but since we're on a new
30262:             // page, nothing will happen.
30262:             br(pc, 0);
30262:         }
30262:     #endif
30262:     }
30262: 
30262:     void Assembler::asm_cmov(LIns *ins) {
30262:         NanoAssert(ins->isop(LIR_cmov) || ins->isop(LIR_qcmov));
30262:         LIns* cond    = ins->oprnd1();
30648:         LIns* iftrue  = ins->oprnd2();
30648:         LIns* iffalse = ins->oprnd3();
33125: 
33125:         NanoAssert(cond->isCmp());
30262:         NanoAssert(iftrue->isQuad() == iffalse->isQuad());
33125: 
30262:         // fixme: we could handle fpu registers here, too, since we're just branching
30262:         Register rr = prepResultReg(ins, GpRegs);
30262:         findSpecificRegFor(iftrue, rr);
30262:         Register rf = findRegFor(iffalse, GpRegs & ~rmask(rr));
30262:         NIns *after = _nIns;
33125:         verbose_only(if (_logc->lcbits & LC_Assembly) outputf("%p:",after);)
30262:         MR(rr, rf);
30262:         asm_branch(false, cond, after);
30262:     }
30262: 
30262:     RegisterMask Assembler::hint(LIns *i, RegisterMask allow) {
30262:         LOpcode op = i->opcode();
30262:         RegisterMask prefer = ~0LL;
30262:         if (op == LIR_icall || op == LIR_qcall)
30262:             prefer = rmask(R3);
30262:         else if (op == LIR_fcall)
30262:             prefer = rmask(F1);
33125:         else if (op == LIR_param) {
33125:             if (i->paramArg() < 8) {
33125:                 prefer = rmask(argRegs[i->paramArg()]);
30262:             }
30262:         }
30262:         // narrow the allow set to whatever is preferred and also free
30262:         if (_allocator.free & allow & prefer)
30262:             allow &= prefer;
30262:         return allow;
30262:     }
30262: 
30262:     void Assembler::asm_neg_not(LIns *ins) {
30262:         Register rr = prepResultReg(ins, GpRegs);
30262:         Register ra = findRegFor(ins->oprnd1(), GpRegs);
30262:         if (ins->isop(LIR_neg)) {
30262:             NEG(rr, ra);
30262:         } else {
30262:             NOT(rr, ra);
30262:         }
30262:     }
30262: 
30262:     void Assembler::asm_qlo(LIns *ins) {
30262:         Register rr = prepResultReg(ins, GpRegs);
30262:         int d = findMemFor(ins->oprnd1());
30262:         LWZ(rr, d+4, FP);
30262:     }
30262: 
30262:     void Assembler::asm_qhi(LIns *ins) {
30262:         Register rr = prepResultReg(ins, GpRegs);
30262:         int d = findMemFor(ins->oprnd1());
30262:         LWZ(rr, d, FP);
30262:         TODO(asm_qhi);
30262:     }
30262: 
30262:     void Assembler::nInit(AvmCore*) {
30262:     }
30262: 
32634:     void Assembler::nBeginAssembly() {
32634:         max_param_size = 0;
32634:     }
32634: 
30262:     void Assembler::nativePageSetup() {
35356:         NanoAssert(!_inExit);
30262:         if (!_nIns) {
33939:             codeAlloc(codeStart, codeEnd, _nIns verbose_only(, codeBytes));
30262:             IF_PEDANTIC( pedanticTop = _nIns; )
30262:         }
30262:     }
30262: 
30262:     void Assembler::nativePageReset()
30262:     {}
30262: 
33939:     // Increment the 32-bit profiling counter at pCtr, without
33939:     // changing any registers.
33939:     verbose_only(
33939:     void Assembler::asm_inc_m32(uint32_t* /*pCtr*/)
33939:     {
33939:     }
33939:     )
33939: 
30262:     void Assembler::nPatchBranch(NIns *branch, NIns *target) {
30262:         // ppc relative offsets are based on the addr of the branch instruction
30262:         ptrdiff_t bd = target - branch;
30262:         if (branch[0] == PPC_b) {
30262:             // unconditional, 24bit offset.  Whoever generated the unpatched jump
30262:             // must have known the final size would fit in 24bits!  otherwise the
30262:             // jump would be (lis,ori,mtctr,bctr) and we'd be patching the lis,ori.
30262:             NanoAssert(isS24(bd));
30262:             branch[0] |= (bd & 0xffffff) << 2;
30262:         }
30262:         else if ((branch[0] & PPC_bc) == PPC_bc) {
30262:             // conditional, 14bit offset. Whoever generated the unpatched jump
30262:             // must have known the final size would fit in 14bits!  otherwise the
30262:             // jump would be (lis,ori,mtctr,bcctr) and we'd be patching the lis,ori below.
30262:             NanoAssert(isS14(bd));
30262:             NanoAssert(((branch[0] & 0x3fff)<<2) == 0);
30262:             branch[0] |= (bd & 0x3fff) << 2;
30262:             TODO(patch_bc);
30262:         }
30262:     #ifdef NANOJIT_64BIT
30262:         // patch 64bit branch
30262:         else if ((branch[0] & ~(31<<21)) == PPC_addis) {
30262:             // general branch, using lis,ori,sldi,oris,ori to load the const 64bit addr.
30262:             Register rd = Register((branch[0] >> 21) & 31);
30262:             NanoAssert(branch[1] == PPC_ori  | GPR(rd)<<21 | GPR(rd)<<16);
30262:             NanoAssert(branch[3] == PPC_oris | GPR(rd)<<21 | GPR(rd)<<16);
30262:             NanoAssert(branch[4] == PPC_ori  | GPR(rd)<<21 | GPR(rd)<<16);
30262:             uint64_t imm = uintptr_t(target);
30262:             uint32_t lo = uint32_t(imm);
30262:             uint32_t hi = uint32_t(imm>>32);
30262:             branch[0] = PPC_addis | GPR(rd)<<21 |               uint16_t(hi>>16);
30262:             branch[1] = PPC_ori   | GPR(rd)<<21 | GPR(rd)<<16 | uint16_t(hi);
30262:             branch[3] = PPC_oris  | GPR(rd)<<21 | GPR(rd)<<16 | uint16_t(lo>>16);
30262:             branch[4] = PPC_ori   | GPR(rd)<<21 | GPR(rd)<<16 | uint16_t(lo);
30262:         }
30262:     #else // NANOJIT_64BIT
30262:         // patch 32bit branch
30262:         else if ((branch[0] & ~(31<<21)) == PPC_addis) {
30262:             // general branch, using lis,ori to load the const addr.
30262:             // patch a lis,ori sequence with a 32bit value
30262:             Register rd = Register((branch[0] >> 21) & 31);
30262:             NanoAssert(branch[1] == PPC_ori | GPR(rd)<<21 | GPR(rd)<<16);
30262:             uint32_t imm = uint32_t(target);
30262:             branch[0] = PPC_addis | GPR(rd)<<21 | uint16_t(imm >> 16); // lis rd, imm >> 16
30262:             branch[1] = PPC_ori | GPR(rd)<<21 | GPR(rd)<<16 | uint16_t(imm); // ori rd, rd, imm & 0xffff
30262:         }
30262:     #endif // !NANOJIT_64BIT
30262:         else {
30262:             TODO(unknown_patch);
30262:         }
30262:     }
30262: 
30262:     static int cntzlw(int set) {
30262:         // On PowerPC, prefer higher registers, to minimize
30262:         // size of nonvolatile area that must be saved.
30262:         register Register i;
30262:         #ifdef __GNUC__
30262:         asm ("cntlzw %0,%1" : "=r" (i) : "r" (set));
30262:         #else // __GNUC__
30262:         # error("unsupported compiler")
30262:         #endif // __GNUC__
30262:         return 31-i;
30262:     }
30262: 
30262:     Register Assembler::nRegisterAllocFromSet(RegisterMask set) {
30262:         Register i;
30262:         // note, deliberate truncation of 64->32 bits
30262:         if (set & 0xffffffff) {
30262:             i = Register(cntzlw(int(set))); // gp reg
30262:         } else {
30262:             i = Register(32+cntzlw(int(set>>32))); // fp reg
30262:         }
30262:         _allocator.free &= ~rmask(i);
30262:         return i;
30262:     }
30262: 
30262:     void Assembler::nRegisterResetAll(RegAlloc &regs) {
30262:         regs.clear();
30262:         regs.free = SavedRegs | 0x1ff8 /* R3-12 */ | 0x3ffe00000000LL /* F1-13 */;
30262:         debug_only(regs.managed = regs.free);
30262:     }
30262: 
30262: #ifdef NANOJIT_64BIT
30262:     void Assembler::asm_qbinop(LIns *ins) {
30262:         LOpcode op = ins->opcode();
30262:         switch (op) {
30262:         case LIR_qaddp:
30262:         case LIR_qior:
30262:         case LIR_qiand:
30262:         case LIR_qursh:
30262:         case LIR_qirsh:
30262:         case LIR_qilsh:
30262:         case LIR_qxor:
33939:         case LIR_qiadd:
30262:             asm_arith(ins);
30262:             break;
30262:         default:
30262:             debug_only(outputf("%s",lirNames[op]));
30262:             TODO(asm_qbinop);
30262:         }
30262:     }
30262: #endif // NANOJIT_64BIT
30262: 
30262:     void Assembler::nFragExit(LIns*) {
30262:         TODO(nFragExit);
30262:     }
35087: 
35087:     void Assembler::asm_jtbl(LIns* ins, NIns** native_table)
35087:     {
35087:         // R0 = index*4, R2 = table, CTR = computed address to jump to.
35087:         // must ensure no page breaks in here because R2 & CTR can get clobbered.
35087:         Register indexreg = findRegFor(ins->oprnd1(), GpRegs);
35087: #ifdef NANOJIT_64BIT
35087:         underrunProtect(9*4);
35087:         BCTR(0);                                // jump to address in CTR
35087:         MTCTR(R2);                              // CTR = R2
35087:         LDX(R2, R2, R0);                        // R2 = [table + index*8]
35087:         SLDI(R0, indexreg, 3);                  // R0 = index*8
35087:         asm_li64(R2, uint64_t(native_table));   // R2 = table (5 instr)
35087: #else // 64bit
35087:         underrunProtect(6*4);
35087:         BCTR(0);                                // jump to address in CTR
35087:         MTCTR(R2);                              // CTR = R2
35087:         LWZX(R2, R2, R0);                       // R2 = [table + index*4]
35087:         SLWI(R0, indexreg, 2);                  // R0 = index*4
35087:         asm_li(R2, int32_t(native_table));      // R2 = table (up to 2 instructions)
35087: #endif // 64bit
35087:     }
35087: 
35356:     void Assembler::swapCodeChunks() {
37698:         if (!_nExitIns) {
37698:             codeAlloc(exitStart, exitEnd, _nExitIns verbose_only(, exitBytes));
37698:         }
35356:         SWAP(NIns*, _nIns, _nExitIns);
35356:         SWAP(NIns*, codeStart, exitStart);
35356:         SWAP(NIns*, codeEnd, exitEnd);
35356:         verbose_only( SWAP(size_t, codeBytes, exitBytes); )
35356:     }
35356: 
30262: } // namespace nanojit
30262: 
30262: #endif // FEATURE_NANOJIT && NANOJIT_PPC
