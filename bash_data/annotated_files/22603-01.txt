18773: /* -*- Mode: C++; tab-width: 8; indent-tabs-mode: nil; c-basic-offset: 4 -*-
18056:  * vim: set ts=4 sw=4 et tw=99:
17181:  *
17181:  * ***** BEGIN LICENSE BLOCK *****
17181:  * Version: MPL 1.1/GPL 2.0/LGPL 2.1
17181:  *
17181:  * The contents of this file are subject to the Mozilla Public License Version
17181:  * 1.1 (the "License"); you may not use this file except in compliance with
17181:  * the License. You may obtain a copy of the License at
17181:  * http://www.mozilla.org/MPL/
17181:  *
17181:  * Software distributed under the License is distributed on an "AS IS" basis,
17181:  * WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
17181:  * for the specific language governing rights and limitations under the
17181:  * License.
17181:  *
17181:  * The Original Code is Mozilla SpiderMonkey JavaScript 1.9 code, released
17181:  * May 28, 2008.
17181:  *
17181:  * The Initial Developer of the Original Code is
17339:  *   Brendan Eich <brendan@mozilla.org>
17181:  *
17181:  * Contributor(s):
17339:  *   Andreas Gal <gal@mozilla.com>
17671:  *   Mike Shaver <shaver@mozilla.org>
17671:  *   David Anderson <danderson@mozilla.com>
17181:  *
17181:  * Alternatively, the contents of this file may be used under the terms of
17181:  * either of the GNU General Public License Version 2 or later (the "GPL"),
17181:  * or the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
17181:  * in which case the provisions of the GPL or the LGPL are applicable instead
17181:  * of those above. If you wish to allow use of your version of this file only
17181:  * under the terms of either the GPL or the LGPL, and not to allow others to
17181:  * use your version of this file under the terms of the MPL, indicate your
17181:  * decision by deleting the provisions above and replace them with the notice
17181:  * and other provisions required by the GPL or the LGPL. If you do not delete
17181:  * the provisions above, a recipient may use your version of this file under
17181:  * the terms of any one of the MPL, the GPL or the LGPL.
17181:  *
17181:  * ***** END LICENSE BLOCK ***** */
17181: 
17666: #include "jsstddef.h"           // always first
17923: #include "jsbit.h"              // low-level (NSPR-based) headers next
17923: #include "jsprf.h"
17666: #include <math.h>               // standard headers next
17666: #ifdef _MSC_VER
17666: #include <malloc.h>
17666: #define alloca _alloca
17666: #endif
19058: #ifdef SOLARIS
19058: #include <alloca.h>
19058: #endif
17181: 
21062: #include "nanojit/nanojit.h"
17666: #include "jsarray.h"            // higher-level library and API headers
17421: #include "jsbool.h"
17630: #include "jscntxt.h"
17949: #include "jsdbgapi.h"
17949: #include "jsemit.h"
17630: #include "jsfun.h"
17630: #include "jsinterp.h"
17899: #include "jsiter.h"
17666: #include "jsobj.h"
17863: #include "jsopcode.h"
18115: #include "jsregexp.h"
17949: #include "jsscope.h"
17630: #include "jsscript.h"
19990: #include "jsdate.h"
20969: #include "jsstaticcheck.h"
17407: #include "jstracer.h"
17293: 
17666: #include "jsautooplen.h"        // generated headers last
17597: 
19995: /* Never use JSVAL_IS_BOOLEAN because it restricts the value (true, false) and 
19995:    the type. What you want to use is JSVAL_TAG(x) == JSVAL_BOOLEAN and then 
19995:    handle the undefined case properly (bug 457363). */
19995: #undef JSVAL_IS_BOOLEAN
19995: #define JSVAL_IS_BOOLEAN(x) JS_STATIC_ASSERT(0) 
19995: 
20399: /* Use a fake tag to represent boxed values, borrowing from the integer tag
20399:    range since we only use JSVAL_INT to indicate integers. */
20399: #define JSVAL_BOXED 3
20399: 
20399: /* Map to translate a type tag into a printable representation. */
20399: static const char typeChar[] = "OIDVS?B?";
20399: 
19591: /* Number of iterations of a loop where we start tracing.  That is, we don't
19592:    start tracing until the beginning of the HOTLOOP-th iteration. */
17821: #define HOTLOOP 2
17821: 
17821: /* Number of times we wait to exit on a side exit before we try to extend the tree. */
18290: #define HOTEXIT 1
17821: 
18051: /* Max call depths for inlining. */
19070: #define MAX_CALLDEPTH 10
17852: 
18051: /* Max number of type mismatchs before we trash the tree. */
21433: #define MAX_MISMATCH 20
21433: 
21433: /* Max blacklist level of inner tree immediate recompiling  */
21433: #define MAX_INNER_RECORD_BLACKLIST  -16
18051: 
18118: /* Max native stack size. */
18118: #define MAX_NATIVE_STACK_SLOTS 1024
18118: 
18133: /* Max call stack size. */
18133: #define MAX_CALL_STACK_ENTRIES 64
18133: 
18781: /* Max number of branches per tree. */
18781: #define MAX_BRANCHES 16
18781: 
21433: /* Macros for demote slot lists */
21472: #define ALLOCA_UNDEMOTE_SLOTLIST(num)     (unsigned*)alloca(((num) + 1) * sizeof(unsigned))
21472: #define ADD_UNDEMOTE_SLOT(list, slot)     list[++list[0]] = slot
21472: #define NUM_UNDEMOTE_SLOTS(list)          list[0]
21472: #define CLEAR_UNDEMOTE_SLOTLIST(list)     list[0] = 0
21433: 
21459: #ifdef JS_JIT_SPEW
18260: #define ABORT_TRACE(msg)   do { debug_only_v(fprintf(stdout, "abort: %d: %s\n", __LINE__, msg);)  return false; } while (0)
17630: #else
17630: #define ABORT_TRACE(msg)   return false
17630: #endif
17630: 
21459: #ifdef JS_JIT_SPEW
19592: struct __jitstats {
19592: #define JITSTAT(x) uint64 x;
19592: #include "jitstats.tbl"
19592: #undef JITSTAT
19623: } jitstats = { 0LL, };
19623: 
19623: JS_STATIC_ASSERT(sizeof(jitstats) % sizeof(uint64) == 0);
19592: 
19592: enum jitstat_ids {
19592: #define JITSTAT(x) STAT ## x ## ID,
19592: #include "jitstats.tbl"
19592: #undef JITSTAT
19598:     STAT_IDS_TOTAL
19592: };
19592: 
19592: static JSPropertySpec jitstats_props[] = {
19592: #define JITSTAT(x) { #x, STAT ## x ## ID, JSPROP_ENUMERATE | JSPROP_READONLY | JSPROP_PERMANENT },
19592: #include "jitstats.tbl"
19592: #undef JITSTAT
19592:     { 0 }
19592: };
19592: 
19592: static JSBool
19592: jitstats_getProperty(JSContext *cx, JSObject *obj, jsid id, jsval *vp)
19592: {
19592:     int index = -1;
19592: 
19592:     if (JSVAL_IS_STRING(id)) {
19592:         JSString* str = JSVAL_TO_STRING(id);
19592:         if (strcmp(JS_GetStringBytes(str), "HOTLOOP") == 0) {
19592:             *vp = INT_TO_JSVAL(HOTLOOP);
19592:             return JS_TRUE;
19592:         }
19592:     }
19592: 
19592:     if (JSVAL_IS_INT(id))
19592:         index = JSVAL_TO_INT(id);
19592: 
19592:     uint64 result = 0;
19592:     switch (index) {
19623: #define JITSTAT(x) case STAT ## x ## ID: result = jitstats.x; break;
19592: #include "jitstats.tbl"
19592: #undef JITSTAT
19592:       default:
19592:         *vp = JSVAL_VOID;
19592:         return JS_TRUE;
19592:     }
19592: 
19592:     if (result < JSVAL_INT_MAX) {
19592:         *vp = INT_TO_JSVAL(result);
19592:         return JS_TRUE;
19592:     }
19592:     char retstr[64];
19606:     JS_snprintf(retstr, sizeof retstr, "%llu", result);
19592:     *vp = STRING_TO_JSVAL(JS_NewStringCopyZ(cx, retstr));
19592:     return JS_TRUE;
19592: }
19592: 
19592: JSClass jitstats_class = {
19592:     "jitstats",
19592:     JSCLASS_HAS_PRIVATE,
19592:     JS_PropertyStub,       JS_PropertyStub,
19592:     jitstats_getProperty,  JS_PropertyStub,
19592:     JS_EnumerateStub,      JS_ResolveStub,
19592:     JS_ConvertStub,        JS_FinalizeStub,
19592:     JSCLASS_NO_OPTIONAL_MEMBERS
19592: };
19592: 
19592: void
19592: js_InitJITStatsClass(JSContext *cx, JSObject *glob)
19592: {
19592:     JS_InitClass(cx, glob, NULL, &jitstats_class, NULL, 0, jitstats_props, NULL, NULL, NULL);
19592: }
19592: 
19623: #define AUDIT(x) (jitstats.x++)
17726: #else
17726: #define AUDIT(x) ((void)0)
21459: #endif /* JS_JIT_SPEW */
17726: 
17870: #define INS_CONST(c)    addName(lir->insImm(c), #c)
18070: #define INS_CONSTPTR(p) addName(lir->insImmPtr((void*) (p)), #p)
17870: 
17409: using namespace avmplus;
17293: using namespace nanojit;
17293: 
17409: static GC gc = GC();
18056: static avmplus::AvmCore s_core = avmplus::AvmCore();
18056: static avmplus::AvmCore* core = &s_core;
17185: 
21459: #ifdef JS_JIT_SPEW
21433: void
21433: js_DumpPeerStability(Fragmento* frago, const void* ip);
21433: #endif
21433: 
17997: /* We really need a better way to configure the JIT. Shaver, where is my fancy JIT object? */
18196: static bool nesting_enabled = true;
19040: #if defined(NANOJIT_IA32)
18333: static bool did_we_check_sse2 = false;
19040: #endif
17997: 
21459: #ifdef JS_JIT_SPEW
18260: static bool verbose_debug = getenv("TRACEMONKEY") && strstr(getenv("TRACEMONKEY"), "verbose");
18260: #define debug_only_v(x) if (verbose_debug) { x; }
18260: #else
18260: #define debug_only_v(x)
18260: #endif
18260: 
17981: /* The entire VM shares one oracle. Collisions and concurrent updates are tolerated and worst
17981:    case cause performance regressions. */
19535: static Oracle oracle;
17981: 
21433: /* Blacklists the root peer fragment at a fragment's PC.  This is so blacklisting stays at the 
21433:    top of the peer list and not scattered around. */
21433: void
21433: js_BlacklistPC(Fragmento* frago, Fragment* frag);
21433: 
17596: Tracker::Tracker()
17213: {
17293:     pagelist = 0;
17293: }
17239: 
17596: Tracker::~Tracker()
17293: {
17293:     clear();
17293: }
17293: 
17596: jsuword
17596: Tracker::getPageBase(const void* v) const
17293: {
17424:     return jsuword(v) & ~jsuword(NJ_PAGE_SIZE-1);
17293: }
17293: 
17596: struct Tracker::Page*
17596: Tracker::findPage(const void* v) const
17293: {
17424:     jsuword base = getPageBase(v);
17596:     struct Tracker::Page* p = pagelist;
17293:     while (p) {
17356:         if (p->base == base) {
17293:             return p;
17356:         }
17293:         p = p->next;
17293:     }
17293:     return 0;
17293: }
17293: 
17596: struct Tracker::Page*
17596: Tracker::addPage(const void* v) {
17424:     jsuword base = getPageBase(v);
17314:     struct Tracker::Page* p = (struct Tracker::Page*)
17596:         GC::Alloc(sizeof(*p) - sizeof(p->map) + (NJ_PAGE_SIZE >> 2) * sizeof(LIns*));
17293:     p->base = base;
17293:     p->next = pagelist;
17293:     pagelist = p;
17293:     return p;
17213: }
17213: 
17596: void
17596: Tracker::clear()
17213: {
17293:     while (pagelist) {
17293:         Page* p = pagelist;
17293:         pagelist = pagelist->next;
17293:         GC::Free(p);
17259:     }
17293: }
17293: 
17773: bool
17773: Tracker::has(const void *v) const
17773: {
17811:     return get(v) != NULL;
17811: }
17811: 
18230: #if defined NANOJIT_64BIT
18230: #define PAGEMASK	0x7ff
18230: #else
18230: #define PAGEMASK	0xfff
18230: #endif
18230: 
17811: LIns*
17811: Tracker::get(const void* v) const
17811: {
17773:     struct Tracker::Page* p = findPage(v);
17773:     if (!p)
17811:         return NULL;
18230:     return p->map[(jsuword(v) & PAGEMASK) >> 2];
17219: }
17247: 
17596: void
17596: Tracker::set(const void* v, LIns* i)
17247: {
17596:     struct Tracker::Page* p = findPage(v);
17293:     if (!p)
17293:         p = addPage(v);
18230:     p->map[(jsuword(v) & PAGEMASK) >> 2] = i;
17293: }
17293: 
17464: static inline bool isNumber(jsval v)
17464: {
17464:     return JSVAL_IS_INT(v) || JSVAL_IS_DOUBLE(v);
17464: }
17464: 
17464: static inline jsdouble asNumber(jsval v)
17464: {
17464:     JS_ASSERT(isNumber(v));
17464:     if (JSVAL_IS_DOUBLE(v))
17464:         return *JSVAL_TO_DOUBLE(v);
17464:     return (jsdouble)JSVAL_TO_INT(v);
17464: }
17464: 
17479: static inline bool isInt32(jsval v)
17479: {
17479:     if (!isNumber(v))
17479:         return false;
17479:     jsdouble d = asNumber(v);
17759:     jsint i;
17759:     return JSDOUBLE_IS_INT(d, i);
17479: }
17479: 
19576: /* Return JSVAL_DOUBLE for all numbers (int and double) and the tag otherwise. */
19576: static inline uint8 getPromotedType(jsval v) 
19576: {
21665:     return JSVAL_IS_INT(v) ? JSVAL_DOUBLE : uint8(JSVAL_TAG(v));
19576: }
19576: 
19576: /* Return JSVAL_INT for all whole numbers that fit into signed 32-bit and the tag otherwise. */
17891: static inline uint8 getCoercedType(jsval v)
17891: {
18702:     return isInt32(v) ? JSVAL_INT : (uint8) JSVAL_TAG(v);
17891: }
17891: 
17981: /* Tell the oracle that a certain global variable should not be demoted. */
17981: void
17993: Oracle::markGlobalSlotUndemotable(JSScript* script, unsigned slot)
17981: {
17981:     _dontDemote.set(&gc, (slot % ORACLE_SIZE));
17981: }
17981: 
17981: /* Consult with the oracle whether we shouldn't demote a certain global variable. */
17981: bool
17993: Oracle::isGlobalSlotUndemotable(JSScript* script, unsigned slot) const
17981: {
21433:     return _dontDemote.get(slot % ORACLE_SIZE);
17981: }
17981: 
17981: /* Tell the oracle that a certain slot at a certain bytecode location should not be demoted. */
17981: void
17981: Oracle::markStackSlotUndemotable(JSScript* script, jsbytecode* ip, unsigned slot)
17981: {
18230:     uint32 hash = uint32(intptr_t(ip)) + (slot << 5);
17981:     hash %= ORACLE_SIZE;
17981:     _dontDemote.set(&gc, hash);
17981: }
17981: 
17981: /* Consult with the oracle whether we shouldn't demote a certain slot. */
17981: bool
17981: Oracle::isStackSlotUndemotable(JSScript* script, jsbytecode* ip, unsigned slot) const
17981: {
18230:     uint32 hash = uint32(intptr_t(ip)) + (slot << 5);
17981:     hash %= ORACLE_SIZE;
21433:     return _dontDemote.get(hash);
17981: }
17981: 
18273: /* Clear the oracle. */
18273: void
18273: Oracle::clear()
18273: {
18273:     _dontDemote.reset();
18273: }
18273: 
20424: #if defined(NJ_SOFTFLOAT)
20923: JS_DEFINE_CALLINFO_1(static, DOUBLE,    i2f, INT32,                 1, 1)
20923: JS_DEFINE_CALLINFO_1(static, DOUBLE,    u2f, UINT32,                1, 1)
20424: #endif
20424: 
18773: static bool isi2f(LInsp i)
18773: {
18773:     if (i->isop(LIR_i2f))
18773:         return true;
18773: 
18777: #if defined(NJ_SOFTFLOAT)
18773:     if (i->isop(LIR_qjoin) &&
18773:         i->oprnd1()->isop(LIR_call) &&
18773:         i->oprnd2()->isop(LIR_callh))
18773:     {
20915:         if (i->oprnd1()->callInfo() == &i2f_ci)
18773:             return true;
18773:     }
18773: #endif
18773: 
18773:     return false;
18773: }
18773: 
18773: static bool isu2f(LInsp i)
18773: {
18773:     if (i->isop(LIR_u2f))
18773:         return true;
18773: 
18777: #if defined(NJ_SOFTFLOAT)
18773:     if (i->isop(LIR_qjoin) &&
18773:         i->oprnd1()->isop(LIR_call) &&
18773:         i->oprnd2()->isop(LIR_callh))
18773:     {
20915:         if (i->oprnd1()->callInfo() == &u2f_ci)
18773:             return true;
18773:     }
18773: #endif
18773: 
18773:     return false;
18773: }
18773: 
18773: static LInsp iu2fArg(LInsp i)
18773: {
18777: #if defined(NJ_SOFTFLOAT)
18773:     if (i->isop(LIR_qjoin))
18773:         return i->oprnd1()->arg(0);
18773: #endif
18773: 
18773:     return i->oprnd1();
18773: }
18773: 
18773: 
17451: static LIns* demote(LirWriter *out, LInsp i)
17451: {
17451:     if (i->isCall())
17451:         return callArgN(i, 0);
18773:     if (isi2f(i) || isu2f(i))
18773:         return iu2fArg(i);
17997:     if (i->isconst())
17997:         return i;
17451:     AvmAssert(i->isconstq());
17451:     double cf = i->constvalf();
17451:     int32_t ci = cf > 0x7fffffff ? uint32_t(cf) : int32_t(cf);
17451:     return out->insImm(ci);
17451: }
17451: 
17451: static bool isPromoteInt(LIns* i)
17451: {
17491:     jsdouble d;
18773:     return isi2f(i) || i->isconst() ||
19068:         (i->isconstq() && (d = i->constvalf()) == jsdouble(jsint(d)) && !JSDOUBLE_IS_NEGZERO(d));
17451: }
17451: 
17451: static bool isPromoteUint(LIns* i)
17451: {
17491:     jsdouble d;
18773:     return isu2f(i) || i->isconst() ||
19578:         (i->isconstq() && (d = i->constvalf()) == (jsdouble)(jsuint)d && !JSDOUBLE_IS_NEGZERO(d));
17451: }
17451: 
17451: static bool isPromote(LIns* i)
17451: {
17758:     return isPromoteInt(i) || isPromoteUint(i);
17451: }
17451: 
17800: static bool isconst(LIns* i, int32_t c)
17800: {
17800:     return i->isconst() && i->constval() == c;
17800: }
17800: 
17796: static bool overflowSafe(LIns* i)
17796: {
17796:     LIns* c;
17796:     return (i->isop(LIR_and) && ((c = i->oprnd2())->isconst()) &&
17796:             ((c->constval() & 0xc0000000) == 0)) ||
17796:            (i->isop(LIR_rsh) && ((c = i->oprnd2())->isconst()) &&
17796:             ((c->constval() > 0)));
17796: }
17796: 
18776: #if defined(NJ_SOFTFLOAT)
20424: /* soft float */
20424: 
20923: JS_DEFINE_CALLINFO_1(static, DOUBLE,    fneg, DOUBLE,               1, 1)
20923: JS_DEFINE_CALLINFO_2(static, INT32,     fcmpeq, DOUBLE, DOUBLE,     1, 1)
20923: JS_DEFINE_CALLINFO_2(static, INT32,     fcmplt, DOUBLE, DOUBLE,     1, 1)
20923: JS_DEFINE_CALLINFO_2(static, INT32,     fcmple, DOUBLE, DOUBLE,     1, 1)
20923: JS_DEFINE_CALLINFO_2(static, INT32,     fcmpgt, DOUBLE, DOUBLE,     1, 1)
20923: JS_DEFINE_CALLINFO_2(static, INT32,     fcmpge, DOUBLE, DOUBLE,     1, 1)
20923: JS_DEFINE_CALLINFO_2(static, DOUBLE,    fmul, DOUBLE, DOUBLE,       1, 1)
20923: JS_DEFINE_CALLINFO_2(static, DOUBLE,    fadd, DOUBLE, DOUBLE,       1, 1)
20923: JS_DEFINE_CALLINFO_2(static, DOUBLE,    fdiv, DOUBLE, DOUBLE,       1, 1)
20923: JS_DEFINE_CALLINFO_2(static, DOUBLE,    fsub, DOUBLE, DOUBLE,       1, 1)
20923: 
20424: jsdouble FASTCALL
20923: fneg(jsdouble x)
20424: {
20424:     return -x;
20424: }
20424: 
20424: jsdouble FASTCALL
20923: i2f(int32 i)
20424: {
20424:     return i;
20424: }
20424: 
20424: jsdouble FASTCALL
20923: u2f(jsuint u)
20424: {
20424:     return u;
20424: }
20424: 
20424: int32 FASTCALL
20923: fcmpeq(jsdouble x, jsdouble y)
20424: {
20424:     return x==y;
20424: }
20424: 
20424: int32 FASTCALL
20923: fcmplt(jsdouble x, jsdouble y)
20424: {
20424:     return x < y;
20424: }
20424: 
20424: int32 FASTCALL
20923: fcmple(jsdouble x, jsdouble y)
20424: {
20424:     return x <= y;
20424: }
20424: 
20424: int32 FASTCALL
20923: fcmpgt(jsdouble x, jsdouble y)
20424: {
20424:     return x > y;
20424: }
20424: 
20424: int32 FASTCALL
20923: fcmpge(jsdouble x, jsdouble y)
20424: {
20424:     return x >= y;
20424: }
20424: 
20424: jsdouble FASTCALL
20923: fmul(jsdouble x, jsdouble y)
20424: {
20424:     return x * y;
20424: }
20424: 
20424: jsdouble FASTCALL
20923: fadd(jsdouble x, jsdouble y)
20424: {
20424:     return x + y;
20424: }
20424: 
20424: jsdouble FASTCALL
20923: fdiv(jsdouble x, jsdouble y)
20424: {
20424:     return x / y;
20424: }
20424: 
20424: jsdouble FASTCALL
20923: fsub(jsdouble x, jsdouble y)
20424: {
20424:     return x - y;
20424: }
20424: 
18773: class SoftFloatFilter: public LirWriter
18773: {
18773: public:
18773:     SoftFloatFilter(LirWriter* out):
18773:         LirWriter(out)
18773:     {
18773:     }
18773: 
20408:     LInsp quadCall(const CallInfo *ci, LInsp args[]) {
18773:         LInsp qlo, qhi;
18773: 
20408:         qlo = out->insCall(ci, args);
18773:         qhi = out->ins1(LIR_callh, qlo);
18773:         return out->qjoin(qlo, qhi);
18773:     }
18773: 
18773:     LInsp ins1(LOpcode v, LInsp s0)
18773:     {
18773:         if (v == LIR_fneg)
20915:             return quadCall(&fneg_ci, &s0);
18773: 
18773:         if (v == LIR_i2f)
20915:             return quadCall(&i2f_ci, &s0);
18773: 
18773:         if (v == LIR_u2f)
20915:             return quadCall(&u2f_ci, &s0);
18773: 
18773:         return out->ins1(v, s0);
18773:     }
18773: 
18773:     LInsp ins2(LOpcode v, LInsp s0, LInsp s1)
18773:     {
18773:         LInsp args[2];
18773:         LInsp bv;
18773: 
18773:         // change the numeric value and order of these LIR opcodes and die
18773:         if (LIR_fadd <= v && v <= LIR_fdiv) {
20915:             static const CallInfo *fmap[] = { &fadd_ci, &fsub_ci, &fmul_ci, &fdiv_ci };
18773: 
18773:             args[0] = s1;
18773:             args[1] = s0;
18773: 
18773:             return quadCall(fmap[v - LIR_fadd], args);
18773:         }
18773: 
18773:         if (LIR_feq <= v && v <= LIR_fge) {
20915:             static const CallInfo *fmap[] = { &fcmpeq_ci, &fcmplt_ci, &fcmpgt_ci, &fcmple_ci, &fcmpge_ci };
18773: 
18773:             args[0] = s1;
18773:             args[1] = s0;
18773: 
18773:             bv = out->insCall(fmap[v - LIR_feq], args);
18773:             return out->ins2(LIR_eq, bv, out->insImm(1));
18773:         }
18773: 
18773:         return out->ins2(v, s0, s1);
18773:     }
18773: 
20408:     LInsp insCall(const CallInfo *ci, LInsp args[])
18773:     {
18773:         // if the return type is ARGSIZE_F, we have
18773:         // to do a quadCall ( qjoin(call,callh) )
20408:         if ((ci->_argtypes & 3) == ARGSIZE_F)
20408:             return quadCall(ci, args);
20408: 
20408:         return out->insCall(ci, args);
18773:     }
18773: };
18773: 
18776: #endif // NJ_SOFTFLOAT
18773: 
17451: class FuncFilter: public LirWriter
17451: {
17451: public:
21799:     FuncFilter(LirWriter* out):
21799:         LirWriter(out)
17451:     {
17451:     }
17451: 
17797:     LInsp ins2(LOpcode v, LInsp s0, LInsp s1)
17451:     {
17451:         if (s0 == s1 && v == LIR_feq) {
17451:             if (isPromote(s0)) {
17451:                 // double(int) and double(uint) cannot be nan
17451:                 return insImm(1);
17451:             }
17451:             if (s0->isop(LIR_fmul) || s0->isop(LIR_fsub) || s0->isop(LIR_fadd)) {
17451:                 LInsp lhs = s0->oprnd1();
17451:                 LInsp rhs = s0->oprnd2();
17451:                 if (isPromote(lhs) && isPromote(rhs)) {
17451:                     // add/sub/mul promoted ints can't be nan
17451:                     return insImm(1);
17451:                 }
17451:             }
17538:         } else if (LIR_feq <= v && v <= LIR_fge) {
17451:             if (isPromoteInt(s0) && isPromoteInt(s1)) {
17451:                 // demote fcmp to cmp
17451:                 v = LOpcode(v + (LIR_eq - LIR_feq));
17797:                 return out->ins2(v, demote(out, s0), demote(out, s1));
17451:             } else if (isPromoteUint(s0) && isPromoteUint(s1)) {
17451:                 // uint compare
17451:                 v = LOpcode(v + (LIR_eq - LIR_feq));
17451:                 if (v != LIR_eq)
17451:                     v = LOpcode(v + (LIR_ult - LIR_lt)); // cmp -> ucmp
17797:                 return out->ins2(v, demote(out, s0), demote(out, s1));
17451:             }
17800:         } else if (v == LIR_or &&
17800:                    s0->isop(LIR_lsh) && isconst(s0->oprnd2(), 16) &&
17800:                    s1->isop(LIR_and) && isconst(s1->oprnd2(), 0xffff)) {
17800:             LIns* msw = s0->oprnd1();
17800:             LIns* lsw = s1->oprnd1();
17800:             LIns* x;
17800:             LIns* y;
17800:             if (lsw->isop(LIR_add) &&
17800:                 lsw->oprnd1()->isop(LIR_and) &&
17800:                 lsw->oprnd2()->isop(LIR_and) &&
17800:                 isconst(lsw->oprnd1()->oprnd2(), 0xffff) &&
17800:                 isconst(lsw->oprnd2()->oprnd2(), 0xffff) &&
17800:                 msw->isop(LIR_add) &&
17800:                 msw->oprnd1()->isop(LIR_add) &&
17800:                 msw->oprnd2()->isop(LIR_rsh) &&
17800:                 msw->oprnd1()->oprnd1()->isop(LIR_rsh) &&
17800:                 msw->oprnd1()->oprnd2()->isop(LIR_rsh) &&
17800:                 isconst(msw->oprnd2()->oprnd2(), 16) &&
17800:                 isconst(msw->oprnd1()->oprnd1()->oprnd2(), 16) &&
17800:                 isconst(msw->oprnd1()->oprnd2()->oprnd2(), 16) &&
17800:                 (x = lsw->oprnd1()->oprnd1()) == msw->oprnd1()->oprnd1()->oprnd1() &&
17800:                 (y = lsw->oprnd2()->oprnd1()) == msw->oprnd1()->oprnd2()->oprnd1() &&
17800:                 lsw == msw->oprnd2()->oprnd1()) {
17800:                 return out->ins2(LIR_add, x, y);
17800:             }
17451:         }
18776: #ifdef NANOJIT_ARM
18776:         else if (v == LIR_lsh ||
18776:                  v == LIR_rsh ||
18776:                  v == LIR_ush)
18776:         {
18776:             // needed on ARM -- arm doesn't mask shifts to 31 like x86 does
18776:             if (s1->isconst())
18776:                 s1->setimm16(s1->constval() & 31);
18776:             else
18776:                 s1 = out->ins2(LIR_and, s1, out->insImm(31));
18776:             return out->ins2(v, s0, s1);
18776:         }
18776: #endif
18776: 
17797:         return out->ins2(v, s0, s1);
17451:     }
17451: 
20408:     LInsp insCall(const CallInfo *ci, LInsp args[])
17451:     {
17478:         LInsp s0 = args[0];
20915:         if (ci == &js_DoubleToUint32_ci) {
17783:             if (s0->isconstq())
17783:                 return out->insImm(js_DoubleToECMAUint32(s0->constvalf()));
18773:             if (isi2f(s0) || isu2f(s0))
18773:                 return iu2fArg(s0);
20915:         } else if (ci == &js_DoubleToInt32_ci) {
17587:             if (s0->isconstq())
17587:                 return out->insImm(js_DoubleToECMAInt32(s0->constvalf()));
21792:             if (s0->isop(LIR_fadd) || s0->isop(LIR_fsub)) {
17451:                 LInsp lhs = s0->oprnd1();
17451:                 LInsp rhs = s0->oprnd2();
17451:                 if (isPromote(lhs) && isPromote(rhs)) {
17451:                     LOpcode op = LOpcode(s0->opcode() & ~LIR64);
17451:                     return out->ins2(op, demote(out, lhs), demote(out, rhs));
17451:                 }
17451:             }
18773:             if (isi2f(s0) || isu2f(s0))
18773:                 return iu2fArg(s0);
20408:             // XXX ARM -- check for qjoin(call(UnboxDouble),call(UnboxDouble))
20915:             if (s0->isCall() && s0->callInfo() == &js_UnboxDouble_ci) {
17847:                 LIns* args2[] = { callArgN(s0, 0) };
20915:                 return out->insCall(&js_UnboxInt32_ci, args2);
20915:             }
20915:             if (s0->isCall() && s0->callInfo() == &js_StringToNumber_ci) {
17912:                 // callArgN's ordering is that as seen by the builtin, not as stored in args here.
17912:                 // True story!
17912:                 LIns* args2[] = { callArgN(s0, 1), callArgN(s0, 0) };
20915:                 return out->insCall(&js_StringToInt32_ci, args2);
20915:             }
20915:         } else if (ci == &js_BoxDouble_ci) {
17478:             JS_ASSERT(s0->isQuad());
17478:             if (s0->isop(LIR_i2f)) {
17588:                 LIns* args2[] = { s0->oprnd1(), args[1] };
20915:                 return out->insCall(&js_BoxInt32_ci, args2);
20915:             }
20915:             if (s0->isCall() && s0->callInfo() == &js_UnboxDouble_ci)
18289:                 return callArgN(s0, 0);
20408:         }
20408:         return out->insCall(ci, args);
17451:     }
17451: };
17451: 
17512: /* In debug mode vpname contains a textual description of the type of the
17512:    slot during the forall iteration over al slots. */
21459: #ifdef JS_JIT_SPEW
17676: #define DEF_VPNAME          const char* vpname; unsigned vpnum
17512: #define SET_VPNAME(name)    do { vpname = name; vpnum = 0; } while(0)
17512: #define INC_VPNUM()         do { ++vpnum; } while(0)
17512: #else
17531: #define DEF_VPNAME          do {} while (0)
17531: #define vpname ""
17531: #define vpnum 0
17512: #define SET_VPNAME(name)    ((void)0)
17512: #define INC_VPNUM()         ((void)0)
17512: #endif
17512: 
17936: /* Iterate over all interned global variables. */
17815: #define FORALL_GLOBAL_SLOTS(cx, ngslots, gslots, code)                        \
17512:     JS_BEGIN_MACRO                                                            \
17531:         DEF_VPNAME;                                                           \
17657:         JSObject* globalObj = JS_GetGlobalForObject(cx, cx->fp->scopeChain);  \
17512:         unsigned n;                                                           \
17512:         jsval* vp;                                                            \
17647:         SET_VPNAME("global");                                                 \
17663:         for (n = 0; n < ngslots; ++n) {                                       \
17663:             vp = &STOBJ_GET_SLOT(globalObj, gslots[n]);                       \
17512:             { code; }                                                         \
17512:             INC_VPNUM();                                                      \
17512:         }                                                                     \
17815:     JS_END_MACRO
17815: 
17936: /* Iterate over all slots in the frame, consisting of args, vars, and stack
17936:    (except for the top-level frame which does not have args or vars. */
17949: #define FORALL_FRAME_SLOTS(fp, depth, code)                                   \
17815:     JS_BEGIN_MACRO                                                            \
17815:         jsval* vp;                                                            \
17512:         jsval* vpstop;                                                        \
17949:         if (fp->callee) {                                                     \
17936:             if (depth == 0) {                                                 \
18187:                 SET_VPNAME("callee");                                         \
18187:                 vp = &fp->argv[-2];                                           \
18187:                 { code; }                                                     \
17688:                 SET_VPNAME("this");                                           \
17949:                 vp = &fp->argv[-1];                                           \
18187:                 { code; }                                                     \
17512:                 SET_VPNAME("argv");                                           \
18425:                 vp = &fp->argv[0]; vpstop = &fp->argv[fp->fun->nargs];        \
17512:                 while (vp < vpstop) { code; ++vp; INC_VPNUM(); }              \
17923:             }                                                                 \
17512:             SET_VPNAME("vars");                                               \
17949:             vp = fp->slots; vpstop = &fp->slots[fp->script->nfixed];          \
17512:             while (vp < vpstop) { code; ++vp; INC_VPNUM(); }                  \
17512:         }                                                                     \
17512:         SET_VPNAME("stack");                                                  \
17949:         vp = StackBase(fp); vpstop = fp->regs->sp;                            \
17512:         while (vp < vpstop) { code; ++vp; INC_VPNUM(); }                      \
18119:         if (fsp < fspstop - 1) {                                              \
18119:             JSStackFrame* fp2 = fsp[1];                                       \
18119:             int missing = fp2->fun->nargs - fp2->argc;                        \
18119:             if (missing > 0) {                                                \
18110:                 SET_VPNAME("missing");                                        \
18119:                 vp = fp->regs->sp;                                            \
18119:                 vpstop = vp + missing;                                        \
18110:                 while (vp < vpstop) { code; ++vp; INC_VPNUM(); }              \
18110:             }                                                                 \
18119:         }                                                                     \
17936:     JS_END_MACRO
17936: 
17936: /* Iterate over all slots in each pending frame. */
18169: #define FORALL_SLOTS_IN_PENDING_FRAMES(cx, callDepth, code)                   \
17936:     JS_BEGIN_MACRO                                                            \
17936:         DEF_VPNAME;                                                           \
17936:         unsigned n;                                                           \
18169:         JSStackFrame* currentFrame = cx->fp;                                  \
17936:         JSStackFrame* entryFrame;                                             \
17936:         JSStackFrame* fp = currentFrame;                                      \
17936:         for (n = 0; n < callDepth; ++n) { fp = fp->down; }                    \
17936:         entryFrame = fp;                                                      \
17936:         unsigned frames = callDepth+1;                                        \
17949:         JSStackFrame** fstack =                                               \
17949:             (JSStackFrame**) alloca(frames * sizeof (JSStackFrame*));         \
17936:         JSStackFrame** fspstop = &fstack[frames];                             \
17936:         JSStackFrame** fsp = fspstop-1;                                       \
17936:         fp = currentFrame;                                                    \
17936:         for (;; fp = fp->down) { *fsp-- = fp; if (fp == entryFrame) break; }  \
17936:         unsigned depth;                                                       \
17936:         for (depth = 0, fsp = fstack; fsp < fspstop; ++fsp, ++depth) {        \
18119:             fp = *fsp;                                                        \
17949:             FORALL_FRAME_SLOTS(fp, depth, code);                              \
17512:         }                                                                     \
17544:     JS_END_MACRO
17512: 
18169: #define FORALL_SLOTS(cx, ngslots, gslots, callDepth, code)                    \
17820:     JS_BEGIN_MACRO                                                            \
17820:         FORALL_GLOBAL_SLOTS(cx, ngslots, gslots, code);                       \
18169:         FORALL_SLOTS_IN_PENDING_FRAMES(cx, callDepth, code);                  \
17820:     JS_END_MACRO
17820: 
18144: /* Calculate the total number of native frame slots we need from this frame
18144:    all the way back to the entry frame, including the current stack usage. */
18425: unsigned
18425: js_NativeStackSlots(JSContext *cx, unsigned callDepth)
18193: {
18193:     JSStackFrame* fp = cx->fp;
18144:     unsigned slots = 0;
18144: #if defined _DEBUG
18144:     unsigned int origCallDepth = callDepth;
18144: #endif
18144:     for (;;) {
18144:         unsigned operands = fp->regs->sp - StackBase(fp);
18144:         slots += operands;
18144:         if (fp->callee)
18144:             slots += fp->script->nfixed;
18144:         if (callDepth-- == 0) {
18617:             if (fp->callee)
18425:                 slots += 2/*callee,this*/ + fp->fun->nargs;
18144: #if defined _DEBUG
18144:             unsigned int m = 0;
18169:             FORALL_SLOTS_IN_PENDING_FRAMES(cx, origCallDepth, m++);
18144:             JS_ASSERT(m == slots);
18144: #endif
18144:             return slots;
18144:         }
18144:         JSStackFrame* fp2 = fp;
18144:         fp = fp->down;
18144:         int missing = fp2->fun->nargs - fp2->argc;
18144:         if (missing > 0)
18144:             slots += missing;
18144:     }
18425:     JS_NOT_REACHED("js_NativeStackSlots");
18144: }
18144: 
17991: /* Capture the type map for the selected slots of the global object. */
17991: void
17992: TypeMap::captureGlobalTypes(JSContext* cx, SlotList& slots)
17991: {
17991:     unsigned ngslots = slots.length();
17991:     uint16* gslots = slots.data();
17991:     setLength(ngslots);
17991:     uint8* map = data();
17991:     uint8* m = map;
17991:     FORALL_GLOBAL_SLOTS(cx, ngslots, gslots,
17991:         uint8 type = getCoercedType(*vp);
19535:         if ((type == JSVAL_INT) && oracle.isGlobalSlotUndemotable(cx->fp->script, gslots[n]))
17991:             type = JSVAL_DOUBLE;
20399:         JS_ASSERT(type != JSVAL_BOXED);
17991:         *m++ = type;
17991:     );
17991: }
17991: 
17985: /* Capture the type map for the currently pending stack frames. */
17985: void
17985: TypeMap::captureStackTypes(JSContext* cx, unsigned callDepth)
17985: {
18425:     setLength(js_NativeStackSlots(cx, callDepth));
17985:     uint8* map = data();
17985:     uint8* m = map;
18169:     FORALL_SLOTS_IN_PENDING_FRAMES(cx, callDepth,
17985:         uint8 type = getCoercedType(*vp);
18202:         if ((type == JSVAL_INT) &&
19535:             oracle.isStackSlotUndemotable(cx->fp->script, cx->fp->regs->pc, unsigned(m - map))) {
17985:             type = JSVAL_DOUBLE;
18202:         }
21456:         debug_only_v(printf("capture %s%d: %d\n", vpname, vpnum, type);)
17985:         *m++ = type;
17985:     );
17986: }
17986: 
17986: /* Compare this type map to another one and see whether they match. */
17986: bool
18239: TypeMap::matches(TypeMap& other) const
18239: {
18239:     if (length() != other.length())
18239:         return false;
17987:     return !memcmp(data(), other.data(), length());
17985: }
17985: 
18198: /* Use the provided storage area to create a new type map that contains the partial type map
18198:    with the rest of it filled up from the complete type map. */
18198: static void
18198: mergeTypeMaps(uint8** partial, unsigned* plength, uint8* complete, unsigned clength, uint8* mem)
18198: {
18198:     unsigned l = *plength;
18198:     JS_ASSERT(l < clength);
18198:     memcpy(mem, *partial, l * sizeof(uint8));
18198:     memcpy(mem + l, complete + l, (clength - l) * sizeof(uint8));
18198:     *partial = mem;
18198:     *plength = clength;
18198: }
18198: 
18650: static void
18650: js_TrashTree(JSContext* cx, Fragment* f);
18650: 
21521: TraceRecorder::TraceRecorder(JSContext* cx, VMSideExit* _anchor, Fragment* _fragment,
18284:         TreeInfo* ti, unsigned ngslots, uint8* globalTypeMap, uint8* stackTypeMap,
21521:         VMSideExit* innermostNestedGuard, Fragment* outerToBlacklist)
18211: {
18211:     JS_ASSERT(!_fragment->vmprivate && ti);
18211: 
17351:     this->cx = cx;
18239:     this->traceMonitor = &JS_TRACE_MONITOR(cx);
17657:     this->globalObj = JS_GetGlobalForObject(cx, cx->fp->scopeChain);
17738:     this->anchor = _anchor;
17397:     this->fragment = _fragment;
17701:     this->lirbuf = _fragment->lirbuf;
18211:     this->treeInfo = ti;
21521:     this->callDepth = _anchor ? _anchor->calldepth : 0;
17611:     this->atoms = cx->fp->script->atomMap.vector;
19068:     this->deepAborted = false;
21526:     this->applyingArguments = false;
18650:     this->trashTree = false;
18667:     this->whichTreeToTrash = _fragment->root;
19653:     this->global_dslots = this->globalObj->dslots;
20416:     this->terminate = false;
21433:     this->outerToBlacklist = outerToBlacklist;
21723:     this->wasRootFragment = _fragment == _fragment->root;
18260: 
21685:     debug_only_v(printf("recording starting from %s:%u@%u\n",
21685:                         cx->fp->script->filename,
21685:                         js_FramePCToLineNumber(cx, cx->fp),
21685:                         FramePCOffset(cx->fp));)
19653:     debug_only_v(printf("globalObj=%p, shape=%d\n", this->globalObj, OBJ_SHAPE(this->globalObj));)
17414: 
17370:     lir = lir_buf_writer = new (&gc) LirBufWriter(lirbuf);
17334: #ifdef DEBUG
18260:     if (verbose_debug)
17370:         lir = verbose_filter = new (&gc) VerboseWriter(&gc, lir, lirbuf->names);
17334: #endif
18776: #ifdef NJ_SOFTFLOAT
18773:     lir = float_filter = new (&gc) SoftFloatFilter(lir);
18773: #endif
17370:     lir = cse_filter = new (&gc) CseFilter(lir, &gc);
17370:     lir = expr_filter = new (&gc) ExprFilter(lir);
21799:     lir = func_filter = new (&gc) FuncFilter(lir);
20893:     lir->ins0(LIR_start);
17663: 
20946:     if (!nanojit::AvmCore::config.tree_opt || fragment->root == fragment) 
20893:         lirbuf->state = addName(lir->insParam(0, 0), "state");
20893: 
18230:     lirbuf->sp = addName(lir->insLoad(LIR_ldp, lirbuf->state, (int)offsetof(InterpState, sp)), "sp");
18230:     lirbuf->rp = addName(lir->insLoad(LIR_ldp, lirbuf->state, offsetof(InterpState, rp)), "rp");
18230:     cx_ins = addName(lir->insLoad(LIR_ldp, lirbuf->state, offsetof(InterpState, cx)), "cx");
18230:     gp_ins = addName(lir->insLoad(LIR_ldp, lirbuf->state, offsetof(InterpState, gp)), "gp");
18230:     eos_ins = addName(lir->insLoad(LIR_ldp, lirbuf->state, offsetof(InterpState, eos)), "eos");
18230:     eor_ins = addName(lir->insLoad(LIR_ldp, lirbuf->state, offsetof(InterpState, eor)), "eor");
17334: 
17997:     /* read into registers all values on the stack and all globals we know so far */
18140:     import(treeInfo, lirbuf->sp, ngslots, callDepth, globalTypeMap, stackTypeMap);
18284: 
18284:     /* If we are attached to a tree call guard, make sure the guard the inner tree exited from
18284:        is what we expect it to be. */
20931:     if (_anchor && _anchor->exitType == NESTED_EXIT) {
18284:         LIns* nested_ins = addName(lir->insLoad(LIR_ldp, lirbuf->state, 
19590:                                                 offsetof(InterpState, lastTreeExitGuard)), 
19590:                                                 "lastTreeExitGuard");
18712:         guard(true, lir->ins2(LIR_eq, nested_ins, INS_CONSTPTR(innermostNestedGuard)), NESTED_EXIT);
18284:     }
17334: }
17334: 
21433: TreeInfo::~TreeInfo()
21433: {
21433:     UnstableExit* temp;
21433: 
21433:     while (unstableExits) {
21433:         temp = unstableExits->next;
21433:         delete unstableExits;
21433:         unstableExits = temp;
21433:     }
21433: }
21433: 
17334: TraceRecorder::~TraceRecorder()
17334: {
21723:     JS_ASSERT(nextRecorderToAbort == NULL);
21723:     JS_ASSERT(treeInfo && (fragment || wasDeepAborted()));
21767: #ifdef DEBUG
21767:     TraceRecorder* tr = JS_TRACE_MONITOR(cx).abortStack;
21767:     while (tr != NULL)
21767:     {
21767:         JS_ASSERT(this != tr);
21767:         tr = tr->nextRecorderToAbort;
21767:     }
21767: #endif
21723:     if (fragment) {
21723:         if (wasRootFragment && !fragment->root->code()) {
18211:             JS_ASSERT(!fragment->root->vmprivate);
18211:             delete treeInfo;
18211:         }
18650:         if (trashTree)
18667:             js_TrashTree(cx, whichTreeToTrash);
21723:     } else if (wasRootFragment) {
21723:         delete treeInfo;
21723:     }
17334: #ifdef DEBUG
17370:     delete verbose_filter;
17334: #endif
17370:     delete cse_filter;
17370:     delete expr_filter;
17453:     delete func_filter;
18776: #ifdef NJ_SOFTFLOAT
18773:     delete float_filter;
18773: #endif
17370:     delete lir_buf_writer;
17319: }
17319: 
21723: void TraceRecorder::removeFragmentoReferences()
21723: {
21723:     fragment = NULL;
21723: }
21723: 
17722: /* Add debug information to a LIR instruction as we emit it. */
17722: inline LIns*
17722: TraceRecorder::addName(LIns* ins, const char* name)
17722: {
17722: #ifdef DEBUG
17722:     lirbuf->names->addName(ins, name);
17722: #endif
17722:     return ins;
17722: }
17722: 
17346: /* Determine the current call depth (starting with the entry frame.) */
17346: unsigned
17442: TraceRecorder::getCallDepth() const
17346: {
17789:     return callDepth;
17346: }
17346: 
17815: /* Determine the offset in the native global frame for a jsval we track */
17811: ptrdiff_t
17815: TraceRecorder::nativeGlobalOffset(jsval* p) const
17815: {
17900:     JS_ASSERT(isGlobal(p));
17900:     if (size_t(p - globalObj->fslots) < JS_INITIAL_NSLOTS)
17900:         return size_t(p - globalObj->fslots) * sizeof(double);
17900:     return ((p - globalObj->dslots) + JS_INITIAL_NSLOTS) * sizeof(double);
17815: }
17815: 
17894: /* Determine whether a value is a global stack slot */
17893: bool
17893: TraceRecorder::isGlobal(jsval* p) const
17893: {
17893:     return ((size_t(p - globalObj->fslots) < JS_INITIAL_NSLOTS) ||
17893:             (size_t(p - globalObj->dslots) < (STOBJ_NSLOTS(globalObj) - JS_INITIAL_NSLOTS)));
17893: }
17893: 
17815: /* Determine the offset in the native stack for a jsval we track */
17815: ptrdiff_t
17815: TraceRecorder::nativeStackOffset(jsval* p) const
17346: {
17814: #ifdef DEBUG
17814:     size_t slow_offset = 0;
18169:     FORALL_SLOTS_IN_PENDING_FRAMES(cx, callDepth,
17814:         if (vp == p) goto done;
17814:         slow_offset += sizeof(double)
17512:     );
17923: 
17923:     /*
17923:      * If it's not in a pending frame, it must be on the stack of the current frame above
17923:      * sp but below fp->slots + script->nslots.
17923:      */
17923:     JS_ASSERT(size_t(p - cx->fp->slots) < cx->fp->script->nslots);
17814:     slow_offset += size_t(p - cx->fp->regs->sp) * sizeof(double);
17923: 
17814: done:
17814: #define RETURN(offset) { JS_ASSERT((offset) == slow_offset); return offset; }
17814: #else
17814: #define RETURN(offset) { return offset; }
17814: #endif
17814:     size_t offset = 0;
17814:     JSStackFrame* currentFrame = cx->fp;
17814:     JSStackFrame* entryFrame;
17814:     JSStackFrame* fp = currentFrame;
17814:     for (unsigned n = 0; n < callDepth; ++n) { fp = fp->down; }
17814:     entryFrame = fp;
17814:     unsigned frames = callDepth+1;
17814:     JSStackFrame** fstack = (JSStackFrame **)alloca(frames * sizeof (JSStackFrame *));
17814:     JSStackFrame** fspstop = &fstack[frames];
17814:     JSStackFrame** fsp = fspstop-1;
17814:     fp = currentFrame;
17814:     for (;; fp = fp->down) { *fsp-- = fp; if (fp == entryFrame) break; }
17814:     for (fsp = fstack; fsp < fspstop; ++fsp) {
17949:         fp = *fsp;
17949:         if (fp->callee) {
17923:             if (fsp == fstack) {
18425:                 if (size_t(p - &fp->argv[-2]) < size_t(2/*callee,this*/ + fp->fun->nargs))
18187:                     RETURN(offset + size_t(p - &fp->argv[-2]) * sizeof(double));
18425:                 offset += (2/*callee,this*/ + fp->fun->nargs) * sizeof(double);
17923:             }
17949:             if (size_t(p - &fp->slots[0]) < fp->script->nfixed)
17949:                 RETURN(offset + size_t(p - &fp->slots[0]) * sizeof(double));
17949:             offset += fp->script->nfixed * sizeof(double);
17949:         }
17949:         jsval* spbase = StackBase(fp);
17949:         if (size_t(p - spbase) < size_t(fp->regs->sp - spbase))
17923:             RETURN(offset + size_t(p - spbase) * sizeof(double));
17949:         offset += size_t(fp->regs->sp - spbase) * sizeof(double);
18119:         if (fsp < fspstop - 1) {
18119:             JSStackFrame* fp2 = fsp[1];
18119:             int missing = fp2->fun->nargs - fp2->argc;
18119:             if (missing > 0) {
18119:                 if (size_t(p - fp->regs->sp) < size_t(missing))
18119:                     RETURN(offset + size_t(p - fp->regs->sp) * sizeof(double));
18119:                 offset += size_t(missing) * sizeof(double);
18119:             }
18110:         }
17923:     }
17923: 
17923:     /*
17923:      * If it's not in a pending frame, it must be on the stack of the current frame above
17923:      * sp but below fp->slots + script->nslots.
17923:      */
17923:     JS_ASSERT(size_t(p - currentFrame->slots) < currentFrame->script->nslots);
17538:     offset += size_t(p - currentFrame->regs->sp) * sizeof(double);
17814:     RETURN(offset);
17814: #undef RETURN
17346: }
17346: 
17397: /* Track the maximum number of native frame slots we need during
17397:    execution. */
17397: void
17815: TraceRecorder::trackNativeStackUse(unsigned slots)
17815: {
17815:     if (slots > treeInfo->maxNativeStackSlots)
17815:         treeInfo->maxNativeStackSlots = slots;
17397: }
17397: 
21433: /* Unbox a jsval into a slot. Slots are wide enough to hold double values directly (instead of 
21433:    storing a pointer to them). We now assert instead of type checking, the caller must ensure the 
21433:    types are compatible. */
21433: static void
18296: ValueToNative(JSContext* cx, jsval v, uint8 type, double* slot)
18296: {
18296:     unsigned tag = JSVAL_TAG(v);
18296:     switch (type) {
18296:       case JSVAL_INT:
17482:         jsint i;
17482:         if (JSVAL_IS_INT(v))
17482:             *(jsint*)slot = JSVAL_TO_INT(v);
18296:         else if ((tag == JSVAL_DOUBLE) && JSDOUBLE_IS_INT(*JSVAL_TO_DOUBLE(v), i))
17469:             *(jsint*)slot = i;
21433:         else
21433:             JS_ASSERT(JSVAL_IS_INT(v));
18260:         debug_only_v(printf("int<%d> ", *(jsint*)slot);)
21433:         return;
18296:       case JSVAL_DOUBLE:
17482:         jsdouble d;
17482:         if (JSVAL_IS_INT(v))
17482:             d = JSVAL_TO_INT(v);
21433:         else
17482:             d = *JSVAL_TO_DOUBLE(v);
21433:         JS_ASSERT(JSVAL_IS_INT(v) || JSVAL_IS_DOUBLE(v));
17482:         *(jsdouble*)slot = d;
18260:         debug_only_v(printf("double<%g> ", d);)
21433:         return;
18296:       case JSVAL_BOOLEAN:
21433:         JS_ASSERT(tag == JSVAL_BOOLEAN);
17884:         *(JSBool*)slot = JSVAL_TO_BOOLEAN(v);
20002:         debug_only_v(printf("boolean<%d> ", *(JSBool*)slot);)
21433:         return;
17387:       case JSVAL_STRING:
21433:         JS_ASSERT(tag == JSVAL_STRING);
17387:         *(JSString**)slot = JSVAL_TO_STRING(v);
18260:         debug_only_v(printf("string<%p> ", *(JSString**)slot);)
21433:         return;
17387:       default:
20399:         /* Note: we should never see JSVAL_BOXED in an entry type map. */
18296:         JS_ASSERT(type == JSVAL_OBJECT);
21433:         JS_ASSERT(tag == JSVAL_OBJECT);
17546:         *(JSObject**)slot = JSVAL_TO_OBJECT(v);
18260:         debug_only_v(printf("object<%p:%s> ", JSVAL_TO_OBJECT(v),
17611:                             JSVAL_IS_NULL(v)
17611:                             ? "null"
17611:                             : STOBJ_GET_CLASS(JSVAL_TO_OBJECT(v))->name);)
21433:         return;
17360:     }
18296: }
17360: 
21514: /* We maintain an emergency recovery pool of doubles so we can recover safely if a trace runs
19987:    out of memory (doubles or objects). */
19991: static jsval
21514: AllocateDoubleFromRecoveryPool(JSContext* cx)
19987: {
19987:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
19987:     JS_ASSERT(tm->recoveryDoublePoolPtr > tm->recoveryDoublePool);
19987:     return *--tm->recoveryDoublePoolPtr;
19987: }
19987: 
19987: static bool
21514: js_ReplenishRecoveryPool(JSContext* cx, JSTraceMonitor* tm)
19991: {
19991:     /* We should not be called with a full pool. */
19991:     JS_ASSERT((size_t) (tm->recoveryDoublePoolPtr - tm->recoveryDoublePool) <
19991:               MAX_NATIVE_STACK_SLOTS);
19991: 
19991:     /*
19991:      * When the GC runs in js_NewDoubleInRootedValue, it resets
21514:      * tm->recoveryDoublePoolPtr back to tm->recoveryDoublePool. 
19991:      */
19991:     JSRuntime* rt = cx->runtime;
19991:     uintN gcNumber = rt->gcNumber;
21514:     jsval* ptr = tm->recoveryDoublePoolPtr; 
21514:     while (ptr < tm->recoveryDoublePool + MAX_NATIVE_STACK_SLOTS) {
21514:         if (!js_NewDoubleInRootedValue(cx, 0.0, ptr)) 
21514:             goto oom;
21514:         if (rt->gcNumber != gcNumber) {
19991:             JS_ASSERT(tm->recoveryDoublePoolPtr == tm->recoveryDoublePool);
21514:             ptr = tm->recoveryDoublePool;
21514:             if (uintN(rt->gcNumber - gcNumber) > uintN(1))
21514:                 goto oom;
21514:             continue;
21514:         }
21514:         ++ptr;
21514:     }
21514:     tm->recoveryDoublePoolPtr = ptr;
21514:     return true;
21514: 
21514: oom:
21514:     /*
21514:      * Already massive GC pressure, no need to hold doubles back.
21514:      * We won't run any native code anyway.
21514:      */
21514:     tm->recoveryDoublePoolPtr = tm->recoveryDoublePool;
21514:     return false;
19987: }
19987: 
17387: /* Box a value from the native stack back into the jsval format. Integers
17387:    that are too large to fit into a jsval are automatically boxed into
17387:    heap-allocated doubles. */
17397: static bool
17857: NativeToValue(JSContext* cx, jsval& v, uint8 type, double* slot)
17841: {
17483:     jsint i;
17469:     jsdouble d;
17494:     switch (type) {
17361:       case JSVAL_BOOLEAN:
20002:         v = BOOLEAN_TO_JSVAL(*(JSBool*)slot);
20002:         debug_only_v(printf("boolean<%d> ", *(JSBool*)slot);)
17393:         break;
17361:       case JSVAL_INT:
17483:         i = *(jsint*)slot;
18260:         debug_only_v(printf("int<%d> ", i);)
17483:       store_int:
17469:         if (INT_FITS_IN_JSVAL(i)) {
17492:             v = INT_TO_JSVAL(i);
17393:             break;
17469:         }
17469:         d = (jsdouble)i;
17483:         goto store_double;
17361:       case JSVAL_DOUBLE:
17469:         d = *slot;
18260:         debug_only_v(printf("double<%g> ", d);)
17483:         if (JSDOUBLE_IS_INT(d, i))
17483:             goto store_int;
19987:       store_double: {
19987:         /* Its not safe to trigger the GC here, so use an emergency heap if we are out of
19987:            double boxes. */
19987:         if (cx->doubleFreeList) {
19987: #ifdef DEBUG
19987:             bool ok =
19987: #endif
19987:                 js_NewDoubleInRootedValue(cx, d, &v);
19987:             JS_ASSERT(ok);
19987:             return true;
19987:         }
21514:         v = AllocateDoubleFromRecoveryPool(cx);
19991:         JS_ASSERT(JSVAL_IS_DOUBLE(v) && *JSVAL_TO_DOUBLE(v) == 0.0);
19991:         *JSVAL_TO_DOUBLE(v) = d;
19987:         return true;
19987:       }
17361:       case JSVAL_STRING:
17492:         v = STRING_TO_JSVAL(*(JSString**)slot);
20427:         JS_ASSERT(JSVAL_TAG(v) == JSVAL_STRING); /* if this fails the pointer was not aligned */
18260:         debug_only_v(printf("string<%p> ", *(JSString**)slot);)
17393:         break;
20399:       case JSVAL_BOXED:
20399:         v = *(jsval*)slot;
20399:         debug_only_v(printf("box<%lx> ", v));
20399:         break;
17361:       default:
17841:         JS_ASSERT(type == JSVAL_OBJECT);
17492:         v = OBJECT_TO_JSVAL(*(JSObject**)slot);
20427:         JS_ASSERT(JSVAL_TAG(v) == JSVAL_OBJECT); /* if this fails the pointer was not aligned */
18260:         debug_only_v(printf("object<%p:%s> ", JSVAL_TO_OBJECT(v),
17812:                             JSVAL_IS_NULL(v)
17812:                             ? "null"
17812:                             : STOBJ_GET_CLASS(JSVAL_TO_OBJECT(v))->name);)
17393:         break;
17393:     }
17361:     return true;
17361: }
17361: 
21433: /* Attempt to unbox the given list of interned globals onto the native global frame. */
21433: static void
17857: BuildNativeGlobalFrame(JSContext* cx, unsigned ngslots, uint16* gslots, uint8* mp, double* np)
17857: {
18260:     debug_only_v(printf("global: ");)
17815:     FORALL_GLOBAL_SLOTS(cx, ngslots, gslots,
21433:         ValueToNative(cx, *vp, *mp, np + gslots[n]);
17900:         ++mp;
17512:     );
18260:     debug_only_v(printf("\n");)
21433: }
21433: 
21433: /* Attempt to unbox the given JS frame onto a native frame. */
21433: static void
17857: BuildNativeStackFrame(JSContext* cx, unsigned callDepth, uint8* mp, double* np)
17857: {
18260:     debug_only_v(printf("stack: ");)
18169:     FORALL_SLOTS_IN_PENDING_FRAMES(cx, callDepth,
18260:         debug_only_v(printf("%s%u=", vpname, vpnum);)
21433:         ValueToNative(cx, *vp, *mp, np);
17857:         ++mp; ++np;
17857:     );
18260:     debug_only_v(printf("\n");)
17857: }
17857: 
17469: /* Box the given native frame into a JS frame. This only fails due to a hard error
17469:    (out of memory for example). */
18170: static int
17857: FlushNativeGlobalFrame(JSContext* cx, unsigned ngslots, uint16* gslots, uint8* mp, double* np)
17857: {
17857:     uint8* mp_base = mp;
17815:     FORALL_GLOBAL_SLOTS(cx, ngslots, gslots,
17900:         if (!NativeToValue(cx, *vp, *mp, np + gslots[n]))
18170:             return -1;
17900:         ++mp;
17815:     );
18260:     debug_only_v(printf("\n");)
18170:     return mp - mp_base;
17857: }
17857: 
19076: /**
19076:  * Box the given native stack frame into the virtual machine stack. This fails
19076:  * only due to a hard error (out of memory for example).
19076:  *
19076:  * @param callDepth the distance between the entry frame into our trace and
19076:  *                  cx->fp when we make this call.  If this is not called as a
19076:  *                  result of a nested exit, callDepth is 0.
19076:  * @param mp pointer to an array of type tags (JSVAL_INT, etc.) that indicate
19076:  *           what the types of the things on the stack are.
19076:  * @param np pointer to the native stack.  We want to copy values from here to
19076:  *           the JS stack as needed.
19076:  * @param stopFrame if non-null, this frame and everything above it should not
19076:  *                  be restored.
19076:  * @return the number of things we popped off of np.
19076:  */
18170: static int
19076: FlushNativeStackFrame(JSContext* cx, unsigned callDepth, uint8* mp, double* np,
19076:                       JSStackFrame* stopFrame)
19076: {
19076:     jsval* stopAt = stopFrame ? &stopFrame->argv[-2] : NULL;
17857:     uint8* mp_base = mp;
17857:     /* Root all string and object references first (we don't need to call the GC for this). */
18169:     FORALL_SLOTS_IN_PENDING_FRAMES(cx, callDepth,
19987:         if (vp == stopAt) goto skip;
19987:         debug_only_v(printf("%s%u=", vpname, vpnum);)
19987:         if (!NativeToValue(cx, *vp, *mp, np))
18170:             return -1;
17857:         ++mp; ++np
17857:     );
19987: skip:
17925:     // Restore thisp from the now-restored argv[-1] in each pending frame.
19076:     // Keep in mind that we didn't restore frames at stopFrame and above!
19076:     // Scope to keep |fp| from leaking into the macros we're using.
19076:     {
19076:         unsigned n = callDepth+1; // +1 to make sure we restore the entry frame
19076:         JSStackFrame* fp = cx->fp;
19076:         if (stopFrame) {
19076:             for (; fp != stopFrame; fp = fp->down) {
19076:                 JS_ASSERT(n != 0);
19076:                 --n;
19076:             }
19076:             // Skip over stopFrame itself.
19076:             JS_ASSERT(n != 0);
19076:             --n;
19076:             fp = fp->down;
19076:         }
19076:         for (; n != 0; fp = fp->down) {
19076:             --n;
19076:             if (fp->callee) { // might not have it if the entry frame is global
19076:                 JS_ASSERT(JSVAL_IS_OBJECT(fp->argv[-1]));
17949:                 fp->thisp = JSVAL_TO_OBJECT(fp->argv[-1]);
19076:             }
19076:         }
19076:     }
18260:     debug_only_v(printf("\n");)
18170:     return mp - mp_base;
17361: }
17361: 
17363: /* Emit load instructions onto the trace that read the initial stack state. */
17321: void
17815: TraceRecorder::import(LIns* base, ptrdiff_t offset, jsval* p, uint8& t,
18045:                       const char *prefix, uintN index, JSStackFrame *fp)
17319: {
17480:     LIns* ins;
17841:     if (t == JSVAL_INT) { /* demoted */
17482:         JS_ASSERT(isInt32(*p));
17480:         /* Ok, we have a valid demotion attempt pending, so insert an integer
17480:            read and promote it to double since all arithmetic operations expect
17480:            to see doubles on entry. The first op to use this slot will emit a
17480:            f2i cast which will cancel out the i2f we insert here. */
17803:         ins = lir->insLoadi(base, offset);
17803:         ins = lir->ins1(LIR_i2f, ins);
17480:     } else {
20399:         JS_ASSERT(t == JSVAL_BOXED || isNumber(*p) == (t == JSVAL_DOUBLE));
18232:         if (t == JSVAL_DOUBLE) {
18232:             ins = lir->insLoad(LIR_ldq, base, offset);
20393:         } else if (t == JSVAL_BOOLEAN) {
20393:             ins = lir->insLoad(LIR_ld, base, offset);
18232:         } else {
18232:             ins = lir->insLoad(LIR_ldp, base, offset);
18232:         }
17480:     }
17372:     tracker.set(p, ins);
17372: #ifdef DEBUG
17737:     char name[64];
17372:     JS_ASSERT(strlen(prefix) < 10);
17925:     void* mark = NULL;
17925:     jsuword* localNames = NULL;
18011:     const char* funName = NULL;
17925:     if (*prefix == 'a' || *prefix == 'v') {
17925:         mark = JS_ARENA_MARK(&cx->tempPool);
18597:         if (JS_GET_LOCAL_NAME_COUNT(fp->fun) != 0)
17925:             localNames = js_GetLocalNameArray(cx, fp->fun, &cx->tempPool);
18011:         funName = fp->fun->atom ? js_AtomToPrintableString(cx, fp->fun->atom) : "<anonymous>";
18011:     }
17737:     if (!strcmp(prefix, "argv")) {
18045:         if (index < fp->fun->nargs) {
17737:             JSAtom *atom = JS_LOCAL_NAME_TO_ATOM(localNames[index]);
18011:             JS_snprintf(name, sizeof name, "$%s.%s", funName, js_AtomToPrintableString(cx, atom));
18011:         } else {
18011:             JS_snprintf(name, sizeof name, "$%s.<arg%d>", funName, index);
18011:         }
17737:     } else if (!strcmp(prefix, "vars")) {
17925:         JSAtom *atom = JS_LOCAL_NAME_TO_ATOM(localNames[fp->fun->nargs + index]);
18011:         JS_snprintf(name, sizeof name, "$%s.%s", funName, js_AtomToPrintableString(cx, atom));
17737:     } else {
17379:         JS_snprintf(name, sizeof name, "$%s%d", prefix, index);
17737:     }
17925: 
17925:     if (mark)
17925:         JS_ARENA_RELEASE(&cx->tempPool, mark);
17721:     addName(ins, name);
17737: 
17587:     static const char* typestr[] = {
17587:         "object", "int", "double", "3", "string", "5", "boolean", "any"
17587:     };
21685:     debug_only_v(printf("import vp=%p name=%s type=%s flags=%d\n",
21685:                         p, name, typestr[t & 7], t >> 3);)
17372: #endif
17317: }
17317: 
17997: void
18140: TraceRecorder::import(TreeInfo* treeInfo, LIns* sp, unsigned ngslots, unsigned callDepth,
18121:                       uint8* globalTypeMap, uint8* stackTypeMap)
17997: {
18214:     /* If we get a partial list that doesn't have all the types (i.e. recording from a side
18214:        exit that was recorded but we added more global slots later), merge the missing types
18214:        from the entry type map. This is safe because at the loop edge we verify that we
18214:        have compatible types for all globals (entry type and loop edge type match). While
18214:        a different trace of the tree might have had a guard with a different type map for
18214:        these slots we just filled in here (the guard we continue from didn't know about them),
18214:        since we didn't take that particular guard the only way we could have ended up here
18214:        is if that other trace had at its end a compatible type distribution with the entry
18214:        map. Since thats exactly what we used to fill in the types our current side exit
18214:        didn't provide, this is always safe to do. */
18214:     unsigned length;
18239:     if (ngslots < (length = traceMonitor->globalTypeMap->length()))
18214:         mergeTypeMaps(&globalTypeMap, &ngslots,
18239:                       traceMonitor->globalTypeMap->data(), length,
18214:                       (uint8*)alloca(sizeof(uint8) * length));
18239:     JS_ASSERT(ngslots == traceMonitor->globalTypeMap->length());
18214: 
17997:     /* the first time we compile a tree this will be empty as we add entries lazily */
18239:     uint16* gslots = traceMonitor->globalSlots->data();
17997:     uint8* m = globalTypeMap;
17997:     FORALL_GLOBAL_SLOTS(cx, ngslots, gslots,
17997:         import(gp_ins, nativeGlobalOffset(vp), vp, *m, vpname, vpnum, NULL);
17997:         m++;
17997:     );
17997:     ptrdiff_t offset = -treeInfo->nativeStackBase;
17997:     m = stackTypeMap;
18169:     FORALL_SLOTS_IN_PENDING_FRAMES(cx, callDepth,
18138:         import(sp, offset, vp, *m, vpname, vpnum, fp);
17997:         m++; offset += sizeof(double);
17997:     );
17997: }
17997: 
17894: /* Lazily import a global slot if we don't already have it in the tracker. */
17891: bool
17892: TraceRecorder::lazilyImportGlobalSlot(unsigned slot)
17891: {
18712:     if (slot != uint16(slot)) /* we use a table of 16-bit ints, bail out if that's not enough */
17891:         return false;
17891:     jsval* vp = &STOBJ_GET_SLOT(globalObj, slot);
17891:     if (tracker.has(vp))
17891:         return true; /* we already have it */
18239:     unsigned index = traceMonitor->globalSlots->length();
18210:     /* If this the first global we are adding, remember the shape of the global object. */
18210:     if (index == 0)
19020:         traceMonitor->globalShape = OBJ_SHAPE(JS_GetGlobalForObject(cx, cx->fp->scopeChain));
18210:     /* Add the slot to the list of interned global slots. */
18239:     traceMonitor->globalSlots->add(slot);
17981:     uint8 type = getCoercedType(*vp);
19535:     if ((type == JSVAL_INT) && oracle.isGlobalSlotUndemotable(cx->fp->script, slot))
17981:         type = JSVAL_DOUBLE;
18239:     traceMonitor->globalTypeMap->add(type);
18088:     import(gp_ins, slot*sizeof(double), vp, type, "global", index, NULL);
17891:     return true;
17891: }
17891: 
18197: /* Write back a value onto the stack or global frames. */
18197: LIns*
18197: TraceRecorder::writeBack(LIns* i, LIns* base, ptrdiff_t offset)
18197: {
17792:     /* Sink all type casts targeting the stack into the side exit by simply storing the original
17792:        (uncasted) value. Each guard generates the side exit map based on the types of the
17792:        last stores to every stack location, so its safe to not perform them on-trace. */
17792:     if (isPromoteInt(i))
17792:         i = ::demote(lir, i);
18197:     return lir->insStorei(i, base, offset);
18197: }
18197: 
18197: /* Update the tracker, then issue a write back store. */
18197: void
18197: TraceRecorder::set(jsval* p, LIns* i, bool initializing)
18197: {
18197:     JS_ASSERT(initializing || tracker.has(p));
18197:     tracker.set(p, i);
17803:     /* If we are writing to this location for the first time, calculate the offset into the
17803:        native frame manually, otherwise just look up the last load or store associated with
17803:        the same source address (p) and use the same offset/base. */
19068:     LIns* x = nativeFrameTracker.get(p);
19068:     if (!x) {
18197:         if (isGlobal(p))
18197:             x = writeBack(i, gp_ins, nativeGlobalOffset(p));
18197:         else
18197:             x = writeBack(i, lirbuf->sp, -treeInfo->nativeStackBase + nativeStackOffset(p));
17962:         nativeFrameTracker.set(p, x);
17803:     } else {
17815: #define ASSERT_VALID_CACHE_HIT(base, offset)                                  \
17815:     JS_ASSERT(base == lirbuf->sp || base == gp_ins);                          \
17815:     JS_ASSERT(offset == ((base == lirbuf->sp)                                 \
17962:         ? -treeInfo->nativeStackBase + nativeStackOffset(p)                   \
17815:         : nativeGlobalOffset(p)));                                            \
17815: 
18123:         if (x->isop(LIR_st) || x->isop(LIR_stq)) {
17815:             ASSERT_VALID_CACHE_HIT(x->oprnd2(), x->oprnd3()->constval());
18197:             writeBack(i, x->oprnd2(), x->oprnd3()->constval());
17803:         } else {
17811:             JS_ASSERT(x->isop(LIR_sti) || x->isop(LIR_stqi));
17815:             ASSERT_VALID_CACHE_HIT(x->oprnd2(), x->immdisp());
18197:             writeBack(i, x->oprnd2(), x->immdisp());
17803:         }
17803:     }
17815: #undef ASSERT_VALID_CACHE_HIT
17320: }
17320: 
17321: LIns*
19084: TraceRecorder::get(jsval* p) const
17320: {
17320:     return tracker.get(p);
17320: }
17320: 
20416: /* Determine whether the current branch instruction terminates the loop. */
17824: static bool
20416: js_IsLoopExit(jsbytecode* pc, jsbytecode* header)
17824: {
17824:     switch (*pc) {
17824:       case JSOP_LT:
17824:       case JSOP_GT:
17824:       case JSOP_LE:
17824:       case JSOP_GE:
17824:       case JSOP_NE:
17824:       case JSOP_EQ:
18772:         /* These ops try to dispatch a JSOP_IFEQ or JSOP_IFNE that follows. */
17824:         JS_ASSERT(js_CodeSpec[*pc].length == 1);
17824:         pc++;
18772:         break;
18772: 
18772:       default:
18772:         for (;;) {
18772:             if (*pc == JSOP_AND || *pc == JSOP_OR)
18772:                 pc += GET_JUMP_OFFSET(pc);
18772:             else if (*pc == JSOP_ANDX || *pc == JSOP_ORX)
18772:                 pc += GET_JUMPX_OFFSET(pc);
18772:             else
18772:                 break;
18772:         }
18772:     }
18772: 
18772:     switch (*pc) {
18279:       case JSOP_IFEQ:
17824:       case JSOP_IFNE:
18279:         /*
18772:          * Forward jumps are usually intra-branch, but for-in loops jump to the
18772:          * trailing enditer to clean up, so check for that case here.
18279:          */
18279:         if (pc[GET_JUMP_OFFSET(pc)] == JSOP_ENDITER)
18279:             return true;
18630:         return pc + GET_JUMP_OFFSET(pc) == header;
17824: 
18772:       case JSOP_IFEQX:
18772:       case JSOP_IFNEX:
18772:         if (pc[GET_JUMPX_OFFSET(pc)] == JSOP_ENDITER)
18772:             return true;
18772:         return pc + GET_JUMPX_OFFSET(pc) == header;
18772: 
17824:       default:;
17824:     }
17824:     return false;
17824: }
17824: 
20416: /* Determine whether the current branch is a loop edge (taken or not taken). */
20416: static bool
20416: js_IsLoopEdge(jsbytecode* pc, jsbytecode* header)
20416: {
20416:     switch (*pc) {
20416:       case JSOP_IFEQ:
20416:       case JSOP_IFNE:
20416:         return ((pc + GET_JUMP_OFFSET(pc)) == header);
20416:       case JSOP_IFEQX:
20416:       case JSOP_IFNEX:
20416:         return ((pc + GET_JUMPX_OFFSET(pc)) == header);
20416:       default:
20416:         JS_ASSERT((*pc == JSOP_AND) || (*pc == JSOP_ANDX) || 
20416:                   (*pc == JSOP_OR) || (*pc == JSOP_ORX));
20416:     }
20416:     return false;
20416: }
20416: 
18250: /* Promote slots if necessary to match the called tree' type map and report error if thats
18250:    impossible. */
18250: bool
21433: TraceRecorder::adjustCallerTypes(Fragment* f, unsigned* demote_slots, bool& trash)
18250: {
18250:     JSTraceMonitor* tm = traceMonitor;
18250:     uint8* m = tm->globalTypeMap->data();
18250:     uint16* gslots = traceMonitor->globalSlots->data();
18250:     unsigned ngslots = traceMonitor->globalSlots->length();
18595:     uint8* map = ((TreeInfo*)f->vmprivate)->stackTypeMap.data();
18595:     bool ok = true;
21433:     trash = false;
18250:     FORALL_GLOBAL_SLOTS(cx, ngslots, gslots, 
18250:         LIns* i = get(vp);
18250:         bool isPromote = isPromoteInt(i);
18250:         if (isPromote && *m == JSVAL_DOUBLE) 
18250:             lir->insStorei(get(vp), gp_ins, nativeGlobalOffset(vp));
18595:         else if (!isPromote && *m == JSVAL_INT) {
21433:             oracle.markGlobalSlotUndemotable(cx->fp->script, nativeGlobalOffset(vp)/sizeof(double));
21433:             trash = true;
18595:             ok = false;
18595:         }
18250:         ++m;
18250:     );
18595:     m = map;
18265:     FORALL_SLOTS_IN_PENDING_FRAMES(cx, 0,
18250:         LIns* i = get(vp);
18250:         bool isPromote = isPromoteInt(i);
21433:         if (isPromote && *m == JSVAL_DOUBLE) {
18250:             lir->insStorei(get(vp), lirbuf->sp, 
18250:                            -treeInfo->nativeStackBase + nativeStackOffset(vp));
21433:             /* Aggressively undo speculation so the inner tree will compile if this fails. */
21472:             ADD_UNDEMOTE_SLOT(demote_slots, unsigned(m - map));
21433:         } else if (!isPromote && *m == JSVAL_INT) {
21433:             debug_only_v(printf("adjusting will fail, %s%d, slot %d\n", vpname, vpnum, m - map);)
18595:             ok = false;
21472:             ADD_UNDEMOTE_SLOT(demote_slots, unsigned(m - map));
21433:         } else if (JSVAL_IS_INT(*vp) && *m == JSVAL_DOUBLE) {
21433:             /* Aggressively undo speculation so the inner tree will compile if this fails. */
21472:             ADD_UNDEMOTE_SLOT(demote_slots, unsigned(m - map));
18595:         }
18250:         ++m;
18250:     );
21433:     /* If this isn't okay, tell the oracle. */
21433:     if (!ok) {
21472:         for (unsigned i = 1; i <= NUM_UNDEMOTE_SLOTS(demote_slots); i++)
21433:             oracle.markStackSlotUndemotable(cx->fp->script, cx->fp->regs->pc, demote_slots[i]);
21433:     }
18650:     JS_ASSERT(f == f->root);
18595:     return ok;
18250: }
18250: 
19084: uint8 
19084: TraceRecorder::determineSlotType(jsval* vp) const
19084: {
19084:     uint8 m;
19084:     LIns* i = get(vp);
19084:     m = isNumber(*vp)
19084:         ? (isPromoteInt(i) ? JSVAL_INT : JSVAL_DOUBLE)
19084:         : JSVAL_TAG(*vp);
19084:     JS_ASSERT((m != JSVAL_INT) || isInt32(*vp));
19084:     return m;
19084: }
19084: 
21685: #define IMACRO_PC_ADJ_BITS   8
21685: #define SCRIPT_PC_ADJ_BITS   (32 - IMACRO_PC_ADJ_BITS)
21685: 
21685: // The stored imacro_pc_adj byte offset is biased by 1.
21685: #define IMACRO_PC_ADJ_LIMIT  (JS_BIT(IMACRO_PC_ADJ_BITS) - 1)
21685: #define SCRIPT_PC_ADJ_LIMIT  JS_BIT(SCRIPT_PC_ADJ_BITS)
21685: 
21685: #define IMACRO_PC_ADJ(ip)    ((uintptr_t)(ip) >> SCRIPT_PC_ADJ_BITS)
21685: #define SCRIPT_PC_ADJ(ip)    ((ip) & JS_BITMASK(SCRIPT_PC_ADJ_BITS))
21685: 
21685: #define FI_SCRIPT_PC(fi,fp)  ((fp)->script->code + SCRIPT_PC_ADJ((fi).ip_adj))
21685: 
21685: #define FI_IMACRO_PC(fi,fp)  (IMACRO_PC_ADJ((fi).ip_adj)                      \
21685:                               ? imacro_code[*FI_SCRIPT_PC(fi, fp)] +          \
21685:                                 IMACRO_PC_ADJ((fi).ip_adj)                    \
21685:                               : NULL)
21685: 
21685: #define IMACRO_PC_OK(fp,pc)  JS_ASSERT(uintN((pc)-imacro_code[*(fp)->imacpc]) \
21685:                                        < JS_BIT(IMACRO_PC_ADJ_BITS))
21685: #define ENCODE_IP_ADJ(fp,pc) ((fp)->imacpc                                    \
21685:                               ? (IMACRO_PC_OK(fp, pc),                        \
21685:                                  (((pc) - imacro_code[*(fp)->imacpc])         \
21685:                                   << SCRIPT_PC_ADJ_BITS) +                    \
21685:                                  (fp)->imacpc - (fp)->script->code)           \
21685:                               : (pc) - (fp)->script->code)
21685: 
21685: #define DECODE_IP_ADJ(ip,fp) (IMACRO_PC_ADJ(ip)                               \
21685:                               ? (fp)->imacpc = (fp)->script->code +           \
21685:                                                SCRIPT_PC_ADJ(ip),             \
21685:                                 (fp)->regs->pc = imacro_code[*(fp)->imacpc] + \
21685:                                                  IMACRO_PC_ADJ(ip)            \
21685:                               : (fp)->regs->pc = (fp)->script->code + (ip))
21685: 
21685: static jsbytecode* imacro_code[JSOP_LIMIT];
21685: 
20931: LIns*
17850: TraceRecorder::snapshot(ExitType exitType)
17850: {
17923:     JSStackFrame* fp = cx->fp;
20969:     JSFrameRegs* regs = fp->regs;
20969:     jsbytecode* pc = regs->pc;
20969:     if (exitType == BRANCH_EXIT && js_IsLoopExit(pc, (jsbytecode*)fragment->root->ip))
17850:         exitType = LOOP_EXIT;
20969: 
20969:     /* Check for a return-value opcode that needs to restart at the next instruction. */
20969:     const JSCodeSpec& cs = js_CodeSpec[*pc];
20969: 
20969:     /* WARNING: don't return before restoring the original pc if (resumeAfter). */
20969:     bool resumeAfter = (pendingTraceableNative &&
20969:                         JSTN_ERRTYPE(pendingTraceableNative) == FAIL_JSVAL);
20969:     if (resumeAfter) {
21466:         JS_ASSERT(*pc == JSOP_CALL || *pc == JSOP_APPLY || *pc == JSOP_NEXTITER);
20969:         pc += cs.length;
20969:         regs->pc = pc;
21476:         MUST_FLOW_THROUGH("restore_pc");
20969:     }
20969: 
20969:     /* Generate the entry map for the (possibly advanced) pc and stash it in the trace. */
18425:     unsigned stackSlots = js_NativeStackSlots(cx, callDepth);
20969: 
18226:     /* It's sufficient to track the native stack use here since all stores above the
17962:        stack watermark defined by guards are killed. */
17962:     trackNativeStackUse(stackSlots + 1);
20969: 
20947:     /* Capture the type map into a temporary location. */
18239:     unsigned ngslots = traceMonitor->globalSlots->length();
20947:     unsigned typemap_size = (stackSlots + ngslots) * sizeof(uint8);
20947:     uint8* typemap = (uint8*)alloca(typemap_size);
20947:     uint8* m = typemap;
20969: 
20947:     /* Determine the type of a store by looking at the current type of the actual value the
20947:        interpreter is using. For numbers we have to check what kind of store we used last
20947:        (integer or double) to figure out what the side exit show reflect in its typemap. */
20947:     FORALL_SLOTS(cx, ngslots, traceMonitor->globalSlots->data(), callDepth,
20947:         *m++ = determineSlotType(vp);
20947:     );
20947:     JS_ASSERT(unsigned(m - typemap) == ngslots + stackSlots);
20969: 
21441:     /* If we are capturing the stack state on a specific instruction, the value on or near
21441:        the top of the stack is a boxed value. Either pc[-cs.length] is JSOP_NEXTITER and we
21441:        want one below top of stack, or else it's JSOP_CALL and we want top of stack. */
20969:     if (resumeAfter) {
21441:         m[(pc[-cs.length] == JSOP_NEXTITER) ? -2 : -1] = JSVAL_BOXED;
20969: 
20969:         /* Now restore the the original pc (after which early returns are ok). */
20969:         MUST_FLOW_LABEL(restore_pc);
20969:         regs->pc = pc - cs.length;
20969:     } else {
20947:         /* If we take a snapshot on a goto, advance to the target address. This avoids inner
20947:            trees returning on a break goto, which the outer recorder then would confuse with
20947:            a break in the outer tree. */
20947:         if (*pc == JSOP_GOTO) 
20947:             pc += GET_JUMP_OFFSET(pc);
20947:         else if (*pc == JSOP_GOTOX)
20947:             pc += GET_JUMPX_OFFSET(pc);
20969:     }
21685:     intptr_t ip_adj = ENCODE_IP_ADJ(fp, pc);
20969: 
20947:     /* Check if we already have a matching side exit. If so use that side exit structure,
20947:        otherwise we have to create our own. */
21521:     VMSideExit** exits = treeInfo->sideExits.data();
20947:     unsigned nexits = treeInfo->sideExits.length();
20957:     if (exitType == LOOP_EXIT) {
20947:         for (unsigned n = 0; n < nexits; ++n) {
21521:             VMSideExit* e = exits[n];
20957:             if (e->ip_adj == ip_adj && 
20947:                 !memcmp(getTypeMap(exits[n]), typemap, typemap_size)) {
20947:                 LIns* data = lir_buf_writer->skip(sizeof(GuardRecord));
20947:                 GuardRecord* rec = (GuardRecord*)data->payload();
20947:                 /* setup guard record structure with shared side exit */
20947:                 memset(rec, 0, sizeof(GuardRecord));
21521:                 VMSideExit* exit = exits[n];
20947:                 rec->exit = exit;
20947:                 exit->addGuard(rec);
20947:                 AUDIT(mergedLoopExits);
20947:                 return data;
20947:             }
20947:         }
20957:     }
20969: 
20947:     /* We couldn't find a matching side exit, so create our own side exit structure. */
20931:     LIns* data = lir_buf_writer->skip(sizeof(GuardRecord) +
21521:                                       sizeof(VMSideExit) + 
20931:                                       (stackSlots + ngslots) * sizeof(uint8));
20931:     GuardRecord* rec = (GuardRecord*)data->payload();
21521:     VMSideExit* exit = (VMSideExit*)(rec + 1);
20931:     /* setup guard record structure */
20931:     memset(rec, 0, sizeof(GuardRecord));
20931:     rec->exit = exit;
17381:     /* setup side exit structure */
21521:     memset(exit, 0, sizeof(VMSideExit));
20931:     exit->from = fragment;
20931:     exit->calldepth = callDepth;
20931:     exit->numGlobalSlots = ngslots;
20931:     exit->numStackSlots = stackSlots;
20931:     exit->numStackSlotsBelowCurrentFrame = cx->fp->callee
18624:         ? nativeStackOffset(&cx->fp->argv[-2])/sizeof(double)
18624:         : 0;
20931:     exit->exitType = exitType;
20946:     exit->addGuard(rec);
20947:     exit->ip_adj = ip_adj;
20931:     exit->sp_adj = (stackSlots * sizeof(double)) - treeInfo->nativeStackBase;
20931:     exit->rp_adj = exit->calldepth * sizeof(FrameInfo);
20947:     memcpy(getTypeMap(exit), typemap, typemap_size);
20969: 
20947:     /* BIG FAT WARNING: If compilation fails, we currently don't reset the lirbuf so its safe
20947:        to keep references to the side exits here. If we ever start rewinding those lirbufs,
20947:        we have to make sure we purge the side exits that then no longer will be in valid
20947:        memory. */
20957:     if (exitType == LOOP_EXIT)
20947:         treeInfo->sideExits.add(exit);
20931:     return data;
17381: }
17381: 
21083: /* Emit a guard for condition (cond), expecting to evaluate to boolean result (expected)
21083:    and using the supplied side exit if the conditon doesn't hold. */
21083: LIns*
21685: TraceRecorder::guard(bool expected, LIns* cond, LIns* exit)
21083: {
21083:     return lir->insGuard(expected ? LIR_xf : LIR_xt, cond, exit);
21083: }
21083: 
21083: /* Emit a guard for condition (cond), expecting to evaluate to boolean result (expected)
21083:    and generate a side exit with type exitType to jump to if the condition does not hold. */
17908: LIns*
17850: TraceRecorder::guard(bool expected, LIns* cond, ExitType exitType)
17323: {
21083:     return guard(expected, cond, snapshot(exitType));
17336: }
17336: 
17894: /* Try to match the type of a slot to type t. checkType is used to verify that the type of
21433:  * values flowing into the loop edge is compatible with the type we expect in the loop header.
21433:  *
21433:  * @param v             Value.
21433:  * @param t             Typemap entry for value.
21433:  * @param stage_val     Outparam for set() address.
21433:  * @param stage_ins     Outparam for set() instruction.
21433:  * @param stage_count   Outparam for set() buffer count.
21433:  * @return              True if types are compatible, false otherwise.
21433:  */
21433: bool
21433: TraceRecorder::checkType(jsval& v, uint8 t, jsval*& stage_val, LIns*& stage_ins, 
21433:                          unsigned& stage_count)
17453: {
17841:     if (t == JSVAL_INT) { /* initially all whole numbers cause the slot to be demoted */
21433:         debug_only_v(printf("checkType(tag=1, t=%d, isnum=%d, i2f=%d) stage_count=%d\n", 
21433:                             t,
21433:                             isNumber(v),
21433:                             isPromoteInt(get(&v)),
21433:                             stage_count);)
17841:         if (!isNumber(v))
17841:             return false; /* not a number? type mismatch */
17472:         LIns* i = get(&v);
21433:         /* This is always a type mismatch, we can't close a double to an int. */
21433:         if (!isPromoteInt(i))
21433:             return false;
20973:         /* Looks good, slot is an int32, the last instruction should be promotable. */
20973:         JS_ASSERT(isInt32(v) && isPromoteInt(i));
20973:         /* Overwrite the value in this slot with the argument promoted back to an integer. */
21433:         stage_val = &v;
21433:         stage_ins = f2i(i);
21433:         stage_count++;
17479:         return true;
17479:     }
18275:     if (t == JSVAL_DOUBLE) {
21433:         debug_only_v(printf("checkType(tag=2, t=%d, isnum=%d, promote=%d) stage_count=%d\n",
21433:                             t,
21433:                             isNumber(v),
21433:                             isPromoteInt(get(&v)),
21433:                             stage_count);)
18275:         if (!isNumber(v))
18275:             return false; /* not a number? type mismatch */
18275:         LIns* i = get(&v);
21685:         /* We sink i2f conversions into the side exit, but at the loop edge we have to make
18275:            sure we promote back to double if at loop entry we want a double. */
21433:         if (isPromoteInt(i)) {
21433:             stage_val = &v;
21433:             stage_ins = lir->ins1(LIR_i2f, i);
21433:             stage_count++;
21433:         }
18275:         return true;
18275:     }
17479:     /* for non-number types we expect a precise match of the type */
17500: #ifdef DEBUG
17841:     if (JSVAL_TAG(v) != t) {
20399:         debug_only_v(printf("Type mismatch: val %c, map %c ", typeChar[JSVAL_TAG(v)],
21685:                             typeChar[t]);)
17500:     }
17500: #endif
21441:     debug_only_v(printf("checkType(tag=%d, t=%d) stage_count=%d\n",
21441:                         (int) JSVAL_TAG(v), t, stage_count);)
17841:     return JSVAL_TAG(v) == t;
17453: }
17453: 
21433: /**
21433:  * Make sure that the current values in the given stack frame and all stack frames
21433:  * up and including entryFrame are type-compatible with the entry map.
21433:  *
21433:  * @param root_peer         First fragment in peer list.
21433:  * @param stable_peer       Outparam for first type stable peer.
21433:  * @param trash             Whether to trash the tree (demotion).
21433:  * @param demotes           Array to store demotable stack slots.
21433:  * @return                  True if type stable, false otherwise.
21433:  */
21433: bool
21433: TraceRecorder::deduceTypeStability(Fragment* root_peer, Fragment** stable_peer, unsigned* demotes)
21433: {
21433:     uint8* m;
21433:     uint8* typemap;
18239:     unsigned ngslots = traceMonitor->globalSlots->length();
18239:     uint16* gslots = traceMonitor->globalSlots->data();
18239:     JS_ASSERT(traceMonitor->globalTypeMap->length() == ngslots);
21433: 
21433:     if (stable_peer)
21433:         *stable_peer = NULL;
21433: 
21472:     CLEAR_UNDEMOTE_SLOTLIST(demotes);
21433: 
21519:     /*
21519:      * Rather than calculate all of this stuff twice, it gets cached locally.  The "stage" buffers 
21433:      * are for calls to set() that will change the exit types.
21433:      */
21433:     bool success;
21519:     bool unstable_from_undemotes;
21433:     unsigned stage_count;
21433:     jsval** stage_vals = (jsval**)alloca(sizeof(jsval*) * (ngslots + treeInfo->stackTypeMap.length()));
21433:     LIns** stage_ins = (LIns**)alloca(sizeof(LIns*) * (ngslots + treeInfo->stackTypeMap.length()));
21433: 
21433:     /* First run through and see if we can close ourselves - best case! */
21433:     stage_count = 0;
21433:     success = false;
21519:     unstable_from_undemotes = false;
21433: 
21433:     debug_only_v(printf("Checking type stability against self=%p\n", fragment);)
21433: 
21433:     m = typemap = traceMonitor->globalTypeMap->data();
17891:     FORALL_GLOBAL_SLOTS(cx, ngslots, gslots,
21433:         debug_only_v(printf("%s%d ", vpname, vpnum);)
21433:         if (!checkType(*vp, *m, stage_vals[stage_count], stage_ins[stage_count], stage_count)) {
21433:             /* If the failure was an int->double, tell the oracle. */
21443:             if (*m == JSVAL_INT && isNumber(*vp) && !isPromoteInt(get(vp)))
19535:                 oracle.markGlobalSlotUndemotable(cx->fp->script, gslots[n]);
21433:             trashTree = true;
21433:             goto checktype_fail_1;
21433:         }
21433:         ++m;
17891:     );
21433:     m = typemap = treeInfo->stackTypeMap.data();
21433:     FORALL_SLOTS_IN_PENDING_FRAMES(cx, 0,
21433:         debug_only_v(printf("%s%d ", vpname, vpnum);)
21433:         if (!checkType(*vp, *m, stage_vals[stage_count], stage_ins[stage_count], stage_count)) {
21443:             if (*m == JSVAL_INT && isNumber(*vp) && !isPromoteInt(get(vp)))
21472:                 ADD_UNDEMOTE_SLOT(demotes, unsigned(m - typemap));
21433:             else
21433:                 goto checktype_fail_1;
21433:         }
21433:         ++m;
17512:     );
21433: 
21519:     /*
21519:      * If there's an exit that's unstable because of undemotable slots, we want to search for 
21519:      * peers just in case we can make a connection.
21519:      */
21519:     if (NUM_UNDEMOTE_SLOTS(demotes))
21519:         unstable_from_undemotes = true;
21519:     else
21433:         success = true;
21433: 
21433: checktype_fail_1:
21433:     /* If we got a success and we don't need to recompile, we should just close here. */
21433:     if (success) {
21433:         for (unsigned i = 0; i < stage_count; i++)
21433:             set(stage_vals[i], stage_ins[i]);
21433:         return true;
21433:     /* If we need to trash, don't bother checking peers. */
21433:     } else if (trashTree) {
21433:         return false;
21433:     } else {
21472:         CLEAR_UNDEMOTE_SLOTLIST(demotes);
21433:     }
21433: 
21433:     /* At this point the tree is about to be incomplete, so let's see if we can connect to any 
21433:      * peer fragment that is type stable.
21433:      */
21433:     Fragment* f;
21433:     TreeInfo* ti;
21433:     for (f = root_peer; f != NULL; f = f->peer) {
21433:         debug_only_v(printf("Checking type stability against peer=%p (code=%p)\n", f, f->code());)
21433:         if (!f->code())
21433:             continue;
21433:         ti = (TreeInfo*)f->vmprivate;
21433:         /* Don't allow varying stack depths */
21433:         if (ti->stackTypeMap.length() != treeInfo->stackTypeMap.length())
21433:             continue;
21433:         stage_count = 0;
21433:         success = false;
21433:         m = ti->stackTypeMap.data();
21433: 
21433:         FORALL_SLOTS_IN_PENDING_FRAMES(cx, 0,
21433:             if (!checkType(*vp, *m, stage_vals[stage_count], stage_ins[stage_count], stage_count))
21433:                 goto checktype_fail_2;
21433:             ++m;
21433:         );
21433: 
21433:         success = true;
21433: 
21433: checktype_fail_2:
21433:         if (success) {
21519:             /*
21519:              * There was a successful match.  We don't care about restoring the saved staging, but 
21519:              * we do need to clear the original undemote list.
21519:              */
21433:             for (unsigned i = 0; i < stage_count; i++)
21433:                 set(stage_vals[i], stage_ins[i]);
21433:             if (stable_peer)
21433:                 *stable_peer = f;
21433:             return false;
21433:         }
21433:     }
21433: 
21519:     JS_ASSERT(NUM_UNDEMOTE_SLOTS(demotes) == 0);
21519: 
21519:     /*
21519:      * If this is a loop trace and it would be stable with demotions, build an undemote list 
21519:      * and return true.  Our caller should sniff this and trash the tree, recording a new one 
21519:      * that will assumedly stabilize.
21519:      */
21519:     if (unstable_from_undemotes && fragment->kind == LoopTrace) {
21519:         typemap = m = treeInfo->stackTypeMap.data();
21519:         FORALL_SLOTS_IN_PENDING_FRAMES(cx, 0,
21519:             if (*m == JSVAL_INT) {
21519:                 JS_ASSERT(isNumber(*vp));
21519:                 if (!isPromoteInt(get(vp)))
21519:                     ADD_UNDEMOTE_SLOT(demotes, unsigned(m - typemap));
21519:             } else if (*m == JSVAL_DOUBLE) {
21519:                 JS_ASSERT(isNumber(*vp));
21519:                 ADD_UNDEMOTE_SLOT(demotes, unsigned(m - typemap));
21519:             } else {
21519:                 JS_ASSERT(*m == JSVAL_TAG(*vp));
21519:             }
21519:             m++;
21519:         );
21519:         return true;
21519:     }
21519: 
21433:     return false;
17453: }
17453: 
17894: /* Check whether the current pc location is the loop header of the loop this recorder records. */
17845: bool
17845: TraceRecorder::isLoopHeader(JSContext* cx) const
17845: {
17845:     return cx->fp->regs->pc == fragment->root->ip;
17845: }
17845: 
18606: /* Compile the current fragment. */
17336: void
18606: TraceRecorder::compile(Fragmento* fragmento)
18606: {
18118:     if (treeInfo->maxNativeStackSlots >= MAX_NATIVE_STACK_SLOTS) {
18260:         debug_only_v(printf("Trace rejected: excessive stack use.\n"));
21433:         js_BlacklistPC(fragmento, fragment);
18118:         return;
18118:     }
18781:     ++treeInfo->branchCount;
21483:     if (lirbuf->outOmem()) {
21483:         fragmento->assm()->setError(nanojit::OutOMem);
21483:         return;
21483:     }
18606:     ::compile(fragmento->assm(), fragment);
20946:     if (anchor) 
17738:         fragmento->assm()->patch(anchor);
21483:     if (fragmento->assm()->error() != nanojit::None)
21483:         return;
18211:     JS_ASSERT(fragment->code());
18211:     JS_ASSERT(!fragment->vmprivate);
18211:     if (fragment == fragment->root)
18211:         fragment->vmprivate = treeInfo;
17884:     /* :TODO: windows support */
17884: #if defined DEBUG && !defined WIN32
21717:     const char* filename = cx->fp->script->filename;
21717:     char* label = (char*)malloc((filename ? strlen(filename) : 7) + 16);
21717:     sprintf(label, "%s:%u", filename ? filename : "<stdin>",
21685:             js_FramePCToLineNumber(cx, cx->fp));
17731:     fragmento->labels->add(fragment, sizeof(Fragment), 0, label);
18056:     free(label);
17731: #endif
18697:     AUDIT(traceCompleted);
18334: }
18334: 
21433: static bool
21521: js_JoinPeersIfCompatible(Fragmento* frago, Fragment* stableFrag, TreeInfo* stableTree, 
21521:                          VMSideExit* exit)
21433: {
21456:     JS_ASSERT(exit->numStackSlots == stableTree->stackTypeMap.length());
21433:     /* Must have a matching type unstable exit. */
21433:     if (memcmp(getTypeMap(exit) + exit->numGlobalSlots,
21433:                stableTree->stackTypeMap.data(), 
21433:                stableTree->stackTypeMap.length()) != 0) {
21433:        return false; 
21433:     }
21433: 
21433:     exit->target = stableFrag;
21433:     frago->assm()->patch(exit);
21433: 
21463:     stableTree->dependentTrees.addUnique(exit->from->root);
21463: 
21433:     return true;
21433: }
21433: 
18606: /* Complete and compile a trace and link it to the existing tree if appropriate. */
21433: bool
21433: TraceRecorder::closeLoop(Fragmento* fragmento, bool& demote, unsigned *demotes)
21433: {
21433:     bool stable;
21433:     LIns* exitIns;
21433:     Fragment* peer;
21521:     VMSideExit* exit;
21433:     Fragment* peer_root;
21433: 
21433:     demote = false;
21433:     
21433:     exitIns = snapshot(UNSTABLE_LOOP_EXIT);
21521:     exit = (VMSideExit*)((GuardRecord*)exitIns->payload())->exit;
21684: 
21684:     if (callDepth != 0) {
21684:         debug_only_v(printf("Stack depth mismatch, possible recursion\n");)
21684:         js_BlacklistPC(fragmento, fragment);
21684:         trashTree = true;
21684:         return false;
21684:     }
21684: 
21684:     JS_ASSERT(exit->numStackSlots == treeInfo->stackTypeMap.length());
21684: 
21433:     peer_root = fragmento->getLoop(fragment->root->ip);
21433:     JS_ASSERT(peer_root != NULL);
21433:     stable = deduceTypeStability(peer_root, &peer, demotes);
21433: 
21433:     #if DEBUG
21472:     if (!stable || NUM_UNDEMOTE_SLOTS(demotes))
18606:         AUDIT(unstableLoopVariable);
21433:     #endif
21433: 
21433:     if (trashTree) {
21433:         debug_only_v(printf("Trashing tree from type instability.\n");)
21433:         return false;
21433:     }
21433: 
21472:     if (stable && NUM_UNDEMOTE_SLOTS(demotes)) {
21519:         JS_ASSERT(fragment->kind == LoopTrace);
21433:         demote = true;
21519:         return false;
21433:     }
21433: 
21433:     if (!stable) {
21433:         fragment->lastIns = lir->insGuard(LIR_x, lir->insImm(1), exitIns);
21433: 
21685:         /*
21685:          * If we didn't find a type stable peer, we compile the loop anyway and
21685:          * hope it becomes stable later.
21433:          */
21433:         if (!peer) {
21685:             /*
21685:              * If such a fragment does not exist, let's compile the loop ahead
21685:              * of time anyway.  Later, if the loop becomes type stable, we will
21685:              * connect these two fragments together.
21433:              */
21685:             debug_only_v(printf("Trace has unstable loop variable with no stable peer, "
21685:                                 "compiling anyway.\n");)
21433:             UnstableExit* uexit = new UnstableExit;
21433:             uexit->fragment = fragment;
21433:             uexit->exit = exit;
21433:             uexit->next = treeInfo->unstableExits;
21433:             treeInfo->unstableExits = uexit;
21433: 
21685:             /*
21685:              * If we walked out of a loop, this exit is wrong. We need to back
21685:              * up to the if operation.
21685:              */
21433:             if (walkedOutOfLoop())
21433:                 exit->ip_adj = terminate_ip_adj;
21433: 
21433:             /* If we were trying to stabilize a promotable tree, trash it. */
21433:             if (promotedPeer)
21433:                 js_TrashTree(cx, promotedPeer);
21433:         } else {
21433:             JS_ASSERT(peer->code());
21433:             exit->target = peer;
21685:             debug_only_v(printf("Joining type-unstable trace to target fragment %p.\n", peer);)
21433:             stable = true;
21783:             ((TreeInfo*)peer->vmprivate)->dependentTrees.addUnique(fragment->root);
21433:         }
21433: 
18606:         compile(fragmento);
21433:     } else {
21433:         exit->target = fragment->root;
21718: #if defined(JS_HAS_OPERATION_COUNT) && !JS_HAS_OPERATION_COUNT
21718:         exit->exitType = TIMEOUT_EXIT;
21718:         guard(false, 
21718:               lir->ins_eq0(lir->insLoadi(cx_ins, 
21718:                                          offsetof(JSContext, operationCount))), 
21718:               exitIns);
21718: #endif
21433:         fragment->lastIns = lir->insGuard(LIR_loop, lir->insImm(1), exitIns);
21433:         compile(fragmento);
21433:     }
21433: 
21483:     if (fragmento->assm()->error() != nanojit::None)
21483:         return false;
21483: 
21433:     joinEdgesToEntry(fragmento, peer_root);
19588: 
21685:     debug_only_v(printf("recording completed at %s:%u@%u via closeLoop\n",
21685:                         cx->fp->script->filename,
21685:                         js_FramePCToLineNumber(cx, cx->fp),
21685:                         FramePCOffset(cx->fp));)
21433:     return true;
21433: }
21433: 
21433: void
21685: TraceRecorder::joinEdgesToEntry(Fragmento* fragmento, Fragment* peer_root)
21433: {
21433:     if (fragment->kind == LoopTrace) {
21433:         TreeInfo* ti;
21433:         Fragment* peer;
21433:         uint8* t1, *t2;
21433:         UnstableExit* uexit, **unext;
21433: 
21433:         unsigned* demotes = (unsigned*)alloca(treeInfo->stackTypeMap.length() * sizeof(unsigned));
21433:         for (peer = peer_root; peer != NULL; peer = peer->peer) {
21433:             if (!peer->code())
21433:                 continue;
21433:             ti = (TreeInfo*)peer->vmprivate;
21433:             uexit = ti->unstableExits;
21433:             unext = &ti->unstableExits;
21433:             while (uexit != NULL) {
21433:                 bool remove = js_JoinPeersIfCompatible(fragmento, fragment, treeInfo, uexit->exit);
21433:                 JS_ASSERT(!remove || fragment != peer);
21433:                 debug_only_v(if (remove) { 
21433:                              printf("Joining type-stable trace to target exit %p->%p.\n", 
21433:                                     uexit->fragment, uexit->exit); });
21433:                 if (!remove) {
21433:                     /* See if this exit contains mismatch demotions, which imply trashing a tree.
21433:                        This is actually faster than trashing the original tree as soon as the 
21433:                        instability is detected, since we could have compiled a fairly stable 
21433:                        tree that ran faster with integers. */
21433:                     unsigned count = 0;
21433:                     t1 = treeInfo->stackTypeMap.data();
21433:                     t2 = getTypeMap(uexit->exit) + uexit->exit->numGlobalSlots;
21433:                     for (unsigned i = 0; i < uexit->exit->numStackSlots; i++) {
21433:                         if (t2[i] == JSVAL_INT && t1[i] == JSVAL_DOUBLE) {
21433:                             demotes[count++] = i;
21433:                         } else if (t2[i] != t1[i]) {
21433:                             count = 0;
21433:                             break;
21433:                         }
21433:                     }
21433:                     if (count) {
21433:                         for (unsigned i = 0; i < count; i++)
21433:                             oracle.markStackSlotUndemotable(cx->fp->script, 
21433:                                                             cx->fp->regs->pc, demotes[i]);
22603:                         trashTree = true;
22603:                         whichTreeToTrash = uexit->fragment->root;
21433:                         break;
21433:                     }
21433:                 }
21433:                 if (remove) {
21433:                     *unext = uexit->next;
21433:                     delete uexit;
21433:                     uexit = *unext;
21433:                 } else {
21433:                     unext = &uexit->next;
21433:                     uexit = uexit->next;
21433:                 }
21433:             } 
21433:         } 
21433:     }
21433: 
21433:     debug_only_v(js_DumpPeerStability(fragmento, peer_root->ip);)
18606: }
18606: 
18606: /* Emit an always-exit guard and compile the tree (used for break statements. */
18606: void
18606: TraceRecorder::endLoop(Fragmento* fragmento)
18606: {
21684:     LIns* exitIns = snapshot(LOOP_EXIT);
21684: 
21684:     if (callDepth != 0) {
21684:         debug_only_v(printf("Stack depth mismatch, possible recursion\n");)
21684:         js_BlacklistPC(fragmento, fragment);
21684:         trashTree = true;
21684:         return;
21684:     }
21684: 
21684:     fragment->lastIns = lir->insGuard(LIR_x, lir->insImm(1), exitIns);
18606:     compile(fragmento);
19659: 
21483:     if (fragmento->assm()->error() != nanojit::None)
21483:         return;
21483: 
21433:     joinEdgesToEntry(fragmento, fragmento->getLoop(fragment->root->ip));
21433: 
21685:     debug_only_v(printf("recording completed at %s:%u@%u via endLoop\n",
21685:                         cx->fp->script->filename,
21685:                         js_FramePCToLineNumber(cx, cx->fp),
21685:                         FramePCOffset(cx->fp));)
17334: }
17334: 
18241: /* Emit code to adjust the stack to match the inner tree's stack expectations. */
17966: void
18250: TraceRecorder::prepareTreeCall(Fragment* inner)
17966: {
18121:     TreeInfo* ti = (TreeInfo*)inner->vmprivate;
18241:     inner_sp_ins = lirbuf->sp;
18215:     /* The inner tree expects to be called from the current frame. If the outer tree (this
18215:        trace) is currently inside a function inlining code (calldepth > 0), we have to advance
18007:        the native stack pointer such that we match what the inner trace expects to see. We
18007:        move it back when we come out of the inner tree call. */
18007:     if (callDepth > 0) {
18121:         /* Calculate the amount we have to lift the native stack pointer by to compensate for
18121:            any outer frames that the inner tree doesn't expect but the outer tree has. */
18280:         ptrdiff_t sp_adj = nativeStackOffset(&cx->fp->argv[-2]);
18133:         /* Calculate the amount we have to lift the call stack by */
18138:         ptrdiff_t rp_adj = callDepth * sizeof(FrameInfo);
18121:         /* Guard that we have enough stack space for the tree we are trying to call on top
18121:            of the new value for sp. */
18260:         debug_only_v(printf("sp_adj=%d outer=%d inner=%d\n",
18187:                           sp_adj, treeInfo->nativeStackBase, ti->nativeStackBase));
18232:         LIns* sp_top = lir->ins2i(LIR_piadd, lirbuf->sp,
18184:                 - treeInfo->nativeStackBase /* rebase sp to beginning of outer tree's stack */
18184:                 + sp_adj /* adjust for stack in outer frame inner tree can't see */
18184:                 + ti->maxNativeStackSlots * sizeof(double)); /* plus the inner tree's stack */
18121:         guard(true, lir->ins2(LIR_lt, sp_top, eos_ins), OOM_EXIT);
18133:         /* Guard that we have enough call stack space. */
18232:         LIns* rp_top = lir->ins2i(LIR_piadd, lirbuf->rp, rp_adj +
18133:                 ti->maxCallDepth * sizeof(FrameInfo));
18133:         guard(true, lir->ins2(LIR_lt, rp_top, eor_ins), OOM_EXIT);
18133:         /* We have enough space, so adjust sp and rp to their new level. */
18241:         lir->insStorei(inner_sp_ins = lir->ins2i(LIR_piadd, lirbuf->sp,
18184:                 - treeInfo->nativeStackBase /* rebase sp to beginning of outer tree's stack */
18184:                 + sp_adj /* adjust for stack in outer frame inner tree can't see */
18184:                 + ti->nativeStackBase), /* plus the inner tree's stack base */
18007:                 lirbuf->state, offsetof(InterpState, sp));
18232:         lir->insStorei(lir->ins2i(LIR_piadd, lirbuf->rp, rp_adj),
18133:                 lirbuf->state, offsetof(InterpState, rp));
18007:     }
18241: }
18241: 
18241: /* Record a call to an inner tree. */
18241: void
21521: TraceRecorder::emitTreeCall(Fragment* inner, VMSideExit* exit)
18241: {
18241:     TreeInfo* ti = (TreeInfo*)inner->vmprivate;
18007:     /* Invoke the inner tree. */
18712:     LIns* args[] = { INS_CONSTPTR(inner), lirbuf->state }; /* reverse order */
20915:     LIns* ret = lir->insCall(&js_CallTree_ci, args);
18116:     /* Read back all registers, in case the called tree changed any of them. */
18241:     import(ti, inner_sp_ins, exit->numGlobalSlots, exit->calldepth,
20931:            getTypeMap(exit), getTypeMap(exit) + exit->numGlobalSlots);
18159:     /* Restore sp and rp to their original values (we still have them in a register). */
18159:     if (callDepth > 0) {
18159:         lir->insStorei(lirbuf->sp, lirbuf->state, offsetof(InterpState, sp));
18159:         lir->insStorei(lirbuf->rp, lirbuf->state, offsetof(InterpState, rp));
18159:     }
18283:     /* Guard that we come out of the inner tree along the same side exit we came out when
18283:        we called the inner tree at recording time. */
20931:     guard(true, lir->ins2(LIR_eq, ret, INS_CONSTPTR(exit)), NESTED_EXIT);
18650:     /* Register us as a dependent tree of the inner tree. */
18650:     ((TreeInfo*)inner->vmprivate)->dependentTrees.addUnique(fragment->root);
18334: }
18334: 
18694: /* Add a if/if-else control-flow merge point to the list of known merge points. */
18694: void
18694: TraceRecorder::trackCfgMerges(jsbytecode* pc)
18694: {
18694:     /* If we hit the beginning of an if/if-else, then keep track of the merge point after it. */
18694:     JS_ASSERT((*pc == JSOP_IFEQ) || (*pc == JSOP_IFEQX));
18694:     jssrcnote* sn = js_GetSrcNote(cx->fp->script, pc);
18694:     if (sn != NULL) {
18694:         if (SN_TYPE(sn) == SRC_IF) {
18694:             cfgMerges.add((*pc == JSOP_IFEQ) 
18694:                           ? pc + GET_JUMP_OFFSET(pc)
18694:                           : pc + GET_JUMPX_OFFSET(pc));
18694:         } else if (SN_TYPE(sn) == SRC_IF_ELSE) 
18694:             cfgMerges.add(pc + js_GetSrcNoteOffset(sn, 0));
18694:     }
18694: }
18694: 
20416: /* Invert the direction of the guard if this is a loop edge that is not 
20416:    taken (thin loop). */
20416: void
20416: TraceRecorder::flipIf(jsbytecode* pc, bool& cond)
20416: {
20416:     if (js_IsLoopEdge(pc, (jsbytecode*)fragment->root->ip)) {
20416:         switch (*pc) {
20416:           case JSOP_IFEQ:
20416:           case JSOP_IFEQX:
20416:             if (!cond)
20416:                 return;
20416:             break;
20416:           case JSOP_IFNE:
20416:           case JSOP_IFNEX:
20416:             if (cond)
20416:                 return;
20416:             break;
20416:           default:
20416:             JS_NOT_REACHED("flipIf");
20416:         }
20416:         /* We are about to walk out of the loop, so terminate it with
20416:            an inverse loop condition. */
20416:         debug_only_v(printf("Walking out of the loop, terminating it anyway.\n");)
20416:         cond = !cond;
20416:         terminate = true;
21433:         /* If when we get to closeLoop the tree is decided to be type unstable, we need to 
21433:            reverse this logic because the loop won't be closed after all.  Store the real 
21433:            value of the IP the interpreter expects, so we can use it in our final LIR_x.
21433:          */
21433:         if (*pc == JSOP_IFEQX || *pc == JSOP_IFNEX)
21685:             pc += GET_JUMPX_OFFSET(pc);
21433:         else
21685:             pc += GET_JUMP_OFFSET(pc);
21685:         terminate_ip_adj = ENCODE_IP_ADJ(cx->fp, pc);
20416:     }
20416: }
20416: 
18694: /* Emit code for a fused IFEQ/IFNE. */
18694: void
18694: TraceRecorder::fuseIf(jsbytecode* pc, bool cond, LIns* x)
18694: {
20416:     if (x->isconst()) // no need to guard if condition is constant
20416:         return;
18696:     if (*pc == JSOP_IFEQ) {
20416:         flipIf(pc, cond);
18694:         guard(cond, x, BRANCH_EXIT);
18694:         trackCfgMerges(pc); 
18696:     } else if (*pc == JSOP_IFNE) {
20416:         flipIf(pc, cond);
18694:         guard(cond, x, BRANCH_EXIT);
18694:     }
17966: }
17966: 
21685: bool
21685: TraceRecorder::hasMethod(JSObject* obj, jsid id)
21685: {
21685:     if (!obj)
21685:         return false;
21685: 
21685:     JSObject* pobj;
21685:     JSProperty* prop;
21685:     int protoIndex = OBJ_LOOKUP_PROPERTY(cx, obj, id, &pobj, &prop);
21685:     if (protoIndex < 0 || !prop)
21685:         return false;
21685: 
21685:     bool found = false;
21685:     if (OBJ_IS_NATIVE(pobj)) {
21685:         JSScope* scope = OBJ_SCOPE(pobj);
21685:         JSScopeProperty* sprop = (JSScopeProperty*) prop;
21685: 
21685:         if (SPROP_HAS_STUB_GETTER(sprop) &&
21685:             SPROP_HAS_VALID_SLOT(sprop, scope)) {
21685:             jsval v = LOCKED_OBJ_GET_SLOT(pobj, sprop->slot);
21685:             if (VALUE_IS_FUNCTION(cx, v)) {
21685:                 found = true;
21685:                 if (!SCOPE_IS_BRANDED(scope)) {
21685:                     SCOPE_MAKE_UNIQUE_SHAPE(cx, scope);
21685:                     SCOPE_SET_BRANDED(scope);
21685:                 }
21685:             }
21685:         }
21685:     }
21685: 
21685:     OBJ_DROP_PROPERTY(cx, pobj, prop);
21685:     return found;
21685: }
21685: 
21685: bool
21685: TraceRecorder::hasToStringMethod(JSObject* obj)
21685: {
21685:     JS_ASSERT(cx->fp->regs->sp + 1 <= cx->fp->slots + cx->fp->script->nslots);
21685: 
21685:     return hasMethod(obj, ATOM_TO_JSID(cx->runtime->atomState.toStringAtom));
21685: }
21685: 
21685: bool
21685: TraceRecorder::hasValueOfMethod(JSObject* obj)
21685: {
21685:     JS_ASSERT(cx->fp->regs->sp + 2 <= cx->fp->slots + cx->fp->script->nslots);
21685: 
21685:     return hasMethod(obj, ATOM_TO_JSID(cx->runtime->atomState.valueOfAtom));
21685: }
21685: 
21685: bool
21685: TraceRecorder::hasIteratorMethod(JSObject* obj)
21685: {
21685:     JS_ASSERT(cx->fp->regs->sp + 2 <= cx->fp->slots + cx->fp->script->nslots);
21685: 
21685:     return hasMethod(obj, ATOM_TO_JSID(cx->runtime->atomState.iteratorAtom));
21685: }
21685: 
17517: int
17517: nanojit::StackFilter::getTop(LInsp guard)
17517: {
21521:     VMSideExit* e = (VMSideExit*)guard->record()->exit;
20893:     if (sp == lirbuf->sp)
21521:         return e->sp_adj;
20893:     JS_ASSERT(sp == lirbuf->rp);
21521:     return e->rp_adj;
17517: }
17517: 
17517: #if defined NJ_VERBOSE
17517: void
17517: nanojit::LirNameMap::formatGuard(LIns *i, char *out)
17517: {
21521:     VMSideExit *x;
21521: 
21521:     x = (VMSideExit *)i->record()->exit;
17517:     sprintf(out,
21685:             "%s: %s %s -> %lu:%lu sp%+ld rp%+ld",
17517:             formatRef(i),
17517:             lirNames[i->opcode()],
17517:             i->oprnd1()->isCond() ? formatRef(i->oprnd1()) : "",
21685:             IMACRO_PC_ADJ(x->ip_adj),
21685:             SCRIPT_PC_ADJ(x->ip_adj),
18612:             (long int)x->sp_adj,
21685:             (long int)x->rp_adj);
17517: }
17517: #endif
17517: 
17517: void
18056: nanojit::Fragment::onDestroy()
18056: {
18056:     if (root == this) {
18056:         delete mergeCounts;
18056:         delete lirbuf;
18056:     }
18056:     delete (TreeInfo *)vmprivate;
18056: }
18056: 
18056: void
17410: js_DeleteRecorder(JSContext* cx)
17293: {
18782:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
18782: 
18702:     /* Aborting and completing a trace end up here. */
18782:     JS_ASSERT(tm->onTrace);
18782:     tm->onTrace = false;
18782: 
17410:     delete tm->recorder;
17410:     tm->recorder = NULL;
17410: }
17410: 
21514: /**
21688:  * Checks whether the shape of the global object has changed.
21514:  */
21514: static inline bool
21514: js_CheckGlobalObjectShape(JSContext* cx, JSTraceMonitor* tm, JSObject* globalObj)
21514: {
21514:     /* Check the global shape. */
21791:     if (OBJ_SHAPE(globalObj) != tm->globalShape) {
21514:         AUDIT(globalShapeMismatchAtEntry);
21514:         debug_only_v(printf("Global shape mismatch (%u vs. %u), flushing cache.\n",
21514:                             OBJ_SHAPE(globalObj), tm->globalShape);)
21514:         return false;
21514:     }
21514:     return true;
21514: }
21514: 
17891: static bool
21521: js_StartRecorder(JSContext* cx, VMSideExit* anchor, Fragment* f, TreeInfo* ti,
18284:                  unsigned ngslots, uint8* globalTypeMap, uint8* stackTypeMap, 
21521:                  VMSideExit* expectedInnerExit, Fragment* outer)
17731: {
18782:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
18782: 
18702:     /*
18702:      * Emulate on-trace semantics and avoid rooting headaches while recording,
18702:      * by suppressing last-ditch GC attempts while recording a trace. This does
18702:      * means that trace recording must not nest or the following assertion will
18702:      * botch.
18702:      */
18782:     JS_ASSERT(!tm->onTrace);
18782:     tm->onTrace = true;
18702: 
17731:     /* start recording if no exception during construction */
19987:     tm->recorder = new (&gc) TraceRecorder(cx, anchor, f, ti,
18702:                                            ngslots, globalTypeMap, stackTypeMap,
21433:                                            expectedInnerExit, outer);
17731:     if (cx->throwing) {
20422:         js_AbortRecording(cx, "setting up recorder failed");
17731:         return false;
17731:     }
17978:     /* clear any leftover error state */
19987:     tm->fragmento->assm()->setError(None);
17731:     return true;
17731: }
17731: 
17853: static void
17889: js_TrashTree(JSContext* cx, Fragment* f)
17889: {
18211:     JS_ASSERT((!f->code()) == (!f->vmprivate));
18650:     JS_ASSERT(f == f->root);
18211:     if (!f->code())
18211:         return;
17889:     AUDIT(treesTrashed);
18260:     debug_only_v(printf("Trashing tree info.\n");)
18051:     Fragmento* fragmento = JS_TRACE_MONITOR(cx).fragmento;
18650:     TreeInfo* ti = (TreeInfo*)f->vmprivate;
17889:     f->vmprivate = NULL;
18051:     f->releaseCode(fragmento);
18650:     Fragment** data = ti->dependentTrees.data();
18650:     unsigned length = ti->dependentTrees.length();
18650:     for (unsigned n = 0; n < length; ++n)
18650:         js_TrashTree(cx, data[n]);
18650:     delete ti;
18650:     JS_ASSERT(!f->code() && !f->vmprivate);
17853: }
17853: 
19663: static int
17923: js_SynthesizeFrame(JSContext* cx, const FrameInfo& fi)
17923: {
17923:     JS_ASSERT(HAS_FUNCTION_CLASS(fi.callee));
17923: 
17923:     JSFunction* fun = GET_FUNCTION_PRIVATE(cx, fi.callee);
17923:     JS_ASSERT(FUN_INTERPRETED(fun));
17923: 
19662:     /* Assert that we have a correct sp distance from cx->fp->slots in fi. */
21726:     JS_ASSERT_IF(!FI_IMACRO_PC(fi, cx->fp),
21726:                  js_ReconstructStackDepth(cx, cx->fp->script, FI_SCRIPT_PC(fi, cx->fp))
21685:                  == uintN(fi.s.spdist - cx->fp->script->nfixed));
19662: 
19662:     uintN nframeslots = JS_HOWMANY(sizeof(JSInlineFrame), sizeof(jsval));
19662:     JSScript* script = fun->u.i.script;
19662:     size_t nbytes = (nframeslots + script->nslots) * sizeof(jsval);
19662: 
19662:     /* Code duplicated from inline_call: case in js_Interpret (FIXME). */
17923:     JSArena* a = cx->stackPool.current;
17923:     void* newmark = (void*) a->avail;
19662:     uintN argc = fi.s.argc & 0x7fff;
19662:     jsval* vp = cx->fp->slots + fi.s.spdist - (2 + argc);
19662:     uintN missing = 0;
19662:     jsval* newsp;
19662: 
19662:     if (fun->nargs > argc) {
19662:         const JSFrameRegs& regs = *cx->fp->regs;
19662: 
19662:         newsp = vp + 2 + fun->nargs;
19662:         JS_ASSERT(newsp > regs.sp);
19662:         if ((jsuword) newsp <= a->limit) {
19662:             if ((jsuword) newsp > a->avail)
19662:                 a->avail = (jsuword) newsp;
19662:             jsval* argsp = newsp;
19662:             do {
19662:                 *--argsp = JSVAL_VOID;
19662:             } while (argsp != regs.sp);
19662:             missing = 0;
19662:         } else {
19662:             missing = fun->nargs - argc;
19662:             nbytes += (2 + fun->nargs) * sizeof(jsval);
19662:         }
19662:     }
17923: 
17923:     /* Allocate the inline frame with its vars and operands. */
17923:     if (a->avail + nbytes <= a->limit) {
17923:         newsp = (jsval *) a->avail;
17923:         a->avail += nbytes;
19662:         JS_ASSERT(missing == 0);
17923:     } else {
17923:         JS_ARENA_ALLOCATE_CAST(newsp, jsval *, &cx->stackPool, nbytes);
17923:         if (!newsp) {
17923:             js_ReportOutOfScriptQuota(cx);
18226:             return 0;
17923:         }
19662: 
19662:         /*
19662:          * Move args if the missing ones overflow arena a, then push
19662:          * undefined for the missing args.
19662:          */
19662:         if (missing) {
19662:             memcpy(newsp, vp, (2 + argc) * sizeof(jsval));
19662:             vp = newsp;
19662:             newsp = vp + 2 + argc;
19662:             do {
19662:                 *newsp++ = JSVAL_VOID;
19662:             } while (--missing != 0);
19662:         }
17923:     }
17923: 
17923:     /* Claim space for the stack frame and initialize it. */
17923:     JSInlineFrame* newifp = (JSInlineFrame *) newsp;
17923:     newsp += nframeslots;
17923: 
17923:     newifp->frame.callobj = NULL;
17923:     newifp->frame.argsobj = NULL;
17923:     newifp->frame.varobj = NULL;
17923:     newifp->frame.script = script;
17923:     newifp->frame.callee = fi.callee;
17923:     newifp->frame.fun = fun;
17923: 
19577:     bool constructing = fi.s.argc & 0x8000;
19577:     newifp->frame.argc = argc;
21685: 
21685:     jsbytecode* imacro_pc = FI_IMACRO_PC(fi, cx->fp);
21685:     jsbytecode* script_pc = FI_SCRIPT_PC(fi, cx->fp);
21685:     newifp->callerRegs.pc = imacro_pc ? imacro_pc : script_pc;
17923:     newifp->callerRegs.sp = cx->fp->slots + fi.s.spdist;
21685:     cx->fp->imacpc = imacro_pc ? script_pc : NULL;
21685: 
19981:     newifp->frame.argv = newifp->callerRegs.sp - argc;
19591:     JS_ASSERT(newifp->frame.argv);
19591: #ifdef DEBUG
19591:     // Initialize argv[-1] to a known-bogus value so we'll catch it if
19591:     // someone forgets to initialize it later.
19591:     newifp->frame.argv[-1] = JSVAL_HOLE;
19591: #endif
19981:     JS_ASSERT(newifp->frame.argv >= StackBase(cx->fp) + 2);
17923: 
17923:     newifp->frame.rval = JSVAL_VOID;
17923:     newifp->frame.down = cx->fp;
17923:     newifp->frame.annotation = NULL;
17923:     newifp->frame.scopeChain = OBJ_GET_PARENT(cx, fi.callee);
17923:     newifp->frame.sharpDepth = 0;
17923:     newifp->frame.sharpArray = NULL;
19577:     newifp->frame.flags = constructing ? JSFRAME_CONSTRUCTING : 0;
17923:     newifp->frame.dormantNext = NULL;
17923:     newifp->frame.xmlNamespace = NULL;
17923:     newifp->frame.blockChain = NULL;
17923:     newifp->mark = newmark;
17925:     newifp->frame.thisp = NULL; // will be set by js_ExecuteTree -> FlushNativeStackFrame
17923: 
17923:     newifp->frame.regs = cx->fp->regs;
17923:     newifp->frame.regs->pc = script->code;
17923:     newifp->frame.regs->sp = newsp + script->nfixed;
21685:     newifp->frame.imacpc = NULL;
17923:     newifp->frame.slots = newsp;
18308:     if (script->staticDepth < JS_DISPLAY_SIZE) {
18308:         JSStackFrame **disp = &cx->display[script->staticDepth];
18308:         newifp->frame.displaySave = *disp;
18308:         *disp = &newifp->frame;
18308:     }
17923: #ifdef DEBUG
17923:     newifp->frame.pcDisabledSave = 0;
17923: #endif
17923: 
21141:     /*
21141:      * Note that cx->fp->script is still the caller's script; set the callee
21141:      * inline frame's idea of caller version from its version.
21141:      */
21141:     newifp->callerVersion = (JSVersion) cx->fp->script->version;
21141: 
17923:     cx->fp->regs = &newifp->callerRegs;
17923:     cx->fp = &newifp->frame;
18226: 
19663:     if (fun->flags & JSFUN_HEAVYWEIGHT) {
21141:         /*
21141:          * Set hookData to null because the failure case for js_GetCallObject
21141:          * involves it calling the debugger hook.
21141:          */
21141:         newifp->hookData = NULL;
19663:         if (!js_GetCallObject(cx, &newifp->frame, newifp->frame.scopeChain))
19663:             return -1;
19663:     }
19663: 
21141:     /*
21141:      * If there's a call hook, invoke it to compute the hookData used by
21141:      * debuggers that cooperate with the interpreter.
21141:      */
21141:     JSInterpreterHook hook = cx->debugHooks->callHook;
21141:     if (hook) {
21141:         newifp->hookData = hook(cx, &newifp->frame, JS_TRUE, 0,
21141:                                 cx->debugHooks->callHookData);
21141:     } else {
21141:         newifp->hookData = NULL;
21141:     }
21141: 
20404:     // FIXME? we must count stack slots from caller's operand stack up to (but not including)
18226:     // callee's, including missing arguments. Could we shift everything down to the caller's
18226:     // fp->slots (where vars start) and avoid some of the complexity?
18226:     return (fi.s.spdist - cx->fp->down->script->nfixed) +
18226:            ((fun->nargs > cx->fp->argc) ? fun->nargs - cx->fp->argc : 0) +
18226:            script->nfixed;
17923: }
17923: 
17939: bool
21433: js_RecordTree(JSContext* cx, JSTraceMonitor* tm, Fragment* f, Fragment* outer, unsigned* demotes)
17939: {
21796:     JS_ASSERT(cx->fp->regs->pc == f->ip && f->root == f);
21796:     
21685:     /* Avoid recording loops in overlarge scripts. */
21685:     if (cx->fp->script->length >= SCRIPT_PC_ADJ_LIMIT) {
21685:         js_AbortRecording(cx, "script too large");
21685:         return false;
21685:     }
21685: 
18239:     /* Make sure the global type map didn't change on us. */
21688:     JSObject* globalObj = JS_GetGlobalForObject(cx, cx->fp->scopeChain);
21688:     if (!js_CheckGlobalObjectShape(cx, tm, globalObj)) {
18271:         js_FlushJITCache(cx);
18271:         return false;
18271:     }
18239:     TypeMap current;
18239:     current.captureGlobalTypes(cx, *tm->globalSlots);
18271:     if (!current.matches(*tm->globalTypeMap)) {
18239:         js_FlushJITCache(cx);
18709:         debug_only_v(printf("Global type map mismatch in RecordTree, flushing cache.\n");)
18239:         return false;
18239:     }
18239: 
17939:     AUDIT(recorderStarted);
18213: 
18213:     /* Try to find an unused peer fragment, or allocate a new one. */
18213:     while (f->code() && f->peer)
18213:         f = f->peer;
18213:     if (f->code())
21433:         f = JS_TRACE_MONITOR(cx).fragmento->getAnchor(f->root->ip);
18213: 
21433:     f->recordAttempts++;
17939:     f->root = f;
17939:     /* allocate space to store the LIR for this tree */
17939:     if (!f->lirbuf) {
20408:         f->lirbuf = new (&gc) LirBuffer(tm->fragmento, NULL);
17939: #ifdef DEBUG
20408:         f->lirbuf->names = new (&gc) LirNameMap(&gc, NULL, tm->fragmento->labels);
17939: #endif
17939:     }
17981: 
21483:     if (f->lirbuf->outOmem()) {
21483:         js_FlushJITCache(cx);
21483:         debug_only_v(printf("Out of memory recording new tree, flushing cache.\n");)
21483:         return false;
21483:     }
21483: 
18211:     JS_ASSERT(!f->code() && !f->vmprivate);
17981: 
17939:     /* setup the VM-private treeInfo structure for this fragment */
18211:     TreeInfo* ti = new (&gc) TreeInfo(f);
17939: 
17985:     /* capture the coerced type of each active slot in the stack type map */
17985:     ti->stackTypeMap.captureStackTypes(cx, 0/*callDepth*/);
17985: 
21433:     if (demotes) {
21458:         /* If we get a list of demotions, an outer tree is telling us our types are not callable. */
21433:         uint8* typeMap = ti->stackTypeMap.data();
21472:         for (unsigned i = 1; i <= NUM_UNDEMOTE_SLOTS(demotes); i++) {
21433:             JS_ASSERT(demotes[i] < ti->stackTypeMap.length());
21458:             if (typeMap[demotes[i]] == JSVAL_INT)
21433:                 typeMap[demotes[i]] = JSVAL_DOUBLE;
21433:         }
21433:     }
21433: 
21433:     /* Check for duplicate entry type maps.  This is always wrong and hints at trace explosion 
21433:        since we are trying to stabilize something without properly connecting peer edges. */
21433:     #ifdef DEBUG
21433:     TreeInfo* ti_other;
21433:     for (Fragment* peer = tm->fragmento->getLoop(f->root->ip); peer != NULL; peer = peer->peer) {
21433:         if (!peer->code() || peer == f)
21433:             continue;
21433:         ti_other = (TreeInfo*)peer->vmprivate;
21433:         JS_ASSERT(ti_other);
21433:         JS_ASSERT(!ti->stackTypeMap.matches(ti_other->stackTypeMap));
21433:     }
21433:     #endif
21433: 
17939:     /* determine the native frame layout at the entry point */
17985:     unsigned entryNativeStackSlots = ti->stackTypeMap.length();
18425:     JS_ASSERT(entryNativeStackSlots == js_NativeStackSlots(cx, 0/*callDepth*/));
17939:     ti->nativeStackBase = (entryNativeStackSlots -
17939:             (cx->fp->regs->sp - StackBase(cx->fp))) * sizeof(double);
17939:     ti->maxNativeStackSlots = entryNativeStackSlots;
17939:     ti->maxCallDepth = 0;
18595:     ti->script = cx->fp->script;
17939: 
17939:     /* recording primary trace */
21433:     if (!js_StartRecorder(cx, NULL, f, ti,
18239:                           tm->globalSlots->length(), tm->globalTypeMap->data(), 
21433:                           ti->stackTypeMap.data(), NULL, outer)) {
21433:         return false;
21433:     }
21433: 
21433:     return true;
17939: }
17939: 
17939: static bool
21521: js_AttemptToStabilizeTree(JSContext* cx, VMSideExit* exit, Fragment* outer)
21433: {
21433:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
21456:     Fragment* from = exit->from->root;
21433:     unsigned* demotes;
21433: 
21463:     JS_ASSERT(exit->from->root->code());
21463:     
21472:     demotes = ALLOCA_UNDEMOTE_SLOTLIST(exit->numStackSlots);
21472:     CLEAR_UNDEMOTE_SLOTLIST(demotes);
21433: 
21433:     uint8* t2 = getTypeMap(exit) + exit->numGlobalSlots;
21433:     for (unsigned i = 0; i < exit->numStackSlots; i++) {
21458:         if (t2[i] == JSVAL_DOUBLE)
21472:             ADD_UNDEMOTE_SLOT(demotes, i);
21472:     }
21472: 
21472:     if (!NUM_UNDEMOTE_SLOTS(demotes))
21433:         demotes = NULL;
21433: 
21433:     if (!js_RecordTree(cx, tm, from->first, outer, demotes))
21433:         return false;
21433: 
21433:     tm->recorder->setPromotedPeer(demotes ? from : NULL);
21433: 
21433:     return true;
21433: }
21433: 
21433: static bool
21521: js_AttemptToExtendTree(JSContext* cx, VMSideExit* anchor, VMSideExit* exitedFrom, Fragment* outer)
18620: {
18620:     Fragment* f = anchor->from->root;
18619:     JS_ASSERT(f->vmprivate);
18781:     TreeInfo* ti = (TreeInfo*)f->vmprivate;
18781: 
18781:     /* Don't grow trees above a certain size to avoid code explosion due to tail duplication. */
18781:     if (ti->branchCount >= MAX_BRANCHES)
18781:         return false;
17939:     
17939:     Fragment* c;
18620:     if (!(c = anchor->target)) {
20931:         c = JS_TRACE_MONITOR(cx).fragmento->createBranch(anchor, cx->fp->regs->pc);
20931:         c->spawnedFrom = anchor;
17939:         c->parent = f;
18620:         anchor->target = c;
17939:         c->root = f;
17939:     }
17939: 
21433:     debug_only_v(printf("trying to attach another branch to the tree (hits = %d)\n", c->hits());)
21433: 
17939:     if (++c->hits() >= HOTEXIT) {
17939:         /* start tracing secondary trace from this point */
17939:         c->lirbuf = f->lirbuf;
18621:         unsigned ngslots;
18621:         uint8* globalTypeMap;
18621:         uint8* stackTypeMap;
18621:         TypeMap fullMap;
18621:         if (exitedFrom == NULL) {
18621:             /* If we are coming straight from a simple side exit, just use that exit's type map
18621:                as starting point. */
20931:             ngslots = anchor->numGlobalSlots;
20931:             globalTypeMap = getTypeMap(anchor);
18621:             stackTypeMap = globalTypeMap + ngslots;
18621:         } else {
18621:             /* If we side-exited on a loop exit and continue on a nesting guard, the nesting
18621:                guard (anchor) has the type information for everything below the current scope, 
18621:                and the actual guard we exited from has the types for everything in the current
18621:                scope (and whatever it inlined). We have to merge those maps here. */
21521:             VMSideExit* e1 = anchor;
21521:             VMSideExit* e2 = exitedFrom;
20931:             fullMap.add(getTypeMap(e1) + e1->numGlobalSlots, e1->numStackSlotsBelowCurrentFrame);
20931:             fullMap.add(getTypeMap(e2) + e2->numGlobalSlots, e2->numStackSlots);
18621:             ngslots = e2->numGlobalSlots;
20931:             globalTypeMap = getTypeMap(e2);
18621:             stackTypeMap = fullMap.data();
18621:         } 
18620:         return js_StartRecorder(cx, anchor, c, (TreeInfo*)f->vmprivate,
21433:                                 ngslots, globalTypeMap, stackTypeMap, exitedFrom, outer);
17939:     }
17939:     return false;
17939: }
17939: 
21521: static VMSideExit*
21433: js_ExecuteTree(JSContext* cx, Fragment* f, uintN& inlineCallCount, 
21521:                VMSideExit** innermostNestedGuardp);
17951: 
21685: static Fragment*
21433: js_FindVMCompatiblePeer(JSContext* cx, Fragment* f);
21433: 
21433: static bool
20416: js_CloseLoop(JSContext* cx)
20416: {
20416:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
20416:     Fragmento* fragmento = tm->fragmento;
20416:     TraceRecorder* r = tm->recorder;
20416:     JS_ASSERT(fragmento && r);
21796:     bool walkedOutOfLoop = r->walkedOutOfLoop();
20416:     
20416:     if (fragmento->assm()->error()) {
20422:         js_AbortRecording(cx, "Error during recording");
21685: 
20416:         /* If we ran out of memory, flush the code cache and abort. */
20416:         if (fragmento->assm()->error() == OutOMem)
20416:             js_FlushJITCache(cx);
21433:         return false;
21433:     }
21433: 
21433:     bool demote;
21433:     Fragment* f = r->getFragment();
21433:     TreeInfo* ti = r->getTreeInfo();
21472:     unsigned* demotes = ALLOCA_UNDEMOTE_SLOTLIST(ti->stackTypeMap.length());
21433:     r->closeLoop(fragmento, demote, demotes);
21472:     JS_ASSERT(!demote || NUM_UNDEMOTE_SLOTS(demotes));
20416:     js_DeleteRecorder(cx);
21796:     
21796:     /*
21798:      * If we just walked out of a thin loop, we can't immediately start the 
21796:      * compiler again here since we didn't return to the loop header.
21796:      */
21796:     if (demote && !walkedOutOfLoop)
21433:         return js_RecordTree(cx, tm, f, NULL, demotes);
21433:     return false;
20416: }
20416: 
17951: bool
20422: js_RecordLoopEdge(JSContext* cx, TraceRecorder* r, uintN& inlineCallCount)
17939: {
17939: #ifdef JS_THREADSAFE
17939:     if (OBJ_SCOPE(JS_GetGlobalForObject(cx, cx->fp->scopeChain))->title.ownercx != cx) {
20422:         js_AbortRecording(cx, "Global object not owned by this context");
17939:         return false; /* we stay away from shared global objects */
17939:     }
17939: #endif
18248:     Fragmento* fragmento = JS_TRACE_MONITOR(cx).fragmento;
18677:     /* If we hit our own loop header, close the loop and compile the trace. */
21433:     if (r->isLoopHeader(cx))
21433:         return js_CloseLoop(cx);
17988:     /* does this branch go to an inner loop? */
17988:     Fragment* f = fragmento->getLoop(cx->fp->regs->pc);
21433:     Fragment* peer_root = f;
21433:     if (nesting_enabled && f) {
21433:         JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
21514:         
21514:         /* Make sure inner tree call will not run into an out-of-memory condition. */
21514:         if (tm->recoveryDoublePoolPtr < (tm->recoveryDoublePool + MAX_NATIVE_STACK_SLOTS) &&
21514:             !js_ReplenishRecoveryPool(cx, tm)) {
21514:             js_AbortRecording(cx, "Couldn't call inner tree (out of memory)");
21514:             return false; 
21514:         }
21514:         
21514:         /* Make sure the shape of the global object still matches (this might flush 
21514:            the JIT cache). */
21514:         JSObject* globalObj = JS_GetGlobalForObject(cx, cx->fp->scopeChain);
21514:         if (!js_CheckGlobalObjectShape(cx, tm, globalObj)) {
21433:             js_AbortRecording(cx, "Couldn't call inner tree (prep failed)");
21433:             return false;
21433:         }
21433:         
21433:         debug_only_v(printf("Looking for type-compatible peer (%s:%d@%d)\n",
21433:                             cx->fp->script->filename,
21685:                             js_FramePCToLineNumber(cx, cx->fp),
21685:                             FramePCOffset(cx->fp));)
21433: 
21433:         /* Find an acceptable peer, make sure our types fit. */
21433:         Fragment* empty;
21433:         bool trash = false;
21433:         bool success = false;
21433:         unsigned* demotes = NULL;
21433: 
21433:         f = r->findNestedCompatiblePeer(f, &empty);
21433:         if (f && f->code()) {
21433:             TreeInfo* ti = (TreeInfo*)f->vmprivate;
21433:             /* alloca docs says it lasts out of scopes. */
21472:             demotes = ALLOCA_UNDEMOTE_SLOTLIST(ti->stackTypeMap.length());
21472:             CLEAR_UNDEMOTE_SLOTLIST(demotes);
21433:             success = r->adjustCallerTypes(f, demotes, trash);
21433:         }
21433: 
21433:         if (!success) {
21433:             AUDIT(noCompatInnerTrees);
21433:             debug_only_v(printf("No compatible inner tree (%p).\n", f);)
21433: 
21433:             if (trash) {
21433:                 js_AbortRecording(cx, "No compatible inner tree (global demotions");
21433:                 return false;
21433:             }
21433: 
21433:             Fragment* old = fragmento->getLoop(tm->recorder->getFragment()->root->ip);
21433:             if (old == NULL)
21433:                 old = tm->recorder->getFragment();
21433:             js_AbortRecording(cx, "No compatible inner tree");
21433:             if (!f && ++peer_root->hits() < MAX_INNER_RECORD_BLACKLIST)
21433:                 return false;
21433:             if (old->recordAttempts < MAX_MISMATCH)
21433:                 old->resetHits();
21433:             f = empty ? empty : tm->fragmento->getAnchor(cx->fp->regs->pc);
21433:             return js_RecordTree(cx, tm, f, old, demotes);
21433:         }
21433: 
18250:         r->prepareTreeCall(f);
21521:         VMSideExit* innermostNestedGuard = NULL;
21521:         VMSideExit* lr = js_ExecuteTree(cx, f, inlineCallCount, &innermostNestedGuard);
18051:         if (!lr) {
20422:             js_AbortRecording(cx, "Couldn't call inner tree");
18051:             return false;
18051:         }
21433:         Fragment* old;
20931:         switch (lr->exitType) {
18051:           case LOOP_EXIT:
18284:             /* If the inner tree exited on an unknown loop exit, grow the tree around it. */
18284:             if (innermostNestedGuard) {
20422:                 js_AbortRecording(cx, "Inner tree took different side exit, abort recording");
21433:                 return js_AttemptToExtendTree(cx, innermostNestedGuard, lr, NULL);
18284:             }
17997:             /* emit a call to the inner tree and continue recording the outer tree trace */
17997:             r->emitTreeCall(f, lr);
17997:             return true;
21433:         case UNSTABLE_LOOP_EXIT:
21433:             /* abort recording so the inner loop can become type stable. */
21433:             old = fragmento->getLoop(tm->recorder->getFragment()->root->ip);
21433:             js_AbortRecording(cx, "Inner tree is trying to stabilize, abort outer recording");
21433:             old->resetHits();
21456:             return js_AttemptToStabilizeTree(cx, lr, old);
18051:         case BRANCH_EXIT:
18051:             /* abort recording the outer tree, extend the inner tree */
21433:             old = fragmento->getLoop(tm->recorder->getFragment()->root->ip);
20422:             js_AbortRecording(cx, "Inner tree is trying to grow, abort outer recording");
21433:             old->resetHits();
21433:             return js_AttemptToExtendTree(cx, lr, NULL, old);
18051:         default:
20931:             debug_only_v(printf("exit_type=%d\n", lr->exitType);)
20422:             js_AbortRecording(cx, "Inner tree not suitable for calling");
18051:             return false;
18051:         }
17988:     }
21433: 
17988:     /* not returning to our own loop header, not an inner loop we can call, abort trace */
17939:     AUDIT(returnToDifferentLoopHeader);
21685:     JS_ASSERT(!cx->fp->imacpc);
20422:     debug_only_v(printf("loop edge to %d, header %d\n",
17939:                  cx->fp->regs->pc - cx->fp->script->code,
17939:                  (jsbytecode*)r->getFragment()->root->ip - cx->fp->script->code));
20422:     js_AbortRecording(cx, "Loop edge does not return to header");
18667:     return false;
17939: }
17939: 
21433: static bool
21433: js_IsEntryTypeCompatible(jsval* vp, uint8* m)
21433: {
21433:     unsigned tag = JSVAL_TAG(*vp);
21433: 
21433:     debug_only_v(printf("%c/%c ", "OIDISIBI"[tag], "OID?S?B?"[*m]);)
21433: 
21433:     switch (*m) {
21433:       case JSVAL_INT:
21433:         jsint i;
21433:         if (JSVAL_IS_INT(*vp))
21433:             return true;
21433:         if ((tag == JSVAL_DOUBLE) && JSDOUBLE_IS_INT(*JSVAL_TO_DOUBLE(*vp), i))
21433:             return true;
21433:         debug_only_v(printf("int != tag%u(value=%lu) ", tag, *vp);)
21433:         return false;
21433:       case JSVAL_DOUBLE:
21433:         if (JSVAL_IS_INT(*vp) || tag == JSVAL_DOUBLE)
21433:             return true;
21433:         debug_only_v(printf("double != tag%u ", tag);)
21433:         return false;
21433:       case JSVAL_BOOLEAN:
21433:         if (tag == JSVAL_BOOLEAN)
21433:             return true;
21433:         debug_only_v(printf("bool != tag%u", tag);)
21433:         return false;
21433:       case JSVAL_STRING:
22601:         if (tag == JSVAL_STRING)
21433:             return true;
21433:         debug_only_v(printf("string != tag%u", tag);)
21433:         return false;
21433:       default:
21433:         JS_ASSERT(*m == JSVAL_OBJECT);
21433:         if (tag == JSVAL_OBJECT)
21433:             return true;
21433:         debug_only_v(printf("object != tag%u", tag);)
21433:         return false;
21433:     }
21433: }
21433: 
21433: Fragment* TraceRecorder::findNestedCompatiblePeer(Fragment* f, Fragment** empty)
21433: {
21433:     Fragment* demote;
21433:     JSTraceMonitor* tm;
21433:     unsigned max_demotes;
21433: 
21433:     if (empty)
21433:         *empty = NULL;
21433:     demote = NULL;
21433: 
21433:     tm = &JS_TRACE_MONITOR(cx);
21433:     unsigned int ngslots = tm->globalSlots->length();
21433:     uint16* gslots = tm->globalSlots->data();
21433:     uint8* m = tm->globalTypeMap->data();
21433: 
21433:     if (ngslots) {
21433:         FORALL_GLOBAL_SLOTS(cx, ngslots, gslots,
21433:             debug_only_v(printf("%s%d=", vpname, vpnum);)
21433:             if (!js_IsEntryTypeCompatible(vp, m))
21433:                 return NULL;
21433:             m++;
21433:         );
21433:     }
21433: 
21433:     /* We keep a maximum tally - we want to select the peer most likely to work so we don't keep 
21433:      * recording.
21433:      */
21433:     max_demotes = 0;
21433: 
21433:     TreeInfo* ti;
21433:     for (; f != NULL; f = f->peer) {
21433:         if (!f->code()) {
21433:             if (empty)
21433:                 *empty = f;
21433:             continue;
21433:         }
21433: 
21433:         unsigned demotes = 0;
21433:         ti = (TreeInfo*)f->vmprivate;
21433:         m = ti->stackTypeMap.data();
21433: 
21456:         debug_only_v(printf("checking nested types %p: ", f);)
21433: 
21433:         FORALL_SLOTS_IN_PENDING_FRAMES(cx, 0,
21433:             debug_only_v(printf("%s%d=", vpname, vpnum);)
21433:             if (!js_IsEntryTypeCompatible(vp, m))
21433:                 goto check_fail;
21468:             if (*m == JSVAL_STRING && *vp == JSVAL_VOID)
21468:                 goto check_fail;
21433:             if (*m == JSVAL_INT && !isPromoteInt(get(vp)))
21433:                 demotes++;
21433:             m++;
21433:         );
21433:         JS_ASSERT(unsigned(m - ti->stackTypeMap.data()) == ti->stackTypeMap.length());
21433: 
21433:         debug_only_v(printf(" (demotes %d)\n", demotes);)
21433: 
21433:         if (demotes) {
21433:             if (demotes > max_demotes) {
21433:                 demote = f;
21433:                 max_demotes = demotes;
21433:             }
21433:             continue;
21433:         } else {
21433:             return f;
21433:         }
21433: 
21433: check_fail:
21433:         debug_only_v(printf("\n"));
21433:         continue;
21433:     }
21433: 
21433:     if (demote)
21433:         return demote;
21433: 
21433:    return NULL;
21433: }
21433: 
21433: /**
21433:  * Check if types are usable for trace execution.
21433:  *
21433:  * @param cx            Context.
21433:  * @param ti            Tree info of peer we're testing.
21433:  * @return              True if compatible (with or without demotions), false otherwise.
21433:  */
21433: static bool
21433: js_CheckEntryTypes(JSContext* cx, TreeInfo* ti)
21433: {
21433:     JSTraceMonitor* tm;
21433: 
21433:     tm = &JS_TRACE_MONITOR(cx);
21433:     unsigned int ngslots = tm->globalSlots->length();
21433:     uint16* gslots = tm->globalSlots->data();
21433:     uint8* m = tm->globalTypeMap->data();
21433: 
21433:     if (ngslots) {
21433:         FORALL_GLOBAL_SLOTS(cx, ngslots, gslots,
21433:             debug_only_v(printf("%s%d=", vpname, vpnum);)
21433:             if (!js_IsEntryTypeCompatible(vp, m))
21433:                 goto check_fail;
21433:             m++;
21433:         );
21433:     }
21433: 
21433:     m = ti->stackTypeMap.data();
21433:     FORALL_SLOTS_IN_PENDING_FRAMES(cx, 0,
21433:         debug_only_v(printf("%s%d=", vpname, vpnum);)
21433:         if (!js_IsEntryTypeCompatible(vp, m))
21433:             goto check_fail;
21433:         m++;
21433:     );
21433:     JS_ASSERT(unsigned(m - ti->stackTypeMap.data()) == ti->stackTypeMap.length());
21433: 
21433:     debug_only_v(printf("\n");)
21433:     return true;
21433: 
21433: check_fail:
21433:     debug_only_v(printf("\n");)
21433:     return false;
21433: }
21433: 
21433: /**
21433:  * Find an acceptable entry tree given a PC.
21433:  *
21433:  * @param cx            Context.
21433:  * @param f             First peer fragment.
21433:  * @param nodemote      If true, will try to find a peer that does not require demotion.
21433:  */
21685: static Fragment*
21433: js_FindVMCompatiblePeer(JSContext* cx, Fragment* f)
21433: {
21433:     for (; f != NULL; f = f->peer) {
21433:         if (f->vmprivate == NULL) 
21433:             continue;
21456:         debug_only_v(printf("checking vm types %p (ip: %p): ", f, f->ip);)
21433:         if (js_CheckEntryTypes(cx, (TreeInfo*)f->vmprivate))
21433:             return f;
21433:     }
21433:     return NULL;
21433: }
21433: 
21433: /**
21514:  * Executes a tree.
21433:  */
21521: static VMSideExit*
21433: js_ExecuteTree(JSContext* cx, Fragment* f, uintN& inlineCallCount, 
21521:                VMSideExit** innermostNestedGuardp)
18209: {
21433:     JS_ASSERT(f->code() && f->vmprivate);
21433: 
21433:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
21514:     JSObject* globalObj = JS_GetGlobalForObject(cx, cx->fp->scopeChain);
18248:     TreeInfo* ti = (TreeInfo*)f->vmprivate;
21433:     unsigned ngslots = tm->globalSlots->length();
21433:     uint16* gslots = tm->globalSlots->data();
21433:     unsigned globalFrameSize = STOBJ_NSLOTS(globalObj);
21433:     double* global = (double*)alloca((globalFrameSize+1) * sizeof(double));
21433:     double stack_buffer[MAX_NATIVE_STACK_SLOTS];
21433:     double* stack = stack_buffer;
21433: 
21433:     /* Make sure the global object is sane. */
21433:     JS_ASSERT(!ngslots || (OBJ_SHAPE(JS_GetGlobalForObject(cx, cx->fp->scopeChain)) == tm->globalShape)); 
21433:     /* Make sure our caller replenished the double pool. */
21433:     JS_ASSERT(tm->recoveryDoublePoolPtr >= tm->recoveryDoublePool + MAX_NATIVE_STACK_SLOTS);
21433: 
21514: #ifdef DEBUG
21433:     memset(stack_buffer, 0xCD, sizeof(stack_buffer));
21433:     memset(global, 0xCD, (globalFrameSize+1)*sizeof(double));
21514: #endif    
21433: 
21433:     debug_only(*(uint64*)&global[globalFrameSize] = 0xdeadbeefdeadbeefLL;)
18773:     debug_only_v(printf("entering trace at %s:%u@%u, native stack slots: %u code: %p\n",
18260:                         cx->fp->script->filename,
21685:                         js_FramePCToLineNumber(cx, cx->fp),
21685:                         FramePCOffset(cx->fp),
21685:                         ti->maxNativeStackSlots,
21685:                         f->code());)
18210:     
21433:     if (ngslots)
21433:         BuildNativeGlobalFrame(cx, ngslots, gslots, tm->globalTypeMap->data(), global);
21433:     BuildNativeStackFrame(cx, 0/*callDepth*/, ti->stackTypeMap.data(), stack);
18051: 
17962:     double* entry_sp = &stack[ti->nativeStackBase/sizeof(double)];
21433:     FrameInfo callstack_buffer[MAX_CALL_STACK_ENTRIES];
21433:     FrameInfo* callstack = callstack_buffer;
18133: 
17485:     InterpState state;
17413:     state.sp = (void*)entry_sp;
18118:     state.eos = ((double*)state.sp) + MAX_NATIVE_STACK_SLOTS;
17659:     state.rp = callstack;
18133:     state.eor = callstack + MAX_CALL_STACK_ENTRIES;
17815:     state.gp = global;
17416:     state.cx = cx;
19590:     state.lastTreeExitGuard = NULL;
19590:     state.lastTreeCallGuard = NULL;
20429:     state.rpAtLastTreeCall = NULL;
17397:     union { NIns *code; GuardRecord* (FASTCALL *func)(InterpState*, Fragment*); } u;
17397:     u.code = f->code();
17923: 
21459: #ifdef JS_JIT_SPEW
19040: #if defined(NANOJIT_IA32) || (defined(NANOJIT_AMD64) && defined(__GNUC__))
17435:     uint64 start = rdtsc();
17435: #endif
18776: #endif
17923: 
18702:     /*
18702:      * We may be called from js_MonitorLoopEdge while not recording, or while
18702:      * recording. Rather than over-generalize by using a counter instead of a
18782:      * flag, we simply sample and update tm->onTrace if necessary.
18702:      */
18782:     bool onTrace = tm->onTrace;
18782:     if (!onTrace)
18782:         tm->onTrace = true;
21521:     VMSideExit* lr;
18723:     
20893:     debug_only(fflush(NULL);)
20931:     GuardRecord* rec;
18723: #if defined(JS_NO_FASTCALL) && defined(NANOJIT_IA32)
20931:     SIMULATE_FASTCALL(rec, &state, NULL, u.func);
18723: #else
20931:     rec = u.func(&state, NULL);
18723: #endif
21521:     lr = (VMSideExit*)rec->exit;
20931: 
21433:     AUDIT(traceTriggered);
21433: 
20931:     JS_ASSERT(lr->exitType != LOOP_EXIT || !lr->calldepth);
19659: 
21433:     tm->onTrace = onTrace;
17923: 
20429:     /* Except if we find that this is a nested bailout, the guard the call returned is the
20429:        one we have to use to adjust pc and sp. */
21521:     VMSideExit* innermost = lr;
20429: 
19588:     /* While executing a tree we do not update state.sp and state.rp even if they grow. Instead,
19588:        guards tell us by how much sp and rp should be incremented in case of a side exit. When
19588:        calling a nested tree, however, we actively adjust sp and rp. If we have such frames
19588:        from outer trees on the stack, then rp will have been adjusted. Before we can process
19588:        the stack of the frames of the tree we directly exited from, we have to first work our
19588:        way through the outer frames and generate interpreter frames for them. Once the call
19588:        stack (rp) is empty, we can process the final frames (which again are not directly
19588:        visible and only the guard we exited on will tells us about). */
19588:     FrameInfo* rp = (FrameInfo*)state.rp;
20931:     if (lr->exitType == NESTED_EXIT) {
21521:         VMSideExit* nested = state.lastTreeCallGuard;
20429:         if (!nested) {
20429:             /* If lastTreeCallGuard is not set in state, we only have a single level of
20429:                nesting in this exit, so lr itself is the innermost and outermost nested
20429:                guard, and hence we set nested to lr. The calldepth of the innermost guard
20429:                is not added to state.rp, so we do it here manually. For a nesting depth
20429:                greater than 1 the CallTree builtin already added the innermost guard's
20429:                calldepth to state.rpAtLastTreeCall. */
20429:             nested = lr;
20429:             rp += lr->calldepth;
20429:         } else {
20429:             /* During unwinding state.rp gets overwritten at every step and we restore
20429:                it here to its state at the innermost nested guard. The builtin already
20429:                added the calldepth of that innermost guard to rpAtLastTreeCall. */
20429:             rp = (FrameInfo*)state.rpAtLastTreeCall;
20429:         }
20429:         innermost = state.lastTreeExitGuard;
18284:         if (innermostNestedGuardp)
20429:             *innermostNestedGuardp = nested;
20429:         JS_ASSERT(nested);
20931:         JS_ASSERT(nested->exitType == NESTED_EXIT);
20429:         JS_ASSERT(state.lastTreeExitGuard);
20931:         JS_ASSERT(state.lastTreeExitGuard->exitType != NESTED_EXIT);
19590:     }
21433: 
19588:     while (callstack < rp) {
19588:         /* Synthesize a stack frame and write out the values in it using the type map pointer
19588:            on the native call stack. */
19663:         if (js_SynthesizeFrame(cx, *callstack) < 0)
19663:             return NULL;
19588:         int slots = FlushNativeStackFrame(cx, 1/*callDepth*/, callstack->typemap, stack, cx->fp);
19588: #ifdef DEBUG
19588:         JSStackFrame* fp = cx->fp;
19588:         debug_only_v(printf("synthesized deep frame for %s:%u@%u, slots=%d\n",
21685:                             fp->script->filename,
21685:                             js_FramePCToLineNumber(cx, fp),
21685:                             FramePCOffset(fp),
21685:                             slots);)
19588: #endif        
18702:         if (slots < 0)
18702:             return NULL;
19588:         /* Keep track of the additional frames we put on the interpreter stack and the native
19588:            stack slots we consumed. */
19588:         ++inlineCallCount;
19588:         ++callstack;
18170:         stack += slots;
18164:     }
19588: 
18164:     /* We already synthesized the frames around the innermost guard. Here we just deal
18164:        with additional frames inside the tree we are bailing out from. */
19588:     JS_ASSERT(rp == callstack);
20429:     unsigned calldepth = innermost->calldepth;
18226:     unsigned calldepth_slots = 0;
19588:     for (unsigned n = 0; n < calldepth; ++n) {
19663:         int nslots = js_SynthesizeFrame(cx, callstack[n]);
19663:         if (nslots < 0)
19663:             return NULL;
19663:         calldepth_slots += nslots;
20429:         ++inlineCallCount;
19588: #ifdef DEBUG        
19588:         JSStackFrame* fp = cx->fp;
19588:         debug_only_v(printf("synthesized shallow frame for %s:%u@%u\n",
21685:                             fp->script->filename, js_FramePCToLineNumber(cx, fp),
21685:                             FramePCOffset(fp));)
19588: #endif        
19588:     }
17923: 
21685:     /* Adjust sp and pc relative to the tree we exited from (not the tree we entered into).
21685:        These are our final values for sp and pc since js_SynthesizeFrame has already taken
21685:        care of all frames in between. */
17923:     JSStackFrame* fp = cx->fp;
18226: 
18193:     /* If we are not exiting from an inlined frame the state->sp is spbase, otherwise spbase
18193:        is whatever slots frames around us consume. */
21685:     DECODE_IP_ADJ(innermost->ip_adj, fp);
20931:     fp->regs->sp = StackBase(fp) + (innermost->sp_adj / sizeof(double)) - calldepth_slots;
21685:     JS_ASSERT_IF(!fp->imacpc,
21685:                  fp->slots + fp->script->nfixed +
18772:                  js_ReconstructStackDepth(cx, fp->script, fp->regs->pc) == fp->regs->sp);
17923:                                               
21685: 
21459: #if defined(JS_JIT_SPEW) && (defined(NANOJIT_IA32) || (defined(NANOJIT_AMD64) && defined(__GNUC__)))
18788:     uint64 cycles = rdtsc() - start;
21459: #elif defined(JS_JIT_SPEW)
19040:     uint64 cycles = 0;
18788: #endif
18788: 
20931:     debug_only_v(printf("leaving trace at %s:%u@%u, op=%s, lr=%p, exitType=%d, sp=%d, "
19588:                         "calldepth=%d, cycles=%llu\n",
21685:                         fp->script->filename,
21685:                         js_FramePCToLineNumber(cx, fp),
21685:                         FramePCOffset(fp),
21685:                         js_CodeName[fp->imacpc ? *fp->imacpc : *fp->regs->pc],
18667:                         lr,
20931:                         lr->exitType,
20931:                         fp->regs->sp - StackBase(fp), 
19588:                         calldepth,
18788:                         cycles));
17923: 
18200:     /* If this trace is part of a tree, later branches might have added additional globals for
18200:        with we don't have any type information available in the side exit. We merge in this
18200:        information from the entry type-map. See also comment in the constructor of TraceRecorder
18200:        why this is always safe to do. */
20931:     unsigned exit_gslots = innermost->numGlobalSlots;
18239:     JS_ASSERT(ngslots == tm->globalTypeMap->length());
18239:     JS_ASSERT(ngslots >= exit_gslots);
20931:     uint8* globalTypeMap = getTypeMap(innermost);
18239:     if (exit_gslots < ngslots)
18239:         mergeTypeMaps(&globalTypeMap, &exit_gslots, tm->globalTypeMap->data(), ngslots,
18239:                       (uint8*)alloca(sizeof(uint8) * ngslots));
18239:     JS_ASSERT(exit_gslots == tm->globalTypeMap->length());
18200: 
18154:     /* write back interned globals */
19588:     int slots = FlushNativeGlobalFrame(cx, exit_gslots, gslots, globalTypeMap, global);
18702:     if (slots < 0)
18702:         return NULL;
19972:     JS_ASSERT_IF(ngslots != 0, globalFrameSize == STOBJ_NSLOTS(globalObj));
17900:     JS_ASSERT(*(uint64*)&global[globalFrameSize] == 0xdeadbeefdeadbeefLL);
17726: 
18154:     /* write back native stack frame */
21685:     slots = FlushNativeStackFrame(cx, innermost->calldepth,
21685:                                   getTypeMap(innermost) + innermost->numGlobalSlots,
21685:                                   stack, NULL);
18702:     if (slots < 0)
18702:         return NULL;
20931:     JS_ASSERT(unsigned(slots) == innermost->numStackSlots);
18154: 
19591: #ifdef DEBUG
19591:     // Verify that our state restoration worked
19591:     for (JSStackFrame* fp = cx->fp; fp; fp = fp->down) {
19591:         JS_ASSERT(!fp->callee || JSVAL_IS_OBJECT(fp->argv[-1]));
19591:         JS_ASSERT(!fp->callee || fp->thisp == JSVAL_TO_OBJECT(fp->argv[-1]));
19591:     }
19591: #endif
19591: 
17726:     AUDIT(sideExitIntoInterpreter);
17397: 
20436:     return innermost;
17772: }
17376: 
17939: bool
20422: js_MonitorLoopEdge(JSContext* cx, uintN& inlineCallCount)
17939: {
17939:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
17939: 
18672:     /* Is the recorder currently active? */
17954:     if (tm->recorder) {
20422:         if (js_RecordLoopEdge(cx, tm->recorder, inlineCallCount))
17954:             return true;
17954:         /* recording was aborted, treat like a regular loop edge hit */
17954:     }
18317:     JS_ASSERT(!tm->recorder);
17939: 
21514:     
21514:     /* Check the recovery pool of doubles (this might trigger a GC). */
21514:     if (tm->recoveryDoublePoolPtr < (tm->recoveryDoublePool + MAX_NATIVE_STACK_SLOTS) &&
21514:         !js_ReplenishRecoveryPool(cx, tm)) {
21514:         return false; /* Out of memory, don't try to record now. */
21514:     }
21514:     
21514:     /* Make sure the shape of the global object still matches (this might flush the JIT cache). */
21514:     JSObject* globalObj = JS_GetGlobalForObject(cx, cx->fp->scopeChain);
21688:     if (!js_CheckGlobalObjectShape(cx, tm, globalObj))
21688:         js_FlushJITCache(cx);
21514:     
17940:     jsbytecode* pc = cx->fp->regs->pc;
21433:     Fragmento* fragmento = tm->fragmento;
17940:     Fragment* f;
21433:     f = fragmento->getLoop(pc);
18204:     if (!f)
21433:         f = fragmento->getAnchor(pc);
21433: 
21433:     /* If we have no code in the anchor and no peers, we definitively won't be able to 
21433:        activate any trees so increment the hit counter and start compiling if appropriate. */
21433:     if (!f->code() && !f->peer) {
21433: monitor_loop:
21433:         if (++f->hits() >= HOTLOOP) {
21433:             /* We can give RecordTree the root peer. If that peer is already taken, it will
21433:                walk the peer list and find us a free slot or allocate a new tree if needed. */
21433:             return js_RecordTree(cx, tm, f->first, NULL, NULL);
21433:         }
21433:         /* Threshold not reached yet. */
21433:         return false;
21433:     }
21433:     
21456:     debug_only_v(printf("Looking for compat peer %d@%d, from %p (ip: %p, hits=%d)\n",
21685:                         js_FramePCToLineNumber(cx, cx->fp), 
21685:                         FramePCOffset(cx->fp),
21685:                         f, f->ip, f->hits());)
21433:     Fragment* match = js_FindVMCompatiblePeer(cx, f);
21433:     /* If we didn't find a tree that actually matched, keep monitoring the loop. */
21433:     if (!match) 
21433:         goto monitor_loop;
21433: 
21521:     VMSideExit* lr = NULL;
21521:     VMSideExit* innermostNestedGuard = NULL;
21433: 
21433:     lr = js_ExecuteTree(cx, match, inlineCallCount, &innermostNestedGuard);
21433:     if (!lr)
21433:         return false;
21433: 
18284:     /* If we exit on a branch, or on a tree call guard, try to grow the inner tree (in case
18284:        of a branch exit), or the tree nested around the tree we exited from (in case of the
18284:        tree call guard). */
20931:     switch (lr->exitType) {
21433:       case UNSTABLE_LOOP_EXIT:
21456:         return js_AttemptToStabilizeTree(cx, lr, NULL);
18284:       case BRANCH_EXIT:
21433:         return js_AttemptToExtendTree(cx, lr, NULL, NULL);
18284:       case LOOP_EXIT:
18284:         if (innermostNestedGuard)
21433:             return js_AttemptToExtendTree(cx, innermostNestedGuard, lr, NULL);
17951:         return false;
18284:       default:
18284:         /* No, this was an unusual exit (i.e. out of memory/GC), so just resume interpretation. */
18284:         return false;
18284:     }
17939: }
17939: 
18683: bool
19093: js_MonitorRecording(TraceRecorder* tr)
19093: {
19093:     JSContext* cx = tr->cx;
19068: 
21483:     if (tr->lirbuf->outOmem()) {
21483:         js_AbortRecording(cx, "no more LIR memory");
21483:         js_FlushJITCache(cx);
21483:         return false;
21483:     }
21483: 
21433:     if (tr->walkedOutOfLoop())
21433:         return js_CloseLoop(cx);
20416: 
21526:     // Clear one-shot state used to communicate between record_JSOP_CALL and mid- and post-
21526:     // opcode-case-guts record hooks (record_EnterFrame, record_FastNativeCallComplete).
21526:     tr->applyingArguments = false;
20969:     tr->pendingTraceableNative = NULL;
19068: 
19653:     // In the future, handle dslots realloc by computing an offset from dslots instead.
19653:     if (tr->global_dslots != tr->globalObj->dslots) {
20422:         js_AbortRecording(cx, "globalObj->dslots reallocated");
19653:         return false;
19653:     }
19653: 
19068:     // Process deepAbort() requests now.
18706:     if (tr->wasDeepAborted()) {
20422:         js_AbortRecording(cx, "deep abort requested");
18706:         return false;
18706:     }
18706: 
18694:     jsbytecode* pc = cx->fp->regs->pc;
19093: 
18683:     /* If we hit a break, end the loop and generate an always taken loop exit guard. For other
18683:        downward gotos (like if/else) continue recording. */
19093:     if (*pc == JSOP_GOTO || *pc == JSOP_GOTOX) {
18694:         jssrcnote* sn = js_GetSrcNote(cx->fp->script, pc);
19093:         if (sn && SN_TYPE(sn) == SRC_BREAK) {
18697:             AUDIT(breakLoopExits);
18706:             tr->endLoop(JS_TRACE_MONITOR(cx).fragmento);
18683:             js_DeleteRecorder(cx);
18683:             return false; /* done recording */
18683:         }
18694:     }
19075: 
19075:     /* An explicit return from callDepth 0 should end the loop, not abort it. */
19075:     if (*pc == JSOP_RETURN && tr->callDepth == 0) {
19075:         AUDIT(returnLoopExits);
19075:         tr->endLoop(JS_TRACE_MONITOR(cx).fragmento);
19075:         js_DeleteRecorder(cx);
19075:         return false; /* done recording */
19075:     }
19075: 
19075:     /* If it's not a break or a return from a loop, continue recording and follow the trace. */
18683:     return true;
18683: }
18683: 
21433: /* If used on a loop trace, blacklists the root peer instead of the given fragment. */
21433: void
21433: js_BlacklistPC(Fragmento* frago, Fragment* frag)
21433: {
21433:     if (frag->kind == LoopTrace)
21433:         frag = frago->getLoop(frag->ip);
21433:     frag->blacklist();
21433: }
21433: 
17253: void
20422: js_AbortRecording(JSContext* cx, const char* reason)
17350: {
18614:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
18614:     JS_ASSERT(tm->recorder != NULL);
20965:     AUDIT(recorderAborted);
21685: 
18614:     /* Abort the trace and blacklist its starting point. */
21685:     JSStackFrame* fp = cx->fp;
21685:     if (fp) {
20422:         debug_only_v(printf("Abort recording (line %d, pc %d): %s.\n",
21685:                             js_FramePCToLineNumber(cx, fp),
21685:                             FramePCOffset(fp),
21685:                             reason);)
18136:     }
20965:     Fragment* f = tm->recorder->getFragment();
20965:     if (!f) {
20965:         js_DeleteRecorder(cx);
20965:         return;
20965:     }
20965:     JS_ASSERT(!f->vmprivate);
21433:     js_BlacklistPC(tm->fragmento, f);
21433:     Fragment* outer = tm->recorder->getOuterToBlacklist();
21433:     /* Give outer two chances to stabilize before we start blacklisting. */
21433:     if (outer != NULL && outer->recordAttempts >= 2)
21433:         js_BlacklistPC(tm->fragmento, outer);
17889:     js_DeleteRecorder(cx);
18614:     /* If this is the primary trace and we didn't succeed compiling, trash the TreeInfo object. */
18614:     if (!f->code() && (f->root == f)) 
17889:         js_TrashTree(cx, f);
17350: }
17350: 
18333: #if defined NANOJIT_IA32
18333: static bool
18333: js_CheckForSSE2()
18333: {
18333:     int features = 0;
18333: #if defined _MSC_VER
18333:     __asm
18333:     {
18333:         pushad
18333:         mov eax, 1
18333:         cpuid
18333:         mov features, edx
18333:         popad
18333:     }
18333: #elif defined __GNUC__
21475:     asm("xchg %%esi, %%ebx\n" /* we can't clobber ebx on gcc (PIC register) */
20953:         "mov $0x01, %%eax\n"
20953:         "cpuid\n"
20953:         "mov %%edx, %0\n"
21475:         "xchg %%esi, %%ebx\n" 
20953:         : "=m" (features)
21475:         : /* We have no inputs */
21475:         : "%eax", "%esi", "%ecx", "%edx"
18788:        );
19058: #elif defined __SUNPRO_C || defined __SUNPRO_CC
19058:     asm("push %%ebx\n"
19058:         "mov $0x01, %%eax\n"
19058:         "cpuid\n"
19058:         "pop %%ebx\n"
19058:         : "=d" (features)
18333:         : /* We have no inputs */
19058:         : "%eax", "%ecx"
18333:        );
18333: #endif
18333:     return (features & (1<<26)) != 0;
18333: }
18333: #endif
18333: 
21685: static void InitIMacroCode();
21685: 
17442: extern void
18068: js_InitJIT(JSTraceMonitor *tm)
18068: {
18333: #if defined NANOJIT_IA32
18333:     if (!did_we_check_sse2) {
19997:         avmplus::AvmCore::cmov_available =
18333:         avmplus::AvmCore::sse2_available = js_CheckForSSE2();
18333:         did_we_check_sse2 = true;
18333:     }
18333: #endif
17442:     if (!tm->fragmento) {
19987:         JS_ASSERT(!tm->globalSlots && !tm->globalTypeMap && !tm->recoveryDoublePool);
17713:         Fragmento* fragmento = new (&gc) Fragmento(core, 24);
17884:         verbose_only(fragmento->labels = new (&gc) LabelMap(core, NULL);)
17442:         tm->fragmento = fragmento;
18264:         tm->globalSlots = new (&gc) SlotList();
18264:         tm->globalTypeMap = new (&gc) TypeMap();
19991:         tm->recoveryDoublePoolPtr = tm->recoveryDoublePool = new jsval[MAX_NATIVE_STACK_SLOTS];
17442:     }
21491:     if (!tm->reFragmento) {
21502:         Fragmento* fragmento = new (&gc) Fragmento(core, 20);
21491:         verbose_only(fragmento->labels = new (&gc) LabelMap(core, NULL);)
21491:         tm->reFragmento = fragmento;
21491:     }
21685:     InitIMacroCode();
17884: #if !defined XP_WIN
19623:     debug_only(memset(&jitstats, 0, sizeof(jitstats)));
17884: #endif
17726: }
17726: 
17726: extern void
18075: js_FinishJIT(JSTraceMonitor *tm)
18068: {
21459: #ifdef JS_JIT_SPEW
17726:     printf("recorder: started(%llu), aborted(%llu), completed(%llu), different header(%llu), "
18697:            "trees trashed(%llu), slot promoted(%llu), unstable loop variable(%llu), "
21433:            "breaks(%llu), returns(%llu), unstableInnerCalls(%llu)\n",
19623:            jitstats.recorderStarted, jitstats.recorderAborted, jitstats.traceCompleted,
19623:            jitstats.returnToDifferentLoopHeader, jitstats.treesTrashed, jitstats.slotPromoted,
21433:            jitstats.unstableLoopVariable, jitstats.breakLoopExits, jitstats.returnLoopExits,
21433:            jitstats.noCompatInnerTrees);
17728:     printf("monitor: triggered(%llu), exits(%llu), type mismatch(%llu), "
19623:            "global mismatch(%llu)\n", jitstats.traceTriggered, jitstats.sideExitIntoInterpreter,
19623:            jitstats.typeMapMismatchAtEntry, jitstats.globalShapeMismatchAtEntry);
17726: #endif
18079:     if (tm->fragmento != NULL) {
19987:         JS_ASSERT(tm->globalSlots && tm->globalTypeMap && tm->recoveryDoublePool);
18056:         verbose_only(delete tm->fragmento->labels;)
18056:         delete tm->fragmento;
18079:         tm->fragmento = NULL;
18238:         delete tm->globalSlots;
18238:         tm->globalSlots = NULL;
18238:         delete tm->globalTypeMap;
18238:         tm->globalTypeMap = NULL;
19991:         delete[] tm->recoveryDoublePool;
19987:         tm->recoveryDoublePool = tm->recoveryDoublePoolPtr = NULL;
18079:     }
21502:     if (tm->reFragmento != NULL) {
21502:         verbose_only(delete tm->reFragmento->labels;)
21502:         delete tm->reFragmento;
21502:     }
17442: }
17442: 
21723: void
21723: TraceRecorder::pushAbortStack()
21723: {
21723:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
21723: 
21723:     JS_ASSERT(tm->abortStack != this);
21723: 
21723:     nextRecorderToAbort = tm->abortStack;
21723:     tm->abortStack = this;
21723: }
21723: 
21723: void
21723: TraceRecorder::popAbortStack()
21723: {
21723:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
21723: 
21723:     JS_ASSERT(tm->abortStack == this);
21723: 
21723:     tm->abortStack = nextRecorderToAbort;
21723:     nextRecorderToAbort = NULL;
21723: }
21723: 
17976: extern void
18277: js_FlushJITOracle(JSContext* cx)
18277: {
18725:     if (!TRACING_ENABLED(cx))
18725:         return;
19535:     oracle.clear();
18277: }
18277: 
18277: extern void
17976: js_FlushJITCache(JSContext* cx)
17976: {
18725:     if (!TRACING_ENABLED(cx))
18725:         return;
21685:     debug_only_v(printf("Flushing cache.\n");)
17976:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
17976:     if (tm->recorder)
20422:         js_AbortRecording(cx, "flush cache");
21723:     TraceRecorder* tr;
21723:     while ((tr = tm->abortStack) != NULL) {
21723:         tr->removeFragmentoReferences();
21723:         tr->deepAbort();
21723:         tr->popAbortStack();
21723:     }
17976:     Fragmento* fragmento = tm->fragmento;
17976:     if (fragmento) {
17976:         fragmento->clearFrags();
17976: #ifdef DEBUG
17976:         JS_ASSERT(fragmento->labels);
17976:         delete fragmento->labels;
17976:         fragmento->labels = new (&gc) LabelMap(core, NULL);
17976: #endif
17976:     }
18271:     if (cx->fp) {
19020:         tm->globalShape = OBJ_SHAPE(JS_GetGlobalForObject(cx, cx->fp->scopeChain));
18271:         tm->globalSlots->clear();
18271:         tm->globalTypeMap->clear();
18271:     }
17976: }
17976: 
17412: jsval&
17412: TraceRecorder::argval(unsigned n) const
17412: {
17799:     JS_ASSERT(n < cx->fp->fun->nargs);
17412:     return cx->fp->argv[n];
17412: }
17412: 
17412: jsval&
17412: TraceRecorder::varval(unsigned n) const
17412: {
18137:     JS_ASSERT(n < cx->fp->script->nslots);
17807:     return cx->fp->slots[n];
17412: }
17412: 
17412: jsval&
17412: TraceRecorder::stackval(int n) const
17412: {
17520:     jsval* sp = cx->fp->regs->sp;
17520:     return sp[n];
17412: }
17412: 
17412: LIns*
18286: TraceRecorder::scopeChain() const
18286: {
18286:     return lir->insLoad(LIR_ldp,
18286:                         lir->insLoad(LIR_ldp, cx_ins, offsetof(JSContext, fp)),
18286:                         offsetof(JSStackFrame, scopeChain));
18286: }
18286: 
18286: static inline bool
18286: FrameInRange(JSStackFrame* fp, JSStackFrame *target, unsigned callDepth)
18286: {
18286:     while (fp != target) {
18286:         if (callDepth-- == 0)
18286:             return false;
18286:         if (!(fp = fp->down))
18286:             return false;
18286:     }
18286:     return true;
18286: }
18286: 
18286: bool
18286: TraceRecorder::activeCallOrGlobalSlot(JSObject* obj, jsval*& vp)
18286: {
18286:     JS_ASSERT(obj != globalObj);
18286: 
18286:     JSAtom* atom = atoms[GET_INDEX(cx->fp->regs->pc)];
18286:     JSObject* obj2;
18286:     JSProperty* prop;
18286:     if (js_FindProperty(cx, ATOM_TO_JSID(atom), &obj, &obj2, &prop) < 0 || !prop)
18286:         ABORT_TRACE("failed to find name in non-global scope chain");
18286: 
18286:     if (obj == globalObj) {
18286:         JSScopeProperty* sprop = (JSScopeProperty*) prop;
18286:         if (obj2 != obj || !SPROP_HAS_VALID_SLOT(sprop, OBJ_SCOPE(obj))) {
18286:             OBJ_DROP_PROPERTY(cx, obj2, prop);
18286:             ABORT_TRACE("prototype or slotless globalObj property");
18286:         }
18286: 
18286:         if (!lazilyImportGlobalSlot(sprop->slot))
18286:              ABORT_TRACE("lazy import of global slot failed");
18286:         vp = &STOBJ_GET_SLOT(obj, sprop->slot);
18286:         OBJ_DROP_PROPERTY(cx, obj2, prop);
18286:         return true;
18286:     }
18286: 
18286:     if (obj == obj2 && OBJ_GET_CLASS(cx, obj) == &js_CallClass) {
18286:         JSStackFrame* cfp = (JSStackFrame*) JS_GetPrivate(cx, obj);
18286:         if (cfp && FrameInRange(cx->fp, cfp, callDepth)) {
18286:             JSScopeProperty* sprop = (JSScopeProperty*) prop;
18286:             uintN slot = sprop->shortid;
18286: 
18426:             vp = NULL;
18286:             if (sprop->getter == js_GetCallArg) {
18286:                 JS_ASSERT(slot < cfp->fun->nargs);
18286:                 vp = &cfp->argv[slot];
18426:             } else if (sprop->getter == js_GetCallVar) {
18286:                 JS_ASSERT(slot < cfp->script->nslots);
18286:                 vp = &cfp->slots[slot];
18286:             }
18286:             OBJ_DROP_PROPERTY(cx, obj2, prop);
18426:             if (!vp)
18426:                 ABORT_TRACE("dynamic property of Call object");
18286:             return true;
18286:         }
18286:     }
18286: 
18286:     OBJ_DROP_PROPERTY(cx, obj2, prop);
18286:     ABORT_TRACE("fp->scopeChain is not global or active call object");
18286: }
18286: 
18286: LIns*
17412: TraceRecorder::arg(unsigned n)
17412: {
17412:     return get(&argval(n));
17412: }
17412: 
17415: void
17415: TraceRecorder::arg(unsigned n, LIns* i)
17415: {
17415:     set(&argval(n), i);
17415: }
17415: 
17412: LIns*
17412: TraceRecorder::var(unsigned n)
17412: {
17412:     return get(&varval(n));
17412: }
17412: 
17415: void
17415: TraceRecorder::var(unsigned n, LIns* i)
17415: {
17415:     set(&varval(n), i);
17415: }
17415: 
17412: LIns*
17412: TraceRecorder::stack(int n)
17412: {
17412:     return get(&stackval(n));
17412: }
17412: 
17412: void
17412: TraceRecorder::stack(int n, LIns* i)
17412: {
17788:     set(&stackval(n), i, n >= 0);
17412: }
17412: 
21799: LIns*
21799: TraceRecorder::alu(LOpcode v, jsdouble v0, jsdouble v1, LIns* s0, LIns* s1)
21799: {
21799:     if (v == LIR_fadd || v == LIR_fsub) {
21799:         jsdouble r;
21799:         if (v == LIR_fadd)
21799:             r = v0 + v1;
21799:         else
21799:             r = v0 - v1;
21799:         /*
21799:          * Calculate the result of the addition for the current values. If the
21799:          * value is not within the integer range, don't even try to demote 
21799:          * here.
21799:          */
21799:         if (!JSDOUBLE_IS_NEGZERO(r) && (jsint(r) == r) && isPromoteInt(s0) && isPromoteInt(s1)) {
21799:             LIns* d0 = ::demote(lir, s0);
21799:             LIns* d1 = ::demote(lir, s1);
21799:             /*
21799:              * If the inputs are constant, generate an integer constant for 
21799:              * this operation.
21799:              */
21799:             if (d0->isconst() && d1->isconst()) 
21799:                 return lir->ins1(LIR_i2f, lir->insImm(jsint(r)));
21799:             /*
21799:              * Speculatively generate code that will perform the addition over
21799:              * the integer inputs as an integer addition/subtraction and exit
21799:              * if that fails.
21799:              */
21799:             v = (LOpcode)((int)v & ~LIR64);
21799:             LIns* result = lir->ins2(v, d0, d1);
21799:             if (!overflowSafe(d0) || !overflowSafe(d1)) {
21799:                 lir->insGuard(LIR_xt, lir->ins1(LIR_ov, result),
21799:                               snapshot(OVERFLOW_EXIT));
21799:             }
21799:             return lir->ins1(LIR_i2f, result);
21799:         }
21799:         /*
21799:          * The result doesn't fit into the integer domain, so either generate
21799:          * a floating point constant or a floating point operation.
21799:          */
21799:         if (s0->isconst() && s1->isconst()) {
21799:             jsdpun u;
21799:             u.d = r;
21799:             return lir->insImmq(u.u64);
21799:         }
21799:         return lir->ins2(v, s0, s1);
21799:     }
21799:     return lir->ins2(v, s0, s1);
21799: }
21799: 
21799: LIns*
21799: TraceRecorder::f2i(LIns* f)
17469: {
20915:     return lir->insCall(&js_DoubleToInt32_ci, &f);
17469: }
17469: 
21799: LIns*
21799: TraceRecorder::makeNumberInt32(LIns* f)
19979: {
19979:     JS_ASSERT(f->isQuad());
19979:     LIns* x;
19979:     if (!isPromote(f)) {
19979:         x = f2i(f);
19979:         guard(true, lir->ins2(LIR_feq, f, lir->ins1(LIR_i2f, x)), MISMATCH_EXIT);
19979:     } else {
19979:         x = ::demote(lir, f);
19979:     }
19979:     return x;
19979: }
19979: 
21447: LIns*
21685: TraceRecorder::stringify(jsval& v)
21685: {
21685:     LIns* v_ins = get(&v);
21447:     if (JSVAL_IS_STRING(v))
21447:         return v_ins;
21447: 
21447:     LIns* args[] = { v_ins, cx_ins };
21447:     const CallInfo* ci;
21447:     if (JSVAL_IS_NUMBER(v)) {
21447:         ci = &js_NumberToString_ci;
21447:     } else if (JSVAL_TAG(v) == JSVAL_BOOLEAN) {
21447:         ci = &js_BooleanOrUndefinedToString_ci;
21447:     } else {
21768:         /* We can't stringify objects here (use imacros instead), just return NULL. */
21447:         return NULL;
21447:     }
21447:     v_ins = lir->insCall(ci, args);
21447:     guard(false, lir->ins_eq0(v_ins), OOM_EXIT);
21447:     return v_ins;
21447: }
21447: 
17926: bool
21685: TraceRecorder::call_imacro(jsbytecode* imacro)
21685: {
21685:     JSStackFrame* fp = cx->fp;
21685:     JSFrameRegs* regs = fp->regs;
21685: 
21808:     if (!fp->imacpc) {
21685:         fp->imacpc = regs->pc;
21719:         fp->flags |= JSFRAME_IMACRO_START;
21685:         regs->pc = imacro;
21685:         atoms = COMMON_ATOMS_START(&cx->runtime->atomState);
21808:     }
21685:     return false;
21685: }
21685: 
21685: bool
20416: TraceRecorder::ifop()
17452: {
17452:     jsval& v = stackval(-1);
19604:     LIns* v_ins = get(&v);
20410:     bool cond;
20410:     LIns* x;
19604:     /* no need to guard if condition is constant */
19604:     if (v_ins->isconst() || v_ins->isconstq())
19604:         return true;
17927:     if (JSVAL_TAG(v) == JSVAL_BOOLEAN) {
20410:         /* test for boolean is true, negate later if we are testing for false */
20410:         cond = JSVAL_TO_BOOLEAN(v) == 1;
20410:         x = lir->ins2i(LIR_eq, v_ins, 1);
17749:     } else if (JSVAL_IS_OBJECT(v)) {
20411:         cond = !JSVAL_IS_NULL(v);
20411:         x = v_ins;
17749:     } else if (isNumber(v)) {
17749:         jsdouble d = asNumber(v);
20411:         cond = !JSDOUBLE_IS_NaN(d) && d;
17749:         jsdpun u;
17749:         u.d = 0;
20410:         x = lir->ins2(LIR_and, 
20410:                       lir->ins2(LIR_feq, v_ins, v_ins),
20410:                       lir->ins_eq0(lir->ins2(LIR_feq, v_ins, lir->insImmq(u.u64))));
17749:     } else if (JSVAL_IS_STRING(v)) {
20411:         cond = JSSTRING_LENGTH(JSVAL_TO_STRING(v)) != 0;
20411:         x = lir->ins2(LIR_piand,
18254:                       lir->insLoad(LIR_ldp, 
19604:                                    v_ins, 
18254:                                    (int)offsetof(JSString, length)),
20411:                       INS_CONSTPTR(JSSTRING_LENGTH_MASK));
17452:     } else {
17927:         JS_NOT_REACHED("ifop");
20410:         return false;
20410:     }
20416:     flipIf(cx->fp->regs->pc, cond);
20412:     bool expected = cond;
20412:     if (!x->isCond()) {
20412:         x = lir->ins_eq0(x);
20412:         expected = !expected;
20412:     }
20412:     guard(expected, x, BRANCH_EXIT); 
17452:     return true;
17452: }
17452: 
17412: bool
18687: TraceRecorder::switchop()
18687: {
18687:     jsval& v = stackval(-1);
19604:     LIns* v_ins = get(&v);
19604:     /* no need to guard if condition is constant */
19604:     if (v_ins->isconst() || v_ins->isconstq())
19604:         return true;
18687:     if (isNumber(v)) {
18687:         jsdouble d = asNumber(v);
18687:         jsdpun u;
18687:         u.d = d;
18687:         guard(true,
19604:               addName(lir->ins2(LIR_feq, v_ins, lir->insImmq(u.u64)),
18687:                       "guard(switch on numeric)"),
18687:               BRANCH_EXIT);
18687:     } else if (JSVAL_IS_STRING(v)) {
19604:         LIns* args[] = { v_ins, INS_CONSTPTR(JSVAL_TO_STRING(v)) };
18687:         guard(true,
20915:               addName(lir->ins_eq0(lir->ins_eq0(lir->insCall(&js_EqualStrings_ci, args))),
18687:                       "guard(switch on string)"),
18687:               BRANCH_EXIT);
19995:     } else if (JSVAL_TAG(v) == JSVAL_BOOLEAN) {
18687:         guard(true,
19604:               addName(lir->ins2(LIR_eq, v_ins, lir->insImm(JSVAL_TO_BOOLEAN(v))),
18687:                       "guard(switch on boolean)"),
18687:               BRANCH_EXIT);
18687:     } else {
19995:         ABORT_TRACE("switch on object or null");
18687:     }
18687:     return true;
18687: }
18687: 
18687: bool
17412: TraceRecorder::inc(jsval& v, jsint incr, bool pre)
17412: {
17782:     LIns* v_ins = get(&v);
17782:     if (!inc(v, v_ins, incr, pre))
17782:         return false;
17782:     set(&v, v_ins);
17782:     return true;
17782: }
17782: 
17782: /*
17782:  * On exit, v_ins is the incremented unboxed value, and the appropriate
17782:  * value (pre- or post-increment as described by pre) is stacked.
17782:  */
17758: bool
17782: TraceRecorder::inc(jsval& v, LIns*& v_ins, jsint incr, bool pre)
17758: {
17758:     if (!isNumber(v))
17782:         ABORT_TRACE("can only inc numbers");
17758: 
17656:     jsdpun u;
17758:     u.d = jsdouble(incr);
17758: 
21799:     LIns* v_after = alu(LIR_fadd, asNumber(v), incr, v_ins, lir->insImmq(u.u64));
17544: 
17544:     const JSCodeSpec& cs = js_CodeSpec[*cx->fp->regs->pc];
17544:     JS_ASSERT(cs.ndefs == 1);
17782:     stack(-cs.nuses, pre ? v_after : v_ins);
17782:     v_ins = v_after;
17412:     return true;
17412: }
17758: 
17758: bool
17758: TraceRecorder::incProp(jsint incr, bool pre)
17758: {
17758:     jsval& l = stackval(-1);
17758:     if (JSVAL_IS_PRIMITIVE(l))
17758:         ABORT_TRACE("incProp on primitive");
17758: 
17758:     JSObject* obj = JSVAL_TO_OBJECT(l);
17758:     LIns* obj_ins = get(&l);
17758: 
17761:     uint32 slot;
17758:     LIns* v_ins;
17761:     if (!prop(obj, obj_ins, slot, v_ins))
17412:         return false;
17761: 
18666:     if (slot == SPROP_INVALID_SLOT)
18666:         ABORT_TRACE("incProp on invalid slot");
18666: 
17761:     jsval& v = STOBJ_GET_SLOT(obj, slot);
17761:     if (!inc(v, v_ins, incr, pre))
17761:         return false;
17761: 
17782:     if (!box_jsval(v, v_ins))
17761:         return false;
17761: 
17761:     LIns* dslots_ins = NULL;
17782:     stobj_set_slot(obj_ins, slot, dslots_ins, v_ins);
17761:     return true;
17758: }
17758: 
17758: bool
17758: TraceRecorder::incElem(jsint incr, bool pre)
17758: {
17758:     jsval& r = stackval(-1);
17758:     jsval& l = stackval(-2);
17758:     jsval* vp;
17758:     LIns* v_ins;
17782:     LIns* addr_ins;
17782:     if (!elem(l, r, vp, v_ins, addr_ins))
17758:         return false;
20972:     if (!addr_ins) // if we read a hole, abort
20972:         return false;
17782:     if (!inc(*vp, v_ins, incr, pre))
17782:         return false;
17782:     if (!box_jsval(*vp, v_ins))
17782:         return false;
17782:     lir->insStorei(v_ins, addr_ins, 0);
17782:     return true;
17412: }
17412: 
19576: static bool
19576: evalCmp(LOpcode op, double result)
19576: {
18017:     bool cond;
18017:     switch (op) {
19576:       case LIR_feq:
19576:         cond = (result == 0);
19576:         break;
18017:       case LIR_flt:
18017:         cond = result < 0;
18017:         break;
18017:       case LIR_fgt:
18017:         cond = result > 0;
18017:         break;
18017:       case LIR_fle:
18017:         cond = result <= 0;
18017:         break;
18017:       case LIR_fge:
18017:         cond = result >= 0;
18017:         break;
18017:       default:
19576:         JS_NOT_REACHED("unexpected comparison op");
19576:         return false;
19576:     }
19576:     return cond;
19576: }
19576: 
19576: static bool
19576: evalCmp(LOpcode op, double l, double r)
19576: {
19576:     return evalCmp(op, l - r);
19576: }
19576: 
19576: static bool
19576: evalCmp(LOpcode op, JSString* l, JSString* r)
19576: {
19576:     if (op == LIR_feq)
19576:         return js_EqualStrings(l, r);
19576:     return evalCmp(op, js_CompareStrings(l, r));
19576: }
19576: 
21724: static struct {
21724:     jsbytecode obj_any[13];
21724:     jsbytecode any_obj[11];
21724:     jsbytecode obj_obj[22];
21724: } binary_imacros = {
21724:     {
21724:         JSOP_SWAP,
21724:         JSOP_CALLPROP, 0, COMMON_ATOM_INDEX(valueOf),
21724:         JSOP_STRING, 0, COMMON_TYPE_ATOM_INDEX(JSTYPE_NUMBER),
21724:         JSOP_CALL, 0, 1,
21724:         JSOP_SWAP,
21724:         JSOP_IMACOP,
21724:         JSOP_STOP
21724:     },
21724: 
21724:     {
21724:         JSOP_CALLPROP, 0, COMMON_ATOM_INDEX(valueOf),
21724:         JSOP_STRING, 0, COMMON_TYPE_ATOM_INDEX(JSTYPE_NUMBER),
21724:         JSOP_CALL, 0, 1,
21724:         JSOP_IMACOP,
21724:         JSOP_STOP
21724:     },
21724: 
21724:     {
21724:         JSOP_SWAP,
21724:         JSOP_CALLPROP, 0, COMMON_ATOM_INDEX(valueOf),
21724:         JSOP_STRING, 0, COMMON_TYPE_ATOM_INDEX(JSTYPE_NUMBER),
21724:         JSOP_CALL, 0, 1,
21724:         JSOP_SWAP,
21724:         JSOP_CALLPROP, 0, COMMON_ATOM_INDEX(valueOf),
21724:         JSOP_STRING, 0, COMMON_TYPE_ATOM_INDEX(JSTYPE_NUMBER),
21724:         JSOP_CALL, 0, 1,
21724:         JSOP_IMACOP,
21724:         JSOP_STOP
21724:     }
21724: };
21724: 
21724: JS_STATIC_ASSERT(sizeof(binary_imacros) < IMACRO_PC_ADJ_LIMIT);
21724: 
19576: bool
19576: TraceRecorder::cmp(LOpcode op, int flags)
19576: {
19576:     jsval& r = stackval(-1);
19576:     jsval& l = stackval(-2);
19576:     LIns* x = NULL;
19576:     bool negate = !!(flags & CMP_NEGATE);
19576:     bool cond;
19576:     LIns* l_ins = get(&l);
19576:     LIns* r_ins = get(&r);
20392:     bool fp = false;
19576: 
21724:     if (op != LIR_feq) {
21724:         if (JSVAL_IS_OBJECT(l) && hasValueOfMethod(l)) {
21724:             if (JSVAL_IS_OBJECT(r) && hasValueOfMethod(r))
21724:                 return call_imacro(binary_imacros.obj_obj);
21724:             return call_imacro(binary_imacros.obj_any);
21724:         }
21724:         if (JSVAL_IS_OBJECT(r) && hasValueOfMethod(r))
21724:             return call_imacro(binary_imacros.any_obj);
21724:     }
21724: 
21719:     // CMP_STRICT is set only for JSOP_STRICTEQ and JSOP_STRICTNE, which correspond to the
19550:     // === and !== operators. negate is true for !== and false for ===. The strict equality
19550:     // operators produce false if the types of the operands differ, i.e. if only one of 
19550:     // them is a number. 
19576:     if ((flags & CMP_STRICT) && getPromotedType(l) != getPromotedType(r)) {
19550:         x = INS_CONST(negate);
19550:         cond = negate;
19576:     } else if (JSVAL_IS_STRING(l) || JSVAL_IS_STRING(r)) {
21802:         // Comparing equality of a string against null always produces false.
21778:         if (op == LIR_feq &&
21778:             ((JSVAL_IS_NULL(l) && l_ins->isconst()) ||
21802:              (JSVAL_IS_NULL(r) && r_ins->isconst()))) {
19576:             x = INS_CONST(negate);
19576:             cond = negate;
21719:         } else {
21719:             if (!JSVAL_IS_STRING(l) || !JSVAL_IS_STRING(r))
19576:                 ABORT_TRACE("unsupported type for cmp vs string");
21719: 
19576:             LIns* args[] = { r_ins, l_ins };
19576:             if (op == LIR_feq)
20915:                 l_ins = lir->ins_eq0(lir->insCall(&js_EqualStrings_ci, args));
19576:             else
20915:                 l_ins = lir->insCall(&js_CompareStrings_ci, args);
19576:             r_ins = lir->insImm(0);
19576:             cond = evalCmp(op, JSVAL_TO_STRING(l), JSVAL_TO_STRING(r));
18017:         }
18082:     } else if (isNumber(l) || isNumber(r)) {
18186:         jsval tmp[2] = {l, r};
18186:         JSAutoTempValueRooter tvr(cx, 2, tmp);
18182:         
20392:         fp = true;
20392: 
18017:         // TODO: coerce non-numbers to numbers if it's not string-on-string above
18082:         jsdouble lnum;
18082:         jsdouble rnum;
18663:         LIns* args[] = { l_ins, cx_ins };
19576:         if (l == JSVAL_NULL && l_ins->isconst()) {
19550:             jsdpun u;
21801:             u.d = (op == LIR_feq) ? js_NaN : 0.0;
19550:             l_ins = lir->insImmq(u.u64);
19550:         } else if (JSVAL_IS_STRING(l)) {
20915:             l_ins = lir->insCall(&js_StringToNumber_ci, args);
18082:         } else if (JSVAL_TAG(l) == JSVAL_BOOLEAN) {
18082:             /*
18082:              * What I really want here is for undefined to be type-specialized
18082:              * differently from real booleans.  Failing that, I want to be able
18082:              * to cmov on quads.  Failing that, I want to have small forward
21719:              * branches.  Failing that, I want to be able to ins_choose on quads
18082:              * without cmov.  Failing that, eat flaming builtin!
18082:              */
21447:             l_ins = lir->insCall(&js_BooleanOrUndefinedToNumber_ci, args);
18082:         } else if (!isNumber(l)) {
18082:             ABORT_TRACE("unsupported LHS type for cmp vs number");
18082:         }
18186:         lnum = js_ValueToNumber(cx, &tmp[0]);
18082: 
19044:         args[0] = r_ins;
18693:         args[1] = cx_ins;
21801:         if (r == JSVAL_NULL && r_ins->isconst()) {
19550:             jsdpun u;
21801:             u.d = (op == LIR_feq) ? js_NaN : 0.0;
19550:             r_ins = lir->insImmq(u.u64);
19550:         } else if (JSVAL_IS_STRING(r)) {
20915:             r_ins = lir->insCall(&js_StringToNumber_ci, args);
18082:         } else if (JSVAL_TAG(r) == JSVAL_BOOLEAN) {
18082:             // See above for the sob story.
21447:             r_ins = lir->insCall(&js_BooleanOrUndefinedToNumber_ci, args);
18082:         } else if (!isNumber(r)) {
18082:             ABORT_TRACE("unsupported RHS type for cmp vs number");
18082:         }
18186:         rnum = js_ValueToNumber(cx, &tmp[1]);
19576:         cond = evalCmp(op, lnum, rnum);
19995:     } else if ((JSVAL_TAG(l) == JSVAL_BOOLEAN) && (JSVAL_TAG(r) == JSVAL_BOOLEAN)) {
18613:         // The well-known values of JSVAL_TRUE and JSVAL_FALSE make this very easy.
18613:         // In particular: JSVAL_TO_BOOLEAN(0) < JSVAL_TO_BOOLEAN(1) so all of these comparisons do
18613:         // the right thing.
19576:         cond = evalCmp(op, l, r);
19995:         // For ==, !=, ===, and !=== the result is magically correct even if undefined (2) is
19995:         // involved. For the relational operations we need some additional cmov magic to make
19995:         // the result always false (since undefined becomes NaN per ECMA and that doesn't
19995:         // compare to anything, even itself). The code for this is emitted a few lines down.
19576:     } else if (JSVAL_IS_OBJECT(l) && JSVAL_IS_OBJECT(r)) {
19576:         if (op != LIR_feq) {
21724:             JS_NOT_REACHED("we should have converted to numbers already");
21724:             return false;
19576:         }
19576:         cond = (l == r); 
18017:     } else {
18082:         ABORT_TRACE("unsupported operand types for cmp");
18017:     }
17758: 
20009:     /* If we didn't generate a constant result yet, then emit the comparison now. */
19576:     if (!x) {
20009:         /* If the result is not a number or it's not a quad, we must use an integer compare. */
20392:         if (!fp) {
19576:             JS_ASSERT(op >= LIR_feq && op <= LIR_fge);
19576:             op = LOpcode(op + (LIR_eq - LIR_feq));
19576:         }
19576:         x = lir->ins2(op, l_ins, r_ins);
19576:         if (negate) {
19576:             x = lir->ins_eq0(x);
19576:             cond = !cond;
19576:         }
19995:         // For boolean comparison we need a bit post-processing to make the result false if
19995:         // either side is undefined.
19995:         if (op != LIR_eq && (JSVAL_TAG(l) == JSVAL_BOOLEAN) && (JSVAL_TAG(r) == JSVAL_BOOLEAN)) {
19995:             x = lir->ins_choose(lir->ins2i(LIR_eq, 
19995:                                            lir->ins2i(LIR_and, 
19995:                                                       lir->ins2(LIR_or, l_ins, r_ins),
19995:                                                       JSVAL_TO_BOOLEAN(JSVAL_VOID)),
19995:                                            JSVAL_TO_BOOLEAN(JSVAL_VOID)),
19995:                                 lir->insImm(JSVAL_TO_BOOLEAN(JSVAL_FALSE)),
19997:                                 x);
20406:             x = lir->ins_eq0(lir->ins_eq0(x));
19995:             if ((l == JSVAL_VOID) || (r == JSVAL_VOID))
19995:                 cond = false;
19995:         }
19576:     }
19576:     
19050:     /* Don't guard if the same path is always taken. */
19576:     if (!x->isconst()) {
18708:         if (flags & CMP_CASE) {
18708:             guard(cond, x, BRANCH_EXIT);
18708:             return true;
18708:         }
18708: 
17413:         /* The interpreter fuses comparisons and the following branch,
17412:            so we have to do that here as well. */
19052:         if (flags & CMP_TRY_BRANCH_AFTER_COND) {
18694:             fuseIf(cx->fp->regs->pc + 1, cond, x);
19050:         }
19052:     } else if (flags & CMP_CASE) {
19052:         return true;
19052:     }
17758: 
17413:     /* We update the stack after the guard. This is safe since
17413:        the guard bails out at the comparison and the interpreter
18680:        will therefore re-execute the comparison. This way the
17413:        value of the condition doesn't have to be calculated and
17413:        saved on the stack in most cases. */
17413:     set(&l, x);
17412:     return true;
17412: }
17412: 
17415: bool
17467: TraceRecorder::unary(LOpcode op)
17467: {
17467:     jsval& v = stackval(-1);
17467:     bool intop = !(op & LIR64);
17467:     if (isNumber(v)) {
17467:         LIns* a = get(&v);
17467:         if (intop)
17469:             a = f2i(a);
17467:         a = lir->ins1(op, a);
17467:         if (intop)
17467:             a = lir->ins1(LIR_i2f, a);
17467:         set(&v, a);
17467:         return true;
17467:     }
17467:     return false;
17467: }
17467: 
17467: bool
17467: TraceRecorder::binary(LOpcode op)
17467: {
17467:     jsval& r = stackval(-1);
17467:     jsval& l = stackval(-2);
21685: 
21685:     if (JSVAL_IS_OBJECT(l) && hasValueOfMethod(l)) {
21685:         if (JSVAL_IS_OBJECT(r) && hasValueOfMethod(r))
21685:             return call_imacro(binary_imacros.obj_obj);
21685:         return call_imacro(binary_imacros.obj_any);
21685:     }
21685:     if (JSVAL_IS_OBJECT(r) && hasValueOfMethod(r))
21685:         return call_imacro(binary_imacros.any_obj);
21685: 
17467:     bool intop = !(op & LIR64);
17467:     LIns* a = get(&l);
17467:     LIns* b = get(&r);
21799: 
21799:     bool leftIsNumber = isNumber(l);
21799:     jsdouble lnum = leftIsNumber ? asNumber(l) : 0;
21799:     
21799:     bool rightIsNumber = isNumber(r);
21799:     jsdouble rnum = rightIsNumber ? asNumber(r) : 0;
21799:     
17910:     if ((op >= LIR_sub && op <= LIR_ush) ||  // sub, mul, (callh), or, xor, (not,) lsh, rsh, ush
17910:         (op >= LIR_fsub && op <= LIR_fdiv)) { // fsub, fmul, fdiv
18693:         LIns* args[2];
17910:         if (JSVAL_IS_STRING(l)) {
17910:             args[0] = a;
18693:             args[1] = cx_ins;
20915:             a = lir->insCall(&js_StringToNumber_ci, args);
21799:             lnum = js_StringToNumber(cx, JSVAL_TO_STRING(l));
21799:             leftIsNumber = true;
17910:         }
17910:         if (JSVAL_IS_STRING(r)) {
17910:             args[0] = b;
18693:             args[1] = cx_ins;
20915:             b = lir->insCall(&js_StringToNumber_ci, args);
21799:             rnum = js_StringToNumber(cx, JSVAL_TO_STRING(r));
21799:             rightIsNumber = true;
17910:         }
17910:     }
21438:     if (JSVAL_TAG(l) == JSVAL_BOOLEAN) {
21438:         LIns* args[] = { a, cx_ins };
21447:         a = lir->insCall(&js_BooleanOrUndefinedToNumber_ci, args);
21799:         lnum = js_BooleanOrUndefinedToNumber(cx, JSVAL_TO_BOOLEAN(l));
21799:         leftIsNumber = true;
20972:     }
21438:     if (JSVAL_TAG(r) == JSVAL_BOOLEAN) {
21438:         LIns* args[] = { b, cx_ins };
21447:         b = lir->insCall(&js_BooleanOrUndefinedToNumber_ci, args);
21799:         rnum = js_BooleanOrUndefinedToNumber(cx, JSVAL_TO_BOOLEAN(r));
21799:         rightIsNumber = true;
21799:     }
21799:     if (leftIsNumber && rightIsNumber) {
17467:         if (intop) {
18693:             LIns *args[] = { a };
20915:             a = lir->insCall(op == LIR_ush ? &js_DoubleToUint32_ci : &js_DoubleToInt32_ci, args);
17469:             b = f2i(b);
17467:         }
21799:         a = alu(op, lnum, rnum, a, b);
17467:         if (intop)
17467:             a = lir->ins1(op == LIR_ush ? LIR_u2f : LIR_i2f, a);
17467:         set(&l, a);
17467:         return true;
17467:     }
17467:     return false;
17467: }
17467: 
18026: JS_STATIC_ASSERT(offsetof(JSObjectOps, newObjectMap) == 0);
18026: 
18026: bool
18026: TraceRecorder::map_is_native(JSObjectMap* map, LIns* map_ins, LIns*& ops_ins, size_t op_offset)
17899: {
18230:     ops_ins = addName(lir->insLoad(LIR_ldp, map_ins, offsetof(JSObjectMap, ops)), "ops");
18230:     LIns* n = lir->insLoad(LIR_ldp, ops_ins, op_offset);
18026: 
18026: #define OP(ops) (*(JSObjectOp*) ((char*)(ops) + op_offset))
18026: 
18026:     if (OP(map->ops) == OP(&js_ObjectOps)) {
18712:         guard(true, addName(lir->ins2(LIR_eq, n, INS_CONSTPTR(OP(&js_ObjectOps))),
18095:                             "guard(native-map)"),
18095:               MISMATCH_EXIT);
17417:         return true;
17417:     }
18026: 
18026: #undef OP
17630:     ABORT_TRACE("non-native map");
17417: }
17417: 
17459: bool
17746: TraceRecorder::test_property_cache(JSObject* obj, LIns* obj_ins, JSObject*& obj2, jsuword& pcval)
17459: {
19093:     jsbytecode* pc = cx->fp->regs->pc;
19093:     JS_ASSERT(*pc != JSOP_INITPROP && *pc != JSOP_SETNAME && *pc != JSOP_SETPROP);
19093: 
18439:     // Mimic the interpreter's special case for dense arrays by skipping up one
18439:     // hop along the proto chain when accessing a named (not indexed) property,
18439:     // typically to find Array.prototype methods.
18026:     JSObject* aobj = obj;
18034:     if (OBJ_IS_DENSE_ARRAY(cx, obj)) {
18026:         aobj = OBJ_GET_PROTO(cx, obj);
18026:         obj_ins = stobj_get_fslot(obj_ins, JSSLOT_PROTO);
18026:     }
18026: 
18230:     LIns* map_ins = lir->insLoad(LIR_ldp, obj_ins, (int)offsetof(JSObject, map));
17899:     LIns* ops_ins;
18026: 
18026:     // Interpreter calls to PROPERTY_CACHE_TEST guard on native object ops
18026:     // (newObjectMap == js_ObjectOps.newObjectMap) which is required to use
18026:     // native objects (those whose maps are scopes), or even more narrow
18026:     // conditions required because the cache miss case will call a particular
18026:     // object-op (js_GetProperty, js_SetProperty).
18026:     //
18026:     // We parameterize using offsetof and guard on match against the hook at
18026:     // the given offset in js_ObjectOps. TraceRecorder::record_JSOP_SETPROP
18026:     // guards the js_SetProperty case.
19093:     uint32 format = js_CodeSpec[*pc].format;
18026:     uint32 mode = JOF_MODE(format);
19090: 
19090:     // No need to guard native-ness of global object.
19090:     JS_ASSERT(OBJ_IS_NATIVE(globalObj));
19090:     if (aobj != globalObj) {
18026:         size_t op_offset = 0;
18026:         if (mode == JOF_PROP || mode == JOF_VARPROP) {
18026:             JS_ASSERT(!(format & JOF_SET));
18026:             op_offset = offsetof(JSObjectOps, getProperty);
18026:         } else {
18026:             JS_ASSERT(mode == JOF_NAME);
18026:         }
18026: 
18026:         if (!map_is_native(aobj->map, map_ins, ops_ins, op_offset))
17459:             return false;
19090:     }
17459: 
17459:     JSAtom* atom;
17746:     JSPropCacheEntry* entry;
19093:     PROPERTY_CACHE_TEST(cx, pc, aobj, obj2, entry, atom);
18439:     if (atom) {
18439:         // Miss: pre-fill the cache for the interpreter, as well as for our needs.
18439:         // FIXME: 452357 - correctly propagate exceptions into the interpreter from
18439:         // js_FindPropertyHelper, js_LookupPropertyWithFlags, and elsewhere.
18439:         jsid id = ATOM_TO_JSID(atom);
17747:         JSProperty* prop;
19093:         if (JOF_OPMODE(*pc) == JOF_NAME) {
18112:             JS_ASSERT(aobj == obj);
17878:             if (js_FindPropertyHelper(cx, id, &obj, &obj2, &prop, &entry) < 0)
17747:                 ABORT_TRACE("failed to find name");
17747:         } else {
19712:             int protoIndex = js_LookupPropertyWithFlags(cx, aobj, id,
19712:                                                         cx->resolveFlags,
19712:                                                         &obj2, &prop);
17878:             if (protoIndex < 0)
17747:                 ABORT_TRACE("failed to lookup property");
17878: 
17998:             if (prop) {
19020:                 js_FillPropertyCache(cx, aobj, OBJ_SHAPE(aobj), 0, protoIndex, obj2,
18439:                                      (JSScopeProperty*) prop, &entry);
17998:             }
17998:         }
17998: 
17998:         if (!prop) {
17998:             // Propagate obj from js_FindPropertyHelper to record_JSOP_BINDNAME
18712:             // via our obj2 out-parameter. If we are recording JSOP_SETNAME and
18712:             // the global it's assigning does not yet exist, create it.
17998:             obj2 = obj;
19093: 
18712:             // Use PCVAL_NULL to return "no such property" to our caller.
17998:             pcval = PCVAL_NULL;
19994:             ABORT_TRACE("failed to find property");
17998:         }
17998: 
17998:         OBJ_DROP_PROPERTY(cx, obj2, prop);
18439:         if (!entry)
17878:             ABORT_TRACE("failed to fill property cache");
17998:     }
17878: 
18439: #ifdef JS_THREADSAFE
18439:     // There's a potential race in any JS_THREADSAFE embedding that's nuts
18439:     // enough to share mutable objects on the scope or proto chain, but we
18439:     // don't care about such insane embeddings. Anyway, the (scope, proto)
18439:     // entry->vcap coordinates must reach obj2 from aobj at this point.
18439:     JS_ASSERT(cx->requestDepth);
18439: #endif
18439: 
18439:     // Emit guard(s), common code for both hit and miss cases.
18439:     // Check for first-level cache hit and guard on kshape if possible.
18439:     // Otherwise guard on key object exact match.
18061:     if (PCVCAP_TAG(entry->vcap) <= 1) {
18112:         if (aobj != globalObj) {
18286:             LIns* shape_ins = addName(lir->insLoad(LIR_ld, map_ins, offsetof(JSScope, shape)),
18286:                                       "shape");
18439:             guard(true, addName(lir->ins2i(LIR_eq, shape_ins, entry->kshape), "guard(kshape)"),
18053:                   MISMATCH_EXIT);
18112:         }
18061:     } else {
18680: #ifdef DEBUG
19093:         JSOp op = JSOp(*pc);
18689:         ptrdiff_t pcoff = (op == JSOP_GETARGPROP) ? ARGNO_LEN :
18689:                           (op == JSOP_GETLOCALPROP) ? SLOTNO_LEN : 0;
19093:         jsatomid index = js_GetIndexFromBytecode(cx, cx->fp->script, pc, pcoff);
18680:         JS_ASSERT(entry->kpc == (jsbytecode*) atoms[index]);
18112:         JS_ASSERT(entry->kshape == jsuword(aobj));
18680: #endif
18439:         if (aobj != globalObj) {
18439:             guard(true, addName(lir->ins2i(LIR_eq, obj_ins, entry->kshape), "guard(kobj)"),
18439:                   MISMATCH_EXIT);
18439:         }
18439:     }
18439: 
21685:     // For any hit that goes up the scope and/or proto chains, we will need to
18439:     // guard on the shape of the object containing the property.
18061:     if (PCVCAP_TAG(entry->vcap) >= 1) {
18439:         jsuword vcap = entry->vcap;
18439:         uint32 vshape = PCVCAP_SHAPE(vcap);
19020:         JS_ASSERT(OBJ_SHAPE(obj2) == vshape);
18439: 
20979:         LIns* obj2_ins;
20979:         if (PCVCAP_TAG(entry->vcap) == 1) {
20979:             // Duplicate the special case in PROPERTY_CACHE_TEST.
20979:             obj2_ins = stobj_get_fslot(obj_ins, JSSLOT_PROTO);
20979:             guard(false, lir->ins_eq0(obj2_ins), MISMATCH_EXIT);
20979:         } else {
20979:             obj2_ins = INS_CONSTPTR(obj2);
20979:         }
18230:         map_ins = lir->insLoad(LIR_ldp, obj2_ins, (int)offsetof(JSObject, map));
18439:         if (!map_is_native(obj2->map, map_ins, ops_ins))
17878:             return false;
18439: 
18439:         LIns* shape_ins = addName(lir->insLoad(LIR_ld, map_ins, offsetof(JSScope, shape)),
18439:                                   "shape");
18086:         guard(true,
18439:               addName(lir->ins2i(LIR_eq, shape_ins, vshape), "guard(vshape)"),
18086:               MISMATCH_EXIT);
17878:     }
17747: 
18439:     pcval = entry->vword;
17459:     return true;
17459: }
17459: 
17541: bool
17541: TraceRecorder::test_property_cache_direct_slot(JSObject* obj, LIns* obj_ins, uint32& slot)
17541: {
17541:     JSObject* obj2;
17746:     jsuword pcval;
17541: 
17541:     /*
17541:      * Property cache ensures that we are dealing with an existing property,
17541:      * and guards the shape for us.
17541:      */
17746:     if (!test_property_cache(obj, obj_ins, obj2, pcval))
17541:         return false;
17541: 
17998:     /* No such property means invalid slot, which callers must check for first. */
17998:     if (PCVAL_IS_NULL(pcval)) {
17998:         slot = SPROP_INVALID_SLOT;
17998:         return true;
17998:     }
17998: 
19558:     /* Insist on obj being the directly addressed object. */
19558:     if (obj2 != obj)
19558:         ABORT_TRACE("test_property_cache_direct_slot hit prototype chain");
17541: 
17969:     /* Don't trace getter or setter calls, our caller wants a direct slot. */
17746:     if (PCVAL_IS_SPROP(pcval)) {
17746:         JSScopeProperty* sprop = PCVAL_TO_SPROP(pcval);
17545: 
19558:         uint32 setflags = (js_CodeSpec[*cx->fp->regs->pc].format & (JOF_SET | JOF_INCDEC));
18115:         if (setflags && !SPROP_HAS_STUB_SETTER(sprop))
17630:             ABORT_TRACE("non-stub setter");
18115:         if (setflags != JOF_SET && !SPROP_HAS_STUB_GETTER(sprop))
17969:             ABORT_TRACE("non-stub getter");
17545:         if (!SPROP_HAS_VALID_SLOT(sprop, OBJ_SCOPE(obj)))
17630:             ABORT_TRACE("no valid slot");
17545:         slot = sprop->slot;
17545:     } else {
17746:         if (!PCVAL_IS_SLOT(pcval))
17630:             ABORT_TRACE("PCE is not a slot");
17746:         slot = PCVAL_TO_SLOT(pcval);
17545:     }
17541:     return true;
17541: }
17541: 
17429: void
17429: TraceRecorder::stobj_set_slot(LIns* obj_ins, unsigned slot, LIns*& dslots_ins, LIns* v_ins)
17426: {
17487:     if (slot < JS_INITIAL_NSLOTS) {
17721:         addName(lir->insStorei(v_ins, obj_ins,
17721:                                offsetof(JSObject, fslots) + slot * sizeof(jsval)),
17721:                 "set_slot(fslots)");
17487:     } else {
17429:         if (!dslots_ins)
18230:             dslots_ins = lir->insLoad(LIR_ldp, obj_ins, offsetof(JSObject, dslots));
17721:         addName(lir->insStorei(v_ins, dslots_ins,
17721:                                (slot - JS_INITIAL_NSLOTS) * sizeof(jsval)),
17721:                 "set_slot(dslots");
17429:     }
17426: }
17426: 
17459: LIns*
17899: TraceRecorder::stobj_get_fslot(LIns* obj_ins, unsigned slot)
17899: {
17899:     JS_ASSERT(slot < JS_INITIAL_NSLOTS);
18230:     return lir->insLoad(LIR_ldp, obj_ins, offsetof(JSObject, fslots) + slot * sizeof(jsval));
17899: }
17899: 
17899: LIns*
17459: TraceRecorder::stobj_get_slot(LIns* obj_ins, unsigned slot, LIns*& dslots_ins)
17426: {
17899:     if (slot < JS_INITIAL_NSLOTS)
17899:         return stobj_get_fslot(obj_ins, slot);
17459: 
17429:     if (!dslots_ins)
18230:         dslots_ins = lir->insLoad(LIR_ldp, obj_ins, offsetof(JSObject, dslots));
18230:     return lir->insLoad(LIR_ldp, dslots_ins, (slot - JS_INITIAL_NSLOTS) * sizeof(jsval));
17426: }
17426: 
17426: bool
17429: TraceRecorder::native_set(LIns* obj_ins, JSScopeProperty* sprop, LIns*& dslots_ins, LIns* v_ins)
17426: {
17426:     if (SPROP_HAS_STUB_SETTER(sprop) && sprop->slot != SPROP_INVALID_SLOT) {
17429:         stobj_set_slot(obj_ins, sprop->slot, dslots_ins, v_ins);
17429:         return true;
17426:     }
17721:     ABORT_TRACE("unallocated or non-stub sprop");
17426: }
17426: 
17426: bool
17429: TraceRecorder::native_get(LIns* obj_ins, LIns* pobj_ins, JSScopeProperty* sprop,
17429:         LIns*& dslots_ins, LIns*& v_ins)
17426: {
17459:     if (!SPROP_HAS_STUB_GETTER(sprop))
17459:         return false;
17459: 
17426:     if (sprop->slot != SPROP_INVALID_SLOT)
17459:         v_ins = stobj_get_slot(pobj_ins, sprop->slot, dslots_ins);
17426:     else
18687:         v_ins = INS_CONST(JSVAL_TO_BOOLEAN(JSVAL_VOID));
17426:     return true;
17426: }
17426: 
18001: // So box_jsval can emit no LIR_or at all to tag an object jsval.
18001: JS_STATIC_ASSERT(JSVAL_OBJECT == 0);
18001: 
17460: bool
17468: TraceRecorder::box_jsval(jsval v, LIns*& v_ins)
17468: {
17470:     if (isNumber(v)) {
17477:         LIns* args[] = { v_ins, cx_ins };
20915:         v_ins = lir->insCall(&js_BoxDouble_ci, args);
18680:         guard(false, lir->ins2(LIR_eq, v_ins, INS_CONST(JSVAL_ERROR_COOKIE)),
17850:               OOM_EXIT);
17468:         return true;
17468:     }
17470:     switch (JSVAL_TAG(v)) {
17470:       case JSVAL_BOOLEAN:
18645:         v_ins = lir->ins2i(LIR_pior, lir->ins2i(LIR_pilsh, v_ins, JSVAL_TAGBITS), JSVAL_BOOLEAN);
17470:         return true;
18001:       case JSVAL_OBJECT:
18001:         return true;
17870:       case JSVAL_STRING:
18645:         v_ins = lir->ins2(LIR_pior, v_ins, INS_CONST(JSVAL_STRING));
17870:         return true;
17470:     }
17470:     return false;
17470: }
17468: 
17468: bool
17460: TraceRecorder::unbox_jsval(jsval v, LIns*& v_ins)
17460: {
17470:     if (isNumber(v)) {
17470:         // JSVAL_IS_NUMBER(v)
17758:         guard(false,
18645:               lir->ins_eq0(lir->ins2(LIR_pior,
18648:                                      lir->ins2(LIR_piand, v_ins, INS_CONST(JSVAL_INT)),
17758:                                      lir->ins2i(LIR_eq,
18232:                                                 lir->ins2(LIR_piand, v_ins,
18648:                                                           INS_CONST(JSVAL_TAGMASK)),
18095:                                                 JSVAL_DOUBLE))),
18095:               MISMATCH_EXIT);
18693:         LIns* args[] = { v_ins };
20915:         v_ins = lir->insCall(&js_UnboxDouble_ci, args);
17460:         return true;
17460:     }
17470:     switch (JSVAL_TAG(v)) {
17470:       case JSVAL_BOOLEAN:
17541:         guard(true,
17541:               lir->ins2i(LIR_eq,
18648:                          lir->ins2(LIR_piand, v_ins, INS_CONST(JSVAL_TAGMASK)),
18095:                          JSVAL_BOOLEAN),
18095:               MISMATCH_EXIT);
17470:          v_ins = lir->ins2i(LIR_ush, v_ins, JSVAL_TAGBITS);
17470:          return true;
17630:        case JSVAL_OBJECT:
17630:         guard(true,
17630:               lir->ins2i(LIR_eq,
18648:                          lir->ins2(LIR_piand, v_ins, INS_CONST(JSVAL_TAGMASK)),
18095:                          JSVAL_OBJECT),
18095:               MISMATCH_EXIT);
17630:         return true;
17870:       case JSVAL_STRING:
17870:         guard(true,
17870:               lir->ins2i(LIR_eq,
18648:                         lir->ins2(LIR_piand, v_ins, INS_CONST(JSVAL_TAGMASK)),
18095:                         JSVAL_STRING),
18095:               MISMATCH_EXIT);
18645:         v_ins = lir->ins2(LIR_piand, v_ins, INS_CONST(~JSVAL_TAGMASK));
17870:         return true;
17470:     }
17470:     return false;
17470: }
17460: 
17630: bool
17688: TraceRecorder::getThis(LIns*& this_ins)
17688: {
17688:     if (cx->fp->callee) { /* in a function */
17688:         if (JSVAL_IS_NULL(cx->fp->argv[-1]))
17688:             return false;
17688:         this_ins = get(&cx->fp->argv[-1]);
18053:         guard(false, lir->ins_eq0(this_ins), MISMATCH_EXIT);
17688:     } else { /* in global code */
18286:         this_ins = scopeChain();
17688:     }
17688:     return true;
17688: }
17688: 
17688: bool
21685: TraceRecorder::guardClass(JSObject* obj, LIns* obj_ins, JSClass* clasp, ExitType exitType)
17899: {
20974:     bool cond = STOBJ_GET_CLASS(obj) == clasp;
17899: 
19020:     LIns* class_ins = lir->insLoad(LIR_ldp, obj_ins, offsetof(JSObject, classword));
18645:     class_ins = lir->ins2(LIR_piand, class_ins, lir->insImm(~3));
17899: 
17899:     char namebuf[32];
17899:     JS_snprintf(namebuf, sizeof namebuf, "guard(class is %s)", clasp->name);
21685:     guard(cond, addName(lir->ins2(LIR_eq, class_ins, INS_CONSTPTR(clasp)), namebuf), exitType);
20974:     return cond;
17630: }
17630: 
17926: bool
21685: TraceRecorder::guardDenseArray(JSObject* obj, LIns* obj_ins, ExitType exitType)
21685: {
21685:     return guardClass(obj, obj_ins, &js_ArrayClass, exitType);
17899: }
17899: 
17926: bool
17926: TraceRecorder::guardDenseArrayIndex(JSObject* obj, jsint idx, LIns* obj_ins,
20972:                                     LIns* dslots_ins, LIns* idx_ins, ExitType exitType)
17429: {
17908:     jsuint length = ARRAY_DENSE_LENGTH(obj);
21083: 
21083:     bool cond = (jsuint(idx) < jsuint(obj->fslots[JSSLOT_ARRAY_LENGTH]) && jsuint(idx) < length);
21083:     if (cond) {
21083:         /* Guard array length */
21083:         LIns* exit = guard(true,
21083:                            lir->ins2(LIR_ult, idx_ins, stobj_get_fslot(obj_ins, JSSLOT_ARRAY_LENGTH)),
21083:                            exitType)->oprnd2();
21083:         /* dslots must not be NULL */
21083:         guard(false,
21083:               lir->ins_eq0(dslots_ins),
21083:               exit);
21083:         /* Guard array capacity */
21083:         guard(true,
21083:               lir->ins2(LIR_ult,
21083:                         idx_ins,
21083:                         lir->insLoad(LIR_ldp, dslots_ins, 0 - (int)sizeof(jsval))),
21083:               exit);
21083:     } else {
21083:         /* If not idx < length, stay on trace (and read value as undefined). */ 
21083:         LIns* br1 = lir->insBranch(LIR_jf, 
21083:                                    lir->ins2(LIR_ult, 
21083:                                              idx_ins, 
21083:                                              stobj_get_fslot(obj_ins, JSSLOT_ARRAY_LENGTH)),
21083:                                    NULL);
21083:         /* If dslots is NULL, stay on trace (and read value as undefined). */
21083:         LIns* br2 = lir->insBranch(LIR_jt, lir->ins_eq0(dslots_ins), NULL);
21083:         /* If not idx < capacity, stay on trace (and read value as undefined). */
21083:         LIns* br3 = lir->insBranch(LIR_jf,
21083:                                    lir->ins2(LIR_ult,
21083:                                              idx_ins,
21083:                                              lir->insLoad(LIR_ldp, dslots_ins, 0 - (int)sizeof(jsval))),
21083:                                    NULL);
21083:         lir->insGuard(LIR_x, lir->insImm(1), snapshot(exitType));
21083:         LIns* label = lir->ins0(LIR_label);
21083:         br1->target(label);
21083:         br2->target(label);
21083:         br3->target(label);
21083:     }
20974:     return cond;    
17427: }
17427: 
19983: /*
19983:  * Guard that a computed property access via an element op (JSOP_GETELEM, etc.)
19983:  * does not find an alias to a global variable, or a property without a slot,
19983:  * or a slot-ful property with a getter or setter (depending on op_offset in
19983:  * JSObjectOps). Finally, beware resolve hooks mutating objects. Oh, and watch
19983:  * out for bears too ;-).
19983:  *
19983:  * One win here is that we do not need to generate a guard that obj_ins does
19983:  * not result in the global object on trace, because we guard on shape and rule
19983:  * out obj's shape being the global object's shape at recording time. This is
19983:  * safe because the global shape cannot change on trace.
19983:  */
19983: bool
19983: TraceRecorder::guardElemOp(JSObject* obj, LIns* obj_ins, jsid id, size_t op_offset, jsval* vp)
19983: {
20017:     LIns* map_ins = lir->insLoad(LIR_ldp, obj_ins, (int)offsetof(JSObject, map));
20017:     LIns* ops_ins;
20017:     if (!map_is_native(obj->map, map_ins, ops_ins, op_offset))
20017:         return false;
20017: 
19983:     uint32 shape = OBJ_SHAPE(obj);
19984:     if (JSID_IS_ATOM(id) && shape == traceMonitor->globalShape)
19983:         ABORT_TRACE("elem op probably aliases global");
19983: 
19983:     JSObject* pobj;
19983:     JSProperty* prop;
19983:     if (!js_LookupProperty(cx, obj, id, &pobj, &prop))
19983:         return false;
19983: 
19983:     if (vp)
19983:         *vp = JSVAL_VOID;
19983:     if (prop) {
19983:         bool traceable_slot = true;
19983:         if (pobj == obj) {
19983:             JSScopeProperty* sprop = (JSScopeProperty*) prop;
19983:             traceable_slot = ((op_offset == offsetof(JSObjectOps, getProperty))
19983:                               ? SPROP_HAS_STUB_GETTER(sprop)
19983:                               : SPROP_HAS_STUB_SETTER(sprop)) &&
19983:                              SPROP_HAS_VALID_SLOT(sprop, OBJ_SCOPE(obj));
19983:             if (vp && traceable_slot)
19983:                 *vp = LOCKED_OBJ_GET_SLOT(obj, sprop->slot);
19983:         }
19983: 
19983:         OBJ_DROP_PROPERTY(cx, pobj, prop);
19983:         if (pobj != obj)
19983:             ABORT_TRACE("elem op hit prototype property, can't shape-guard");
19983:         if (!traceable_slot)
19983:             ABORT_TRACE("elem op hit direct and slotless getter or setter");
19983:     }
19983: 
19983:     // If we got this far, we're almost safe -- but we must check for a rogue resolve hook.
19983:     if (OBJ_SHAPE(obj) != shape)
19983:         ABORT_TRACE("resolve hook mutated elem op base object");
19983: 
19983:     LIns* shape_ins = addName(lir->insLoad(LIR_ld, map_ins, offsetof(JSScope, shape)), "shape");
19983:     guard(true, addName(lir->ins2i(LIR_eq, shape_ins, shape), "guard(shape)"), MISMATCH_EXIT);
19983:     return true;
19983: }
19983: 
17818: void
17818: TraceRecorder::clearFrameSlotsFromCache()
17818: {
17815:     /* Clear out all slots of this frame in the nativeFrameTracker. Different locations on the
17811:        VM stack might map to different locations on the native stack depending on the
17811:        number of arguments (i.e.) of the next call, so we have to make sure we map
17811:        those in to the cache with the right offsets. */
17811:     JSStackFrame* fp = cx->fp;
17811:     jsval* vp;
17811:     jsval* vpstop;
18136:     if (fp->callee) {
18187:         vp = &fp->argv[-2];
18425:         vpstop = &fp->argv[fp->fun->nargs];
18136:         while (vp < vpstop)
18136:             nativeFrameTracker.set(vp++, (LIns*)0);
18136:     }
18136:     vp = &fp->slots[0];
18136:     vpstop = &fp->slots[fp->script->nslots];
18136:     while (vp < vpstop)
18136:         nativeFrameTracker.set(vp++, (LIns*)0);
17818: }
17818: 
17818: bool
17818: TraceRecorder::record_EnterFrame()
17818: {
19078:     JSStackFrame* fp = cx->fp;
19078: 
17852:     if (++callDepth >= MAX_CALLDEPTH)
17852:         ABORT_TRACE("exceeded maximum call depth");
20899:     // FIXME: Allow and attempt to inline a single level of recursion until we compile 
20899:     //        recursive calls as independent trees (459301).
20899:     if (fp->script == fp->down->script && fp->down->down && fp->down->down->script == fp->script)
19078:         ABORT_TRACE("recursive call");
19078:     
18260:     debug_only_v(printf("EnterFrame %s, callDepth=%d\n",
18140:                         js_AtomToPrintableString(cx, cx->fp->fun->atom),
21685:                         callDepth);)
18687:     LIns* void_ins = INS_CONST(JSVAL_TO_BOOLEAN(JSVAL_VOID));
18119: 
18119:     jsval* vp = &fp->argv[fp->argc];
19567:     jsval* vpstop = vp + ptrdiff_t(fp->fun->nargs) - ptrdiff_t(fp->argc);
21526:     if (applyingArguments) {
21526:         applyingArguments = false;
21526:         while (vp < vpstop) {
21526:             JS_ASSERT(vp >= fp->down->regs->sp);
21526:             nativeFrameTracker.set(vp, (LIns*)0);
21526:             LIns* arg_ins = get(&fp->down->argv[fp->argc + (vp - vpstop)]);
21526:             set(vp++, arg_ins, true);
21526:         }
21526:     } else {
18119:         while (vp < vpstop) {
18119:             if (vp >= fp->down->regs->sp)
18119:                 nativeFrameTracker.set(vp, (LIns*)0);
18119:             set(vp++, void_ins, true);
18119:         }
21526:     }
18119: 
18119:     vp = &fp->slots[0];
18119:     vpstop = vp + fp->script->nfixed;
18119:     while (vp < vpstop)
18119:         set(vp++, void_ins, true);
17811:     return true;
17811: }
17818: 
17818: bool
17818: TraceRecorder::record_LeaveFrame()
17818: {
18260:     debug_only_v(
18142:         if (cx->fp->fun)
18150:             printf("LeaveFrame (back to %s), callDepth=%d\n",
18140:                    js_AtomToPrintableString(cx, cx->fp->fun->atom),
18140:                    callDepth);
18260:         );
17818:     if (callDepth-- <= 0)
18286:         ABORT_TRACE("returned out of a loop we started tracing");
18001: 
18001:     // LeaveFrame gets called after the interpreter popped the frame and
18001:     // stored rval, so cx->fp not cx->fp->down, and -1 not 0.
17818:     atoms = cx->fp->script->atomMap.vector;
18150:     set(&stackval(-1), rval_ins, true);
17818:     return true;
17805: }
17805: 
21685: bool
21685: TraceRecorder::record_JSOP_INTERRUPT()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_PUSH()
17409: {
18687:     stack(0, INS_CONST(JSVAL_TO_BOOLEAN(JSVAL_VOID)));
17456:     return true;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_POPV()
17926: {
20907:     jsval& rval = stackval(-1);
20907:     LIns *rval_ins = get(&rval);
20907:     if (!box_jsval(rval, rval_ins))
20907:         return false;
20907: 
20907:     // Store it in cx->fp->rval. NB: Tricky dependencies. cx->fp is the right
20907:     // frame because POPV appears only in global and eval code and we don't
20907:     // trace JSOP_EVAL or leaving the frame where tracing started.
20907:     LIns *fp_ins = lir->insLoad(LIR_ldp, cx_ins, offsetof(JSContext, fp));
20907:     lir->insStorei(rval_ins, fp_ins, offsetof(JSStackFrame, rval));
20907:     return true;
17409: }
17926: 
21685: bool
21685: TraceRecorder::record_JSOP_ENTERWITH()
21685: {
21685:     return false;
21685: }
21685: 
21685: bool
21685: TraceRecorder::record_JSOP_LEAVEWITH()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_RETURN()
17409: {
18001:     jsval& rval = stackval(-1);
18661:     JSStackFrame *fp = cx->fp;
18785:     if ((cx->fp->flags & JSFRAME_CONSTRUCTING) && JSVAL_IS_PRIMITIVE(rval)) {
18661:         JS_ASSERT(OBJECT_TO_JSVAL(fp->thisp) == fp->argv[-1]);
18661:         rval_ins = get(&fp->argv[-1]);
18661:     } else {
18661:         rval_ins = get(&rval);
18001:     }
21685:     debug_only_v(printf("returning from %s\n", js_AtomToPrintableString(cx, cx->fp->fun->atom));)
17818:     clearFrameSlotsFromCache();
17689:     return true;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_GOTO()
17409: {
17577:     return true;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_IFEQ()
17409: {
18694:     trackCfgMerges(cx->fp->regs->pc);
20416:     return ifop();
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_IFNE()
17409: {
20416:     return ifop();
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_ARGUMENTS()
17409: {
19978: #if 1
19978:     ABORT_TRACE("can't trace arguments yet");
19978: #else
19068:     LIns* args[] = { cx_ins };
20915:     LIns* a_ins = lir->insCall(&js_Arguments_ci, args);
19068:     guard(false, lir->ins_eq0(a_ins), OOM_EXIT);
19068:     stack(0, a_ins);
19068:     return true;
19978: #endif
17409: }
17899: 
17926: bool
17926: TraceRecorder::record_JSOP_DUP()
17409: {
17448:     stack(0, get(&stackval(-1)));
17448:     return true;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_DUP2()
17409: {
17448:     stack(0, get(&stackval(-2)));
17448:     stack(1, get(&stackval(-1)));
17448:     return true;
17409: }
17926: 
17926: bool
21685: TraceRecorder::record_JSOP_SWAP()
21685: {
21685:     jsval& l = stackval(-2);
21685:     jsval& r = stackval(-1);
21685:     LIns* l_ins = get(&l);
21685:     LIns* r_ins = get(&r);
21685:     set(&r, l_ins);
21685:     set(&l, r_ins);
21685:     return true;
21685: }
21685: 
21685: bool
17926: TraceRecorder::record_JSOP_SETCONST()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_BITOR()
17409: {
17469:     return binary(LIR_or);
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_BITXOR()
17409: {
17469:     return binary(LIR_xor);
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_BITAND()
17409: {
17469:     return binary(LIR_and);
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_EQ()
17409: {
19576:     return cmp(LIR_feq, CMP_TRY_BRANCH_AFTER_COND);
18687: }
18687: 
17926: bool
17926: TraceRecorder::record_JSOP_NE()
17409: {
19576:     return cmp(LIR_feq, CMP_NEGATE | CMP_TRY_BRANCH_AFTER_COND);
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_LT()
17409: {
18687:     return cmp(LIR_flt, CMP_TRY_BRANCH_AFTER_COND);
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_LE()
17409: {
18687:     return cmp(LIR_fle, CMP_TRY_BRANCH_AFTER_COND);
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_GT()
17409: {
18687:     return cmp(LIR_fgt, CMP_TRY_BRANCH_AFTER_COND);
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_GE()
17409: {
18687:     return cmp(LIR_fge, CMP_TRY_BRANCH_AFTER_COND);
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_LSH()
17409: {
17469:     return binary(LIR_lsh);
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_RSH()
17409: {
17469:     return binary(LIR_rsh);
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_URSH()
17409: {
17469:     return binary(LIR_ush);
17409: }
17926: 
21685: static struct {
21685:     jsbytecode obj_any[10];
21685:     jsbytecode any_obj[8];
21685:     jsbytecode obj_obj[16];
21685: } add_imacros = {
21685:     {
21685:         JSOP_SWAP,
21685:         JSOP_CALLPROP, 0, COMMON_ATOM_INDEX(toString),
21685:         JSOP_CALL, 0, 0,
21685:         JSOP_SWAP,
21685:         JSOP_ADD,
21685:         JSOP_STOP
21685:     },
21685: 
21685:     {
21685:         JSOP_CALLPROP, 0, COMMON_ATOM_INDEX(toString),
21685:         JSOP_CALL, 0, 0,
21685:         JSOP_ADD,
21685:         JSOP_STOP
21685:     },
21685: 
21685:     {
21685:         JSOP_SWAP,
21685:         JSOP_CALLPROP, 0, COMMON_ATOM_INDEX(toString),
21685:         JSOP_CALL, 0, 0,
21685:         JSOP_SWAP,
21685:         JSOP_CALLPROP, 0, COMMON_ATOM_INDEX(toString),
21685:         JSOP_CALL, 0, 0,
21685:         JSOP_ADD,
21685:         JSOP_STOP
21685:     }
21685: };
21685: 
21685: JS_STATIC_ASSERT(sizeof(add_imacros) < IMACRO_PC_ADJ_LIMIT);
21685: 
17926: bool
17926: TraceRecorder::record_JSOP_ADD()
17409: {
17872:     jsval& r = stackval(-1);
17872:     jsval& l = stackval(-2);
21685: 
21685:     if (JSVAL_IS_OBJECT(l) && hasToStringMethod(l)) {
21685:         if (JSVAL_IS_OBJECT(r) && hasToStringMethod(r))
21685:             return call_imacro(add_imacros.obj_obj);
21685:         return call_imacro(add_imacros.obj_any);
21685:     }
21685:     if (JSVAL_IS_OBJECT(r) && hasToStringMethod(r))
21685:         return call_imacro(add_imacros.any_obj);
21685: 
21447:     if (JSVAL_IS_STRING(l) || JSVAL_IS_STRING(r)) {
21685:         LIns* args[] = { stringify(r), stringify(l), cx_ins };
21447:         if (!args[0] || !args[1])
21447:             ABORT_TRACE("can't stringify objects");
20915:         LIns* concat = lir->insCall(&js_ConcatStrings_ci, args);
17873:         guard(false, lir->ins_eq0(concat), OOM_EXIT);
17872:         set(&l, concat);
17872:         return true;
17872:     }
21685: 
17469:     return binary(LIR_fadd);
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_SUB()
17409: {
17469:     return binary(LIR_fsub);
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_MUL()
17409: {
17469:     return binary(LIR_fmul);
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_DIV()
17409: {
17469:     return binary(LIR_fdiv);
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_MOD()
17409: {
17683:     jsval& r = stackval(-1);
17683:     jsval& l = stackval(-2);
21685: 
21685:     if (JSVAL_IS_OBJECT(l) && hasValueOfMethod(l)) {
21685:         if (JSVAL_IS_OBJECT(r) && hasValueOfMethod(r))
21685:             return call_imacro(binary_imacros.obj_obj);
21685:         return call_imacro(binary_imacros.obj_any);
21685:     }
21685:     if (JSVAL_IS_OBJECT(r) && hasValueOfMethod(r))
21685:         return call_imacro(binary_imacros.any_obj);
21685: 
17683:     if (isNumber(l) && isNumber(r)) {
19974:         LIns* l_ins = get(&l);
19974:         LIns* r_ins = get(&r);
19974:         LIns* x;
19974:         /* We can't demote this in a filter since we need the actual values of l and r. */
19974:         if (isPromote(l_ins) && isPromote(r_ins) && asNumber(l) >= 0 && asNumber(r) > 0) {
19974:             LIns* args[] = { ::demote(lir, r_ins), ::demote(lir, l_ins) };
20915:             x = lir->insCall(&js_imod_ci, args);
19974:             guard(false, lir->ins2(LIR_eq, x, lir->insImm(-1)), BRANCH_EXIT);
19974:             x = lir->ins1(LIR_i2f, x);
19974:         } else {
19974:             LIns* args[] = { r_ins, l_ins };
20915:             x = lir->insCall(&js_dmod_ci, args);
19974:         }
19974:         set(&l, x);
17683:         return true;
17683:     }
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_NOT()
17409: {
17436:     jsval& v = stackval(-1);
19070:     if (JSVAL_TAG(v) == JSVAL_BOOLEAN) {
19070:         set(&v, lir->ins_eq0(lir->ins2i(LIR_eq, get(&v), 1)));
19070:         return true;
19070:     } 
19554:     if (isNumber(v)) {
21776:         LIns* v_ins = get(&v);
21776:         set(&v, lir->ins2(LIR_or, lir->ins2(LIR_feq, v_ins, lir->insImmq(0)),
21776:                                   lir->ins_eq0(lir->ins2(LIR_feq, v_ins, v_ins))));
19554:         return true;
19554:     } 
19554:     if (JSVAL_IS_OBJECT(v)) {
19554:         set(&v, lir->ins_eq0(get(&v)));
18769:         return true;
18769:     }
20435:     JS_ASSERT(JSVAL_IS_STRING(v));
19580:     set(&v, lir->ins_eq0(lir->ins2(LIR_piand, 
19580:                                    lir->insLoad(LIR_ldp, get(&v), (int)offsetof(JSString, length)),
19580:                                    INS_CONSTPTR(JSSTRING_LENGTH_MASK))));
17436:     return true;
17436: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_BITNOT()
17409: {
17469:     return unary(LIR_not);
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_NEG()
17409: {
18787:     jsval& v = stackval(-1);
18787:     if (isNumber(v)) {
18787:         LIns* a = get(&v);
18787: 
18787:         /* If we're a promoted integer, we have to watch out for 0s since -0 is a double.
18787:            Only follow this path if we're not an integer that's 0 and we're not a double 
18787:            that's zero.
18787:          */
18787:         if (isPromoteInt(a) &&
18787:             (!JSVAL_IS_INT(v) || JSVAL_TO_INT(v) != 0) &&
18787:             (!JSVAL_IS_DOUBLE(v) || !JSDOUBLE_IS_NEGZERO(*JSVAL_TO_DOUBLE(v))))  {
18787:             a = lir->ins1(LIR_neg, ::demote(lir, a));
18787:             lir->insGuard(LIR_xt, lir->ins1(LIR_ov, a), snapshot(OVERFLOW_EXIT));
18787:             lir->insGuard(LIR_xt, lir->ins2(LIR_eq, a, lir->insImm(0)), snapshot(OVERFLOW_EXIT));
18787:             a = lir->ins1(LIR_i2f, a);
18787:         } else {
18787:             a = lir->ins1(LIR_fneg, a);
18787:         }
18787: 
18787:         set(&v, a);
18787:         return true;
18787:     }
18787:     return false;
17409: }
17926: 
18300: JSBool
18300: js_Array(JSContext* cx, JSObject* obj, uintN argc, jsval* argv, jsval* rval);
18300: 
18712: JSBool
18712: js_Object(JSContext *cx, JSObject *obj, uintN argc, jsval *argv, jsval *rval);
18712: 
20402: JSBool
20402: js_Date(JSContext *cx, JSObject *obj, uintN argc, jsval *argv, jsval *rval);
20402: 
21526: JSBool
21526: js_fun_apply(JSContext* cx, uintN argc, jsval* vp);
21526: 
21526: bool
21526: TraceRecorder::functionCall(bool constructing)
20431: {
20431:     JSStackFrame* fp = cx->fp;
20431:     jsbytecode *pc = fp->regs->pc;
21526:     uintN argc = GET_ARGC(pc);
21511: 
18300:     jsval& fval = stackval(0 - (2 + argc));
20431:     JS_ASSERT(&fval >= StackBase(fp));
18001: 
21511:     if (!VALUE_IS_FUNCTION(cx, fval))
21511:         ABORT_TRACE("callee is not a function");
21511: 
18641:     jsval& tval = stackval(0 - (argc + 1));
18641:     LIns* this_ins = get(&tval);
20431: 
21526:     if (this_ins->isconstp() && !this_ins->constvalp() && !guardShapelessCallee(fval))
18641:         return false;
18641: 
18001:     /*
18001:      * Require that the callee be a function object, to avoid guarding on its
18001:      * class here. We know if the callee and this were pushed by JSOP_CALLNAME
18030:      * or JSOP_CALLPROP that callee is a *particular* function, since these hit
18030:      * the property cache and guard on the object (this) in which the callee
18030:      * was found. So it's sufficient to test here that the particular function
18030:      * is interpreted, not guard on that condition.
18001:      *
18030:      * Bytecode sequences that push shapeless callees must guard on the callee
18030:      * class being Function and the function being interpreted.
18001:      */
18001:     JSFunction* fun = GET_FUNCTION_PRIVATE(cx, JSVAL_TO_OBJECT(fval));
18001: 
18300:     if (FUN_INTERPRETED(fun)) {
20431:         if (constructing) {
18300:             LIns* args[] = { get(&fval), cx_ins };
20915:             LIns* tv_ins = lir->insCall(&js_FastNewObject_ci, args);
18300:             guard(false, lir->ins_eq0(tv_ins), OOM_EXIT);
18830:             set(&tval, tv_ins);
20431:         }
20431:         return interpretedFunctionCall(fval, fun, argc, constructing);
20431:     }
20431: 
21526:     LIns* arg1_ins = NULL;
21526:     jsval arg1 = JSVAL_VOID;
21526:     jsval thisval = tval;
21526:     if (!constructing && FUN_FAST_NATIVE(fun) == js_fun_apply) {
21526:         if (argc != 2)
21526:             ABORT_TRACE("can't trace Function.prototype.apply with other than 2 args");
21526: 
21526:         if (!guardShapelessCallee(tval))
21526:             return false;
21526:         JSObject* tfunobj = JSVAL_TO_OBJECT(tval);
21526:         JSFunction* tfun = GET_FUNCTION_PRIVATE(cx, tfunobj);
21526: 
21526:         jsval& oval = stackval(-2);
21526:         if (JSVAL_IS_PRIMITIVE(oval))
21526:             ABORT_TRACE("can't trace Function.prototype.apply with primitive 1st arg");
21526: 
21526:         jsval& aval = stackval(-1);
21526:         if (JSVAL_IS_PRIMITIVE(aval))
21526:             ABORT_TRACE("can't trace Function.prototype.apply with primitive 2nd arg");
21526:         JSObject* aobj = JSVAL_TO_OBJECT(aval);
21526: 
21526:         LIns* aval_ins = get(&aval);
21526:         if (!aval_ins->isCall())
21526:             ABORT_TRACE("can't trace Function.prototype.apply on non-builtin-call 2nd arg");
21526: 
21526:         if (aval_ins->callInfo() == &js_Arguments_ci) {
21526:             JS_ASSERT(OBJ_GET_CLASS(cx, aobj) == &js_ArgumentsClass);
21526:             JS_ASSERT(OBJ_GET_PRIVATE(cx, aobj) == fp);
21526:             if (!FUN_INTERPRETED(tfun))
21526:                 ABORT_TRACE("can't trace Function.prototype.apply(native_function, arguments)");
21526: 
21526:             // We can only fasttrack applys where the argument array we pass in has the
21526:             // same length (fp->argc) as the number of arguments the function expects (tfun->nargs).
21526:             argc = fp->argc;
21526:             if (tfun->nargs != argc || fp->fun->nargs != argc)
21526:                 ABORT_TRACE("can't trace Function.prototype.apply(scripted_function, arguments)");
21526: 
21526:             jsval* sp = fp->regs->sp - 4;
21526:             set(sp, get(&tval));
21526:             *sp++ = tval;
21526:             set(sp, get(&oval));
21526:             *sp++ = oval;
21526:             jsval* newsp = sp + argc;
21526:             if (newsp > fp->slots + fp->script->nslots) {
21526:                 JSArena* a = cx->stackPool.current;
21526:                 if (jsuword(newsp) > a->limit)
21526:                     ABORT_TRACE("can't grow stack for Function.prototype.apply");
21526:                 if (jsuword(newsp) > a->avail)
21526:                     a->avail = jsuword(newsp);
21526:             }
21526: 
21526:             jsval* argv = fp->argv;
21526:             for (uintN i = 0; i < JS_MIN(argc, 2); i++) {
21526:                 set(&sp[i], get(&argv[i]));
21526:                 sp[i] = argv[i];
21526:             }
21526:             applyingArguments = true;
21526:             return interpretedFunctionCall(tval, tfun, argc, false);
21526:         }
21526: 
21526:         if (aval_ins->callInfo() != &js_Array_1str_ci)
21526:             ABORT_TRACE("can't trace Function.prototype.apply on other than [str] 2nd arg");
21526: 
21526:         JS_ASSERT(OBJ_IS_ARRAY(cx, aobj));
21526:         JS_ASSERT(aobj->fslots[JSSLOT_ARRAY_LENGTH] == 1);
21526:         JS_ASSERT(JSVAL_IS_STRING(aobj->dslots[0]));
21526: 
21526:         if (FUN_INTERPRETED(tfun))
21526:             ABORT_TRACE("can't trace Function.prototype.apply for scripted functions");
21526: 
21526:         if (!(tfun->flags & JSFUN_TRACEABLE))
21526:             ABORT_TRACE("Function.prototype.apply on untraceable native");
21526: 
21526:         thisval = oval;
21526:         this_ins = get(&oval);
21526:         arg1_ins = callArgN(aval_ins, 2);
21526:         arg1 = aobj->dslots[0];
21526:         fun = tfun;
21526:         argc = 1;
21526:     }
21526: 
20431:     if (!constructing && !(fun->flags & JSFUN_TRACEABLE))
20431:         ABORT_TRACE("untraceable native");
20431: 
18300:     static JSTraceableNative knownNatives[] = {
20915:         { (JSFastNative)js_Array,  &js_FastNewArray_ci,   "pC", "",    FAIL_NULL | JSTN_MORE },
20915:         { (JSFastNative)js_Array,  &js_Array_1int_ci,     "pC", "i",   FAIL_NULL | JSTN_MORE },
20915:         { (JSFastNative)js_Array,  &js_Array_2obj_ci,     "pC", "oo",  FAIL_NULL | JSTN_MORE },
20915:         { (JSFastNative)js_Array,  &js_Array_3num_ci,     "pC", "ddd", FAIL_NULL | JSTN_MORE },
20915:         { (JSFastNative)js_Object, &js_FastNewObject_ci,  "fC", "",    FAIL_NULL | JSTN_MORE },
20915:         { (JSFastNative)js_Date,   &js_FastNewDate_ci,    "pC", "",    FAIL_NULL },
17651:     };
17632: 
20431:     LIns* args[5];
20431:     JSTraceableNative* known = constructing ? knownNatives : FUN_TRCINFO(fun);
20431:     do {
20431:         if (constructing && (JSFastNative)fun->u.n.native != known->native)
17651:             continue;
17634: 
17871:         uintN knownargc = strlen(known->argtypes);
17870:         if (argc != knownargc)
18115:             continue;
17870: 
17870:         intN prefixc = strlen(known->prefix);
20431:         JS_ASSERT(prefixc <= 3);
17870:         LIns** argp = &args[argc + prefixc - 1];
17870:         char argtype;
17870: 
18172: #if defined _DEBUG
18172:         memset(args, 0xCD, sizeof(args));
18172: #endif
18172: 
20431:         uintN i;
20431:         for (i = prefixc; i--; ) {
20431:             argtype = known->prefix[i];
20431:             if (argtype == 'C') {
20431:                 *argp = cx_ins;
20431:             } else if (argtype == 'T') {   /* this, as an object */
21526:                 if (!JSVAL_IS_OBJECT(thisval))
20431:                     goto next_specialization;
20431:                 *argp = this_ins;
20431:             } else if (argtype == 'S') {   /* this, as a string */
21526:                 if (!JSVAL_IS_STRING(thisval))
20431:                     goto next_specialization;
20431:                 *argp = this_ins;
20431:             } else if (argtype == 'f') {
20431:                 *argp = INS_CONSTPTR(JSVAL_TO_OBJECT(fval));
20431:             } else if (argtype == 'p') {
20431:                 JSObject* ctor = JSVAL_TO_OBJECT(fval);
20431:                 jsval pval;
20431:                 if (!OBJ_GET_PROPERTY(cx, ctor,
20431:                                       ATOM_TO_JSID(cx->runtime->atomState
20431:                                                    .classPrototypeAtom),
20431:                                       &pval)) {
20431:                     ABORT_TRACE("error getting prototype from constructor");
20431:                 }
20431:                 if (!JSVAL_IS_OBJECT(pval))
20431:                     ABORT_TRACE("got primitive prototype from constructor");
20431:                 *argp = INS_CONSTPTR(JSVAL_TO_OBJECT(pval));
20431:             } else if (argtype == 'R') {
20431:                 *argp = INS_CONSTPTR(cx->runtime);
20431:             } else if (argtype == 'P') {
20431:                 *argp = INS_CONSTPTR(pc);
20431:             } else if (argtype == 'D') {  /* this, as a number */
21526:                 if (!isNumber(thisval))
20431:                     goto next_specialization;
20431:                 *argp = this_ins;
20431:             } else {
20431:                 JS_NOT_REACHED("unknown prefix arg type");
20431:             }
20431:             argp--;
20431:         }
20431: 
20431:         for (i = knownargc; i--; ) {
21665:             jsval& arg = (!constructing && i == 0 && arg1_ins) ? arg1 : stackval(0 - (i + 1));
21526:             *argp = (!constructing && i == 0 && arg1_ins) ? arg1_ins : get(&arg);
20431: 
20431:             argtype = known->argtypes[i];
20431:             if (argtype == 'd' || argtype == 'i') {
20431:                 if (!isNumber(arg))
20431:                     goto next_specialization;
20431:                 if (argtype == 'i')
20431:                     *argp = f2i(*argp);
20431:             } else if (argtype == 'o') {
20431:                 if (!JSVAL_IS_OBJECT(arg))
20431:                     goto next_specialization;
20431:             } else if (argtype == 's') {
20431:                 if (!JSVAL_IS_STRING(arg))
20431:                     goto next_specialization;
20431:             } else if (argtype == 'r') {
20431:                 if (!VALUE_IS_REGEXP(cx, arg))
20431:                     goto next_specialization;
20431:             } else if (argtype == 'f') {
20431:                 if (!VALUE_IS_FUNCTION(cx, arg))
20431:                     goto next_specialization;
20431:             } else if (argtype == 'v') {
20431:                 if (!box_jsval(arg, *argp))
20431:                     return false;
20431:             } else {
20431:                 goto next_specialization;
20431:             }
20431:             argp--;
20431:         }
20431: 
20431:         /*
20431:          * If we got this far, and we have a charCodeAt, check that charCodeAt
20431:          * isn't going to return a NaN.
20431:          */
20915:         if (!constructing && known->builtin == &js_String_p_charCodeAt_ci) {
21526:             JSString* str = JSVAL_TO_STRING(thisval);
21526:             jsval& arg = arg1_ins ? arg1 : stackval(-1);
21526: 
21526:             JS_ASSERT(JSVAL_IS_STRING(thisval));
20431:             JS_ASSERT(isNumber(arg));
20431: 
20431:             if (JSVAL_IS_INT(arg)) {
20431:                 if (size_t(JSVAL_TO_INT(arg)) >= JSSTRING_LENGTH(str))
20431:                     ABORT_TRACE("invalid charCodeAt index");
20431:             } else {
20431:                 double d = js_DoubleToInteger(*JSVAL_TO_DOUBLE(arg));
20431:                 if (d < 0 || JSSTRING_LENGTH(str) <= d)
20431:                     ABORT_TRACE("invalid charCodeAt index");
20431:             }
20431:         }
20431:         goto success;
20431: 
20431: next_specialization:;
20431:     } while ((known++)->flags & JSTN_MORE);
20431: 
20431:     if (!constructing)
20431:         ABORT_TRACE("unknown native");
20431:     if (!(fun->flags & JSFUN_TRACEABLE) && FUN_CLASP(fun))
20431:         ABORT_TRACE("can't trace native constructor");
20431:     ABORT_TRACE("can't trace unknown constructor");
20431: 
20431: success:
18172: #if defined _DEBUG
18172:     JS_ASSERT(args[0] != (LIns *)0xcdcdcdcd);
18172: #endif
18172: 
17870:     LIns* res_ins = lir->insCall(known->builtin, args);
20431:     if (!constructing)
20431:         rval_ins = res_ins;
20408:     switch (JSTN_ERRTYPE(known)) {
18115:       case FAIL_NULL:
17873:         guard(false, lir->ins_eq0(res_ins), OOM_EXIT);
18115:         break;
18115:       case FAIL_NEG:
18115:       {
17895:         res_ins = lir->ins1(LIR_i2f, res_ins);
17895:         jsdpun u;
17895:         u.d = 0.0;
17895:         guard(false, lir->ins2(LIR_flt, res_ins, lir->insImmq(u.u64)), OOM_EXIT);
18115:         break;
18115:       }
18115:       case FAIL_VOID:
18680:         guard(false, lir->ins2i(LIR_eq, res_ins, JSVAL_TO_BOOLEAN(JSVAL_VOID)), OOM_EXIT);
18115:         break;
18115:       default:;
17895:     }
17870:     set(&fval, res_ins);
20431: 
20431:     if (!constructing) {
20431:         /*
20431:          * The return value will be processed by FastNativeCallComplete since
20431:          * we have to know the actual return value type for calls that return
20431:          * jsval (like Array_p_pop).
20431:          */
20431:         pendingTraceableNative = known;
20431:     }
20431: 
20431:     return true;
20431: }
20431: 
20431: bool
20431: TraceRecorder::record_JSOP_NEW()
20431: {
21526:     return functionCall(true);
18300: }
18300: 
18300: bool
18300: TraceRecorder::record_JSOP_DELNAME()
18300: {
18300:     return false;
18300: }
18300: 
18300: bool
18300: TraceRecorder::record_JSOP_DELPROP()
18300: {
18300:     return false;
18300: }
18300: 
18300: bool
18300: TraceRecorder::record_JSOP_DELELEM()
18300: {
18300:     return false;
18300: }
18300: 
18300: bool
18300: TraceRecorder::record_JSOP_TYPEOF()
18300: {
18300:     jsval& r = stackval(-1);
18300:     LIns* type;
18300:     if (JSVAL_IS_STRING(r)) {
18663:         type = INS_CONSTPTR(ATOM_TO_STRING(cx->runtime->atomState.typeAtoms[JSTYPE_STRING]));
18300:     } else if (isNumber(r)) {
18663:         type = INS_CONSTPTR(ATOM_TO_STRING(cx->runtime->atomState.typeAtoms[JSTYPE_NUMBER]));
18300:     } else {
18300:         LIns* args[] = { get(&r), cx_ins };
18300:         if (JSVAL_TAG(r) == JSVAL_BOOLEAN) {
18300:             // We specialize identically for boolean and undefined. We must not have a hole here.
18300:             // Pass the unboxed type here, since TypeOfBoolean knows how to handle it.
18300:             JS_ASSERT(JSVAL_TO_BOOLEAN(r) <= 2);
20915:             type = lir->insCall(&js_TypeOfBoolean_ci, args);
18300:         } else {
18300:             JS_ASSERT(JSVAL_IS_OBJECT(r));
20915:             type = lir->insCall(&js_TypeOfObject_ci, args);
18300:         }
18300:     }
18300:     set(&r, type);
18300:     return true;
18300: }
18300: 
18300: bool
18300: TraceRecorder::record_JSOP_VOID()
18300: {
18687:     stack(-1, INS_CONST(JSVAL_TO_BOOLEAN(JSVAL_VOID)));
18300:     return true;
18300: }
18300: 
18300: bool
18300: TraceRecorder::record_JSOP_INCNAME()
18300: {
18300:     return incName(1);
18300: }
18300: 
18300: bool
18300: TraceRecorder::record_JSOP_INCPROP()
18300: {
18300:     return incProp(1);
18300: }
18300: 
18300: bool
18300: TraceRecorder::record_JSOP_INCELEM()
18300: {
18300:     return incElem(1);
18300: }
18300: 
18300: bool
18300: TraceRecorder::record_JSOP_DECNAME()
18300: {
18300:     return incName(-1);
18300: }
18300: 
18300: bool
18300: TraceRecorder::record_JSOP_DECPROP()
18300: {
18300:     return incProp(-1);
18300: }
18300: 
18300: bool
18300: TraceRecorder::record_JSOP_DECELEM()
18300: {
18300:     return incElem(-1);
18300: }
18300: 
18300: bool
18300: TraceRecorder::incName(jsint incr, bool pre)
18300: {
18300:     jsval* vp;
18300:     if (!name(vp))
18300:         return false;
18300:     LIns* v_ins = get(vp);
18300:     if (!inc(*vp, v_ins, incr, pre))
18300:         return false;
18300:     set(vp, v_ins);
18300:     return true;
18300: }
18300: 
18300: bool
18300: TraceRecorder::record_JSOP_NAMEINC()
18300: {
18300:     return incName(1, false);
18300: }
18300: 
18300: bool
18300: TraceRecorder::record_JSOP_PROPINC()
18300: {
18300:     return incProp(1, false);
18300: }
18300: 
18300: // XXX consolidate with record_JSOP_GETELEM code...
18300: bool
18300: TraceRecorder::record_JSOP_ELEMINC()
18300: {
18300:     return incElem(1, false);
18300: }
18300: 
18300: bool
18300: TraceRecorder::record_JSOP_NAMEDEC()
18300: {
21805:     return incName(-1, false);
18300: }
18300: 
18300: bool
18300: TraceRecorder::record_JSOP_PROPDEC()
18300: {
18300:     return incProp(-1, false);
18300: }
18300: 
18300: bool
18300: TraceRecorder::record_JSOP_ELEMDEC()
18300: {
18300:     return incElem(-1, false);
18300: }
18300: 
18300: bool
18300: TraceRecorder::record_JSOP_GETPROP()
18300: {
18300:     return getProp(stackval(-1));
18300: }
18300: 
18300: bool
18300: TraceRecorder::record_JSOP_SETPROP()
18300: {
19093:     jsval& l = stackval(-2);
19093:     if (JSVAL_IS_PRIMITIVE(l))
19093:         ABORT_TRACE("primitive this for SETPROP");
19093: 
19093:     JSObject* obj = JSVAL_TO_OBJECT(l);
19093:     if (obj->map->ops->setProperty != js_SetProperty)
19093:         ABORT_TRACE("non-native JSObjectOps::setProperty");
19093:     return true;
19093: }
19093: 
19093: bool
19171: TraceRecorder::record_SetPropHit(JSPropCacheEntry* entry, JSScopeProperty* sprop)
19093: {
20012:     if (sprop->setter == js_watch_set)
20012:         ABORT_TRACE("watchpoint detected");
20012: 
19093:     jsbytecode* pc = cx->fp->regs->pc;
18300:     jsval& r = stackval(-1);
18300:     jsval& l = stackval(-2);
18300: 
19093:     JS_ASSERT(!JSVAL_IS_PRIMITIVE(l));
18300:     JSObject* obj = JSVAL_TO_OBJECT(l);
18300:     LIns* obj_ins = get(&l);
18300: 
19093:     if (obj == globalObj) {
19093:         JS_ASSERT(SPROP_HAS_VALID_SLOT(sprop, OBJ_SCOPE(obj)));
19093:         uint32 slot = sprop->slot;
19093:         if (!lazilyImportGlobalSlot(slot))
19093:             ABORT_TRACE("lazy import of global slot failed");
19093: 
19093:         LIns* r_ins = get(&r);
19093:         set(&STOBJ_GET_SLOT(obj, slot), r_ins);
19093: 
19093:         JS_ASSERT(*pc != JSOP_INITPROP);
19093:         if (pc[JSOP_SETPROP_LENGTH] != JSOP_POP)
19093:             set(&l, r_ins);
19093:         return true;
19093:     }
19093: 
19093:     // The global object's shape is guarded at trace entry, all others need a guard here.
18300:     LIns* map_ins = lir->insLoad(LIR_ldp, obj_ins, (int)offsetof(JSObject, map));
18300:     LIns* ops_ins;
18300:     if (!map_is_native(obj->map, map_ins, ops_ins, offsetof(JSObjectOps, setProperty)))
18300:         return false;
18300: 
18300:     LIns* shape_ins = addName(lir->insLoad(LIR_ld, map_ins, offsetof(JSScope, shape)), "shape");
19171:     guard(true, addName(lir->ins2i(LIR_eq, shape_ins, entry->kshape), "guard(shape)"),
19171:           MISMATCH_EXIT);
19171: 
19171:     if (entry->kshape != PCVCAP_SHAPE(entry->vcap)) {
18712:         LIns* args[] = { INS_CONSTPTR(sprop), obj_ins, cx_ins };
20915:         LIns* ok_ins = lir->insCall(&js_AddProperty_ci, args);
19171:         guard(false, lir->ins_eq0(ok_ins), OOM_EXIT);
18300:     }
18300: 
18300:     LIns* dslots_ins = NULL;
18300:     LIns* v_ins = get(&r);
18300:     LIns* boxed_ins = v_ins;
18300:     if (!box_jsval(r, boxed_ins))
18300:         return false;
18300:     if (!native_set(obj_ins, sprop, dslots_ins, boxed_ins))
18300:         return false;
19093: 
19093:     if (*pc != JSOP_INITPROP && pc[JSOP_SETPROP_LENGTH] != JSOP_POP)
19093:         set(&l, v_ins);
19093:     return true;
19093: }
19093: 
19093: bool
19093: TraceRecorder::record_SetPropMiss(JSPropCacheEntry* entry)
19093: {
19171:     if (entry->kpc != cx->fp->regs->pc || !PCVAL_IS_SPROP(entry->vword))
19148:         ABORT_TRACE("can't trace uncacheable property set");
19148: 
19171:     JSScopeProperty* sprop = PCVAL_TO_SPROP(entry->vword);
19171: 
19171: #ifdef DEBUG
19171:     jsval& l = stackval(-2);
19171:     JSObject* obj = JSVAL_TO_OBJECT(l);
19171:     JSScope* scope = OBJ_SCOPE(obj);
19171:     JS_ASSERT(scope->object == obj);
19171:     JS_ASSERT(scope->shape == PCVCAP_SHAPE(entry->vcap));
19171:     JS_ASSERT(SCOPE_HAS_PROPERTY(scope, sprop));
19171: #endif
19171: 
19171:     return record_SetPropHit(entry, sprop);
18300: }
18300: 
18300: bool
18300: TraceRecorder::record_JSOP_GETELEM()
18300: {
19979:     jsval& idx = stackval(-1);
19983:     jsval& lval = stackval(-2);
19983: 
19983:     LIns* obj_ins = get(&lval);
19979:     LIns* idx_ins = get(&idx);
19979:     
19983:     if (JSVAL_IS_STRING(lval) && JSVAL_IS_INT(idx)) {
19979:         int i = JSVAL_TO_INT(idx);
19983:         if ((size_t)i >= JSSTRING_LENGTH(JSVAL_TO_STRING(lval)))
18692:             ABORT_TRACE("Invalid string index in JSOP_GETELEM");
19979:         idx_ins = makeNumberInt32(idx_ins);
19979:         LIns* args[] = { idx_ins, obj_ins, cx_ins };
20915:         LIns* unitstr_ins = lir->insCall(&js_String_getelem_ci, args);
18300:         guard(false, lir->ins_eq0(unitstr_ins), MISMATCH_EXIT);
19983:         set(&lval, unitstr_ins);
19983:         return true;
19983:     }
19983: 
19983:     if (JSVAL_IS_PRIMITIVE(lval))
19979:         ABORT_TRACE("JSOP_GETLEM on a primitive");
19979: 
19983:     JSObject* obj = JSVAL_TO_OBJECT(lval);
19979:     jsval id;
18300:     jsval v;
19979:     LIns* v_ins;
19979: 
19979:     /* Property access using a string name. */
19979:     if (JSVAL_IS_STRING(idx)) {
19979:         if (!js_ValueToStringId(cx, idx, &id))
19979:             return false;
19983:         // Store the interned string to the stack to save the interpreter from redoing this work.
19983:         idx = ID_TO_VALUE(id);
20404:         jsuint index;
21685:         if (js_IdIsIndex(idx, &index) && guardDenseArray(obj, obj_ins, BRANCH_EXIT)) {
20404:             v = (index >= ARRAY_DENSE_LENGTH(obj)) ? JSVAL_HOLE : obj->dslots[index];
20404:             if (v == JSVAL_HOLE)
20404:                 ABORT_TRACE("can't see through hole in dense array");
20404:         } else {
19983:             if (!guardElemOp(obj, obj_ins, id, offsetof(JSObjectOps, getProperty), &v))
19979:                 return false;
20404:         }
19979:         LIns* args[] = { idx_ins, obj_ins, cx_ins };
20915:         v_ins = lir->insCall(&js_Any_getprop_ci, args);
19545:         guard(false, lir->ins2(LIR_eq, v_ins, INS_CONST(JSVAL_ERROR_COOKIE)), MISMATCH_EXIT);
18300:         if (!unbox_jsval(v, v_ins))
18300:             ABORT_TRACE("JSOP_GETELEM");
19983:         set(&lval, v_ins);
19979:         return true;
19979:     }
19979: 
19979:     /* At this point we expect a whole number or we bail. */
19979:     if (!JSVAL_IS_INT(idx))
19979:         ABORT_TRACE("non-string, non-int JSOP_GETELEM index");
20000:     if (JSVAL_TO_INT(idx) < 0)
20000:         ABORT_TRACE("negative JSOP_GETELEM index");
19979: 
19979:     /* Accessing an object using integer index but not a dense array. */
19983:     if (!OBJ_IS_DENSE_ARRAY(cx, obj)) {
19979:         idx_ins = makeNumberInt32(idx_ins);
20000:         LIns* args[] = { idx_ins, obj_ins, cx_ins };
19979:         if (!js_IndexToId(cx, JSVAL_TO_INT(idx), &id))
19979:             return false;
19984:         idx = ID_TO_VALUE(id);
19984:         if (!guardElemOp(obj, obj_ins, id, offsetof(JSObjectOps, getProperty), &v))
19979:             return false;
20915:         LIns* v_ins = lir->insCall(&js_Any_getelem_ci, args);
19979:         guard(false, lir->ins2(LIR_eq, v_ins, INS_CONST(JSVAL_ERROR_COOKIE)), MISMATCH_EXIT);
19979:         if (!unbox_jsval(v, v_ins))
19979:             ABORT_TRACE("JSOP_GETELEM");
19983:         set(&lval, v_ins);
18300:         return true;
18300:     }
18300: 
18300:     jsval* vp;
18300:     LIns* addr_ins;
19983:     if (!elem(lval, idx, vp, v_ins, addr_ins))
19983:         return false;
19983:     set(&lval, v_ins);
18300:     return true;
18300: }
18300: 
18300: bool
18300: TraceRecorder::record_JSOP_SETELEM()
18300: {
18300:     jsval& v = stackval(-1);
19979:     jsval& idx = stackval(-2);
19983:     jsval& lval = stackval(-3);
18300: 
18300:     /* no guards for type checks, trace specialized this already */
19983:     if (JSVAL_IS_PRIMITIVE(lval))
18300:         ABORT_TRACE("left JSOP_SETELEM operand is not an object");
19979: 
19983:     JSObject* obj = JSVAL_TO_OBJECT(lval);
19983:     LIns* obj_ins = get(&lval);
19979:     LIns* idx_ins = get(&idx);
18300:     LIns* v_ins = get(&v);
20000:     jsid id;
19979: 
19993:     LIns* boxed_v_ins = v_ins;
19993:     if (!box_jsval(v, boxed_v_ins))
20000:         ABORT_TRACE("boxing JSOP_SETELEM value");
19993: 
19979:     if (JSVAL_IS_STRING(idx)) {
19983:         if (!js_ValueToStringId(cx, idx, &id))
19983:             return false;
19983:         // Store the interned string to the stack to save the interpreter from redoing this work.
19983:         idx = ID_TO_VALUE(id);
19983:         if (!guardElemOp(obj, obj_ins, id, offsetof(JSObjectOps, setProperty), NULL))
19979:             return false;
19993:         LIns* args[] = { boxed_v_ins, idx_ins, obj_ins, cx_ins };
20915:         LIns* ok_ins = lir->insCall(&js_Any_setprop_ci, args);
18300:         guard(false, lir->ins_eq0(ok_ins), MISMATCH_EXIT);    
19993:     } else if (JSVAL_IS_INT(idx)) {
20000:         if (JSVAL_TO_INT(idx) < 0)
20010:             ABORT_TRACE("negative JSOP_SETELEM index");
19993:         idx_ins = makeNumberInt32(idx_ins);
19993:         LIns* args[] = { boxed_v_ins, idx_ins, obj_ins, cx_ins };
19993:         LIns* res_ins;
21685:         if (guardDenseArray(obj, obj_ins, BRANCH_EXIT)) {
20915:             res_ins = lir->insCall(&js_Array_dense_setelem_ci, args);
20000:         } else {
20000:             if (!js_IndexToId(cx, JSVAL_TO_INT(idx), &id))
20000:                 return false;
20000:             idx = ID_TO_VALUE(id);
20010:             if (!guardElemOp(obj, obj_ins, id, offsetof(JSObjectOps, setProperty), NULL))
20000:                 return false;
20915:             res_ins = lir->insCall(&js_Any_setelem_ci, args);
20000:         }
19993:         guard(false, lir->ins_eq0(res_ins), MISMATCH_EXIT);
19993:     } else {
18300:         ABORT_TRACE("non-string, non-int JSOP_SETELEM index");
19993:     }
18300: 
18300:     jsbytecode* pc = cx->fp->regs->pc;
18300:     if (*pc == JSOP_SETELEM && pc[JSOP_SETELEM_LENGTH] != JSOP_POP)
19983:         set(&lval, v_ins);
19993: 
18300:     return true;
18300: }
18300: 
18300: bool
18300: TraceRecorder::record_JSOP_CALLNAME()
18300: {
18300:     JSObject* obj = cx->fp->scopeChain;
18300:     if (obj != globalObj) {
18300:         jsval* vp;
18300:         if (!activeCallOrGlobalSlot(obj, vp))
18300:             return false;
18300:         stack(0, get(vp));
21492:         stack(1, INS_CONSTPTR(globalObj));
18300:         return true;
18300:     }
18300: 
18300:     LIns* obj_ins = scopeChain();
18300:     JSObject* obj2;
18300:     jsuword pcval;
18300:     if (!test_property_cache(obj, obj_ins, obj2, pcval))
18300:         return false;
18300: 
18300:     if (PCVAL_IS_NULL(pcval) || !PCVAL_IS_OBJECT(pcval))
18300:         ABORT_TRACE("callee is not an object");
18300:     JS_ASSERT(HAS_FUNCTION_CLASS(PCVAL_TO_OBJECT(pcval)));
18300: 
18712:     stack(0, INS_CONSTPTR(PCVAL_TO_OBJECT(pcval)));
18300:     stack(1, obj_ins);
18300:     return true;
18300: }
18300: 
18300: bool
18308: TraceRecorder::record_JSOP_GETUPVAR()
18308: {
18308:     ABORT_TRACE("GETUPVAR");
18308: }
18308: 
18308: bool
18308: TraceRecorder::record_JSOP_CALLUPVAR()
18308: {
18308:     ABORT_TRACE("CALLUPVAR");
18308: }
18308: 
18308: bool
21526: TraceRecorder::guardShapelessCallee(jsval& callee)
21526: {
18300:     guard(true,
18300:           addName(lir->ins2(LIR_eq, get(&callee), INS_CONSTPTR(JSVAL_TO_OBJECT(callee))),
21526:                   "guard(shapeless callee)"),
18300:           MISMATCH_EXIT);
18300:     return true;
18300: }
18300: 
18300: bool
19577: TraceRecorder::interpretedFunctionCall(jsval& fval, JSFunction* fun, uintN argc, bool constructing)
18300: {
19149:     if (JS_GetGlobalForObject(cx, JSVAL_TO_OBJECT(fval)) != globalObj)
19149:         ABORT_TRACE("JSOP_CALL or JSOP_NEW crosses global scopes");
19149: 
18300:     JSStackFrame* fp = cx->fp;
18300: 
18300:     // TODO: track the copying via the tracker...
18300:     if (argc < fun->nargs &&
18300:         jsuword(fp->regs->sp + (fun->nargs - argc)) > cx->stackPool.current->limit) {
18300:         ABORT_TRACE("can't trace calls with too few args requiring argv move");
18300:     }
18300: 
19085:     // Generate a type map for the outgoing frame and stash it in the LIR
19085:     unsigned stackSlots = js_NativeStackSlots(cx, 0/*callDepth*/);
19085:     LIns* data = lir_buf_writer->skip(stackSlots * sizeof(uint8));
19085:     uint8* typemap = (uint8 *)data->payload();
19085:     uint8* m = typemap;
19085:     /* Determine the type of a store by looking at the current type of the actual value the
19085:        interpreter is using. For numbers we have to check what kind of store we used last
19085:        (integer or double) to figure out what the side exit show reflect in its typemap. */
19085:     FORALL_SLOTS_IN_PENDING_FRAMES(cx, 0/*callDepth*/,
19085:         *m++ = determineSlotType(vp);
19085:     );
19085: 
19577:     if (argc >= 0x8000)
19577:         ABORT_TRACE("too many arguments");
19577: 
18300:     FrameInfo fi = {
18300:         JSVAL_TO_OBJECT(fval),
21685:         ENCODE_IP_ADJ(fp, fp->regs->pc),
19085:         typemap,
19577:         { { fp->regs->sp - fp->slots, argc | (constructing ? 0x8000 : 0) } }
18300:     };
18300: 
18300:     unsigned callDepth = getCallDepth();
18300:     if (callDepth >= treeInfo->maxCallDepth)
18300:         treeInfo->maxCallDepth = callDepth + 1;
18300: 
18712:     lir->insStorei(INS_CONSTPTR(fi.callee), lirbuf->rp,
18300:                    callDepth * sizeof(FrameInfo) + offsetof(FrameInfo, callee));
21685:     lir->insStorei(INS_CONSTPTR(fi.ip_adj), lirbuf->rp,
21685:                    callDepth * sizeof(FrameInfo) + offsetof(FrameInfo, ip_adj));
19588:     lir->insStorei(INS_CONSTPTR(fi.typemap), lirbuf->rp,
19588:                    callDepth * sizeof(FrameInfo) + offsetof(FrameInfo, typemap));
18687:     lir->insStorei(INS_CONST(fi.word), lirbuf->rp,
18300:                    callDepth * sizeof(FrameInfo) + offsetof(FrameInfo, word));
18300: 
18300:     atoms = fun->u.i.script->atomMap.vector;
18300:     return true;
18300: }
18300: 
18300: bool
18300: TraceRecorder::record_JSOP_CALL()
18300: {
21526:     return functionCall(false);
19582: }
19582: 
19582: bool
21452: TraceRecorder::record_JSOP_APPLY()
21452: {
21526:     return functionCall(false);
21452: }
21452: 
21452: bool
20405: TraceRecorder::record_FastNativeCallComplete()
20405: {
20405:     JS_ASSERT(pendingTraceableNative);
20405:     
20405:     /* At this point the generated code has already called the native function
20405:        and we can no longer fail back to the original pc location (JSOP_CALL)
20405:        because that would cause the interpreter to re-execute the native 
20969:        function, which might have side effects.
20969: 
20969:        Instead, snapshot(), which is invoked from unbox_jsval(), will see that
21441:        we are currently parked on a traceable native's JSOP_CALL instruction,
21441:        and it will advance the pc to restore by the length of the current
21441:        opcode, and indicate in the type map that the element on top of the
21441:        stack is a boxed value which doesn't need to be boxed if the type guard
21441:        generated by unbox_jsval() fails. */
21466:     JS_ASSERT(*cx->fp->regs->pc == JSOP_CALL || 
21466:               *cx->fp->regs->pc == JSOP_APPLY);
20405: 
20405:     jsval& v = stackval(-1);
20405:     LIns* v_ins = get(&v);
20405:     
20405:     bool ok = true;
20966:     switch (JSTN_ERRTYPE(pendingTraceableNative)) {
20966:       case FAIL_JSVAL:
20405:         ok = unbox_jsval(v, v_ins);
20405:         if (ok)
20405:             set(&v, v_ins);
20966:         break;
20966:       case FAIL_NEG:
20966:         /* Already added i2f in functionCall. */
20966:         JS_ASSERT(JSVAL_IS_NUMBER(v));
20966:         break;
20966:       default:
20966:         /* Convert the result to double if the builtin returns int32. */
20966:         if (JSVAL_IS_NUMBER(v) &&
20966:             (pendingTraceableNative->builtin->_argtypes & 3) == nanojit::ARGSIZE_LO) {
20966:             set(&v, lir->ins1(LIR_i2f, v_ins));
20966:         }
20405:     }
20405: 
20969:     // We'll null pendingTraceableNative in js_MonitorRecording, on the next op cycle.
20969:     // There must be a next op since the stack is non-empty.
20405:     return ok;
20405: }
20405: 
20405: bool
18096: TraceRecorder::name(jsval*& vp)
18096: {
18096:     JSObject* obj = cx->fp->scopeChain;
18096:     if (obj != globalObj)
18286:         return activeCallOrGlobalSlot(obj, vp);
18115: 
18115:     /* Can't use prop here, because we don't want unboxing from global slots. */
18286:     LIns* obj_ins = scopeChain();
18096:     uint32 slot;
18096:     if (!test_property_cache_direct_slot(obj, obj_ins, slot))
18096:         return false;
18096: 
18096:     if (slot == SPROP_INVALID_SLOT)
18096:         ABORT_TRACE("name op can't find named property");
18096: 
18096:     if (!lazilyImportGlobalSlot(slot))
18096:         ABORT_TRACE("lazy import of global slot failed");
18096: 
18096:     vp = &STOBJ_GET_SLOT(obj, slot);
18096:     return true;
18096: }
18096: 
18096: bool
17761: TraceRecorder::prop(JSObject* obj, LIns* obj_ins, uint32& slot, LIns*& v_ins)
17758: {
17758:     /*
17758:      * Can't specialize to assert obj != global, must guard to avoid aliasing
17758:      * stale homes of stacked global variables.
17758:      */
17758:     if (obj == globalObj)
17758:         ABORT_TRACE("prop op aliases global");
18712:     guard(false, lir->ins2(LIR_eq, obj_ins, INS_CONSTPTR(globalObj)), MISMATCH_EXIT);
17758: 
18115:     /*
18115:      * Property cache ensures that we are dealing with an existing property,
18115:      * and guards the shape for us.
18115:      */
18115:     JSObject* obj2;
18115:     jsuword pcval;
18115:     if (!test_property_cache(obj, obj_ins, obj2, pcval))
17665:         return false;
17665: 
17998:     /* Check for non-existent property reference, which results in undefined. */
17998:     const JSCodeSpec& cs = js_CodeSpec[*cx->fp->regs->pc];
18115:     if (PCVAL_IS_NULL(pcval)) {
18687:         v_ins = INS_CONST(JSVAL_TO_BOOLEAN(JSVAL_VOID));
17998:         JS_ASSERT(cs.ndefs == 1);
17998:         stack(-cs.nuses, v_ins);
18666:         slot = SPROP_INVALID_SLOT;
17998:         return true;
17998:     }
17998: 
18115:     /* Insist if setting on obj being the directly addressed object. */
18115:     uint32 setflags = (cs.format & (JOF_SET | JOF_INCDEC));
18143:     LIns* dslots_ins = NULL;
18143:     if (obj2 != obj) {
18143:         if (setflags)
18115:             ABORT_TRACE("JOF_SET opcode hit prototype chain");
18115: 
18143:         /*
18143:          * We're getting a proto-property. Walk up the prototype chain emitting
18143:          * proto slot loads, updating obj as we go, leaving obj set to obj2 with
18143:          * obj_ins the last proto-load.
18143:          */
18143:         while (obj != obj2) {
18143:             obj_ins = stobj_get_slot(obj_ins, JSSLOT_PROTO, dslots_ins);
18143:             obj = STOBJ_GET_PROTO(obj);
18143:         }
18143:     }
18143: 
18115:     /* Don't trace getter or setter calls, our caller wants a direct slot. */
18115:     if (PCVAL_IS_SPROP(pcval)) {
18115:         JSScopeProperty* sprop = PCVAL_TO_SPROP(pcval);
18115: 
18115:         if (setflags && !SPROP_HAS_STUB_SETTER(sprop))
18115:             ABORT_TRACE("non-stub setter");
18115:         if (setflags != JOF_SET && !SPROP_HAS_STUB_GETTER(sprop)) {
18115:             // FIXME 450335: generalize this away from regexp built-in getters.
18115:             if (setflags == 0 &&
18115:                 sprop->getter == js_RegExpClass.getProperty &&
18115:                 sprop->shortid < 0) {
19986:                 if (sprop->shortid == REGEXP_LAST_INDEX)
19986:                     ABORT_TRACE("can't trace regexp.lastIndex yet");
18115:                 LIns* args[] = { INS_CONSTPTR(sprop), obj_ins, cx_ins };
20915:                 v_ins = lir->insCall(&js_CallGetter_ci, args);
18766:                 guard(false, lir->ins2(LIR_eq, v_ins, INS_CONST(JSVAL_ERROR_COOKIE)), OOM_EXIT);
18286:                 if (!unbox_jsval((sprop->shortid == REGEXP_SOURCE) ? JSVAL_STRING : JSVAL_BOOLEAN,
18115:                                  v_ins)) {
18115:                     ABORT_TRACE("unboxing");
18115:                 }
18115:                 JS_ASSERT(cs.ndefs == 1);
18115:                 stack(-cs.nuses, v_ins);
18115:                 return true;
18115:             }
18115:             ABORT_TRACE("non-stub getter");
18115:         }
18115:         if (!SPROP_HAS_VALID_SLOT(sprop, OBJ_SCOPE(obj)))
18115:             ABORT_TRACE("no valid slot");
18115:         slot = sprop->slot;
18115:     } else {
18115:         if (!PCVAL_IS_SLOT(pcval))
18115:             ABORT_TRACE("PCE is not a slot");
18115:         slot = PCVAL_TO_SLOT(pcval);
18115:     }
18115: 
17758:     v_ins = stobj_get_slot(obj_ins, slot, dslots_ins);
17761:     if (!unbox_jsval(STOBJ_GET_SLOT(obj, slot), v_ins))
17665:         ABORT_TRACE("unboxing");
17758:     return true;
17758: }
17758: 
17758: bool
19983: TraceRecorder::elem(jsval& oval, jsval& idx, jsval*& vp, LIns*& v_ins, LIns*& addr_ins)
17758: {
17758:     /* no guards for type checks, trace specialized this already */
19983:     if (JSVAL_IS_PRIMITIVE(oval) || !JSVAL_IS_INT(idx))
19983:         return false;
19983: 
19983:     JSObject* obj = JSVAL_TO_OBJECT(oval);
19983:     LIns* obj_ins = get(&oval);
17758: 
17758:     /* make sure the object is actually a dense array */
17899:     if (!guardDenseArray(obj, obj_ins))
17758:         return false;
17758: 
17758:     /* check that the index is within bounds */
19979:     jsint i = JSVAL_TO_INT(idx);
19979:     LIns* idx_ins = makeNumberInt32(get(&idx));
17899: 
18230:     LIns* dslots_ins = lir->insLoad(LIR_ldp, obj_ins, offsetof(JSObject, dslots));
20972:     if (!guardDenseArrayIndex(obj, i, obj_ins, dslots_ins, idx_ins, BRANCH_EXIT)) {
20972:         LIns* rt_ins = lir->insLoad(LIR_ldp, cx_ins, offsetof(JSContext, runtime));
20972:         guard(true, 
20972:               lir->ins_eq0(lir->insLoad(LIR_ldp, rt_ins,
20972:                                         offsetof(JSRuntime, anyArrayProtoHasElement))),
20972:               MISMATCH_EXIT);
20972:         // Return undefined and indicate that we didn't actually read this (addr_ins).
20972:         v_ins = lir->insImm(JSVAL_TO_BOOLEAN(JSVAL_VOID));
20972:         addr_ins = NULL; 
20972:         return true;
20972:     }
20404: 
20404:     // We can't "see through" a hole to a possible Array.prototype property, so
20404:     // we abort here and guard below (after unboxing).
19983:     vp = &obj->dslots[i];
20404:     if (*vp == JSVAL_HOLE)
20404:         ABORT_TRACE("can't see through hole in dense array");
17758: 
18232:     addr_ins = lir->ins2(LIR_piadd, dslots_ins,
18232:                          lir->ins2i(LIR_pilsh, idx_ins, (sizeof(jsval) == 4) ? 2 : 3));
17899: 
20404:     /* Load the value and guard on its type to unbox it. */
18230:     v_ins = lir->insLoad(LIR_ldp, addr_ins, 0);
19053:     if (!unbox_jsval(*vp, v_ins))
19053:         return false;
20404: 
19053:     if (JSVAL_TAG(*vp) == JSVAL_BOOLEAN) {
20404:         // Optimize to guard for a hole only after untagging, so we know that
20404:         // we have a boolean, to avoid an extra guard for non-boolean values.
20404:         guard(false, lir->ins2(LIR_eq, v_ins, INS_CONST(JSVAL_TO_BOOLEAN(JSVAL_HOLE))),
20404:               MISMATCH_EXIT);
19053:     }
20972:     return true;
17758: }
17758: 
17758: bool
17758: TraceRecorder::getProp(JSObject* obj, LIns* obj_ins)
17758: {
17761:     uint32 slot;
17758:     LIns* v_ins;
17761:     if (!prop(obj, obj_ins, slot, v_ins))
17758:         return false;
17758: 
17758:     const JSCodeSpec& cs = js_CodeSpec[*cx->fp->regs->pc];
17758:     JS_ASSERT(cs.ndefs == 1);
17758:     stack(-cs.nuses, v_ins);
17665:     return true;
17665: }
17665: 
17665: bool
17688: TraceRecorder::getProp(jsval& v)
17665: {
17665:     if (JSVAL_IS_PRIMITIVE(v))
17665:         ABORT_TRACE("primitive lhs");
17665: 
17688:     return getProp(JSVAL_TO_OBJECT(v), get(&v));
17665: }
17665: 
17926: bool
17926: TraceRecorder::record_JSOP_NAME()
17409: {
18096:     jsval* vp;
18096:     if (!name(vp))
17908:         return false;
18096:     stack(0, get(vp));
17459:     return true;
17409: }
17541: 
17926: bool
17926: TraceRecorder::record_JSOP_DOUBLE()
17409: {
17611:     jsval v = jsval(atoms[GET_INDEX(cx->fp->regs->pc)]);
17656:     jsdpun u;
17656:     u.d = *JSVAL_TO_DOUBLE(v);
17656:     stack(0, lir->insImmq(u.u64));
17453:     return true;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_STRING()
17409: {
17902:     JSAtom* atom = atoms[GET_INDEX(cx->fp->regs->pc)];
17902:     JS_ASSERT(ATOM_IS_STRING(atom));
18712:     stack(0, INS_CONSTPTR(ATOM_TO_STRING(atom)));
17902:     return true;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_ZERO()
17409: {
17656:     jsdpun u;
17656:     u.d = 0.0;
17656:     stack(0, lir->insImmq(u.u64));
17418:     return true;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_ONE()
17409: {
17656:     jsdpun u;
17656:     u.d = 1.0;
17656:     stack(0, lir->insImmq(u.u64));
17418:     return true;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_NULL()
17409: {
18712:     stack(0, INS_CONSTPTR(NULL));
17418:     return true;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_THIS()
17409: {
17688:     LIns* this_ins;
17688:     if (!getThis(this_ins))
17688:         return false;
17688:     stack(0, this_ins);
17682:     return true;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_FALSE()
17409: {
17418:     stack(0, lir->insImm(0));
17418:     return true;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_TRUE()
17409: {
17418:     stack(0, lir->insImm(1));
17418:     return true;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_OR()
17409: {
20416:     return ifop();
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_AND()
17409: {
20416:     return ifop();
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_TABLESWITCH()
17409: {
18687:     return switchop();
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_LOOKUPSWITCH()
17409: {
18687:     return switchop();
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_STRICTEQ()
17409: {
19576:     return cmp(LIR_feq, CMP_STRICT);
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_STRICTNE()
17409: {
19576:     return cmp(LIR_feq, CMP_STRICT | CMP_NEGATE);
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_OBJECT()
17409: {
18027:     JSStackFrame* fp = cx->fp;
18027:     JSScript* script = fp->script;
18027:     unsigned index = atoms - script->atomMap.vector + GET_INDEX(fp->regs->pc);
18027: 
18027:     JSObject* obj;
18027:     JS_GET_SCRIPT_OBJECT(script, index, obj);
18712:     stack(0, INS_CONSTPTR(obj));
18027:     return true;
17409: }
17899: 
17926: bool
17926: TraceRecorder::record_JSOP_POP()
17409: {
17456:     return true;
17409: }
17899: 
17926: bool
17926: TraceRecorder::record_JSOP_POS()
17409: {
17899:     jsval& r = stackval(-1);
17899:     return isNumber(r);
17899: }
17899: 
17926: bool
17926: TraceRecorder::record_JSOP_TRAP()
17899: {
17409:     return false;
17409: }
17899: 
17926: bool
17926: TraceRecorder::record_JSOP_GETARG()
17409: {
17412:     stack(0, arg(GET_ARGNO(cx->fp->regs->pc)));
17412:     return true;
17409: }
17899: 
17926: bool
17926: TraceRecorder::record_JSOP_SETARG()
17409: {
17415:     arg(GET_ARGNO(cx->fp->regs->pc), stack(-1));
17415:     return true;
17409: }
17899: 
17926: bool
18005: TraceRecorder::record_JSOP_GETLOCAL()
17409: {
17807:     stack(0, var(GET_SLOTNO(cx->fp->regs->pc)));
17412:     return true;
17409: }
17899: 
17926: bool
18005: TraceRecorder::record_JSOP_SETLOCAL()
17409: {
17807:     var(GET_SLOTNO(cx->fp->regs->pc), stack(-1));
17415:     return true;
17409: }
17899: 
17926: bool
17926: TraceRecorder::record_JSOP_UINT16()
17409: {
17656:     jsdpun u;
17656:     u.d = (jsdouble)GET_UINT16(cx->fp->regs->pc);
17656:     stack(0, lir->insImmq(u.u64));
17412:     return true;
17409: }
17899: 
17926: bool
17926: TraceRecorder::record_JSOP_NEWINIT()
17409: {
18036:     JSProtoKey key = JSProtoKey(GET_INT8(cx->fp->regs->pc));
18712:     JSObject* obj;
20408:     const CallInfo *ci;
18712:     if (key == JSProto_Array) {
18712:         if (!js_GetClassPrototype(cx, globalObj, INT_TO_JSID(key), &obj))
17409:             return false;
20915:         ci = &js_FastNewArray_ci;
18712:     } else {
19985:         jsval v_obj;
19985:         if (!js_FindClassObject(cx, globalObj, INT_TO_JSID(key), &v_obj))
19985:             return false;
19985:         if (JSVAL_IS_PRIMITIVE(v_obj))
19985:             ABORT_TRACE("primitive Object value");
19985:         obj = JSVAL_TO_OBJECT(v_obj);
20915:         ci = &js_FastNewObject_ci;
18712:     }
18712:     LIns* args[] = { INS_CONSTPTR(obj), cx_ins };
20408:     LIns* v_ins = lir->insCall(ci, args);
18036:     guard(false, lir->ins_eq0(v_ins), OOM_EXIT);
18036:     stack(0, v_ins);
18036:     return true;
17409: }
17899: 
17926: bool
17926: TraceRecorder::record_JSOP_ENDINIT()
17409: {
18712:     jsval& v = stackval(-1);
18712:     JS_ASSERT(!JSVAL_IS_PRIMITIVE(v));
18712:     JSObject* obj = JSVAL_TO_OBJECT(v);
18712:     if (OBJ_IS_DENSE_ARRAY(cx, obj)) {
18712:         // Until we get JSOP_NEWARRAY working, we do our optimizing here...
18714:         if (obj->fslots[JSSLOT_ARRAY_LENGTH] == 1 &&
18714:             obj->dslots && JSVAL_IS_STRING(obj->dslots[0])) {
18712:             LIns* v_ins = get(&v);
20915:             JS_ASSERT(v_ins->isCall() && v_ins->callInfo() == &js_FastNewArray_ci);
18712:             LIns* args[] = { stack(1), callArgN(v_ins, 1), cx_ins };
20915:             v_ins = lir->insCall(&js_Array_1str_ci, args);
18712:             set(&v, v_ins);
18712:         }
18712:     }
18036:     return true;
17409: }
17899: 
17926: bool
17926: TraceRecorder::record_JSOP_INITPROP()
17409: {
19093:     // All the action is in record_SetPropHit.
19093:     return true;
17409: }
17899: 
17926: bool
17926: TraceRecorder::record_JSOP_INITELEM()
17409: {
18036:     return record_JSOP_SETELEM();
17409: }
17899: 
17926: bool
17926: TraceRecorder::record_JSOP_DEFSHARP()
17409: {
17409:     return false;
17409: }
17899: 
17926: bool
17926: TraceRecorder::record_JSOP_USESHARP()
17409: {
17409:     return false;
17409: }
17899: 
17926: bool
17926: TraceRecorder::record_JSOP_INCARG()
17409: {
17544:     return inc(argval(GET_ARGNO(cx->fp->regs->pc)), 1);
17409: }
17899: 
17926: bool
18005: TraceRecorder::record_JSOP_INCLOCAL()
17409: {
17807:     return inc(varval(GET_SLOTNO(cx->fp->regs->pc)), 1);
17409: }
17899: 
17926: bool
17926: TraceRecorder::record_JSOP_DECARG()
17409: {
17544:     return inc(argval(GET_ARGNO(cx->fp->regs->pc)), -1);
17409: }
17899: 
17926: bool
18005: TraceRecorder::record_JSOP_DECLOCAL()
17409: {
17807:     return inc(varval(GET_SLOTNO(cx->fp->regs->pc)), -1);
17409: }
17899: 
17926: bool
17926: TraceRecorder::record_JSOP_ARGINC()
17409: {
17412:     return inc(argval(GET_ARGNO(cx->fp->regs->pc)), 1, false);
17409: }
17899: 
17926: bool
18005: TraceRecorder::record_JSOP_LOCALINC()
17409: {
17807:     return inc(varval(GET_SLOTNO(cx->fp->regs->pc)), 1, false);
17409: }
17899: 
17926: bool
17926: TraceRecorder::record_JSOP_ARGDEC()
17409: {
17412:     return inc(argval(GET_ARGNO(cx->fp->regs->pc)), -1, false);
17409: }
17899: 
17926: bool
18005: TraceRecorder::record_JSOP_LOCALDEC()
17409: {
17807:     return inc(varval(GET_SLOTNO(cx->fp->regs->pc)), -1, false);
17409: }
17899: 
17926: bool
21685: TraceRecorder::record_JSOP_IMACOP()
21685: {
21685:     JS_ASSERT(cx->fp->imacpc);
21685:     return true;
21685: }
21685: 
21685: static struct {
21685:     jsbytecode for_in[10];
21685:     jsbytecode for_each[10];
21685: } iter_imacros = {
21685:     {
21685:         JSOP_CALLPROP, 0, COMMON_ATOM_INDEX(iterator),
21685:         JSOP_INT8, JSITER_ENUMERATE,
21685:         JSOP_CALL, 0, 1,
21685:         JSOP_PUSH,
21685:         JSOP_STOP
21685:     },
21685: 
21685:     {
21685:         JSOP_CALLPROP, 0, COMMON_ATOM_INDEX(iterator),
21685:         JSOP_INT8, JSITER_ENUMERATE | JSITER_FOREACH,
21685:         JSOP_CALL, 0, 1,
21685:         JSOP_PUSH,
21685:         JSOP_STOP
21685:     }
21685: };
21685: 
21685: JS_STATIC_ASSERT(sizeof(iter_imacros) < IMACRO_PC_ADJ_LIMIT);
21685: 
21685: bool
17926: TraceRecorder::record_JSOP_ITER()
17409: {
17958:     jsval& v = stackval(-1);
17958:     if (!JSVAL_IS_PRIMITIVE(v)) {
18136:         jsuint flags = cx->fp->regs->pc[1];
21685: 
21685:         if (!hasIteratorMethod(v)) {
18136:             LIns* args[] = { get(&v), INS_CONST(flags), cx_ins };
20915:             LIns* v_ins = lir->insCall(&js_FastValueToIterator_ci, args);
18053:             guard(false, lir->ins_eq0(v_ins), MISMATCH_EXIT);
17958:             set(&v, v_ins);
21441: 
21441:             LIns* void_ins = INS_CONST(JSVAL_TO_BOOLEAN(JSVAL_VOID));
21441:             stack(0, void_ins);
17899:             return true;
17899:         }
17899: 
21685:         if (flags == JSITER_ENUMERATE)
21685:             return call_imacro(iter_imacros.for_in);
21686:         if (flags == (JSITER_ENUMERATE | JSITER_FOREACH))
21685:             return call_imacro(iter_imacros.for_each);
21685:         ABORT_TRACE("unimplemented JSITER_* flags");
21685:     }
21685: 
17958:     ABORT_TRACE("for-in on a primitive value");
17958: }
17958: 
21441: static JSTraceableNative js_FastCallIteratorNext_tn = {
21441:     NULL,                               // JSFastNative            native;
21441:     &js_FastCallIteratorNext_ci,        // const nanojit::CallInfo *builtin;
21441:     "C",                                // const char              *prefix;
21441:     "o",                                // const char              *argtypes;
21441:     FAIL_JSVAL                          // uintN                   flags;
21441: };
21441: 
21685: static jsbytecode nextiter_imacro[] = {
21685:     JSOP_POP,
21685:     JSOP_DUP,
21685:     JSOP_CALLPROP, 0, COMMON_ATOM_INDEX(next),
21685:     JSOP_CALL, 0, 0,
21685:     JSOP_TRUE,
21685:     JSOP_STOP
21685: };
21685: 
21685: JS_STATIC_ASSERT(sizeof(nextiter_imacro) < IMACRO_PC_ADJ_LIMIT);
21685: 
21441: bool
21441: TraceRecorder::record_JSOP_NEXTITER()
21441: {
21441:     jsval& iterobj_val = stackval(-2);
18136:     if (!JSVAL_IS_PRIMITIVE(iterobj_val)) {
21685:         LIns* iterobj_ins = get(&iterobj_val);
21685: 
21685:         if (guardClass(JSVAL_TO_OBJECT(iterobj_val), iterobj_ins, &js_IteratorClass, BRANCH_EXIT)) {
21685:             LIns* args[] = { iterobj_ins, cx_ins };
20915:             LIns* v_ins = lir->insCall(&js_FastCallIteratorNext_ci, args);
18680:             guard(false, lir->ins2(LIR_eq, v_ins, INS_CONST(JSVAL_ERROR_COOKIE)), OOM_EXIT);
18680: 
18680:             LIns* flag_ins = lir->ins_eq0(lir->ins2(LIR_eq, v_ins, INS_CONST(JSVAL_HOLE)));
21441:             stack(-1, v_ins);
18136:             stack(0, flag_ins);
21441: 
21441:             pendingTraceableNative = &js_FastCallIteratorNext_tn;
17899:             return true;
17899:         }
18136: 
21685:         // Custom iterator, possibly a generator.
21685:         return call_imacro(nextiter_imacro);
21685:     }
21685: 
18136:     ABORT_TRACE("for-in on a primitive value");
17899: }
17899: 
17926: bool
21441: TraceRecorder::record_IteratorNextComplete()
21441: {
21441:     JS_ASSERT(*cx->fp->regs->pc == JSOP_NEXTITER);
21685:     JS_ASSERT(pendingTraceableNative == &js_FastCallIteratorNext_tn);
21441: 
21441:     jsval& v = stackval(-2);
21441:     LIns* v_ins = get(&v);
21441:     if (unbox_jsval(v, v_ins)) {
21441:         set(&v, v_ins);
21441:         return true;
21441:     }
21441:     return false;
21441: }
21441: 
21441: bool
18085: TraceRecorder::record_JSOP_ENDITER()
18085: {
21441:     LIns* args[] = { stack(-2), cx_ins };
20915:     LIns* ok_ins = lir->insCall(&js_CloseIterator_ci, args);
18085:     guard(false, lir->ins_eq0(ok_ins), MISMATCH_EXIT);
18085:     return true;
18085: }
18085: 
18085: bool
18085: TraceRecorder::record_JSOP_FORNAME()
18085: {
18136:     jsval* vp;
21441:     if (name(vp)) {
21441:         set(vp, stack(-1));
21441:         return true;
21441:     }
21441:     return false;
18136: }
18136: 
18136: bool
18136: TraceRecorder::record_JSOP_FORPROP()
18136: {
18085:     return false;
18136: }
18136: 
18136: bool
18136: TraceRecorder::record_JSOP_FORELEM()
18136: {
21441:     return record_JSOP_DUP();
18085: }
18085: 
18085: bool
18085: TraceRecorder::record_JSOP_FORARG()
18085: {
21441:     return record_JSOP_SETARG();
18085: }
18085: 
18085: bool
18085: TraceRecorder::record_JSOP_FORLOCAL()
18085: {
21441:     return record_JSOP_SETLOCAL();
17899: }
17899: 
17926: bool
17926: TraceRecorder::record_JSOP_POPN()
17409: {
17456:     return true;
17409: }
17899: 
17926: bool
17926: TraceRecorder::record_JSOP_BINDNAME()
17409: {
17544:     JSObject* obj = cx->fp->scopeChain;
17657:     if (obj != globalObj)
17998:         ABORT_TRACE("JSOP_BINDNAME crosses global scopes");
17544: 
18286:     LIns* obj_ins = scopeChain();
17544:     JSObject* obj2;
17746:     jsuword pcval;
17746:     if (!test_property_cache(obj, obj_ins, obj2, pcval))
17544:         return false;
18669:     if (obj2 != obj)
18669:         ABORT_TRACE("JSOP_BINDNAME found a non-direct property on the global object");
17544: 
17544:     stack(0, obj_ins);
17417:     return true;
17409: }
17541: 
17926: bool
17926: TraceRecorder::record_JSOP_SETNAME()
17409: {
17545:     jsval& l = stackval(-2);
18085:     JS_ASSERT(!JSVAL_IS_PRIMITIVE(l));
17519: 
17541:     /*
17541:      * Trace cases that are global code or in lightweight functions scoped by
17541:      * the global object only.
17541:      */
17541:     JSObject* obj = JSVAL_TO_OBJECT(l);
17657:     if (obj != cx->fp->scopeChain || obj != globalObj)
19093:         ABORT_TRACE("JSOP_SETNAME left operand is not the global object");
19093: 
19093:     // The rest of the work is in record_SetPropHit.
17519:     return true;
17409: }
17519: 
17926: bool
17926: TraceRecorder::record_JSOP_THROW()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_IN()
17409: {
18495:     jsval& rval = stackval(-1);
21784:     jsval& lval = stackval(-2);
21784: 
18495:     if (JSVAL_IS_PRIMITIVE(rval))
18495:         ABORT_TRACE("JSOP_IN on non-object right operand");
21784:     JSObject* obj = JSVAL_TO_OBJECT(rval);
21784:     LIns* obj_ins = get(&rval);
18495: 
18495:     jsid id;
21784:     LIns* x;
18495:     if (JSVAL_IS_INT(lval)) {
18495:         id = INT_JSVAL_TO_JSID(lval);
21784:         LIns* args[] = { makeNumberInt32(get(&lval)), obj_ins, cx_ins };
21784:         x = lir->insCall(&js_HasNamedPropertyInt32_ci, args);
21784:     } else if (JSVAL_IS_STRING(lval)) {
21784:         if (!js_ValueToStringId(cx, lval, &id))
21784:             ABORT_TRACE("left operand of JSOP_IN didn't convert to a string-id");
21784:         LIns* args[] = { get(&lval), obj_ins, cx_ins };
21784:         x = lir->insCall(&js_HasNamedProperty_ci, args);
18495:     } else {
21784:         ABORT_TRACE("string or integer expected");
21784:     }        
21784: 
21784:     guard(false, lir->ins2i(LIR_eq, x, JSVAL_TO_BOOLEAN(JSVAL_VOID)), OOM_EXIT);
21784:     x = lir->ins2i(LIR_eq, x, 1);
18495: 
18495:     JSObject* obj2;
18495:     JSProperty* prop;
18495:     if (!OBJ_LOOKUP_PROPERTY(cx, obj, id, &obj2, &prop))
18495:         ABORT_TRACE("OBJ_LOOKUP_PROPERTY failed in JSOP_IN");
21784:     bool cond = prop != NULL;
18495:     if (prop)
18495:         OBJ_DROP_PROPERTY(cx, obj2, prop);
18495:     
18495:     /* The interpreter fuses comparisons and the following branch,
18495:        so we have to do that here as well. */
18694:     fuseIf(cx->fp->regs->pc + 1, cond, x);
18495: 
18495:     /* We update the stack after the guard. This is safe since
18495:        the guard bails out at the comparison and the interpreter
18680:        will therefore re-execute the comparison. This way the
18495:        value of the condition doesn't have to be calculated and
18495:        saved on the stack in most cases. */
18495:     set(&lval, x);
18495:     return true;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_INSTANCEOF()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_DEBUGGER()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_GOSUB()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_RETSUB()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_EXCEPTION()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_LINENO()
17409: {
17456:     return true;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_CONDSWITCH()
17409: {
17456:     return true;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_CASE()
17409: {
19576:     return cmp(LIR_feq, CMP_CASE);
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_DEFAULT()
17409: {
18687:     return true;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_EVAL()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_ENUMELEM()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_GETTER()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_SETTER()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_DEFFUN()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_DEFCONST()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_DEFVAR()
17409: {
17409:     return false;
17409: }
17926: 
17763: /*
17763:  * XXX could hoist out to jsinterp.h and share with jsinterp.cpp, but
17763:  * XXX jsopcode.cpp has different definitions of same-named macros.
17763:  */
17763: #define GET_FULL_INDEX(PCOFF)                                                 \
17763:     (atoms - script->atomMap.vector + GET_INDEX(regs.pc + PCOFF))
17763: 
17763: #define LOAD_FUNCTION(PCOFF)                                                  \
17763:     JS_GET_SCRIPT_FUNCTION(script, GET_FULL_INDEX(PCOFF), fun)
17764: 
17926: bool
18020: TraceRecorder::record_JSOP_ANONFUNOBJ()
17409: {
17763:     JSFunction* fun;
17763:     JSFrameRegs& regs = *cx->fp->regs;
17763:     JSScript* script = cx->fp->script;
18020:     LOAD_FUNCTION(0); // needs script, regs, fun
18020: 
18020:     JSObject* obj = FUN_OBJECT(fun);
18020:     if (OBJ_GET_PARENT(cx, obj) != cx->fp->scopeChain)
18020:         ABORT_TRACE("can't trace with activation object on scopeChain");
18020: 
18712:     stack(0, INS_CONSTPTR(obj));
18020:     return true;
18020: }
18020: 
18020: bool
18020: TraceRecorder::record_JSOP_NAMEDFUNOBJ()
18020: {
18020:     return false;
18020: }
18020: 
18020: bool
18020: TraceRecorder::record_JSOP_SETLOCALPOP()
18020: {
20394:     var(GET_SLOTNO(cx->fp->regs->pc), stack(-1));
20394:     return true;
18020: }
18020: 
18020: bool
18020: TraceRecorder::record_JSOP_SETCALL()
18020: {
18020:     return false;
18020: }
18020: 
18020: bool
18020: TraceRecorder::record_JSOP_TRY()
18020: {
18020:     return true;
18020: }
18020: 
18020: bool
18020: TraceRecorder::record_JSOP_FINALLY()
18020: {
18020:     return true;
18020: }
18020: 
18020: bool
18020: TraceRecorder::record_JSOP_NOP()
18020: {
18020:     return true;
18020: }
18020: 
18020: bool
18020: TraceRecorder::record_JSOP_ARGSUB()
18020: {
19068:     JSStackFrame* fp = cx->fp;
19068:     if (!(fp->fun->flags & JSFUN_HEAVYWEIGHT)) {
19068:         uintN slot = GET_ARGNO(fp->regs->pc);
19579:         if (slot < fp->fun->nargs && slot < fp->argc && !fp->argsobj) {
19068:             stack(0, get(&cx->fp->argv[slot]));
19068:             return true;
19068:         }
19068:     }
19068:     ABORT_TRACE("can't trace JSOP_ARGSUB hard case");
18020: }
18020: 
18020: bool
18020: TraceRecorder::record_JSOP_ARGCNT()
18020: {
19068:     if (!(cx->fp->fun->flags & JSFUN_HEAVYWEIGHT)) {
19068:         jsdpun u;
19068:         u.d = cx->fp->argc;
19068:         stack(0, lir->insImmq(u.u64));
19068:         return true;
19068:     }
19068:     ABORT_TRACE("can't trace heavyweight JSOP_ARGCNT");
18020: }
18020: 
18020: bool
19970: TraceRecorder::record_DefLocalFunSetSlot(uint32 slot, JSObject* obj)
19970: {
19970:     var(slot, INS_CONSTPTR(obj));
19970:     return true;
19970: }
19970: 
19970: bool
18020: TraceRecorder::record_JSOP_DEFLOCALFUN()
18020: {
17763:     return true;
17409: }
17764: 
17926: bool
17926: TraceRecorder::record_JSOP_GOTOX()
17409: {
18677:     return true;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_IFEQX()
17409: {
18694:     trackCfgMerges(cx->fp->regs->pc);
17611:     return record_JSOP_IFEQ();
17611: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_IFNEX()
17409: {
17611:     return record_JSOP_IFNE();
17611: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_ORX()
17409: {
17611:     return record_JSOP_OR();
17611: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_ANDX()
17409: {
17611:     return record_JSOP_AND();
17611: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_GOSUBX()
17409: {
17611:     return record_JSOP_GOSUB();
17611: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_CASEX()
17409: {
19576:     return cmp(LIR_feq, CMP_CASE);
17611: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_DEFAULTX()
17409: {
18687:     return true;
17611: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_TABLESWITCHX()
17409: {
18687:     return switchop();
17611: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_LOOKUPSWITCHX()
17409: {
18687:     return switchop();
17611: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_BACKPATCH()
17409: {
17456:     return true;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_BACKPATCH_POP()
17409: {
17456:     return true;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_THROWING()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_SETRVAL()
17409: {
17844:     // If we implement this, we need to update JSOP_STOP.
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_RETRVAL()
17409: {
17409:     return false;
17409: }
17899: 
17926: bool
17926: TraceRecorder::record_JSOP_GETGVAR()
17409: {
17807:     jsval slotval = cx->fp->slots[GET_SLOTNO(cx->fp->regs->pc)];
17468:     if (JSVAL_IS_NULL(slotval))
17468:         return true; // We will see JSOP_NAME from the interpreter's jump, so no-op here.
17545: 
17468:     uint32 slot = JSVAL_TO_INT(slotval);
17891: 
17892:     if (!lazilyImportGlobalSlot(slot))
17891:          ABORT_TRACE("lazy import of global slot failed");
17891: 
19973:     stack(0, get(&STOBJ_GET_SLOT(globalObj, slot)));
17468:     return true;
17409: }
17545: 
17926: bool
17926: TraceRecorder::record_JSOP_SETGVAR()
17409: {
17807:     jsval slotval = cx->fp->slots[GET_SLOTNO(cx->fp->regs->pc)];
17494:     if (JSVAL_IS_NULL(slotval))
17494:         return true; // We will see JSOP_NAME from the interpreter's jump, so no-op here.
17545: 
17494:     uint32 slot = JSVAL_TO_INT(slotval);
17891: 
17892:     if (!lazilyImportGlobalSlot(slot))
17891:          ABORT_TRACE("lazy import of global slot failed");
17891: 
19973:     set(&STOBJ_GET_SLOT(globalObj, slot), stack(-1));
17494:     return true;
17409: }
17545: 
17926: bool
17926: TraceRecorder::record_JSOP_INCGVAR()
17409: {
17807:     jsval slotval = cx->fp->slots[GET_SLOTNO(cx->fp->regs->pc)];
17468:     if (JSVAL_IS_NULL(slotval))
17468:         return true; // We will see JSOP_INCNAME from the interpreter's jump, so no-op here.
17545: 
17468:     uint32 slot = JSVAL_TO_INT(slotval);
17891: 
17892:     if (!lazilyImportGlobalSlot(slot))
17891:          ABORT_TRACE("lazy import of global slot failed");
17891: 
19973:     return inc(STOBJ_GET_SLOT(globalObj, slot), 1);
17409: }
17545: 
17926: bool
17926: TraceRecorder::record_JSOP_DECGVAR()
17409: {
17807:     jsval slotval = cx->fp->slots[GET_SLOTNO(cx->fp->regs->pc)];
17494:     if (JSVAL_IS_NULL(slotval))
17494:         return true; // We will see JSOP_INCNAME from the interpreter's jump, so no-op here.
17545: 
17494:     uint32 slot = JSVAL_TO_INT(slotval);
17891: 
17892:     if (!lazilyImportGlobalSlot(slot))
17891:          ABORT_TRACE("lazy import of global slot failed");
17891: 
19973:     return inc(STOBJ_GET_SLOT(globalObj, slot), -1);
17409: }
17545: 
17926: bool
17926: TraceRecorder::record_JSOP_GVARINC()
17409: {
17807:     jsval slotval = cx->fp->slots[GET_SLOTNO(cx->fp->regs->pc)];
17494:     if (JSVAL_IS_NULL(slotval))
17494:         return true; // We will see JSOP_INCNAME from the interpreter's jump, so no-op here.
17545: 
17494:     uint32 slot = JSVAL_TO_INT(slotval);
17891: 
17892:     if (!lazilyImportGlobalSlot(slot))
17891:          ABORT_TRACE("lazy import of global slot failed");
17891: 
19973:     return inc(STOBJ_GET_SLOT(globalObj, slot), 1, false);
17409: }
17545: 
17926: bool
17926: TraceRecorder::record_JSOP_GVARDEC()
17409: {
17807:     jsval slotval = cx->fp->slots[GET_SLOTNO(cx->fp->regs->pc)];
17494:     if (JSVAL_IS_NULL(slotval))
17494:         return true; // We will see JSOP_INCNAME from the interpreter's jump, so no-op here.
17545: 
17494:     uint32 slot = JSVAL_TO_INT(slotval);
17891: 
17892:     if (!lazilyImportGlobalSlot(slot))
17891:          ABORT_TRACE("lazy import of global slot failed");
17891: 
19973:     return inc(STOBJ_GET_SLOT(globalObj, slot), -1, false);
17409: }
17545: 
17926: bool
17926: TraceRecorder::record_JSOP_REGEXP()
17409: {
17409:     return false;
17409: }
17926: 
17926: // begin JS_HAS_XML_SUPPORT
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_DEFXMLNS()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_ANYNAME()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_QNAMEPART()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_QNAMECONST()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_QNAME()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_TOATTRNAME()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_TOATTRVAL()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_ADDATTRNAME()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_ADDATTRVAL()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_BINDXMLNAME()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_SETXMLNAME()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_XMLNAME()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_DESCENDANTS()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_FILTER()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_ENDFILTER()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_TOXML()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_TOXMLLIST()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_XMLTAGEXPR()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_XMLELTEXPR()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_XMLOBJECT()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_XMLCDATA()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_XMLCOMMENT()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_XMLPI()
17409: {
17409:     return false;
17409: }
17642: 
17926: bool
17926: TraceRecorder::record_JSOP_GETFUNNS()
17926: {
17926:     return false;
17926: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_STARTXML()
17926: {
17926:     return false;
17926: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_STARTXMLEXPR()
17926: {
17926:     return false;
17926: }
17926: 
17926: // end JS_HAS_XML_SUPPORT
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_CALLPROP()
17409: {
17630:     jsval& l = stackval(-1);
17870:     JSObject* obj;
17870:     LIns* obj_ins;
20427:     LIns* this_ins;
17870:     if (!JSVAL_IS_PRIMITIVE(l)) {
17870:         obj = JSVAL_TO_OBJECT(l);
17870:         obj_ins = get(&l);
20427:         this_ins = obj_ins; // |this| for subsequent call
22602:         if (JSVAL_IS_NULL(l))
22602:             ABORT_TRACE("callprop on null");
22602:         if (!this_ins->isconstp())
22602:             guard(false, lir->ins_eq0(this_ins), MISMATCH_EXIT);
17870:     } else {
17870:         jsint i;
17998:         debug_only(const char* protoname = NULL;)
17870:         if (JSVAL_IS_STRING(l)) {
17870:             i = JSProto_String;
17998:             debug_only(protoname = "String.prototype";)
17870:         } else if (JSVAL_IS_NUMBER(l)) {
17870:             i = JSProto_Number;
17998:             debug_only(protoname = "Number.prototype";)
19995:         } else if (JSVAL_TAG(l) == JSVAL_BOOLEAN) {
19995:             if (l == JSVAL_VOID)
19995:                 ABORT_TRACE("callprop on void");
19995:             guard(false, lir->ins2i(LIR_eq, get(&l), JSVAL_TO_BOOLEAN(JSVAL_VOID)), MISMATCH_EXIT);
17870:             i = JSProto_Boolean;
17998:             debug_only(protoname = "Boolean.prototype";)
17870:         } else {
17870:             JS_ASSERT(JSVAL_IS_NULL(l) || JSVAL_IS_VOID(l));
17870:             ABORT_TRACE("callprop on null or void");
17870:         }
17870: 
17870:         if (!js_GetClassPrototype(cx, NULL, INT_TO_JSID(i), &obj))
17870:             ABORT_TRACE("GetClassPrototype failed!");
17870: 
18712:         obj_ins = INS_CONSTPTR(obj);
17998:         debug_only(obj_ins = addName(obj_ins, protoname);)
20427:         this_ins = get(&l); // use primitive as |this|
17870:     }
17870: 
17632:     JSObject* obj2;
17746:     jsuword pcval;
17746:     if (!test_property_cache(obj, obj_ins, obj2, pcval))
17998:         return false;
17998: 
17998:     if (PCVAL_IS_NULL(pcval) || !PCVAL_IS_OBJECT(pcval))
17998:         ABORT_TRACE("callee is not an object");
17998:     JS_ASSERT(HAS_FUNCTION_CLASS(PCVAL_TO_OBJECT(pcval)));
17630: 
19054:     if (JSVAL_IS_PRIMITIVE(l)) {
19054:         JSFunction* fun = GET_FUNCTION_PRIVATE(cx, PCVAL_TO_OBJECT(pcval));
19054:         if (!PRIMITIVE_THIS_TEST(fun, l))
19054:             ABORT_TRACE("callee does not accept primitive |this|");
19054:     }
19054: 
20427:     stack(0, this_ins);
18712:     stack(-1, INS_CONSTPTR(PCVAL_TO_OBJECT(pcval)));
17630:     return true;
17409: }
17642: 
17926: bool
17926: TraceRecorder::record_JSOP_DELDESC()
17409: {
17409:     return false;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_UINT24()
17409: {
17656:     jsdpun u;
17656:     u.d = (jsdouble)GET_UINT24(cx->fp->regs->pc);
17656:     stack(0, lir->insImmq(u.u64));
17412:     return true;
17409: }
17926: 
17926: bool
17926: TraceRecorder::record_JSOP_INDEXBASE()
17611: {
17611:     atoms += GET_INDEXBASE(cx->fp->regs->pc);
17611:     return true;
17611: }
17611: 
17926: bool
17926: TraceRecorder::record_JSOP_RESETBASE()
17611: {
17611:     atoms = cx->fp->script->atomMap.vector;
17611:     return true;
17611: }
17611: 
17926: bool
17926: TraceRecorder::record_JSOP_RESETBASE0()
17611: {
17611:     atoms = cx->fp->script->atomMap.vector;
17611:     return true;
17611: }
17611: 
17926: bool
17926: TraceRecorder::record_JSOP_CALLELEM()
17409: {
17409:     return false;
17409: }
17611: 
17926: bool
17926: TraceRecorder::record_JSOP_STOP()
17611: {
21685:     JSStackFrame *fp = cx->fp;
21685: 
21685:     if (fp->imacpc) {
21685:         // End of imacro, so return true to the interpreter immediately. The
21685:         // interpreter's JSOP_STOP case will return from the imacro, back to
21685:         // the pc after the calling op, still in the same JSStackFrame.
21721:         atoms = fp->script->atomMap.vector;
21685:         return true;
21685:     }
21685: 
18001:     /*
18001:      * We know falling off the end of a constructor returns the new object that
18001:      * was passed in via fp->argv[-1], while falling off the end of a function
18001:      * returns undefined.
18001:      *
18001:      * NB: we do not support script rval (eval, API users who want the result
18001:      * of the last expression-statement, debugger API calls).
18001:      */
18001:     if (fp->flags & JSFRAME_CONSTRUCTING) {
18001:         JS_ASSERT(OBJECT_TO_JSVAL(fp->thisp) == fp->argv[-1]);
18001:         rval_ins = get(&fp->argv[-1]);
18001:     } else {
18687:         rval_ins = INS_CONST(JSVAL_TO_BOOLEAN(JSVAL_VOID));
18001:     }
17818:     clearFrameSlotsFromCache();
17818:     return true;
17611: }
17611: 
17926: bool
17926: TraceRecorder::record_JSOP_GETXPROP()
17409: {
17686:     jsval& l = stackval(-1);
17686:     if (JSVAL_IS_PRIMITIVE(l))
17686:         ABORT_TRACE("primitive-this for GETXPROP?");
17686: 
17686:     JSObject* obj = JSVAL_TO_OBJECT(l);
18096:     if (obj != cx->fp->scopeChain || obj != globalObj)
17686:         return false;
18096: 
18096:     jsval* vp;
18096:     if (!name(vp))
18096:         return false;
18096:     stack(-1, get(vp));
17686:     return true;
17409: }
17611: 
17926: bool
17926: TraceRecorder::record_JSOP_CALLXMLNAME()
17409: {
17409:     return false;
17409: }
17611: 
17926: bool
17926: TraceRecorder::record_JSOP_TYPEOFEXPR()
17409: {
18019:     return record_JSOP_TYPEOF();
17409: }
17611: 
17926: bool
17926: TraceRecorder::record_JSOP_ENTERBLOCK()
17409: {
17409:     return false;
17409: }
17611: 
17926: bool
17926: TraceRecorder::record_JSOP_LEAVEBLOCK()
17409: {
17409:     return false;
17409: }
17611: 
17926: bool
18005: TraceRecorder::record_JSOP_GENERATOR()
17409: {
17409:     return false;
21685: #if 0
21685:     JSStackFrame* fp = cx->fp;
21685:     if (fp->callobj || fp->argsobj || fp->varobj)
21685:         ABORT_TRACE("can't trace hard-case generator");
21685: 
21685:     // Generate a type map for the outgoing frame and stash it in the LIR
21685:     unsigned stackSlots = js_NativeStackSlots(cx, 0/*callDepth*/);
21685:     LIns* data = lir_buf_writer->skip(stackSlots * sizeof(uint8));
21685:     uint8* typemap = (uint8 *)data->payload();
21685:     uint8* m = typemap;
21685:     /* Determine the type of a store by looking at the current type of the actual value the
21685:        interpreter is using. For numbers we have to check what kind of store we used last
21685:        (integer or double) to figure out what the side exit show reflect in its typemap. */
21685:     FORALL_SLOTS_IN_PENDING_FRAMES(cx, 0/*callDepth*/,
21685:         *m++ = determineSlotType(vp);
21685:     );
21685:     FlushNativeStackFrame(cx, 0, typemap, state.???, NULL);
21685: 
21685:     LIns* args[] = { INS_CONST(fp->argc), INS_CONSTPTR(fp->callee), cx_ins };
21685:     LIns* g_ins = lir->insCall(&js_FastNewGenerator_ci, args);
21685:     guard(false, lir->ins_eq0(g_ins), OOM_EXIT);
21685:     return true;
21685: #endif
17409: }
17611: 
17926: bool
18005: TraceRecorder::record_JSOP_YIELD()
17409: {
17409:     return false;
17409: }
17611: 
17926: bool
17926: TraceRecorder::record_JSOP_ARRAYPUSH()
17409: {
17409:     return false;
17409: }
17611: 
17926: bool
17926: TraceRecorder::record_JSOP_ENUMCONSTELEM()
17409: {
17409:     return false;
17409: }
17611: 
17926: bool
17926: TraceRecorder::record_JSOP_LEAVEBLOCKEXPR()
17409: {
17409:     return false;
17409: }
17611: 
17926: bool
17926: TraceRecorder::record_JSOP_GETTHISPROP()
17409: {
17688:     LIns* this_ins;
17758: 
17688:     /* its safe to just use cx->fp->thisp here because getThis() returns false if thisp
17688:        is not available */
17688:     return getThis(this_ins) && getProp(cx->fp->thisp, this_ins);
17409: }
17611: 
17926: bool
17926: TraceRecorder::record_JSOP_GETARGPROP()
17409: {
17688:     return getProp(argval(GET_ARGNO(cx->fp->regs->pc)));
17409: }
17611: 
17926: bool
18005: TraceRecorder::record_JSOP_GETLOCALPROP()
17409: {
17807:     return getProp(varval(GET_SLOTNO(cx->fp->regs->pc)));
17409: }
17611: 
17926: bool
17926: TraceRecorder::record_JSOP_INDEXBASE1()
17611: {
17611:     atoms += 1 << 16;
17611:     return true;
17611: }
17611: 
17926: bool
17926: TraceRecorder::record_JSOP_INDEXBASE2()
17611: {
17611:     atoms += 2 << 16;
17611:     return true;
17611: }
17611: 
17926: bool
17926: TraceRecorder::record_JSOP_INDEXBASE3()
17611: {
17611:     atoms += 3 << 16;
17611:     return true;
17611: }
17611: 
17926: bool
17926: TraceRecorder::record_JSOP_CALLGVAR()
17409: {
18003:     jsval slotval = cx->fp->slots[GET_SLOTNO(cx->fp->regs->pc)];
18003:     if (JSVAL_IS_NULL(slotval))
18003:         return true; // We will see JSOP_CALLNAME from the interpreter's jump, so no-op here.
18003: 
18003:     uint32 slot = JSVAL_TO_INT(slotval);
18003: 
18003:     if (!lazilyImportGlobalSlot(slot))
18003:          ABORT_TRACE("lazy import of global slot failed");
18003: 
18031:     jsval& v = STOBJ_GET_SLOT(cx->fp->scopeChain, slot);
18031:     stack(0, get(&v));
18712:     stack(1, INS_CONSTPTR(NULL));
18280:     return true;
17409: }
17611: 
17926: bool
18005: TraceRecorder::record_JSOP_CALLLOCAL()
17409: {
18003:     uintN slot = GET_SLOTNO(cx->fp->regs->pc);
18003:     stack(0, var(slot));
18712:     stack(1, INS_CONSTPTR(NULL));
18280:     return true;
17409: }
17611: 
17926: bool
17926: TraceRecorder::record_JSOP_CALLARG()
17409: {
18003:     uintN slot = GET_ARGNO(cx->fp->regs->pc);
18003:     stack(0, arg(slot));
18712:     stack(1, INS_CONSTPTR(NULL));
18280:     return true;
17409: }
17611: 
17926: bool
18031: TraceRecorder::record_JSOP_NULLTHIS()
18031: {
18712:     stack(0, INS_CONSTPTR(NULL));
18280:     return true;
18031: }
18031: 
18031: bool
17926: TraceRecorder::record_JSOP_INT8()
17409: {
17656:     jsdpun u;
17656:     u.d = (jsdouble)GET_INT8(cx->fp->regs->pc);
17656:     stack(0, lir->insImmq(u.u64));
17412:     return true;
17409: }
17611: 
17926: bool
17926: TraceRecorder::record_JSOP_INT32()
17409: {
17656:     jsdpun u;
17656:     u.d = (jsdouble)GET_INT32(cx->fp->regs->pc);
17656:     stack(0, lir->insImmq(u.u64));
17412:     return true;
17409: }
17611: 
17926: bool
17926: TraceRecorder::record_JSOP_LENGTH()
17409: {
17715:     jsval& l = stackval(-1);
17869:     if (JSVAL_IS_PRIMITIVE(l)) {
17869:         if (!JSVAL_IS_STRING(l))
17869:             ABORT_TRACE("non-string primitives unsupported");
17869:         LIns* str_ins = get(&l);
18254:         LIns* len_ins = lir->insLoad(LIR_ldp, str_ins, (int)offsetof(JSString, length));
18254: 
18261:         LIns* masked_len_ins = lir->ins2(LIR_piand,
18070:                                          len_ins,
18254:                                          INS_CONSTPTR(JSSTRING_LENGTH_MASK));
18070: 
18070:         LIns *choose_len_ins =
18232:             lir->ins_choose(lir->ins_eq0(lir->ins2(LIR_piand,
18070:                                                    len_ins,
18070:                                                    INS_CONSTPTR(JSSTRFLAG_DEPENDENT))),
18070:                             masked_len_ins,
18232:                             lir->ins_choose(lir->ins_eq0(lir->ins2(LIR_piand,
18070:                                                                    len_ins,
18070:                                                                    INS_CONSTPTR(JSSTRFLAG_PREFIX))),
18232:                                             lir->ins2(LIR_piand,
18070:                                                       len_ins,
18070:                                                       INS_CONSTPTR(JSSTRDEP_LENGTH_MASK)),
19997:                                             masked_len_ins));
18070: 
18070:         set(&l, lir->ins1(LIR_i2f, choose_len_ins));
17869:         return true;
17869:     }
17869: 
17869:     JSObject* obj = JSVAL_TO_OBJECT(l);
17869:     if (!OBJ_IS_DENSE_ARRAY(cx, obj))
17715:         ABORT_TRACE("only dense arrays supported");
17899:     if (!guardDenseArray(obj, get(&l)))
17715:         ABORT_TRACE("OBJ_IS_DENSE_ARRAY but not?!?");
17899:     LIns* v_ins = lir->ins1(LIR_i2f, stobj_get_fslot(get(&l), JSSLOT_ARRAY_LENGTH));
17715:     set(&l, v_ins);
17715:     return true;
17409: }
17611: 
17926: bool
17926: TraceRecorder::record_JSOP_NEWARRAY()
17409: {
17409:     return false;
17409: }
17611: 
17926: bool
17926: TraceRecorder::record_JSOP_HOLE()
17409: {
18687:     stack(0, INS_CONST(JSVAL_TO_BOOLEAN(JSVAL_HOLE)));
17456:     return true;
17409: }
18005: 
21459: #ifdef JS_JIT_SPEW
21433: /* Prints information about entry typemaps and unstable exits for all peers at a PC */
21433: void
21433: js_DumpPeerStability(Fragmento* frago, const void* ip)
21433: {
21433:     Fragment* f;
21433:     TreeInfo* ti;
21456:     bool looped = false;
21685:     unsigned length = 0;
21433: 
21433:     for (f = frago->getLoop(ip); f != NULL; f = f->peer) {
21433:         if (!f->vmprivate)
21433:             continue;
21433:         printf("fragment %p:\nENTRY: ", f);
21433:         ti = (TreeInfo*)f->vmprivate;
21456:         if (looped)
21456:             JS_ASSERT(ti->stackTypeMap.length() == length);
21433:         for (unsigned i = 0; i < ti->stackTypeMap.length(); i++)
21433:             printf("%d ", ti->stackTypeMap.data()[i]);
21433:         printf("\n");
21433:         UnstableExit* uexit = ti->unstableExits;
21433:         while (uexit != NULL) {
21433:             printf("EXIT:  ");
21433:             uint8* m = getTypeMap(uexit->exit) + uexit->exit->numGlobalSlots;
21433:             for (unsigned i = 0; i < uexit->exit->numStackSlots; i++)
21433:                 printf("%d ", m[i]);
21433:             printf("\n");
21433:             uexit = uexit->next;
21433:         }
21456:         length = ti->stackTypeMap.length();
21456:         looped = true;
21433:     }
21433: }
21433: #endif
21433: 
21685: /*
21685:  * 17 potentially-converting binary operators:
21685:  *  | ^ & == != < <= > >= << >> >>> + - * / %
21685:  */
21685: JS_STATIC_ASSERT((uintN)(JSOP_MOD - JSOP_BITOR) == 16);
21685: 
21685: /*
21685:  * Use an old <ctype.h> trick: bias imacro_code[op] by -1 to allow zero high
21685:  * FrameInfo.ip_adj byte to mean "not in an imacro".
21685:  */
21685: static void
21685: InitIMacroCode()
21685: {
21685:     if (imacro_code[JSOP_NEXTITER]) {
21685:         JS_ASSERT(imacro_code[JSOP_NEXTITER] == nextiter_imacro - 1);
21685:         return;
21685:     }
21685: 
21685:     for (uintN op = JSOP_BITOR; op <= JSOP_MOD; op++)
21685:         imacro_code[op] = (jsbytecode*)&binary_imacros - 1;
21685: 
21685:     // NB: above loop mis-set JSOP_ADD's entry, so order here is crucial.
21685:     imacro_code[JSOP_ADD] = (jsbytecode*)&add_imacros - 1;
21685: 
21685:     imacro_code[JSOP_ITER] = (jsbytecode*)&iter_imacros - 1;
21685:     imacro_code[JSOP_NEXTITER] = nextiter_imacro - 1;
21685: }
21685: 
20969: #define UNUSED(n) bool TraceRecorder::record_JSOP_UNUSED##n() { return false; }
20969: 
20969: UNUSED(131)
20969: UNUSED(201)
20969: UNUSED(202)
20969: UNUSED(203)
20969: UNUSED(204)
20969: UNUSED(205)
20969: UNUSED(206)
20969: UNUSED(207)
21441: UNUSED(208)
21441: UNUSED(209)
20969: UNUSED(219)
20969: UNUSED(226)
