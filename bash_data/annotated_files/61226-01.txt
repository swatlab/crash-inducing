22705: /* -*- Mode: C++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*-
18056:  * vim: set ts=4 sw=4 et tw=99:
17181:  *
17181:  * ***** BEGIN LICENSE BLOCK *****
17181:  * Version: MPL 1.1/GPL 2.0/LGPL 2.1
17181:  *
17181:  * The contents of this file are subject to the Mozilla Public License Version
17181:  * 1.1 (the "License"); you may not use this file except in compliance with
17181:  * the License. You may obtain a copy of the License at
17181:  * http://www.mozilla.org/MPL/
17181:  *
17181:  * Software distributed under the License is distributed on an "AS IS" basis,
17181:  * WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
17181:  * for the specific language governing rights and limitations under the
17181:  * License.
17181:  *
17181:  * The Original Code is Mozilla SpiderMonkey JavaScript 1.9 code, released
17181:  * May 28, 2008.
17181:  *
17181:  * The Initial Developer of the Original Code is
17339:  *   Brendan Eich <brendan@mozilla.org>
17181:  *
17181:  * Contributor(s):
17339:  *   Andreas Gal <gal@mozilla.com>
17671:  *   Mike Shaver <shaver@mozilla.org>
17671:  *   David Anderson <danderson@mozilla.com>
17181:  *
17181:  * Alternatively, the contents of this file may be used under the terms of
17181:  * either of the GNU General Public License Version 2 or later (the "GPL"),
17181:  * or the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
17181:  * in which case the provisions of the GPL or the LGPL are applicable instead
17181:  * of those above. If you wish to allow use of your version of this file only
17181:  * under the terms of either the GPL or the LGPL, and not to allow others to
17181:  * use your version of this file under the terms of the MPL, indicate your
17181:  * decision by deleting the provisions above and replace them with the notice
17181:  * and other provisions required by the GPL or the LGPL. If you do not delete
17181:  * the provisions above, a recipient may use your version of this file under
17181:  * the terms of any one of the MPL, the GPL or the LGPL.
17181:  *
17181:  * ***** END LICENSE BLOCK ***** */
17181: 
26316: #include "jsstdint.h"
17923: #include "jsbit.h"              // low-level (NSPR-based) headers next
17923: #include "jsprf.h"
17666: #include <math.h>               // standard headers next
26053: 
26053: #if defined(_MSC_VER) || defined(__MINGW32__)
26053: #include <malloc.h>
17666: #ifdef _MSC_VER
17666: #define alloca _alloca
17666: #endif
26053: #endif
19058: #ifdef SOLARIS
19058: #include <alloca.h>
19058: #endif
25627: #include <limits.h>
17181: 
21062: #include "nanojit/nanojit.h"
25718: #include "jsapi.h"              // higher-level library and API headers
25718: #include "jsarray.h"
17421: #include "jsbool.h"
17630: #include "jscntxt.h"
59733: #include "jscompartment.h"
32738: #include "jsdate.h"
17949: #include "jsdbgapi.h"
17949: #include "jsemit.h"
17630: #include "jsfun.h"
17630: #include "jsinterp.h"
17899: #include "jsiter.h"
31900: #include "jsmath.h"
17666: #include "jsobj.h"
17863: #include "jsopcode.h"
18115: #include "jsregexp.h"
17949: #include "jsscope.h"
17630: #include "jsscript.h"
20969: #include "jsstaticcheck.h"
56750: #include "jstl.h"
17407: #include "jstracer.h"
28175: #include "jsxml.h"
37754: #include "jstypedarray.h"
17293: 
30283: #include "jsatominlines.h"
42714: #include "jscntxtinlines.h"
51097: #include "jsfuninlines.h"
53840: #include "jsinterpinlines.h"
40359: #include "jspropertycacheinlines.h"
37685: #include "jsobjinlines.h"
34349: #include "jsscopeinlines.h"
32737: #include "jsscriptinlines.h"
48470: #include "jscntxtinlines.h"
54855: #include "jsopcodeinlines.h"
54427: 
56217: #ifdef JS_METHODJIT
56217: #include "methodjit/MethodJIT.h"
56217: #endif
30283: 
17666: #include "jsautooplen.h"        // generated headers last
23075: #include "imacros.c.out"
17597: 
37762: #if defined(NANOJIT_ARM) && defined(__GNUC__) && defined(AVMPLUS_LINUX)
37741: #include <stdlib.h>
37741: #include <unistd.h>
37741: #include <sys/types.h>
37741: #include <sys/stat.h>
37741: #include <sys/mman.h>
37741: #include <fcntl.h>
37741: #include <string.h>
37741: #include <elf.h>
37741: #endif
37741: 
57765: #ifdef DEBUG
57765: namespace js {
57765: static const char*
57765: getExitName(ExitType type)
57765: {
57765:     static const char* exitNames[] =
57765:     {
57765:     #define MAKE_EXIT_STRING(x) #x,
57765:     JS_TM_EXITCODES(MAKE_EXIT_STRING)
57765:     #undef MAKE_EXIT_STRING
57765:     NULL
57765:     };
57765: 
57765:     JS_ASSERT(type < TOTAL_EXIT_TYPES);
57765: 
57765:     return exitNames[type];
57765: }
57765: }
57765: #endif /* DEBUG */
57765: 
37741: namespace nanojit {
37741: using namespace js;
54707: using namespace js::gc;
56750: using namespace js::tjit;
37741: 
61053: /*
61053:  * This macro is just like JS_NOT_REACHED but it exists in non-debug builds
61053:  * too.  Its presence indicates shortcomings in jstracer's handling of some
61053:  * OOM situations:
61053:  * - OOM failures in constructors, which lack a return value to pass back a
61053:  *   failure code (though it can and should be done indirectly).
61053:  * - OOM failures in the "infallible" allocators used for Nanojit.
61053:  *
61053:  * FIXME: bug 624590 is open to fix these problems.
61053:  */
61053: #define OUT_OF_MEMORY_ABORT(msg)    JS_Assert(msg, __FILE__, __LINE__);
61053: 
37741: /* Implement embedder-specific nanojit members. */
37741: 
61053: /* 
61053:  * Nanojit requires infallible allocations most of the time.  We satisfy this
61053:  * by reserving some space in each allocator which is used as a fallback if
61053:  * js_calloc() fails.  Ideallly this reserve space should be big enough to
61053:  * allow for all infallible requests made to the allocator until the next OOM
61053:  * check occurs, but it turns out that's impossible to guarantee (though it
61053:  * should be unlikely).  So we abort if the reserve runs out;  this is better
61053:  * than allowing memory errors to occur.
61053:  *
61053:  * The space calculations are as follows... between OOM checks, each
61053:  * VMAllocator can do (ie. has been seen to do) the following maximum
61053:  * allocations on 64-bits:
61053:  *
61053:  * - dataAlloc: 31 minimum-sized chunks (MIN_CHUNK_SZB) in assm->compile()
61053:  *   (though arbitrarily more could occur due to LabelStateMap additions done
61053:  *   when handling labels):  62,248 bytes.  This one is the most likely to
61053:  *   overflow.
61053:  *
61053:  * - traceAlloc: 1 minimum-sized chunk:  2,008 bytes.
61053:  *
61053:  * - tempAlloc: 1 LIR code chunk (CHUNK_SZB) and 5 minimum-sized chunks for
61053:  *   sundry small allocations:  18,048 bytes.
61053:  *
61053:  * The reserve sizes are chosen by exceeding this by a reasonable amount.
61053:  * Reserves for 32-bits are slightly more than half, because most of the
61053:  * allocated space is used to hold pointers.
61053:  *
61053:  * FIXME: Bug 624590 is open to get rid of all this.
61053:  */
61053: static const size_t DataReserveSize  = 12500 * sizeof(uintptr_t);
61053: static const size_t TraceReserveSize =  5000 * sizeof(uintptr_t);
61053: static const size_t TempReserveSize  =  1000 * sizeof(uintptr_t);
61053: 
37741: void*
61053: nanojit::Allocator::allocChunk(size_t nbytes, bool fallible)
37741: {
37741:     VMAllocator *vma = (VMAllocator*)this;
61053:     /*
61053:      * Nb: it's conceivable that request 1 might fail (in which case
61053:      * mOutOfMemory will be set) and then request 2 succeeds.  The subsequent
61053:      * OOM check will still fail, which is what we want, and the success of
61053:      * request 2 makes it less likely that the reserve space will overflow.
61053:      */
41802:     void *p = js_calloc(nbytes);
61053:     if (p) {
61053:         vma->mSize += nbytes;
61053:     } else {
37741:         vma->mOutOfMemory = true;
61053:         if (!fallible) {
61053:             p = (void *)vma->mReserveCurr;
61053:             vma->mReserveCurr += nbytes;
61053:             if (vma->mReserveCurr > vma->mReserveLimit)
61053:                 OUT_OF_MEMORY_ABORT("nanojit::Allocator::allocChunk: out of memory");
61053:             memset(p, 0, nbytes);
37741:             vma->mSize += nbytes;
61053:         }
61053:     }
37741:     return p;
37741: }
37741: 
37741: void
37741: nanojit::Allocator::freeChunk(void *p) {
37741:     VMAllocator *vma = (VMAllocator*)this;
61053:     if (p < vma->mReserve || uintptr_t(p) >= vma->mReserveLimit)
41802:         js_free(p);
37741: }
37741: 
37741: void
37741: nanojit::Allocator::postReset() {
37741:     VMAllocator *vma = (VMAllocator*)this;
37741:     vma->mOutOfMemory = false;
37741:     vma->mSize = 0;
61053:     vma->mReserveCurr = uintptr_t(vma->mReserve);
37741: }
37741: 
40248: int
40248: StackFilter::getTop(LIns* guard)
37741: {
37741:     VMSideExit* e = (VMSideExit*)guard->record()->exit;
40248:     return e->sp_adj;
37741: }
37741: 
37741: #if defined NJ_VERBOSE
57765: static void
57765: formatGuardExit(InsBuf *buf, LIns *ins)
57765: {
40300:     VMSideExit *x = (VMSideExit *)ins->record()->exit;
57765:     RefBuf b1;
57765:     if (LogController.lcbits & LC_FragProfile)
57765:         VMPI_snprintf(b1.buf, b1.len, " (GuardID=%03d)", ins->record()->profGuardID);
57765:     else
57765:         b1.buf[0] = '\0';
40300:     VMPI_snprintf(buf->buf, buf->len,
57765:                   " -> exit=%p pc=%p imacpc=%p sp%+ld rp%+ld %s%s",
57765:                   (void *)x,
37741:                   (void *)x->pc,
37741:                   (void *)x->imacpc,
37741:                   (long int)x->sp_adj,
37741:                   (long int)x->rp_adj,
57765:                   getExitName(x->exitType),
57765:                   b1.buf);
57765: }
57765: 
57765: void
57765: LInsPrinter::formatGuard(InsBuf *buf, LIns *ins)
57765: {
57765:     RefBuf b1, b2;
57765:     InsBuf b3;
57765:     formatGuardExit(&b3, ins);
57765:     VMPI_snprintf(buf->buf, buf->len,
57765:                   "%s: %s %s%s",
57765:                   formatRef(&b1, ins),
57765:                   lirNames[ins->opcode()],
57765:                   ins->oprnd1() ? formatRef(&b2, ins->oprnd1()) : "",
57765:                   b3.buf);
37741: }
38603: 
38603: void
40300: LInsPrinter::formatGuardXov(InsBuf *buf, LIns *ins)
40300: {
40300:     RefBuf b1, b2, b3;
57765:     InsBuf b4;
57765:     formatGuardExit(&b4, ins);
40300:     VMPI_snprintf(buf->buf, buf->len,
57765:                   "%s = %s %s, %s%s",
40300:                   formatRef(&b1, ins),
40300:                   lirNames[ins->opcode()],
40300:                   formatRef(&b2, ins->oprnd1()),
40300:                   formatRef(&b3, ins->oprnd2()),
57765:                   b4.buf);
38603: }
48613: 
48613: const char*
48613: nanojit::LInsPrinter::accNames[] = {
56180:     "state",        // (1 <<  0) == ACCSET_STATE
56180:     "sp",           // (1 <<  1) == ACCSET_STACK
56180:     "rp",           // (1 <<  2) == ACCSET_RSTACK
56180:     "cx",           // (1 <<  3) == ACCSET_CX
60574:     "tm",           // (1 <<  4) == ACCSET_TM
60574:     "eos",          // (1 <<  5) == ACCSET_EOS
60574:     "alloc",        // (1 <<  6) == ACCSET_ALLOC
60574:     "regs",         // (1 <<  7) == ACCSET_FRAMEREGS
60574:     "sf",           // (1 <<  8) == ACCSET_STACKFRAME
60574:     "rt",           // (1 <<  9) == ACCSET_RUNTIME
60574: 
60574:     "objclasp",     // (1 << 10) == ACCSET_OBJ_CLASP
60574:     "objflags",     // (1 << 11) == ACCSET_OBJ_FLAGS
60574:     "objshape",     // (1 << 12) == ACCSET_OBJ_SHAPE
60574:     "objproto",     // (1 << 13) == ACCSET_OBJ_PROTO
60574:     "objparent",    // (1 << 14) == ACCSET_OBJ_PARENT
60574:     "objprivate",   // (1 << 15) == ACCSET_OBJ_PRIVATE
60574:     "objcapacity",  // (1 << 16) == ACCSET_OBJ_CAPACITY
60574:     "objslots",     // (1 << 17) == ACCSET_OBJ_SLOTS
60574: 
60574:     "slots",        // (1 << 18) == ACCSET_SLOTS
60574:     "tarray",       // (1 << 19) == ACCSET_TARRAY
60574:     "tdata",        // (1 << 20) == ACCSET_TARRAY_DATA
60574:     "iter",         // (1 << 21) == ACCSET_ITER
60574:     "iterprops",    // (1 << 22) == ACCSET_ITER_PROPS
60574:     "str",          // (1 << 23) == ACCSET_STRING
60574:     "strmchars",    // (1 << 24) == ACCSET_STRING_MCHARS
60574:     "typemap",      // (1 << 25) == ACCSET_TYPEMAP
60574:     "fcslots",      // (1 << 26) == ACCSET_FCSLOTS
60574:     "argsdata",     // (1 << 27) == ACCSET_ARGS_DATA
56180: 
56180:     "?!"            // this entry should never be used, have it just in case
48613: };
60574: 
60574: JS_STATIC_ASSERT(JS_ARRAY_LENGTH(nanojit::LInsPrinter::accNames) == TM_NUM_USED_ACCS + 1);
37741: #endif
37741: 
37741: } /* namespace nanojit */
37741: 
48613: JS_DEFINE_CALLINFO_2(extern, STRING, js_IntToString, CONTEXT, INT32, 1, nanojit::ACCSET_NONE)
42641: 
37741: namespace js {
37741: 
30860: using namespace nanojit;
30860: 
28175: #if JS_HAS_XML_SUPPORT
33542: #define RETURN_VALUE_IF_XML(val, ret)                                         \
28175:     JS_BEGIN_MACRO                                                            \
48470:         if (!val.isPrimitive() && val.toObject().isXML())        \
33542:             RETURN_VALUE("xml detected", ret);                                \
28175:     JS_END_MACRO
28175: #else
33542: #define RETURN_IF_XML(val, ret) ((void) 0)
33542: #endif
33542: 
33542: #define RETURN_IF_XML_A(val) RETURN_VALUE_IF_XML(val, ARECORD_STOP)
33542: #define RETURN_IF_XML(val)   RETURN_VALUE_IF_XML(val, RECORD_STOP)
30860: 
48470: JS_STATIC_ASSERT(sizeof(JSValueType) == 1);
37037: JS_STATIC_ASSERT(offsetof(TraceNativeStorage, stack_global_buf) % 16 == 0);
27541: 
20399: /* Map to translate a type tag into a printable representation. */
48470: #ifdef DEBUG
48470: static char
48470: TypeToChar(JSValueType type)
48470: {
48470:     switch (type) {
48470:       case JSVAL_TYPE_DOUBLE: return 'D';
48470:       case JSVAL_TYPE_INT32: return 'I';
48470:       case JSVAL_TYPE_STRING: return 'S';
48470:       case JSVAL_TYPE_OBJECT: return '!';
48470:       case JSVAL_TYPE_BOOLEAN: return 'B';
48470:       case JSVAL_TYPE_NULL: return 'N';
48470:       case JSVAL_TYPE_UNDEFINED: return 'U';
48470:       case JSVAL_TYPE_MAGIC: return 'M';
48470:       case JSVAL_TYPE_FUNOBJ: return 'F';
48470:       case JSVAL_TYPE_NONFUNOBJ: return 'O';
48470:       case JSVAL_TYPE_BOXED: return '#';
48470:       case JSVAL_TYPE_STRORNULL: return 's';
48470:       case JSVAL_TYPE_OBJORNULL: return 'o';
48470:     }
48470:     return '?';
48470: }
48470: 
48470: static char
48470: ValueToTypeChar(const Value &v)
48470: {
48470:     if (v.isInt32()) return 'I';
48470:     if (v.isDouble()) return 'D';
48470:     if (v.isString()) return 'S';
48470:     if (v.isObject()) return v.toObject().isFunction() ? 'F' : 'O';
48470:     if (v.isBoolean()) return 'B';
48470:     if (v.isNull()) return 'N';
48470:     if (v.isUndefined()) return 'U';
48470:     if (v.isMagic()) return 'M';
48470:     return '?';
48470: }
48470: #endif
48470: 
20399: 
25627: /* Blacklist parameters. */
25627: 
30860: /*
30860:  * Number of iterations of a loop where we start tracing.  That is, we don't
30860:  * start tracing until the beginning of the HOTLOOP-th iteration.
30860:  */
53626: #define HOTLOOP 8
17821: 
25627: /* Attempt recording this many times before blacklisting permanently. */
25937: #define BL_ATTEMPTS 2
25627: 
28409: /* Skip this many hits before attempting recording again, after an aborted attempt. */
25627: #define BL_BACKOFF 32
25627: 
56551: /*
56551:  * If, after running a trace CHECK_LOOP_ITERS times, it hasn't done MIN_LOOP_ITERS
56551:  * iterations, we blacklist it.
56551: */
56551: #define MIN_LOOP_ITERS 200
56551: #define LOOP_CHECK_ITERS 10
56551: 
56551: #ifdef DEBUG
56551: #define LOOP_COUNT_MAX 100000000
56551: #else
56551: #define LOOP_COUNT_MAX MIN_LOOP_ITERS
56551: #endif
53324: 
17821: /* Number of times we wait to exit on a side exit before we try to extend the tree. */
18290: #define HOTEXIT 1
17821: 
25627: /* Number of times we try to extend the tree along a side exit. */
25627: #define MAXEXIT 3
25627: 
25627: /* Maximum number of peer trees allowed. */
25627: #define MAXPEERS 9
25627: 
18051: /* Max call depths for inlining. */
19070: #define MAX_CALLDEPTH 10
17852: 
34322: /* Max number of slots in a table-switch. */
34322: #define MAX_TABLE_SWITCH 256
34322: 
23447: /* Max memory needed to rebuild the interpreter stack when falling off trace. */
23447: #define MAX_INTERP_STACK_BYTES                                                \
48470:     (MAX_NATIVE_STACK_SLOTS * sizeof(Value) +                                 \
28086:      MAX_CALL_STACK_ENTRIES * sizeof(JSInlineFrame) +                         \
28086:      sizeof(JSInlineFrame)) /* possibly slow native frame at top of stack */
23447: 
18781: /* Max number of branches per tree. */
26534: #define MAX_BRANCHES 32
18781: 
27933: #define CHECK_STATUS(expr)                                                    \
27933:     JS_BEGIN_MACRO                                                            \
33542:         RecordingStatus _status = (expr);                                     \
33542:         if (_status != RECORD_CONTINUE)                                       \
27933:           return _status;                                                     \
27933:     JS_END_MACRO
27933: 
33542: #define CHECK_STATUS_A(expr)                                                  \
33542:     JS_BEGIN_MACRO                                                            \
33542:         AbortableRecordingStatus _status = InjectStatus((expr));              \
33542:         if (_status != ARECORD_CONTINUE)                                      \
33542:           return _status;                                                     \
33542:     JS_END_MACRO
33542: 
21459: #ifdef JS_JIT_SPEW
33542: #define RETURN_VALUE(msg, value)                                              \
27933:     JS_BEGIN_MACRO                                                            \
33542:         debug_only_printf(LC_TMAbort, "trace stopped: %d: %s\n", __LINE__, (msg)); \
27933:         return (value);                                                       \
27933:     JS_END_MACRO
17630: #else
33542: #define RETURN_VALUE(msg, value)   return (value)
33542: #endif
33542: 
33542: #define RETURN_STOP(msg)     RETURN_VALUE(msg, RECORD_STOP)
33542: #define RETURN_STOP_A(msg)   RETURN_VALUE(msg, ARECORD_STOP)
33542: #define RETURN_ERROR(msg)    RETURN_VALUE(msg, RECORD_ERROR)
33542: #define RETURN_ERROR_A(msg)  RETURN_VALUE(msg, ARECORD_ERROR)
27933: 
21459: #ifdef JS_JIT_SPEW
19592: struct __jitstats {
19592: #define JITSTAT(x) uint64 x;
19592: #include "jitstats.tbl"
19592: #undef JITSTAT
19623: } jitstats = { 0LL, };
19623: 
19623: JS_STATIC_ASSERT(sizeof(jitstats) % sizeof(uint64) == 0);
19592: 
19592: enum jitstat_ids {
19592: #define JITSTAT(x) STAT ## x ## ID,
19592: #include "jitstats.tbl"
19592: #undef JITSTAT
19598:     STAT_IDS_TOTAL
19592: };
19592: 
40792: static JSBool
40792: jitstats_getOnTrace(JSContext *cx, JSObject *obj, jsid id, jsval *vp)
40792: {
40792:     *vp = BOOLEAN_TO_JSVAL(JS_ON_TRACE(cx));
40792:     return true;
40792: }
40792: 
19592: static JSPropertySpec jitstats_props[] = {
19592: #define JITSTAT(x) { #x, STAT ## x ## ID, JSPROP_ENUMERATE | JSPROP_READONLY | JSPROP_PERMANENT },
19592: #include "jitstats.tbl"
19592: #undef JITSTAT
40792:     { "onTrace", 0, JSPROP_ENUMERATE | JSPROP_READONLY | JSPROP_PERMANENT, jitstats_getOnTrace, NULL },
19592:     { 0 }
19592: };
19592: 
19592: static JSBool
19592: jitstats_getProperty(JSContext *cx, JSObject *obj, jsid id, jsval *vp)
19592: {
19592:     int index = -1;
19592: 
48470:     if (JSID_IS_STRING(id)) {
59890:         JSAtom* str = JSID_TO_ATOM(id);
59890:         if (StringEqualsAscii(str, "HOTLOOP")) {
19592:             *vp = INT_TO_JSVAL(HOTLOOP);
19592:             return JS_TRUE;
19592:         }
56551: 
58011: #ifdef JS_METHODJIT
59890:         if (StringEqualsAscii(str, "profiler")) {
56551:             *vp = BOOLEAN_TO_JSVAL(cx->profilingEnabled);
56551:             return JS_TRUE;
56551:         }
58011: #endif
19592:     }
19592: 
48470:     if (JSID_IS_INT(id))
48470:         index = JSID_TO_INT(id);
19592: 
19592:     uint64 result = 0;
19592:     switch (index) {
19623: #define JITSTAT(x) case STAT ## x ## ID: result = jitstats.x; break;
19592: #include "jitstats.tbl"
19592: #undef JITSTAT
19592:       default:
19592:         *vp = JSVAL_VOID;
19592:         return JS_TRUE;
19592:     }
19592: 
19592:     if (result < JSVAL_INT_MAX) {
33166:         *vp = INT_TO_JSVAL(jsint(result));
19592:         return JS_TRUE;
19592:     }
19592:     char retstr[64];
19606:     JS_snprintf(retstr, sizeof retstr, "%llu", result);
19592:     *vp = STRING_TO_JSVAL(JS_NewStringCopyZ(cx, retstr));
19592:     return JS_TRUE;
19592: }
19592: 
19592: JSClass jitstats_class = {
19592:     "jitstats",
31452:     0,
19592:     JS_PropertyStub,       JS_PropertyStub,
19592:     jitstats_getProperty,  JS_PropertyStub,
19592:     JS_EnumerateStub,      JS_ResolveStub,
30654:     JS_ConvertStub,        NULL,
19592:     JSCLASS_NO_OPTIONAL_MEMBERS
19592: };
19592: 
19592: void
37741: InitJITStatsClass(JSContext *cx, JSObject *glob)
19592: {
19592:     JS_InitClass(cx, glob, NULL, &jitstats_class, NULL, 0, jitstats_props, NULL, NULL, NULL);
19592: }
19592: 
19623: #define AUDIT(x) (jitstats.x++)
17726: #else
17726: #define AUDIT(x) ((void)0)
21459: #endif /* JS_JIT_SPEW */
17726: 
18056: static avmplus::AvmCore s_core = avmplus::AvmCore();
18056: static avmplus::AvmCore* core = &s_core;
17185: 
21459: #ifdef JS_JIT_SPEW
30860: static void
37741: DumpPeerStability(TraceMonitor* tm, const void* ip, JSObject* globalObj, uint32 globalShape, uint32 argc);
30860: #endif
30860: 
30860: /*
30860:  * We really need a better way to configure the JIT. Shaver, where is
30860:  * my fancy JIT object?
30860:  *
30860:  * NB: this is raced on, if jstracer.cpp should ever be running MT.
30860:  * I think it's harmless tho.
30860:  */
26545: static bool did_we_check_processor_features = false;
17997: 
29883: /* ------ Debug logging control ------ */
29883: 
30860: /*
30860:  * All the logging control stuff lives in here.  It is shared between
30860:  * all threads, but I think that's OK.
30860:  */
37741: LogControl LogController;
29883: 
21459: #ifdef JS_JIT_SPEW
29883: 
30860: /*
30860:  * NB: this is raced on too, if jstracer.cpp should ever be running MT.
30860:  * Also harmless.
30860:  */
29883: static bool did_we_set_up_debug_logging = false;
29883: 
29883: static void
30860: InitJITLogController()
29883: {
29883:     char *tm, *tmf;
29883:     uint32_t bits;
29883: 
37741:     LogController.lcbits = 0;
29883: 
29883:     tm = getenv("TRACEMONKEY");
32636:     if (tm) {
32636:         fflush(NULL);
32636:         printf(
32636:             "The environment variable $TRACEMONKEY has been replaced by $TMFLAGS.\n"
32636:             "Try 'TMFLAGS=help js -j' for a list of options.\n"
32636:         );
32636:         exit(0);
32636:     }
29883: 
29883:     tmf = getenv("TMFLAGS");
29883:     if (!tmf) return;
29883: 
32636:     /* Using strstr() is really a cheap hack as far as flag decoding goes. */
32636:     if (strstr(tmf, "help")) {
32636:         fflush(NULL);
32636:         printf(
32636:             "usage: TMFLAGS=option,option,option,... where options can be:\n"
32636:             "\n"
32636:             "  help         show this message\n"
32636:             "  ------ options for jstracer & jsregexp ------\n"
32636:             "  minimal      ultra-minimalist output; try this first\n"
36379:             "  full         everything except 'treevis' and 'fragprofile'\n"
32636:             "  tracer       tracer lifetime (FIXME:better description)\n"
32636:             "  recorder     trace recording stuff (FIXME:better description)\n"
32636:             "  abort        show trace recording aborts\n"
32636:             "  stats        show trace recording stats\n"
32636:             "  regexp       show compilation & entry for regexps\n"
56551:             "  profiler     show loop profiles as they are profiled\n"
32636:             "  treevis      spew that tracevis/tree.py can parse\n"
32636:             "  ------ options for Nanojit ------\n"
32784:             "  fragprofile  count entries and exits for each fragment\n"
42670:             "  liveness     show LIR liveness at start of reader pipeline\n"
32636:             "  readlir      show LIR as it enters the reader pipeline\n"
32703:             "  aftersf      show LIR after StackFilter\n"
42670:             "  afterdce     show LIR after dead code elimination\n"
42670:             "  native       show native code (interleaved with 'afterdce')\n"
60810:             "  nativebytes  show native code bytes in 'native' output\n"
42670:             "  regalloc     show regalloc state in 'native' output\n"
42670:             "  activation   show activation state in 'native' output\n"
32636:             "\n"
32636:         );
32636:         exit(0);
32636:         /*NOTREACHED*/
32636:     }
29883: 
29883:     bits = 0;
30860: 
29883:     /* flags for jstracer.cpp */
32636:     if (strstr(tmf, "minimal")  || strstr(tmf, "full")) bits |= LC_TMMinimal;
32636:     if (strstr(tmf, "tracer")   || strstr(tmf, "full")) bits |= LC_TMTracer;
32636:     if (strstr(tmf, "recorder") || strstr(tmf, "full")) bits |= LC_TMRecorder;
32636:     if (strstr(tmf, "abort")    || strstr(tmf, "full")) bits |= LC_TMAbort;
32636:     if (strstr(tmf, "stats")    || strstr(tmf, "full")) bits |= LC_TMStats;
56551:     if (strstr(tmf, "profiler") || strstr(tmf, "full")) bits |= LC_TMProfiler;
31903:     if (strstr(tmf, "treevis"))                         bits |= LC_TMTreeVis;
30860: 
29883:     /* flags for nanojit */
32784:     if (strstr(tmf, "fragprofile"))                       bits |= LC_FragProfile;
32636:     if (strstr(tmf, "liveness")   || strstr(tmf, "full")) bits |= LC_Liveness;
32636:     if (strstr(tmf, "readlir")    || strstr(tmf, "full")) bits |= LC_ReadLIR;
32703:     if (strstr(tmf, "aftersf")    || strstr(tmf, "full")) bits |= LC_AfterSF;
42670:     if (strstr(tmf, "afterdce")   || strstr(tmf, "full")) bits |= LC_AfterDCE;
42670:     if (strstr(tmf, "native")     || strstr(tmf, "full")) bits |= LC_Native;
60810:     if (strstr(tmf, "nativebytes")|| strstr(tmf, "full")) bits |= LC_Bytes;
32636:     if (strstr(tmf, "regalloc")   || strstr(tmf, "full")) bits |= LC_RegAlloc;
42670:     if (strstr(tmf, "activation") || strstr(tmf, "full")) bits |= LC_Activation;
29883: 
37741:     LogController.lcbits = bits;
29883:     return;
29883: 
29883: }
18260: #endif
18260: 
32784: /* ------------------ Frag-level profiling support ------------------ */
32784: 
32784: #ifdef JS_JIT_SPEW
32784: 
32784: /*
32784:  * All the allocations done by this profile data-collection and
37741:  * display machinery, are done in TraceMonitor::profAlloc.  That is
61053:  * emptied out at the end of FinishJIT.  It has a lifetime from
61053:  * InitJIT to FinishJIT, which exactly matches the span
32784:  * js_FragProfiling_init to js_FragProfiling_showResults.
32784:  */
32784: template<class T>
32784: static
32784: Seq<T>* reverseInPlace(Seq<T>* seq)
32784: {
32784:     Seq<T>* prev = NULL;
32784:     Seq<T>* curr = seq;
32784:     while (curr) {
32784:         Seq<T>* next = curr->tail;
32784:         curr->tail = prev;
32784:         prev = curr;
32784:         curr = next;
32784:     }
32784:     return prev;
32784: }
32784: 
32784: // The number of top blocks to show in the profile
32784: #define N_TOP_BLOCKS 50
32784: 
32784: // Contains profile info for a single guard
32784: struct GuardPI {
32784:     uint32_t guardID; // identifying number
32784:     uint32_t count;   // count.
32784: };
32784: 
32784: struct FragPI {
32784:     uint32_t count;          // entry count for this Fragment
32784:     uint32_t nStaticExits;   // statically: the number of exits
32784:     size_t nCodeBytes;       // statically: the number of insn bytes in the main fragment
32784:     size_t nExitBytes;       // statically: the number of insn bytes in the exit paths
32784:     Seq<GuardPI>* guards;    // guards, each with its own count
32784:     uint32_t largestGuardID; // that exists in .guards
32784: };
32784: 
32784: void
37741: FragProfiling_FragFinalizer(Fragment* f, TraceMonitor* tm)
32784: {
32784:     // Recover profiling data from 'f', which is logically at the end
32784:     // of its useful lifetime.
37741:     if (!(LogController.lcbits & LC_FragProfile))
32784:         return;
32784: 
32784:     NanoAssert(f);
32784:     // Valid profFragIDs start at 1
32784:     NanoAssert(f->profFragID >= 1);
32784:     // Should be called exactly once per Fragment.  This will assert if
32784:     // you issue the same FragID to more than one Fragment.
32784:     NanoAssert(!tm->profTab->containsKey(f->profFragID));
32784: 
32784:     FragPI pi = { f->profCount,
32784:                   f->nStaticExits,
32784:                   f->nCodeBytes,
32784:                   f->nExitBytes,
32784:                   NULL, 0 };
32784: 
32784:     // Begin sanity check on the guards
32784:     SeqBuilder<GuardPI> guardsBuilder(*tm->profAlloc);
32784:     GuardRecord* gr;
32784:     uint32_t nGs = 0;
32784:     uint32_t sumOfDynExits = 0;
32784:     for (gr = f->guardsForFrag; gr; gr = gr->nextInFrag) {
32784:          nGs++;
32784:          // Also copy the data into our auxiliary structure.
32784:          // f->guardsForFrag is in reverse order, and so this
32784:          // copy preserves that ordering (->add adds at end).
32784:          // Valid profGuardIDs start at 1.
32784:          NanoAssert(gr->profGuardID > 0);
32784:          sumOfDynExits += gr->profCount;
32784:          GuardPI gpi = { gr->profGuardID, gr->profCount };
32784:          guardsBuilder.add(gpi);
32784:          if (gr->profGuardID > pi.largestGuardID)
32784:              pi.largestGuardID = gr->profGuardID;
32784:     }
32784:     pi.guards = guardsBuilder.get();
32784:     // And put the guard list in forwards order
32784:     pi.guards = reverseInPlace(pi.guards);
32784: 
32784:     // Why is this so?  Because nGs is the number of guards
32784:     // at the time the LIR was generated, whereas f->nStaticExits
32784:     // is the number of them observed by the time it makes it
32784:     // through to the assembler.  It can be the case that LIR
32784:     // optimisation removes redundant guards; hence we expect
32784:     // nGs to always be the same or higher.
32784:     NanoAssert(nGs >= f->nStaticExits);
32784: 
32784:     // Also we can assert that the sum of the exit counts
32784:     // can't exceed the entry count.  It'd be nice to assert that
32784:     // they are exactly equal, but we can't because we don't know
32784:     // how many times we got to the end of the trace.
32784:     NanoAssert(f->profCount >= sumOfDynExits);
32784: 
32784:     // End sanity check on guards
32784: 
32784:     tm->profTab->put(f->profFragID, pi);
32784: }
32784: 
32784: static void
37741: FragProfiling_showResults(TraceMonitor* tm)
32784: {
32784:     uint32_t topFragID[N_TOP_BLOCKS];
32784:     FragPI   topPI[N_TOP_BLOCKS];
32784:     uint64_t totCount = 0, cumulCount;
32784:     uint32_t totSE = 0;
32784:     size_t   totCodeB = 0, totExitB = 0;
40229:     PodArrayZero(topFragID);
40229:     PodArrayZero(topPI);
32784:     FragStatsMap::Iter iter(*tm->profTab);
32784:     while (iter.next()) {
32784:         uint32_t fragID  = iter.key();
32784:         FragPI   pi      = iter.value();
32784:         uint32_t count   = pi.count;
32784:         totCount += (uint64_t)count;
32784:         /* Find the rank for this entry, in tops */
32784:         int r = N_TOP_BLOCKS-1;
32784:         while (true) {
32784:             if (r == -1)
32784:                 break;
32784:             if (topFragID[r] == 0) {
32784:                 r--;
32784:                 continue;
32784:             }
32784:             if (count > topPI[r].count) {
32784:                 r--;
32784:                 continue;
32784:             }
32784:             break;
32784:         }
32784:         r++;
40370:         NanoAssert(r >= 0 && r <= N_TOP_BLOCKS);
32784:         /* This entry should be placed at topPI[r], and entries
32784:            at higher numbered slots moved up one. */
32784:         if (r < N_TOP_BLOCKS) {
32784:             for (int s = N_TOP_BLOCKS-1; s > r; s--) {
32784:                 topFragID[s] = topFragID[s-1];
32784:                 topPI[s]     = topPI[s-1];
32784:             }
32784:             topFragID[r] = fragID;
32784:             topPI[r]     = pi;
32784:         }
32784:     }
32784: 
37741:     LogController.printf(
32784:         "\n----------------- Per-fragment execution counts ------------------\n");
37741:     LogController.printf(
33132:         "\nTotal count = %llu\n\n", (unsigned long long int)totCount);
32784: 
37741:     LogController.printf(
32784:         "           Entry counts         Entry counts       ----- Static -----\n");
37741:     LogController.printf(
32784:         "         ------Self------     ----Cumulative---   Exits  Cbytes Xbytes   FragID\n");
37741:     LogController.printf("\n");
32784: 
32784:     if (totCount == 0)
32784:         totCount = 1; /* avoid division by zero */
32784:     cumulCount = 0;
32784:     int r;
32784:     for (r = 0; r < N_TOP_BLOCKS; r++) {
32784:         if (topFragID[r] == 0)
32784:             break;
32784:         cumulCount += (uint64_t)topPI[r].count;
37741:         LogController.printf("%3d:     %5.2f%% %9u     %6.2f%% %9llu"
32784:                              "     %3d   %5u  %5u   %06u\n",
32784:                              r,
32784:                              (double)topPI[r].count * 100.0 / (double)totCount,
32784:                              topPI[r].count,
32784:                              (double)cumulCount * 100.0 / (double)totCount,
33132:                              (unsigned long long int)cumulCount,
32784:                              topPI[r].nStaticExits,
33132:                              (unsigned int)topPI[r].nCodeBytes,
33132:                              (unsigned int)topPI[r].nExitBytes,
32784:                              topFragID[r]);
32784:         totSE += (uint32_t)topPI[r].nStaticExits;
32784:         totCodeB += topPI[r].nCodeBytes;
32784:         totExitB += topPI[r].nExitBytes;
32784:     }
37741:     LogController.printf("\nTotal displayed code bytes = %u, "
32784:                             "exit bytes = %u\n"
32784:                             "Total displayed static exits = %d\n\n",
33132:                             (unsigned int)totCodeB, (unsigned int)totExitB, totSE);
32784: 
37741:     LogController.printf("Analysis by exit counts\n\n");
32784: 
32784:     for (r = 0; r < N_TOP_BLOCKS; r++) {
32784:         if (topFragID[r] == 0)
32784:             break;
37741:         LogController.printf("FragID=%06u, total count %u:\n", topFragID[r],
32784:                                 topPI[r].count);
32784:         uint32_t madeItToEnd = topPI[r].count;
32784:         uint32_t totThisFrag = topPI[r].count;
32784:         if (totThisFrag == 0)
32784:             totThisFrag = 1;
32784:         GuardPI gpi;
32784:         // visit the guards, in forward order
32784:         for (Seq<GuardPI>* guards = topPI[r].guards; guards; guards = guards->tail) {
32784:             gpi = (*guards).head;
32784:             if (gpi.count == 0)
32784:                 continue;
32784:             madeItToEnd -= gpi.count;
37741:             LogController.printf("   GuardID=%03u    %7u (%5.2f%%)\n",
32784:                                     gpi.guardID, gpi.count,
32784:                                     100.0 * (double)gpi.count / (double)totThisFrag);
32784:         }
37741:         LogController.printf("   Looped (%03u)   %7u (%5.2f%%)\n",
32784:                                 topPI[r].largestGuardID+1,
32784:                                 madeItToEnd,
32784:                                 100.0 * (double)madeItToEnd /  (double)totThisFrag);
32784:         NanoAssert(madeItToEnd <= topPI[r].count); // else unsigned underflow
37741:         LogController.printf("\n");
32784:     }
32784: 
32784:     tm->profTab = NULL;
32784: }
32784: 
32784: #endif
32784: 
32784: /* ----------------------------------------------------------------- */
32784: 
33157: #ifdef DEBUG
33157: static JSBool FASTCALL
33157: PrintOnTrace(char* format, uint32 argc, double *argv)
33157: {
33157:     union {
33157:         struct {
33157:             uint32 lo;
33157:             uint32 hi;
33157:         } i;
33157:         double   d;
33157:         char     *cstr;
33157:         JSObject *o;
33157:         JSString *s;
33157:     } u;
33157: 
33157: #define GET_ARG() JS_BEGIN_MACRO          \
33157:         if (argi >= argc) { \
33157:         fprintf(out, "[too few args for format]"); \
33157:         break;       \
33157: } \
33157:     u.d = argv[argi++]; \
33157:     JS_END_MACRO
33157: 
33157:     FILE *out = stderr;
33157: 
33157:     uint32 argi = 0;
33157:     for (char *p = format; *p; ++p) {
33157:         if (*p != '%') {
33157:             putc(*p, out);
33157:             continue;
33157:         }
33157:         char ch = *++p;
33157:         if (!ch) {
33157:             fprintf(out, "[trailing %%]");
33157:             continue;
33157:         }
33157: 
33157:         switch (ch) {
33157:         case 'a':
33157:             GET_ARG();
33162:             fprintf(out, "[%u:%u 0x%x:0x%x %f]", u.i.lo, u.i.hi, u.i.lo, u.i.hi, u.d);
33157:             break;
33157:         case 'd':
33157:             GET_ARG();
33157:             fprintf(out, "%d", u.i.lo);
33157:             break;
33157:         case 'u':
33157:             GET_ARG();
33157:             fprintf(out, "%u", u.i.lo);
33157:             break;
33157:         case 'x':
33157:             GET_ARG();
33157:             fprintf(out, "%x", u.i.lo);
33157:             break;
33157:         case 'f':
33157:             GET_ARG();
33162:             fprintf(out, "%f", u.d);
33157:             break;
33157:         case 'o':
33157:             GET_ARG();
33157:             js_DumpObject(u.o);
33157:             break;
33157:         case 's':
33157:             GET_ARG();
33157:             {
33157:                 size_t length = u.s->length();
33157:                 // protect against massive spew if u.s is a bad pointer.
33157:                 if (length > 1 << 16)
33157:                     length = 1 << 16;
59890:                 if (u.s->isRope()) {
59890:                     fprintf(out, "<rope>");
59890:                     break;
59890:                 }
59890:                 const jschar *chars = u.s->nonRopeChars();
33157:                 for (unsigned i = 0; i < length; ++i) {
33157:                     jschar co = chars[i];
33157:                     if (co < 128)
33157:                         putc(co, out);
33157:                     else if (co < 256)
33157:                         fprintf(out, "\\u%02x", co);
33157:                     else
33157:                         fprintf(out, "\\u%04x", co);
33157:                 }
33157:             }
33157:             break;
33157:         case 'S':
33157:             GET_ARG();
33157:             fprintf(out, "%s", u.cstr);
33157:             break;
48470:         case 'v': {
48470:             GET_ARG();
48470:             Value *v = (Value *) u.i.lo;
48470:             js_DumpValue(*v);
48470:             break;
48470:         }
33157:         default:
33157:             fprintf(out, "[invalid %%%c]", *p);
33157:         }
33157:     }
33157: 
33157: #undef GET_ARG
33157: 
33157:     return JS_TRUE;
33157: }
33157: 
48613: JS_DEFINE_CALLINFO_3(extern, BOOL, PrintOnTrace, CHARPTR, UINT32, DOUBLEPTR, 0, ACCSET_STORE_ANY)
33157: 
33157: // This version is not intended to be called directly: usually it is easier to
33157: // use one of the other overloads.
33157: void
33157: TraceRecorder::tprint(const char *format, int count, nanojit::LIns *insa[])
33157: {
33157:     size_t size = strlen(format) + 1;
34347:     char* data = (char*) traceMonitor->traceAlloc->alloc(size);
33157:     memcpy(data, format, size);
33157: 
34347:     double *args = (double*) traceMonitor->traceAlloc->alloc(count * sizeof(double));
56750:     LIns* argsp_ins = w.nameImmpNonGC(args);
56750:     for (int i = 0; i < count; ++i)
56750:         w.stTprintArg(insa, argsp_ins, i);
56750: 
56750:     LIns* args_ins[] = { w.nameImmpNonGC(args), w.nameImmi(count), w.nameImmpNonGC(data) };
56750:     LIns* call_ins = w.call(&PrintOnTrace_ci, args_ins);
56750:     guard(false, w.eqi0(call_ins), MISMATCH_EXIT);
33157: }
33157: 
33157: // Generate a 'printf'-type call from trace for debugging.
33157: void
33157: TraceRecorder::tprint(const char *format)
33157: {
33157:     LIns* insa[] = { NULL };
33157:     tprint(format, 0, insa);
33157: }
33157: 
33157: void
33157: TraceRecorder::tprint(const char *format, LIns *ins)
33157: {
33157:     LIns* insa[] = { ins };
33157:     tprint(format, 1, insa);
33157: }
33157: 
33157: void
33157: TraceRecorder::tprint(const char *format, LIns *ins1, LIns *ins2)
33157: {
33157:     LIns* insa[] = { ins1, ins2 };
33157:     tprint(format, 2, insa);
33157: }
33157: 
33157: void
33157: TraceRecorder::tprint(const char *format, LIns *ins1, LIns *ins2, LIns *ins3)
33157: {
33157:     LIns* insa[] = { ins1, ins2, ins3 };
33157:     tprint(format, 3, insa);
33157: }
33157: 
33157: void
33157: TraceRecorder::tprint(const char *format, LIns *ins1, LIns *ins2, LIns *ins3, LIns *ins4)
33157: {
33157:     LIns* insa[] = { ins1, ins2, ins3, ins4 };
33157:     tprint(format, 4, insa);
33157: }
33157: 
33157: void
33157: TraceRecorder::tprint(const char *format, LIns *ins1, LIns *ins2, LIns *ins3, LIns *ins4,
33157:                       LIns *ins5)
33157: {
33157:     LIns* insa[] = { ins1, ins2, ins3, ins4, ins5 };
33157:     tprint(format, 5, insa);
33157: }
33157: 
33157: void
33157: TraceRecorder::tprint(const char *format, LIns *ins1, LIns *ins2, LIns *ins3, LIns *ins4,
33157:                       LIns *ins5, LIns *ins6)
33157: {
33157:     LIns* insa[] = { ins1, ins2, ins3, ins4, ins5, ins6 };
33157:     tprint(format, 6, insa);
33157: }
29894: #endif
29893: 
17596: Tracker::Tracker()
17213: {
33550:     pagelist = NULL;
17293: }
17239: 
17596: Tracker::~Tracker()
17293: {
17293:     clear();
17293: }
17293: 
33550: inline jsuword
33177: Tracker::getTrackerPageBase(const void* v) const
33177: {
33550:     return jsuword(v) & ~TRACKER_PAGE_MASK;
33550: }
33550: 
33550: inline jsuword
33550: Tracker::getTrackerPageOffset(const void* v) const
33550: {
33550:     return (jsuword(v) & TRACKER_PAGE_MASK) >> 2;
33177: }
33177: 
33177: struct Tracker::TrackerPage*
33177: Tracker::findTrackerPage(const void* v) const
33177: {
33177:     jsuword base = getTrackerPageBase(v);
33177:     struct Tracker::TrackerPage* p = pagelist;
17293:     while (p) {
33550:         if (p->base == base)
17293:             return p;
17293:         p = p->next;
17293:     }
33550:     return NULL;
17293: }
17293: 
33177: struct Tracker::TrackerPage*
33550: Tracker::addTrackerPage(const void* v)
33550: {
33177:     jsuword base = getTrackerPageBase(v);
41802:     struct TrackerPage* p = (struct TrackerPage*) js_calloc(sizeof(*p));
17293:     p->base = base;
17293:     p->next = pagelist;
17293:     pagelist = p;
17293:     return p;
17213: }
17213: 
17596: void
17596: Tracker::clear()
17213: {
17293:     while (pagelist) {
33177:         TrackerPage* p = pagelist;
17293:         pagelist = pagelist->next;
41802:         js_free(p);
17259:     }
17293: }
17293: 
17773: bool
17773: Tracker::has(const void *v) const
17773: {
17811:     return get(v) != NULL;
17811: }
17811: 
17811: LIns*
17811: Tracker::get(const void* v) const
17811: {
33177:     struct Tracker::TrackerPage* p = findTrackerPage(v);
17773:     if (!p)
17811:         return NULL;
33550:     return p->map[getTrackerPageOffset(v)];
17219: }
17247: 
17596: void
17596: Tracker::set(const void* v, LIns* i)
17247: {
33177:     struct Tracker::TrackerPage* p = findTrackerPage(v);
17293:     if (!p)
33177:         p = addTrackerPage(v);
33550:     p->map[getTrackerPageOffset(v)] = i;
17293: }
17293: 
30860: static inline bool
48470: hasInt32Repr(const Value &v)
48470: {
48470:     if (!v.isNumber())
48470:         return false;
48470:     if (v.isInt32())
48470:         return true;
48470:     int32_t _;
48470:     return JSDOUBLE_IS_INT32(v.toDouble(), &_);
48470: }
48470: 
48470: static inline jsint
48470: asInt32(const Value &v)
48470: {
48470:     JS_ASSERT(v.isNumber());
48470:     if (v.isInt32())
48470:         return v.toInt32();
48470: #ifdef DEBUG
48470:     int32_t _;
48470:     JS_ASSERT(JSDOUBLE_IS_INT32(v.toDouble(), &_));
48470: #endif
48470:     return jsint(v.toDouble());
48470: }
48470: 
48490: /*
48490:  * Return JSVAL_TYPE_DOUBLE for all numbers (int and double). Split
48490:  * JSVAL_TYPE_OBJECT into JSVAL_TYPE_FUNOBJ  and JSVAL_TYPE_NONFUNOBJ.
48490:  * Otherwise, just return the value's type.
48490:  */
48470: static inline JSValueType
48490: getPromotedType(const Value &v)
48470: {
48470:     if (v.isNumber())
48470:         return JSVAL_TYPE_DOUBLE;
48470:     if (v.isObject())
48470:         return v.toObject().isFunction() ? JSVAL_TYPE_FUNOBJ : JSVAL_TYPE_NONFUNOBJ;
48470:     return v.extractNonDoubleObjectTraceType();
48470: }
48470: 
48490: /*
48490:  * Return JSVAL_TYPE_INT32 for all whole numbers that fit into signed 32-bit.
48490:  * Split JSVAL_TYPE_OBJECT into JSVAL_TYPE_FUNOBJ and JSVAL_TYPE_NONFUNOBJ.
48490:  * Otherwise, just return the value's type.
48490:  */
48470: static inline JSValueType
48470: getCoercedType(const Value &v)
48470: {
48470:     if (v.isNumber()) {
48470:         int32_t _;
48470:         return (v.isInt32() || JSDOUBLE_IS_INT32(v.toDouble(), &_))
48470:                ? JSVAL_TYPE_INT32
48470:                : JSVAL_TYPE_DOUBLE;
48470:     }
48470:     if (v.isObject())
48470:         return v.toObject().isFunction() ? JSVAL_TYPE_FUNOBJ : JSVAL_TYPE_NONFUNOBJ;
48470:     return v.extractNonDoubleObjectTraceType();
48470: }
48470: 
48470: static inline JSValueType
53840: getFrameObjPtrTraceType(void *p, JSStackFrame *fp)
53840: {
53840:     if (p == fp->addressOfScopeChain()) {
53840:         JS_ASSERT(*(JSObject **)p != NULL);
53840:         return JSVAL_TYPE_NONFUNOBJ;
53840:     }
53840:     JS_ASSERT(p == fp->addressOfArgs());
53840:     return fp->hasArgsObj() ? JSVAL_TYPE_NONFUNOBJ : JSVAL_TYPE_NULL;
17464: }
17464: 
30860: static inline bool
53840: isFrameObjPtrTraceType(JSValueType t)
53840: {
53840:     return t == JSVAL_TYPE_NULL || t == JSVAL_TYPE_NONFUNOBJ;
17891: }
17891: 
30860: /* Constant seed and accumulate step borrowed from the DJB hash. */
30860: 
30860: const uintptr_t ORACLE_MASK = ORACLE_SIZE - 1;
30860: JS_STATIC_ASSERT((ORACLE_MASK & ORACLE_SIZE) == 0);
30860: 
30860: const uintptr_t FRAGMENT_TABLE_MASK = FRAGMENT_TABLE_SIZE - 1;
30860: JS_STATIC_ASSERT((FRAGMENT_TABLE_MASK & FRAGMENT_TABLE_SIZE) == 0);
30860: 
30860: const uintptr_t HASH_SEED = 5381;
22613: 
22613: static inline void
30860: HashAccum(uintptr_t& h, uintptr_t i, uintptr_t mask)
24307: {
24307:     h = ((h << 5) + h + (mask & i)) & mask;
22613: }
22613: 
30860: static JS_REQUIRES_STACK inline int
33564: StackSlotHash(JSContext* cx, unsigned slot, const void* pc)
22613: {
24307:     uintptr_t h = HASH_SEED;
53840:     HashAccum(h, uintptr_t(cx->fp()->script()), ORACLE_MASK);
33564:     HashAccum(h, uintptr_t(pc), ORACLE_MASK);
30860:     HashAccum(h, uintptr_t(slot), ORACLE_MASK);
22614:     return int(h);
22614: }
22614: 
30860: static JS_REQUIRES_STACK inline int
30860: GlobalSlotHash(JSContext* cx, unsigned slot)
22613: {
24307:     uintptr_t h = HASH_SEED;
51446:     JSStackFrame* fp = cx->fp();
22613: 
53840:     while (fp->prev())
53840:         fp = fp->prev();
22613: 
51056:     HashAccum(h, uintptr_t(fp->maybeScript()), ORACLE_MASK);
53840:     HashAccum(h, uintptr_t(fp->scopeChain().getGlobal()->shape()), ORACLE_MASK);
30860:     HashAccum(h, uintptr_t(slot), ORACLE_MASK);
22614:     return int(h);
22613: }
22613: 
29354: static inline int
30860: PCHash(jsbytecode* pc)
29354: {
29354:     return int(uintptr_t(pc) & ORACLE_MASK);
29354: }
29354: 
24290: Oracle::Oracle()
24290: {
28171:     /* Grow the oracle bitsets to their (fixed) size here, once. */
31485:     _stackDontDemote.set(ORACLE_SIZE-1);
31485:     _globalDontDemote.set(ORACLE_SIZE-1);
24290:     clear();
24290: }
24290: 
17981: /* Tell the oracle that a certain global variable should not be demoted. */
23456: JS_REQUIRES_STACK void
22613: Oracle::markGlobalSlotUndemotable(JSContext* cx, unsigned slot)
22613: {
31485:     _globalDontDemote.set(GlobalSlotHash(cx, slot));
17981: }
17981: 
17981: /* Consult with the oracle whether we shouldn't demote a certain global variable. */
23456: JS_REQUIRES_STACK bool
22613: Oracle::isGlobalSlotUndemotable(JSContext* cx, unsigned slot) const
22613: {
30860:     return _globalDontDemote.get(GlobalSlotHash(cx, slot));
17981: }
17981: 
29354: /* Tell the oracle that a certain slot at a certain stack slot should not be demoted. */
23456: JS_REQUIRES_STACK void
33564: Oracle::markStackSlotUndemotable(JSContext* cx, unsigned slot, const void* pc)
33564: {
33564:     _stackDontDemote.set(StackSlotHash(cx, slot, pc));
33564: }
33564: 
33564: JS_REQUIRES_STACK void
22613: Oracle::markStackSlotUndemotable(JSContext* cx, unsigned slot)
22613: {
42717:     markStackSlotUndemotable(cx, slot, cx->regs->pc);
17981: }
17981: 
17981: /* Consult with the oracle whether we shouldn't demote a certain slot. */
23456: JS_REQUIRES_STACK bool
33564: Oracle::isStackSlotUndemotable(JSContext* cx, unsigned slot, const void* pc) const
33564: {
33564:     return _stackDontDemote.get(StackSlotHash(cx, slot, pc));
33564: }
33564: 
33564: JS_REQUIRES_STACK bool
22613: Oracle::isStackSlotUndemotable(JSContext* cx, unsigned slot) const
22613: {
42717:     return isStackSlotUndemotable(cx, slot, cx->regs->pc);
17981: }
17981: 
29354: /* Tell the oracle that a certain slot at a certain bytecode location should not be demoted. */
29354: void
29354: Oracle::markInstructionUndemotable(jsbytecode* pc)
29354: {
31485:     _pcDontDemote.set(PCHash(pc));
29354: }
29354: 
29354: /* Consult with the oracle whether we shouldn't demote a certain bytecode location. */
29354: bool
29354: Oracle::isInstructionUndemotable(jsbytecode* pc) const
29354: {
30860:     return _pcDontDemote.get(PCHash(pc));
29354: }
29354: 
56185: /* Tell the oracle that the instruction at bytecode location should use a stronger (slower) test for -0. */
56185: void
56185: Oracle::markInstructionSlowZeroTest(jsbytecode* pc)
56185: {
56185:     _pcSlowZeroTest.set(PCHash(pc));
56185: }
56185: 
56185: /* Consult with the oracle whether we should use a stronger (slower) test for -0. */
56185: bool
56185: Oracle::isInstructionSlowZeroTest(jsbytecode* pc) const
56185: {
56185:     return _pcSlowZeroTest.get(PCHash(pc));
56185: }
56185: 
24290: void
24290: Oracle::clearDemotability()
18273: {
22613:     _stackDontDemote.reset();
22613:     _globalDontDemote.reset();
29354:     _pcDontDemote.reset();
56185:     _pcSlowZeroTest.reset();
22613: }
22613: 
41802: JS_REQUIRES_STACK void
41802: TraceRecorder::markSlotUndemotable(LinkableFragment* f, unsigned slot)
36361: {
36361:     if (slot < f->nStackTypes) {
46181:         traceMonitor->oracle->markStackSlotUndemotable(cx, slot);
31473:         return;
31473:     }
31473: 
36361:     uint16* gslots = f->globalSlots->data();
46181:     traceMonitor->oracle->markGlobalSlotUndemotable(cx, gslots[slot - f->nStackTypes]);
41802: }
41802: 
41802: JS_REQUIRES_STACK void
41802: TraceRecorder::markSlotUndemotable(LinkableFragment* f, unsigned slot, const void* pc)
36361: {
36361:     if (slot < f->nStackTypes) {
46181:         traceMonitor->oracle->markStackSlotUndemotable(cx, slot, pc);
33564:         return;
33564:     }
33564: 
36361:     uint16* gslots = f->globalSlots->data();
46181:     traceMonitor->oracle->markGlobalSlotUndemotable(cx, gslots[slot - f->nStackTypes]);
41802: }
41802: 
41802: static JS_REQUIRES_STACK bool
41802: IsSlotUndemotable(Oracle* oracle, JSContext* cx, LinkableFragment* f, unsigned slot, const void* ip)
36361: {
36361:     if (slot < f->nStackTypes)
46181:         return !oracle || oracle->isStackSlotUndemotable(cx, slot, ip);
33564: 
36361:     uint16* gslots = f->globalSlots->data();
46181:     return !oracle || oracle->isGlobalSlotUndemotable(cx, gslots[slot - f->nStackTypes]);
31495: }
31495: 
33563: class FrameInfoCache
33563: {
38568:     struct HashPolicy
38568:     {
38568:         typedef FrameInfo *Lookup;
38568:         static HashNumber hash(const FrameInfo* fi) {
48470:             size_t len = sizeof(FrameInfo) + fi->callerHeight * sizeof(JSValueType);
38568:             HashNumber h = 0;
33563:             const unsigned char *s = (const unsigned char*)fi;
33563:             for (size_t i = 0; i < len; i++, s++)
33563:                 h = JS_ROTATE_LEFT32(h, 4) ^ *s;
33563:             return h;
33563:         }
33563: 
38568:         static bool match(const FrameInfo* fi1, const FrameInfo* fi2) {
38568:             if (memcmp(fi1, fi2, sizeof(FrameInfo)) != 0)
38568:                 return false;
38568:             return memcmp(fi1->get_typemap(), fi2->get_typemap(),
48470:                           fi1->callerHeight * sizeof(JSValueType)) == 0;
38568:         }
38568:     };
38568: 
38568:     typedef HashSet<FrameInfo *, HashPolicy, SystemAllocPolicy> FrameSet;
38568: 
38568:     FrameSet set;
33563:     VMAllocator *allocator;
33563: 
33563:   public:
38568: 
38568:     FrameInfoCache(VMAllocator *allocator);
38568: 
38568:     void reset() {
38568:         set.clear();
38568:     }
38568: 
38568:     FrameInfo *memoize(FrameInfo *fi) {
38568:         FrameSet::AddPtr p = set.lookupForAdd(fi);
38568:         if (!p) {
33563:             FrameInfo* n = (FrameInfo*)
48470:                 allocator->alloc(sizeof(FrameInfo) + fi->callerHeight * sizeof(JSValueType));
48470:             memcpy(n, fi, sizeof(FrameInfo) + fi->callerHeight * sizeof(JSValueType));
38568:             if (!set.add(p, n))
38568:                 return NULL;
38568:         }
38568: 
38568:         return *p;
33563:     }
33563: };
33563: 
38568: FrameInfoCache::FrameInfoCache(VMAllocator *allocator)
38568:   : allocator(allocator)
38568: {
38568:     if (!set.init())
61053:         OUT_OF_MEMORY_ABORT("FrameInfoCache::FrameInfoCache(): out of memory");
38568: }
28105: 
28105: #define PC_HASH_COUNT 1024
28105: 
25627: static void
30860: Blacklist(jsbytecode* pc)
28105: {
29875:     AUDIT(blacklisted);
56217:     JS_ASSERT(*pc == JSOP_TRACE || *pc == JSOP_NOTRACE);
56217:     *pc = JSOP_NOTRACE;
56217: }
56217: 
56217: static void
56217: Unblacklist(JSScript *script, jsbytecode *pc)
56217: {
56217:     JS_ASSERT(*pc == JSOP_NOTRACE || *pc == JSOP_TRACE);
56217:     if (*pc == JSOP_NOTRACE) {
56217:         *pc = JSOP_TRACE;
56217: 
56217: #ifdef JS_METHODJIT
56217:         /* This code takes care of unblacklisting in the method JIT. */
60567:         js::mjit::ResetTraceHint(script, pc, GET_UINT16(pc), false);
56217: #endif
56217:     }
25627: }
33564: 
33564: static bool
33564: IsBlacklisted(jsbytecode* pc)
33564: {
56217:     if (*pc == JSOP_NOTRACE)
33564:         return true;
33564:     if (*pc == JSOP_CALL)
56217:         return *(pc + JSOP_CALL_LENGTH) == JSOP_NOTRACE;
33564:     return false;
33564: }
25627: 
25627: static void
30860: Backoff(JSContext *cx, jsbytecode* pc, Fragment* tree = NULL)
28105: {
33542:     /* N.B. This code path cannot assume the recorder is/is not alive. */
38568:     RecordAttemptMap &table = *JS_TRACE_MONITOR(cx).recordAttempts;
38568:     if (RecordAttemptMap::AddPtr p = table.lookupForAdd(pc)) {
38568:         if (p->value++ > (BL_ATTEMPTS * MAXPEERS)) {
38568:             p->value = 0;
30860:             Blacklist(pc);
25627:             return;
25627:         }
38568:     } else {
38568:         table.add(p, pc, 0);
28105:     }
28105: 
28105:     if (tree) {
25627:         tree->hits() -= BL_BACKOFF;
28105: 
28105:         /*
28105:          * In case there is no entry or no table (due to OOM) or some
28105:          * serious imbalance in the recording-attempt distribution on a
28105:          * multitree, give each tree another chance to blacklist here as
28105:          * well.
28105:          */
28105:         if (++tree->recordAttempts > BL_ATTEMPTS)
30860:             Blacklist(pc);
28105:     }
28105: }
28105: 
28105: static void
30860: ResetRecordingAttempts(JSContext *cx, jsbytecode* pc)
28105: {
38568:     RecordAttemptMap &table = *JS_TRACE_MONITOR(cx).recordAttempts;
38568:     if (RecordAttemptMap::Ptr p = table.lookup(pc))
38568:         p->value = 0;
25627: }
25627: 
24307: static inline size_t
30860: FragmentHash(const void *ip, JSObject* globalObj, uint32 globalShape, uint32 argc)
24307: {
24307:     uintptr_t h = HASH_SEED;
30860:     HashAccum(h, uintptr_t(ip), FRAGMENT_TABLE_MASK);
30860:     HashAccum(h, uintptr_t(globalObj), FRAGMENT_TABLE_MASK);
30860:     HashAccum(h, uintptr_t(globalShape), FRAGMENT_TABLE_MASK);
30860:     HashAccum(h, uintptr_t(argc), FRAGMENT_TABLE_MASK);
24307:     return size_t(h);
24307: }
24307: 
33748: static void
37741: RawLookupFirstPeer(TraceMonitor* tm, const void *ip, JSObject* globalObj,
33748:                    uint32 globalShape, uint32 argc,
35044:                    TreeFragment*& firstInBucket, TreeFragment**& prevTreeNextp)
33748: {
33748:     size_t h = FragmentHash(ip, globalObj, globalShape, argc);
35044:     TreeFragment** ppf = &tm->vmfragments[h];
33748:     firstInBucket = *ppf;
35044:     for (; TreeFragment* pf = *ppf; ppf = &pf->next) {
33748:         if (pf->globalObj == globalObj &&
33748:             pf->globalShape == globalShape &&
33748:             pf->ip == ip &&
33748:             pf->argc == argc) {
33748:             prevTreeNextp = ppf;
33748:             return;
33748:         }
33748:     }
33748:     prevTreeNextp = ppf;
33748:     return;
33748: }
33748: 
35044: static TreeFragment*
37741: LookupLoop(TraceMonitor* tm, const void *ip, JSObject* globalObj,
33748:                 uint32 globalShape, uint32 argc)
33748: {
35044:     TreeFragment *_, **prevTreeNextp;
33748:     RawLookupFirstPeer(tm, ip, globalObj, globalShape, argc, _, prevTreeNextp);
33748:     return *prevTreeNextp;
24307: }
24307: 
35044: static TreeFragment*
37741: LookupOrAddLoop(TraceMonitor* tm, const void *ip, JSObject* globalObj,
33748:                 uint32 globalShape, uint32 argc)
33748: {
35044:     TreeFragment *firstInBucket, **prevTreeNextp;
33748:     RawLookupFirstPeer(tm, ip, globalObj, globalShape, argc, firstInBucket, prevTreeNextp);
35044:     if (TreeFragment *f = *prevTreeNextp)
33748:         return f;
33748: 
32784:     verbose_only(
37741:     uint32_t profFragID = (LogController.lcbits & LC_FragProfile)
32784:                           ? (++(tm->lastFragID)) : 0;
32784:     )
36361:     TreeFragment* f = new (*tm->dataAlloc) TreeFragment(ip, tm->dataAlloc, globalObj, globalShape,
36361:                                                         argc verbose_only(, profFragID));
33748:     f->root = f;                /* f is the root of a new tree */
33748:     *prevTreeNextp = f;         /* insert f at the end of the vmfragments bucket-list */
33748:     f->next = NULL;
33748:     f->first = f;               /* initialize peer-list at f */
33748:     f->peer = NULL;
33748:     return f;
33748: }
33748: 
35044: static TreeFragment*
37741: AddNewPeerToPeerList(TraceMonitor* tm, TreeFragment* peer)
33748: {
33748:     JS_ASSERT(peer);
33748:     verbose_only(
37741:     uint32_t profFragID = (LogController.lcbits & LC_FragProfile)
33748:                           ? (++(tm->lastFragID)) : 0;
33748:     )
36361:     TreeFragment* f = new (*tm->dataAlloc) TreeFragment(peer->ip, tm->dataAlloc, peer->globalObj,
33748:                                                         peer->globalShape, peer->argc
33748:                                                         verbose_only(, profFragID));
33748:     f->root = f;                /* f is the root of a new tree */
33748:     f->first = peer->first;     /* add f to peer list */
33748:     f->peer = peer->peer;
33748:     peer->peer = f;
33748:     /* only the |first| Fragment of a peer list needs a valid |next| field */
35044:     debug_only(f->next = (TreeFragment*)0xcdcdcdcd);
24307:     return f;
24307: }
24307: 
36361: JS_REQUIRES_STACK void
46181: TreeFragment::initialize(JSContext* cx, SlotList *globalSlots, bool speculate)
36361: {
36361:     this->dependentTrees.clear();
36361:     this->linkedTrees.clear();
36361:     this->globalSlots = globalSlots;
36361: 
36361:     /* Capture the coerced type of each active slot in the type map. */
46181:     this->typeMap.captureTypes(cx, globalObj, *globalSlots, 0 /* callDepth */, speculate);
36361:     this->nStackTypes = this->typeMap.length() - globalSlots->length();
51446:     this->spOffsetAtEntry = cx->regs->sp - cx->fp()->base();
36361: 
36361: #ifdef DEBUG
53840:     this->treeFileName = cx->fp()->script()->filename;
51446:     this->treeLineNumber = js_FramePCToLineNumber(cx, cx->fp());
51446:     this->treePCOffset = FramePCOffset(cx, cx->fp());
51446: #endif
53840:     this->script = cx->fp()->script();
36361:     this->gcthings.clear();
52503:     this->shapes.clear();
36361:     this->unstableExits = NULL;
36361:     this->sideExits.clear();
36361: 
36361:     /* Determine the native frame layout at the entry point. */
51446:     this->nativeStackBase = (nStackTypes - (cx->regs->sp - cx->fp()->base())) *
36361:                              sizeof(double);
36361:     this->maxNativeStackSlots = nStackTypes;
36361:     this->maxCallDepth = 0;
53324:     this->execs = 0;
56551:     this->iters = 0;
36361: }
36361: 
36361: UnstableExit*
36361: TreeFragment::removeUnstableExit(VMSideExit* exit)
36361: {
36361:     /* Now erase this exit from the unstable exit list. */
36361:     UnstableExit** tail = &this->unstableExits;
36361:     for (UnstableExit* uexit = this->unstableExits; uexit != NULL; uexit = uexit->next) {
36361:         if (uexit->exit == exit) {
36361:             *tail = uexit->next;
36361:             return *tail;
36361:         }
36361:         tail = &uexit->next;
36361:     }
36361:     JS_NOT_REACHED("exit not in unstable exit list");
36361:     return NULL;
36361: }
36361: 
28913: #ifdef DEBUG
28913: static void
37741: AssertTreeIsUnique(TraceMonitor* tm, TreeFragment* f)
28913: {
28913:     JS_ASSERT(f->root == f);
30860: 
28913:     /*
28913:      * Check for duplicate entry type maps.  This is always wrong and hints at
28913:      * trace explosion since we are trying to stabilize something without
28913:      * properly connecting peer edges.
28913:      */
35044:     for (TreeFragment* peer = LookupLoop(tm, f->ip, f->globalObj, f->globalShape, f->argc);
28913:          peer != NULL;
28913:          peer = peer->peer) {
28913:         if (!peer->code() || peer == f)
28913:             continue;
36361:         JS_ASSERT(!f->typeMap.matches(peer->typeMap));
28913:     }
28913: }
28913: #endif
28913: 
25937: static void
56217: AttemptCompilation(JSContext *cx, JSObject* globalObj,
56217:                    JSScript* script, jsbytecode* pc, uint32 argc)
40226: {
40226:     TraceMonitor *tm = &JS_TRACE_MONITOR(cx);
40226: 
30860:     /* If we already permanently blacklisted the location, undo that. */
56217:     Unblacklist(script, pc);
30860:     ResetRecordingAttempts(cx, pc);
30860: 
30860:     /* Breathe new life into all peer fragments at the designated loop header. */
40847:     TreeFragment* f = LookupLoop(tm, pc, globalObj, globalObj->shape(), argc);
26726:     if (!f) {
26726:         /*
26726:          * If the global object's shape changed, we can't easily find the
26726:          * corresponding loop header via a hash table lookup. In this
26726:          * we simply bail here and hope that the fragment has another
26726:          * outstanding compilation attempt. This case is extremely rare.
26726:          */
26726:         return;
26726:     }
25937:     JS_ASSERT(f->root == f);
25937:     f = f->first;
25937:     while (f) {
25937:         JS_ASSERT(f->root == f);
25937:         --f->recordAttempts;
25937:         f->hits() = HOTLOOP;
25937:         f = f->peer;
25937:     }
25937: }
18273: 
36402: static const CallInfo *
56750: fcallinfo(LIns *ins)
56750: {
56750:     return ins->isop(LIR_calld) ? ins->callInfo() : NULL;
17451: }
17451: 
29354: /*
29354:  * Determine whether this operand is guaranteed to not overflow the specified
29354:  * integer operation.
29354:  */
54554: static void
54554: ChecksRequired(LOpcode op, LIns* op1, LIns* op2,
54554:                bool* needsOverflowCheck, bool* needsNegZeroCheck)
54554: {
54554:     Interval x = Interval::of(op1, 3);
54554:     Interval y = Interval::of(op2, 3);
54554:     Interval z(0, 0);
54554: 
29354:     switch (op) {
42688:       case LIR_addi:
54554:         z = Interval::add(x, y);
54554:         *needsNegZeroCheck = false;
54554:         break;
54554: 
42688:       case LIR_subi:
54554:         z = Interval::sub(x, y);
54554:         *needsNegZeroCheck = false;
54554:         break;
54554:         
54554:       case LIR_muli: {
54554:         z = Interval::mul(x, y);
54554:         // A would-be negative zero result can only occur if we have 
54554:         // mul(0, -n) or mul(-n, 0), where n != 0.  In particular, a multiply
54554:         // where one operand is a positive immediate cannot result in negative
54554:         // zero.
54554:         //
54554:         // This assumes that -0 cannot be an operand;  if one had occurred we
54554:         // would have already exited the trace in order to promote the
54554:         // computation back to doubles.
54554:         *needsNegZeroCheck = (x.canBeZero() && y.canBeNegative()) ||
54554:                              (y.canBeZero() && x.canBeNegative());
54554:         break;
54554:       }
54554: 
29354:       default:
54554:         JS_NOT_REACHED("needsOverflowCheck");
54554:     }
54554: 
54554:     *needsOverflowCheck = z.hasOverflowed;
17796: }
17796: 
30248: /*
53840:  * JSStackFrame::numActualArgs is only defined for function frames. Since the
53840:  * actual arguments of the entry frame are kept on trace, argc is included in
53840:  * the tuple identifying a fragment so that two fragments for the same loop but
53840:  * recorded with different number of actual arguments are treated as two
53840:  * completely separate trees. For this particular use, we define the number of
53840:  * actuals for global and eval frames to be 0.
53840:  */
53840: static inline uintN
53840: entryFrameArgc(JSContext *cx)
53840: {
53840:     JSStackFrame *fp = cx->fp();
53840:     return fp->isGlobalFrame() || fp->isEvalFrame() ? 0 : fp->numActualArgs();
53840: }
53840: 
53840: template <typename Visitor>
53840: static JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
53840: VisitStackAndArgs(Visitor &visitor, JSStackFrame *fp, JSStackFrame *next, Value *stack)
53840: {
53840:     if (JS_LIKELY(!next->hasOverflowArgs()))
53840:         return visitor.visitStackSlots(stack, next->formalArgsEnd() - stack, fp);
53840: 
53840:     /*
53840:      * In the case of nactual > nformal, the formals are copied by the VM onto
53840:      * the top of the stack. We only want to mark the formals once, so we
53840:      * carefully mark only the canonical actual arguments (as defined by
53840:      * JSStackFrame::canonicalActualArg).
53840:      */
53840:     uintN nactual = next->numActualArgs();
53840:     Value *actuals = next->actualArgs();
53840:     size_t nstack = (actuals - 2 /* callee,this */) - stack;
53840:     if (!visitor.visitStackSlots(stack, nstack, fp))
53840:         return false;
53840:     uintN nformal = next->numFormalArgs();
53840:     Value *formals = next->formalArgs();
53840:     if (!visitor.visitStackSlots(formals - 2, 2 + nformal, fp))
53840:         return false;
53840:     return visitor.visitStackSlots(actuals + nformal, nactual - nformal, fp);
53840: }
53840: 
53840: /*
30860:  * Visit the values in the given JSStackFrame that the tracer cares about. This
30860:  * visitor function is (implicitly) the primary definition of the native stack
30860:  * area layout. There are a few other independent pieces of code that must be
30860:  * maintained to assume the same layout. They are marked like this:
30248:  *
30248:  *   Duplicate native stack layout computation: see VisitFrameSlots header comment.
30248:  */
29880: template <typename Visitor>
30860: static JS_REQUIRES_STACK bool
53840: VisitFrameSlots(Visitor &visitor, JSContext *cx, unsigned depth, JSStackFrame *fp,
53840:                 JSStackFrame *next)
53840: {
53840:     JS_ASSERT_IF(!next, cx->fp() == fp);
53840: 
53840:     if (depth > 0 && !VisitFrameSlots(visitor, cx, depth-1, fp->prev(), fp))
29880:         return false;
29880: 
29880:     if (depth == 0) {
53840:         if (fp->isGlobalFrame()) {
53840:             visitor.setStackSlotKind("global");
53840:             Value *base = fp->slots() + fp->globalScript()->nfixed;
53840:             if (next)
53840:                 return VisitStackAndArgs(visitor, fp, next, base);
53840:             return visitor.visitStackSlots(base, cx->regs->sp - base, fp);
53840:         }
53840: 
53840:         if (JS_UNLIKELY(fp->isEvalFrame())) {
53840:             visitor.setStackSlotKind("eval");
53840:             if (!visitor.visitStackSlots(&fp->calleeValue(), 2, fp))
53840:                 return false;
53840:         } else {
53840:             /*
53840:              * Only the bottom function frame must visit its arguments; for all
53840:              * other frames, arguments are visited by the prev-frame.
53840:              */
29880:             visitor.setStackSlotKind("args");
53840:             uintN nformal = fp->numFormalArgs();
53840:             if (!visitor.visitStackSlots(fp->formalArgs() - 2, 2 + nformal, fp))
29880:                 return false;
53840:             if (JS_UNLIKELY(fp->hasOverflowArgs())) {
53840:                 if (!visitor.visitStackSlots(fp->actualArgs() + nformal,
53840:                                              fp->numActualArgs() - nformal, fp))
53840:                     return false;
53840:             }
53840:         }
53840:     }
53840: 
53840:     JS_ASSERT(fp->isFunctionFrame());
53840: 
53840:     /*
53840:      * We keep two members of JSStackFrame on trace: the args obj pointer and
53840:      * the scope chain pointer. The visitor must take care not to treat these
53840:      * as js::Value-typed variables, since they are unboxed pointers.
53840:      * Moreover, JSStackFrame compresses the args obj pointer with nactual, so
53840:      * fp->addressOfArgs() is not really a JSObject**: the visitor must treat
53840:      * !fp->hasArgsObj() as a null args obj pointer. Hence, visitFrameObjPtr
53840:      * is only passed a void *.
53840:      */
30248:     visitor.setStackSlotKind("arguments");
53840:     if (!visitor.visitFrameObjPtr(fp->addressOfArgs(), fp))
30248:         return false;
37694:     visitor.setStackSlotKind("scopeChain");
53840:     if (!visitor.visitFrameObjPtr(fp->addressOfScopeChain(), fp))
37694:         return false;
53840: 
53840:     visitor.setStackSlotKind("slots");
53840:     if (next)
53840:         return VisitStackAndArgs(visitor, fp, next, fp->slots());
53840:     return visitor.visitStackSlots(fp->slots(), cx->regs->sp - fp->slots(), fp);
29880: }
29880: 
37694: // Number of native frame slots used for 'special' values between args and vars.
37694: // Currently the two values are |arguments| (args object) and |scopeChain|.
37694: const int SPECIAL_FRAME_SLOTS = 2;
37694: 
29880: template <typename Visitor>
30860: static JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
29882: VisitStackSlots(Visitor &visitor, JSContext *cx, unsigned callDepth)
29882: {
53840:     return VisitFrameSlots(visitor, cx, callDepth, cx->fp(), NULL);
29880: }
29880: 
29880: template <typename Visitor>
30860: static JS_REQUIRES_STACK JS_ALWAYS_INLINE void
29882: VisitGlobalSlots(Visitor &visitor, JSContext *cx, JSObject *globalObj,
29882:                  unsigned ngslots, uint16 *gslots)
29882: {
29880:     for (unsigned n = 0; n < ngslots; ++n) {
29880:         unsigned slot = gslots[n];
40410:         visitor.visitGlobalSlot(&globalObj->getSlotRef(slot), n, slot);
29880:     }
29880: }
29880: 
29880: class AdjustCallerTypeVisitor;
29880: 
29880: template <typename Visitor>
30860: static JS_REQUIRES_STACK JS_ALWAYS_INLINE void
29882: VisitGlobalSlots(Visitor &visitor, JSContext *cx, SlotList &gslots)
29882: {
53840:     VisitGlobalSlots(visitor, cx, cx->fp()->scopeChain().getGlobal(),
29880:                      gslots.length(), gslots.data());
29880: }
29880: 
29880: 
29880: template <typename Visitor>
30860: static JS_REQUIRES_STACK JS_ALWAYS_INLINE void
29882: VisitSlots(Visitor& visitor, JSContext* cx, JSObject* globalObj,
29882:            unsigned callDepth, unsigned ngslots, uint16* gslots)
29882: {
29882:     if (VisitStackSlots(visitor, cx, callDepth))
29882:         VisitGlobalSlots(visitor, cx, globalObj, ngslots, gslots);
29880: }
29880: 
29880: template <typename Visitor>
30860: static JS_REQUIRES_STACK JS_ALWAYS_INLINE void
29882: VisitSlots(Visitor& visitor, JSContext* cx, unsigned callDepth,
29882:            unsigned ngslots, uint16* gslots)
29882: {
53840:     VisitSlots(visitor, cx, cx->fp()->scopeChain().getGlobal(),
29880:                callDepth, ngslots, gslots);
29880: }
29880: 
29880: template <typename Visitor>
30860: static JS_REQUIRES_STACK JS_ALWAYS_INLINE void
29882: VisitSlots(Visitor &visitor, JSContext *cx, JSObject *globalObj,
29882:            unsigned callDepth, const SlotList& slots)
29882: {
29882:     VisitSlots(visitor, cx, globalObj, callDepth, slots.length(),
29880:                slots.data());
29880: }
29880: 
29880: template <typename Visitor>
30860: static JS_REQUIRES_STACK JS_ALWAYS_INLINE void
29882: VisitSlots(Visitor &visitor, JSContext *cx, unsigned callDepth,
29882:            const SlotList& slots)
29882: {
53840:     VisitSlots(visitor, cx, cx->fp()->scopeChain().getGlobal(),
29880:                callDepth, slots.length(), slots.data());
29880: }
29880: 
29880: 
29880: class SlotVisitorBase {
33564: #if defined JS_JIT_SPEW
29880: protected:
29880:     char const *mStackSlotKind;
29880: public:
29882:     SlotVisitorBase() : mStackSlotKind(NULL) {}
29882:     JS_ALWAYS_INLINE const char *stackSlotKind() { return mStackSlotKind; }
29880:     JS_ALWAYS_INLINE void setStackSlotKind(char const *k) {
29880:         mStackSlotKind = k;
29880:     }
29882: #else
29882: public:
29882:     JS_ALWAYS_INLINE const char *stackSlotKind() { return NULL; }
29882:     JS_ALWAYS_INLINE void setStackSlotKind(char const *k) {}
29880: #endif
29880: };
29880: 
29882: struct CountSlotsVisitor : public SlotVisitorBase
29882: {
29880:     unsigned mCount;
29880:     bool mDone;
48470:     const void* mStop;
29880: public:
48470:     JS_ALWAYS_INLINE CountSlotsVisitor(const void* stop = NULL) :
29880:         mCount(0),
29880:         mDone(false),
29880:         mStop(stop)
29880:     {}
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
48470:     visitStackSlots(Value *vp, size_t count, JSStackFrame* fp) {
29880:         if (mDone)
29880:             return false;
48470:         if (mStop && size_t(((const Value *)mStop) - vp) < count) {
48470:             mCount += size_t(((const Value *)mStop) - vp);
29880:             mDone = true;
29880:             return false;
29880:         }
29880:         mCount += count;
29880:         return true;
29880:     }
29880: 
48470:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
53840:     visitFrameObjPtr(void* p, JSStackFrame* fp) {
48470:         if (mDone)
48470:             return false;
48470:         if (mStop && mStop == p) {
48470:             mDone = true;
48470:             return false;
48470:         }
48470:         mCount++;
48470:         return true;
48470:     }
48470: 
29880:     JS_ALWAYS_INLINE unsigned count() {
29880:         return mCount;
29880:     }
29880: 
29880:     JS_ALWAYS_INLINE bool stopped() {
29880:         return mDone;
29880:     }
29880: };
17820: 
53840: static JS_REQUIRES_STACK JS_ALWAYS_INLINE unsigned
53840: CountStackAndArgs(JSStackFrame *next, Value *stack)
53840: {
53840:     if (JS_LIKELY(!next->hasOverflowArgs()))
53840:         return (Value *)next - stack;
53840:     size_t nvals = (next->formalArgs() - 2 /* callee, this */) - stack;
54405:     JS_ASSERT(nvals == unsigned((next->actualArgs() - 2) - stack) + (2 + next->numActualArgs()));
53840:     return nvals;
53840: }
53840: 
53840: static JS_ALWAYS_INLINE uintN
53840: NumSlotsBeforeFixed(JSStackFrame *fp)
53840: {
53840:     uintN numArgs = fp->isEvalFrame() ? 0 : Max(fp->numActualArgs(), fp->numFormalArgs());
53840:     return 2 + numArgs + SPECIAL_FRAME_SLOTS;
53840: }
53840: 
30860: /*
30860:  * Calculate the total number of native frame slots we need from this frame all
30860:  * the way back to the entry frame, including the current stack usage.
53840:  *
53840:  * Duplicate native stack layout computation: see VisitFrameSlots header comment.
30860:  */
22652: JS_REQUIRES_STACK unsigned
30860: NativeStackSlots(JSContext *cx, unsigned callDepth)
18193: {
53840:     JSStackFrame *fp = cx->fp();
53840:     JSStackFrame *next = NULL;
18144:     unsigned slots = 0;
29880:     unsigned depth = callDepth;
53840: 
53840:     for (; depth > 0; --depth, next = fp, fp = fp->prev()) {
53840:         JS_ASSERT(fp->isFunctionFrame() && !fp->isEvalFrame());
53840:         slots += SPECIAL_FRAME_SLOTS;
53840:         if (next)
53840:             slots += CountStackAndArgs(next, fp->slots());
53840:         else
53840:             slots += cx->regs->sp - fp->slots();
53840:     }
53840: 
53840:     Value *start;
53840:     if (fp->isGlobalFrame()) {
53840:         start = fp->slots() + fp->globalScript()->nfixed;
53840:     } else {
53840:         start = fp->slots();
53840:         slots += NumSlotsBeforeFixed(fp);
53840:     }
53840:     if (next)
53840:         slots += CountStackAndArgs(next, start);
53840:     else
53840:         slots += cx->regs->sp - start;
53840: 
29880: #ifdef DEBUG
29880:     CountSlotsVisitor visitor;
29882:     VisitStackSlots(visitor, cx, callDepth);
29880:     JS_ASSERT(visitor.count() == slots && !visitor.stopped());
18144: #endif
18144:     return slots;
18144: }
18144: 
29882: class CaptureTypesVisitor : public SlotVisitorBase
29882: {
29880:     JSContext* mCx;
48470:     JSValueType* mTypeMap;
48470:     JSValueType* mPtr;
46181:     Oracle   * mOracle;
29880: 
29880: public:
48470:     JS_ALWAYS_INLINE CaptureTypesVisitor(JSContext* cx, JSValueType* typeMap, bool speculate) :
29880:         mCx(cx),
29880:         mTypeMap(typeMap),
46181:         mPtr(typeMap),
46181:         mOracle(speculate ? JS_TRACE_MONITOR(cx).oracle : NULL) {}
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE void
48470:     visitGlobalSlot(Value *vp, unsigned n, unsigned slot) {
48470:             JSValueType type = getCoercedType(*vp);
48470:             if (type == JSVAL_TYPE_INT32 && (!mOracle || mOracle->isGlobalSlotUndemotable(mCx, slot)))
48470:                 type = JSVAL_TYPE_DOUBLE;
48470:             JS_ASSERT(type != JSVAL_TYPE_BOXED);
29883:             debug_only_printf(LC_TMTracer,
48470:                               "capture type global%d: %c\n",
48470:                               n, TypeToChar(type));
29880:             *mPtr++ = type;
29880:     }
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
48470:     visitStackSlots(Value *vp, int count, JSStackFrame* fp) {
29880:         for (int i = 0; i < count; ++i) {
48470:             JSValueType type = getCoercedType(vp[i]);
48470:             if (type == JSVAL_TYPE_INT32 && (!mOracle || mOracle->isStackSlotUndemotable(mCx, length())))
48470:                 type = JSVAL_TYPE_DOUBLE;
48470:             JS_ASSERT(type != JSVAL_TYPE_BOXED);
29883:             debug_only_printf(LC_TMTracer,
48470:                               "capture type %s%d: %c\n",
48470:                               stackSlotKind(), i, TypeToChar(type));
29880:             *mPtr++ = type;
29880:         }
29880:         return true;
29880:     }
29880: 
48470:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
53840:     visitFrameObjPtr(void* p, JSStackFrame* fp) {
53840:         JSValueType type = getFrameObjPtrTraceType(p, fp);
48470:         debug_only_printf(LC_TMTracer,
48470:                           "capture type %s%d: %c\n",
48470:                           stackSlotKind(), 0, TypeToChar(type));
48470:         *mPtr++ = type;
48470:         return true;
48470:     }
48470: 
29880:     JS_ALWAYS_INLINE uintptr_t length() {
29880:         return mPtr - mTypeMap;
29880:     }
29880: };
29880: 
36401: void
36401: TypeMap::set(unsigned stackSlots, unsigned ngslots,
48470:              const JSValueType* stackTypeMap, const JSValueType* globalTypeMap)
36401: {
36401:     setLength(ngslots + stackSlots);
48470:     memcpy(data(), stackTypeMap, stackSlots * sizeof(JSValueType));
48470:     memcpy(data() + stackSlots, globalTypeMap, ngslots * sizeof(JSValueType));
36401: }
36401: 
24246: /*
24246:  * Capture the type map for the selected slots of the global object and currently pending
24246:  * stack frames.
24246:  */
22652: JS_REQUIRES_STACK void
46181: TypeMap::captureTypes(JSContext* cx, JSObject* globalObj, SlotList& slots, unsigned callDepth,
46181:                       bool speculate)
29880: {
30860:     setLength(NativeStackSlots(cx, callDepth) + slots.length());
46181:     CaptureTypesVisitor visitor(cx, data(), speculate);
29882:     VisitSlots(visitor, cx, globalObj, callDepth, slots);
29880:     JS_ASSERT(visitor.length() == length());
24246: }
24246: 
22652: JS_REQUIRES_STACK void
46181: TypeMap::captureMissingGlobalTypes(JSContext* cx, JSObject* globalObj, SlotList& slots, unsigned stackSlots,
46181:                                    bool speculate)
24246: {
24246:     unsigned oldSlots = length() - stackSlots;
24246:     int diff = slots.length() - oldSlots;
24246:     JS_ASSERT(diff >= 0);
24246:     setLength(length() + diff);
46181:     CaptureTypesVisitor visitor(cx, data() + stackSlots + oldSlots, speculate);
29882:     VisitGlobalSlots(visitor, cx, globalObj, diff, slots.data() + oldSlots);
17986: }
17986: 
17986: /* Compare this type map to another one and see whether they match. */
17986: bool
18239: TypeMap::matches(TypeMap& other) const
18239: {
18239:     if (length() != other.length())
18239:         return false;
17987:     return !memcmp(data(), other.data(), length());
17985: }
17985: 
31495: void
48470: TypeMap::fromRaw(JSValueType* other, unsigned numSlots)
31495: {
31495:     unsigned oldLength = length();
31495:     setLength(length() + numSlots);
31495:     for (unsigned i = 0; i < numSlots; i++)
31495:         get(oldLength + i) = other[i];
31495: }
31495: 
30860: /*
30860:  * Use the provided storage area to create a new type map that contains the
30860:  * partial type map with the rest of it filled up from the complete type
30860:  * map.
30860:  */
28956: static void
48470: MergeTypeMaps(JSValueType** partial, unsigned* plength, JSValueType* complete, unsigned clength, JSValueType* mem)
28956: {
28956:     unsigned l = *plength;
28956:     JS_ASSERT(l < clength);
48470:     memcpy(mem, *partial, l * sizeof(JSValueType));
48470:     memcpy(mem + l, complete + l, (clength - l) * sizeof(JSValueType));
28956:     *partial = mem;
28956:     *plength = clength;
28956: }
28956: 
36397: /*
36397:  * Specializes a tree to any specifically missing globals, including any
36397:  * dependent trees.
36397:  */
36397: static JS_REQUIRES_STACK void
48470: SpecializeTreesToLateGlobals(JSContext* cx, TreeFragment* root, JSValueType* globalTypeMap,
36397:                             unsigned numGlobalSlots)
36397: {
36397:     for (unsigned i = root->nGlobalTypes(); i < numGlobalSlots; i++)
36397:         root->typeMap.add(globalTypeMap[i]);
36397: 
36397:     JS_ASSERT(root->nGlobalTypes() == numGlobalSlots);
36397: 
36397:     for (unsigned i = 0; i < root->dependentTrees.length(); i++) {
36397:         TreeFragment* tree = root->dependentTrees[i];
36397:         if (tree->code() && tree->nGlobalTypes() < numGlobalSlots)
36397:             SpecializeTreesToLateGlobals(cx, tree, globalTypeMap, numGlobalSlots);
36397:     }
36397:     for (unsigned i = 0; i < root->linkedTrees.length(); i++) {
36397:         TreeFragment* tree = root->linkedTrees[i];
36397:         if (tree->code() && tree->nGlobalTypes() < numGlobalSlots)
36397:             SpecializeTreesToLateGlobals(cx, tree, globalTypeMap, numGlobalSlots);
36397:     }
36397: }
36397: 
25491: /* Specializes a tree to any missing globals, including any dependent trees. */
25509: static JS_REQUIRES_STACK void
36361: SpecializeTreesToMissingGlobals(JSContext* cx, JSObject* globalObj, TreeFragment* root)
36361: {
46181:     /* If we already have a bunch of peer trees, try to be as generic as possible. */
46181:     size_t count = 0;
46181:     for (TreeFragment *f = root->first; f; f = f->peer, ++count);
46181:     bool speculate = count < MAXPEERS-1;
46181: 
46181:     root->typeMap.captureMissingGlobalTypes(cx, globalObj, *root->globalSlots, root->nStackTypes,
46181:                                             speculate);
36361:     JS_ASSERT(root->globalSlots->length() == root->typeMap.length() - root->nStackTypes);
25491: 
36397:     SpecializeTreesToLateGlobals(cx, root, root->globalTypeMap(), root->nGlobalTypes());
30860: }
30860: 
36712: static void
35083: ResetJITImpl(JSContext* cx);
35083: 
35083: #ifdef MOZ_TRACEVIS
36712: static JS_INLINE void
35083: ResetJIT(JSContext* cx, TraceVisFlushReason r)
35083: {
37741:     LogTraceVisEvent(cx, S_RESET, r);
35083:     ResetJITImpl(cx);
35083: }
35083: #else
37741: # define ResetJIT(cx, reason) ResetJITImpl(cx)
35083: #endif
35083: 
36712: void
37741: FlushJITCache(JSContext *cx)
36712: {
36712:     ResetJIT(cx, FR_OOM);
36712: }
36712: 
18650: static void
54718: TrashTree(TreeFragment* f);
18650: 
22652: JS_REQUIRES_STACK
35044: TraceRecorder::TraceRecorder(JSContext* cx, VMSideExit* anchor, VMFragment* fragment,
48470:                              unsigned stackSlots, unsigned ngslots, JSValueType* typeMap,
56217:                              VMSideExit* innermost, JSScript* outerScript, jsbytecode* outerPC,
56217:                              uint32 outerArgc, bool speculate)
34351:   : cx(cx),
34351:     traceMonitor(&JS_TRACE_MONITOR(cx)),
46181:     oracle(speculate ? JS_TRACE_MONITOR(cx).oracle : NULL),
34351:     fragment(fragment),
36361:     tree(fragment->root),
36361:     globalObj(tree->globalObj),
56217:     outerScript(outerScript),
56217:     outerPC(outerPC),
34351:     outerArgc(outerArgc),
34351:     anchor(anchor),
34351:     cx_ins(NULL),
34351:     eos_ins(NULL),
34351:     eor_ins(NULL),
34351:     loopLabel(NULL),
36401:     importTypeMap(&tempAlloc()),
36377:     lirbuf(new (tempAlloc()) LirBuffer(tempAlloc())),
34351:     mark(*traceMonitor->traceAlloc),
36361:     numSideExitsBefore(tree->sideExits.length()),
34351:     tracker(),
34351:     nativeFrameTracker(),
56180:     global_slots(NULL),
34351:     callDepth(anchor ? anchor->calldepth : 0),
51446:     atoms(FrameAtomBase(cx, cx->fp())),
59221:     consts(JSScript::isValidOffset(cx->fp()->script()->constOffset)
53840:            ? cx->fp()->script()->consts()->vector
51446:            : NULL),
54169:     strictModeCode_ins(NULL),
34351:     cfgMerges(&tempAlloc()),
34351:     trashSelf(false),
34351:     whichTreesToTrash(&tempAlloc()),
38568:     guardedShapeTable(cx),
58056:     initDepth(0),
58056:     hadNewInit(false),
60811: #ifdef DEBUG
60811:     addPropShapeBefore(NULL),
60811: #endif
34351:     rval_ins(NULL),
34351:     native_rval_ins(NULL),
34351:     newobj_ins(NULL),
34351:     pendingSpecializedNative(NULL),
34351:     pendingUnboxSlot(NULL),
34351:     pendingGuardCondition(NULL),
60559:     pendingGlobalSlotsToSet(cx),
34351:     pendingLoop(true),
34351:     generatedSpecializedNative(),
56750:     tempTypeMap(cx),
56750:     w(&tempAlloc(), lirbuf)
18211: {
53840:     JS_ASSERT(globalObj == cx->fp()->scopeChain().getGlobal());
52503:     JS_ASSERT(globalObj->hasOwnShape());
42717:     JS_ASSERT(cx->regs->pc == (jsbytecode*)fragment->ip);
34351: 
36377: #ifdef DEBUG
48613:     lirbuf->printer = new (tempAlloc()) LInsPrinter(tempAlloc(), TM_NUM_USED_ACCS);
36377: #endif
36377: 
34351:     /*
34351:      * Reset the fragment state we care about in case we got a recycled
34351:      * fragment.  This includes resetting any profiling data we might have
34351:      * accumulated.
34351:      */
34351:     fragment->lastIns = NULL;
34351:     fragment->setCode(NULL);
34351:     fragment->lirbuf = lirbuf;
34351:     verbose_only( fragment->profCount = 0; )
34351:     verbose_only( fragment->nStaticExits = 0; )
34351:     verbose_only( fragment->nCodeBytes = 0; )
34351:     verbose_only( fragment->nExitBytes = 0; )
34351:     verbose_only( fragment->guardNumberer = 1; )
34351:     verbose_only( fragment->guardsForFrag = NULL; )
34351:     verbose_only( fragment->loopLabel = NULL; )
34351: 
34351:     /*
34351:      * Don't change fragment->profFragID, though.  Once the identity of the
34351:      * Fragment is set up (for profiling purposes), we can't change it.
34351:      */
32779: 
38568:     if (!guardedShapeTable.init())
61053:         OUT_OF_MEMORY_ABORT("TraceRecorder::TraceRecorder: out of memory");
33560: 
29883: #ifdef JS_JIT_SPEW
29883:     debug_only_print0(LC_TMMinimal, "\n");
32784:     debug_only_printf(LC_TMMinimal, "Recording starting from %s:%u@%u (FragID=%06u)\n",
36361:                       tree->treeFileName, tree->treeLineNumber, tree->treePCOffset,
34351:                       fragment->profFragID);
29883: 
29883:     debug_only_printf(LC_TMTracer, "globalObj=%p, shape=%d\n",
40847:                       (void*)this->globalObj, this->globalObj->shape());
31937:     debug_only_printf(LC_TMTreeVis, "TREEVIS RECORD FRAG=%p ANCHOR=%p\n", (void*)fragment,
31937:                       (void*)anchor);
29883: #endif
17414: 
56750:     /* This creates the LIR writer pipeline. */
56750:     w.init(&LogController);
56750: 
56750:     w.start();
17663: 
32600:     for (int i = 0; i < NumSavedRegs; ++i)
56750:         w.paramp(i, 1);
32600: #ifdef DEBUG
32600:     for (int i = 0; i < NumSavedRegs; ++i)
56750:         w.name(lirbuf->savedRegs[i], regNames[REGNUM(Assembler::savedRegs[i])]);
56750: #endif
56750: 
56750:     lirbuf->state = w.name(w.paramp(0, 0), "state");
32643: 
56719:     if (fragment == fragment->root) {
56750:         w.comment("begin-loop");
56750:         InitConst(loopLabel) = w.label();
56750:     }
56750:     w.comment("begin-setup");
32600: 
32784:     // if profiling, drop a label, so the assembler knows to put a
32784:     // frag-entry-counter increment at this point.  If there's a
32784:     // loopLabel, use that; else we'll have to make a dummy label
32784:     // especially for this purpose.
37741:     verbose_only( if (LogController.lcbits & LC_FragProfile) {
32784:         LIns* entryLabel = NULL;
32784:         if (fragment == fragment->root) {
32784:             entryLabel = loopLabel;
32784:         } else {
56750:             entryLabel = w.label();
32784:         }
32784:         NanoAssert(entryLabel);
32784:         NanoAssert(!fragment->loopLabel);
32784:         fragment->loopLabel = entryLabel;
32784:     })
32784: 
57747:     lirbuf->sp = w.name(w.ldpStateField(sp), "sp");
57747:     lirbuf->rp = w.name(w.ldpStateField(rp), "rp");
57747:     InitConst(cx_ins) = w.name(w.ldpStateField(cx), "cx");
57747:     InitConst(eos_ins) = w.name(w.ldpStateField(eos), "eos");
57747:     InitConst(eor_ins) = w.name(w.ldpStateField(eor), "eor");
56750: 
56750:     strictModeCode_ins = w.name(w.immi(cx->fp()->script()->strictModeCode), "strict");
54169: 
24246:     /* If we came from exit, we might not have enough global types. */
36361:     if (tree->globalSlots->length() > tree->nGlobalTypes())
36361:         SpecializeTreesToMissingGlobals(cx, globalObj, tree);
17334: 
17997:     /* read into registers all values on the stack and all globals we know so far */
36361:     import(tree, lirbuf->sp, stackSlots, ngslots, callDepth, typeMap);
18284: 
22615:     if (fragment == fragment->root) {
25087:         /*
25087:          * We poll the operation callback request flag. It is updated asynchronously whenever
56750:          * the callback is to be invoked. We can use w.nameImmpNonGC here as JIT-ed code is per
48613:          * thread and cannot outlive the corresponding JSThreadData.
48613:          */
56750:         w.comment("begin-interruptFlags-check");
59737:         /* FIXME: See bug 621140 for moving interruptCounter to the compartment. */
59737: #ifdef JS_THREADSAFE
59737:         void *interrupt = (void*) &cx->runtime->interruptCounter;
59737: #else
59737:         void *interrupt = (void*) &JS_THREAD_DATA(cx)->interruptFlags;
59737: #endif
59737:         LIns* flagptr = w.nameImmpNonGC(interrupt);
56750:         LIns* x = w.ldiVolatile(flagptr);
56750:         guard(true, w.eqi0(x), TIMEOUT_EXIT);
56750:         w.comment("end-interruptFlags-check");
53578: 
53578:         /*
53578:          * Count the number of iterations run by a trace, so that we can blacklist if
53579:          * the trace runs too few iterations to be worthwhile. Do this only if the methodjit
53579:          * is on--otherwise we must try to trace as much as possible.
53579:          */
54560: #ifdef JS_METHODJIT
54175:         if (cx->methodJitEnabled) {
56750:             w.comment("begin-count-loop-iterations");
59736:             LIns* counterPtr = w.nameImmpNonGC((void *) &traceMonitor->iterationCounter);
56750:             LIns* counterValue = w.ldiVolatile(counterPtr);
56750:             LIns* test = w.ltiN(counterValue, LOOP_COUNT_MAX);
56750:             LIns *branch = w.jfUnoptimizable(test);
56750:             /* 
56750:              * stiVolatile() uses ACCSET_STORE_ANY;  If LICM is implemented
56750:              * (bug 545406) this counter will need its own region.
56750:              */
57719:             w.stiVolatile(w.addi(counterValue, w.immi(1)), counterPtr);
56750:             w.label(branch);
56750:             w.comment("end-count-loop-iterations");
54427:         }
54560: #endif
53579:     }
22615: 
30860:     /*
30860:      * If we are attached to a tree call guard, make sure the guard the inner
30860:      * tree exited from is what we expect it to be.
30860:      */
34351:     if (anchor && anchor->exitType == NESTED_EXIT) {
57719:         LIns* nested_ins = w.ldpStateField(outermostTreeExitGuard);
56750:         guard(true, w.eqp(nested_ins, w.nameImmpNonGC(innermost)), NESTED_EXIT);
56750:     }
56750: 
56750:     w.comment("end-setup");
17334: }
17334: 
32779: TraceRecorder::~TraceRecorder()
17334: {
35083:     /* Should already have been adjusted by callers before calling delete. */
35083:     JS_ASSERT(traceMonitor->recorder != this);
33171: 
22609:     if (trashSelf)
54718:         TrashTree(fragment->root);
22609: 
22609:     for (unsigned int i = 0; i < whichTreesToTrash.length(); i++)
54718:         TrashTree(whichTreesToTrash[i]);
33167: 
33167:     /* Purge the tempAlloc used during recording. */
34351:     tempAlloc().reset();
33560: 
33560:     forgetGuardedShapes();
17319: }
17319: 
35083: inline bool
37741: TraceMonitor::outOfMemory() const
35083: {
35083:     return dataAlloc->outOfMemory() ||
35083:            tempAlloc->outOfMemory() ||
35083:            traceAlloc->outOfMemory();
35083: }
35083: 
35083: /*
35083:  * This function destroys the recorder after a successful recording, possibly
35083:  * starting a suspended outer recorder.
35083:  */
35083: AbortableRecordingStatus
35083: TraceRecorder::finishSuccessfully()
35083: {
60157:     JS_ASSERT(!traceMonitor->profile);
35083:     JS_ASSERT(traceMonitor->recorder == this);
35083:     JS_ASSERT(fragment->lastIns && fragment->code());
35083: 
35083:     AUDIT(traceCompleted);
35083:     mark.commit();
35083: 
60811:     /* Grab local copies of members needed after destruction of |this|. */
35083:     JSContext* localcx = cx;
37741:     TraceMonitor* localtm = traceMonitor;
35083: 
35083:     localtm->recorder = NULL;
60811:     /* We can't (easily) use js_delete() here because the constructor is private. */
60811:     this->~TraceRecorder();
60811:     js_free(this);
35083: 
35083:     /* Catch OOM that occurred during recording. */
59733:     if (localtm->outOfMemory() || OverfullJITCache(localcx, localtm)) {
35083:         ResetJIT(localcx, FR_OOM);
35083:         return ARECORD_ABORTED;
35083:     }
35083:     return ARECORD_COMPLETED;
35083: }
35083: 
35083: /* This function aborts a recorder and any pending outer recorders. */
54835: JS_REQUIRES_STACK TraceRecorder::AbortResult
35083: TraceRecorder::finishAbort(const char* reason)
35083: {
60157:     JS_ASSERT(!traceMonitor->profile);
35083:     JS_ASSERT(traceMonitor->recorder == this);
35083: 
35083:     AUDIT(recorderAborted);
35083: #ifdef DEBUG
59244:     debug_only_printf(LC_TMMinimal | LC_TMAbort,
35083:                       "Abort recording of tree %s:%d@%d at %s:%d@%d: %s.\n",
36361:                       tree->treeFileName,
36361:                       tree->treeLineNumber,
36361:                       tree->treePCOffset,
53840:                       cx->fp()->script()->filename,
51446:                       js_FramePCToLineNumber(cx, cx->fp()),
51446:                       FramePCOffset(cx, cx->fp()),
35083:                       reason);
35083: #endif
35083:     Backoff(cx, (jsbytecode*) fragment->root->ip, fragment->root);
35083: 
35083:     /*
35083:      * If this is the primary trace and we didn't succeed compiling, trash the
36361:      * tree. Otherwise, remove the VMSideExits we added while recording, which
36361:      * are about to be invalid.
35085:      *
35085:      * BIG FAT WARNING: resetting the length is only a valid strategy as long as
35085:      * there may be only one recorder active for a single TreeInfo at a time.
35085:      * Otherwise, we may be throwing away another recorder's valid side exits.
35085:      */
35085:     if (fragment->root == fragment) {
54718:         TrashTree(fragment->toTreeFragment());
35085:     } else {
36361:         JS_ASSERT(numSideExitsBefore <= fragment->root->sideExits.length());
36361:         fragment->root->sideExits.setLength(numSideExitsBefore);
35085:     }
35083: 
60811:     /* Grab local copies of members needed after destruction of |this|. */
35083:     JSContext* localcx = cx;
37741:     TraceMonitor* localtm = traceMonitor;
35083: 
35083:     localtm->recorder = NULL;
60811:     /* We can't (easily) use js_delete() here because the constructor is private. */
60811:     this->~TraceRecorder();
60811:     js_free(this);
60811: 
60811:     /* Catch OOM that occurred during recording. */
59733:     if (localtm->outOfMemory() || OverfullJITCache(localcx, localtm)) {
35083:         ResetJIT(localcx, FR_OOM);
54835:         return JIT_RESET;
54835:     }
54835:     return NORMAL_ABORT;
33159: }
33159: 
17722: inline LIns*
56750: TraceRecorder::w_immpObjGC(JSObject* obj)
56750: {
60780:     JS_ASSERT(obj);
56750:     tree->gcthings.addUnique(ObjectValue(*obj));
56750:     return w.immpNonGC((void*)obj);
17722: }
17722: 
31843: inline LIns*
56750: TraceRecorder::w_immpFunGC(JSFunction* fun)
56750: {
60780:     JS_ASSERT(fun);
56750:     tree->gcthings.addUnique(ObjectValue(*fun));
56750:     return w.immpNonGC((void*)fun);
56719: }
56719: 
56719: inline LIns*
56750: TraceRecorder::w_immpStrGC(JSString* str)
56750: {
60780:     JS_ASSERT(str);
56750:     tree->gcthings.addUnique(StringValue(str));
56750:     return w.immpNonGC((void*)str);
31843: }
31843: 
31843: inline LIns*
56750: TraceRecorder::w_immpShapeGC(const Shape* shape)
56750: {
60780:     JS_ASSERT(shape);
56750:     tree->shapes.addUnique(shape);
56750:     return w.immpNonGC((void*)shape);
31843: }
31843: 
31843: inline LIns*
56750: TraceRecorder::w_immpIdGC(jsid id)
48470: {
48470:     if (JSID_IS_GCTHING(id))
48470:         tree->gcthings.addUnique(IdToValue(id));
56750:     return w.immpNonGC((void*)JSID_BITS(id));
32746: }
32746: 
36401: ptrdiff_t
48470: TraceRecorder::nativeGlobalSlot(const Value* p) const
36401: {
36401:     JS_ASSERT(isGlobal(p));
55746:     return ptrdiff_t(p - globalObj->slots);
36401: }
36401: 
30860: /* Determine the offset in the native global frame for a jsval we track. */
17811: ptrdiff_t
48470: TraceRecorder::nativeGlobalOffset(const Value* p) const
17815: {
36401:     return nativeGlobalSlot(p) * sizeof(double);
17815: }
17815: 
30860: /* Determine whether a value is a global stack slot. */
17893: bool
48470: TraceRecorder::isGlobal(const Value* p) const
17893: {
55746:     return (size_t(p - globalObj->slots) < globalObj->numSlots());
17893: }
17893: 
48470: bool
48470: TraceRecorder::isVoidPtrGlobal(const void* p) const
48470: {
48470:     return isGlobal((const Value *)p);
48470: }
48470: 
30248: /*
30248:  * Return the offset in the native stack for the given jsval. More formally,
30248:  * |p| must be the address of a jsval that is represented in the native stack
41276:  * area. The return value is the offset, from TracerState::stackBase, in bytes,
30860:  * where the native representation of |*p| is stored. To get the offset
41276:  * relative to TracerState::sp, subtract TreeFragment::nativeStackBase.
30248:  */
22652: JS_REQUIRES_STACK ptrdiff_t
48470: TraceRecorder::nativeStackOffsetImpl(const void* p) const
17346: {
29880:     CountSlotsVisitor visitor(p);
29882:     VisitStackSlots(visitor, cx, callDepth);
29880:     size_t offset = visitor.count() * sizeof(double);
30860: 
30860:     /*
30860:      * If it's not in a pending frame, it must be on the stack of the current
42714:      * frame above sp but below fp->slots() + script->nslots.
17923:      */
29880:     if (!visitor.stopped()) {
48470:         const Value *vp = (const Value *)p;
53840:         JS_ASSERT(size_t(vp - cx->fp()->slots()) < cx->fp()->numSlots());
48470:         offset += size_t(vp - cx->regs->sp) * sizeof(double);
29880:     }
29880:     return offset;
17346: }
36401: 
48470: JS_REQUIRES_STACK inline ptrdiff_t
48470: TraceRecorder::nativeStackOffset(const Value* p) const
48470: {
48470:     return nativeStackOffsetImpl(p);
48470: }
48470: 
48470: JS_REQUIRES_STACK inline ptrdiff_t
48470: TraceRecorder::nativeStackSlotImpl(const void* p) const
48470: {
48470:     return nativeStackOffsetImpl(p) / sizeof(double);
48470: }
48470: 
48470: JS_REQUIRES_STACK inline ptrdiff_t
48470: TraceRecorder::nativeStackSlot(const Value* p) const
48470: {
48470:     return nativeStackSlotImpl(p);
36401: }
36401: 
35083: /*
41276:  * Return the offset, from TracerState:sp, for the given jsval. Shorthand for:
36361:  *  -TreeFragment::nativeStackBase + nativeStackOffset(p).
35083:  */
35083: inline JS_REQUIRES_STACK ptrdiff_t
48470: TraceRecorder::nativespOffsetImpl(const void* p) const
48470: {
48470:     return -tree->nativeStackBase + nativeStackOffsetImpl(p);
48470: }
48470: 
48470: inline JS_REQUIRES_STACK ptrdiff_t
48470: TraceRecorder::nativespOffset(const Value* p) const
48470: {
48470:     return nativespOffsetImpl(p);
35083: }
17346: 
30860: /* Track the maximum number of native frame slots we need during execution. */
35083: inline void
17815: TraceRecorder::trackNativeStackUse(unsigned slots)
17815: {
36361:     if (slots > tree->maxNativeStackSlots)
36361:         tree->maxNativeStackSlots = slots;
17397: }
17397: 
30860: /*
30860:  * Unbox a jsval into a slot. Slots are wide enough to hold double values
30860:  * directly (instead of storing a pointer to them). We assert instead of
30860:  * type checking. The caller must ensure the types are compatible.
30860:  */
48470: static inline void
48470: ValueToNative(const Value &v, JSValueType type, double* slot)
48470: {
48470:     JS_ASSERT(type <= JSVAL_UPPER_INCL_TYPE_OF_BOXABLE_SET);
48470:     if (type > JSVAL_UPPER_INCL_TYPE_OF_NUMBER_SET)
48470:         v.unboxNonDoubleTo((uint64 *)slot);
48470:     else if (type == JSVAL_TYPE_INT32)
48470:         *(int32_t *)slot = v.isInt32() ? v.toInt32() : (int32_t)v.toDouble();
48470:     else
48470:         *(double *)slot = v.toNumber();
48470: 
48470: #ifdef DEBUG
48470:     int32_t _;
18296:     switch (type) {
48470:       case JSVAL_TYPE_NONFUNOBJ: {
48470:         JS_ASSERT(!IsFunctionObject(v));
29883:         debug_only_printf(LC_TMTracer,
48470:                           "object<%p:%s> ", (void*)*(JSObject **)slot,
48470:                           v.toObject().getClass()->name);
27542:         return;
48470:       }
48470: 
48470:       case JSVAL_TYPE_INT32:
48470:         JS_ASSERT(v.isInt32() || (v.isDouble() && JSDOUBLE_IS_INT32(v.toDouble(), &_)));
29883:         debug_only_printf(LC_TMTracer, "int<%d> ", *(jsint *)slot);
21433:         return;
30860: 
48470:       case JSVAL_TYPE_DOUBLE:
48470:         JS_ASSERT(v.isNumber());
48470:         debug_only_printf(LC_TMTracer, "double<%g> ", *(jsdouble *)slot);
21433:         return;
30860: 
48470:       case JSVAL_TYPE_BOXED:
29896:         JS_NOT_REACHED("found jsval type in an entry type map");
27541:         return;
30860: 
48470:       case JSVAL_TYPE_STRING:
48470:         JS_ASSERT(v.isString());
48470:         debug_only_printf(LC_TMTracer, "string<%p> ", (void*)*(JSString**)slot);
27542:         return;
30860: 
48470:       case JSVAL_TYPE_NULL:
48470:         JS_ASSERT(v.isNull());
29883:         debug_only_print0(LC_TMTracer, "null ");
27542:         return;
30860: 
48470:       case JSVAL_TYPE_BOOLEAN:
48470:         JS_ASSERT(v.isBoolean());
40307:         debug_only_printf(LC_TMTracer, "special<%d> ", *(JSBool*)slot);
40307:         return;
40307: 
48470:       case JSVAL_TYPE_UNDEFINED:
48470:         JS_ASSERT(v.isUndefined());
40307:         debug_only_print0(LC_TMTracer, "undefined ");
21433:         return;
30860: 
48470:       case JSVAL_TYPE_MAGIC:
48470:         JS_ASSERT(v.isMagic());
43263:         debug_only_print0(LC_TMTracer, "hole ");
43263:         return;
43263: 
48470:       case JSVAL_TYPE_FUNOBJ: {
48470:         JS_ASSERT(IsFunctionObject(v));
48470:         JSFunction* fun = GET_FUNCTION_PRIVATE(cx, &v.toObject());
57721: #if defined JS_JIT_SPEW
57721:         if (LogController.lcbits & LC_TMTracer) {
57721:             char funName[40];
57721:             if (fun->atom)
59889:                 JS_PutEscapedFlatString(funName, sizeof funName, ATOM_TO_STRING(fun->atom), 0);
57721:             else
57721:                 strcpy(funName, "unnamed");
57721:             LogController.printf("function<%p:%s> ", (void*)*(JSObject **)slot, funName);
57721:         }
57721: #endif
27541:         return;
27541:       }
36401:       default:
27542:         JS_NOT_REACHED("unexpected type");
36401:         break;
36401:     }
48470: #endif
18296: }
17360: 
31843: void
37741: TraceMonitor::flush()
31843: {
35083:     /* flush should only be called after all recorders have been aborted. */
35083:     JS_ASSERT(!recorder);
60157:     JS_ASSERT(!profile);
33108:     AUDIT(cacheFlushed);
33108: 
32784:     // recover profiling data from expiring Fragments
32784:     verbose_only(
32784:         for (size_t i = 0; i < FRAGMENT_TABLE_SIZE; ++i) {
35044:             for (TreeFragment *f = vmfragments[i]; f; f = f->next) {
32784:                 JS_ASSERT(f->root == f);
35044:                 for (TreeFragment *p = f; p; p = p->peer)
37741:                     FragProfiling_FragFinalizer(p, this);
32784:             }
32784:         }
32784:     )
32784: 
32784:     verbose_only(
32784:         for (Seq<Fragment*>* f = branches; f; f = f->tail)
37741:             FragProfiling_FragFinalizer(f->head, this);
32784:     )
32784: 
56551:     flushEpoch++;
56551: 
60567: #ifdef JS_METHODJIT
60567:     if (loopProfiles) {
60567:         for (LoopProfileMap::Enum e(*loopProfiles); !e.empty(); e.popFront()) {
60567:             jsbytecode *pc = e.front().key;
60567:             LoopProfile *prof = e.front().value;
60567:             /* This code takes care of resetting all methodjit state. */
60567:             js::mjit::ResetTraceHint(prof->entryScript, pc, GET_UINT16(pc), true);
60567:         }
60567:     }
60567: #endif
60567: 
33563:     frameCache->reset();
33159:     dataAlloc->reset();
33545:     traceAlloc->reset();
32768:     codeAlloc->reset();
35047:     tempAlloc->reset();
41802:     oracle->clear();
56551:     loopProfiles->clear();
31920: 
31843:     for (size_t i = 0; i < MONITOR_N_GLOBAL_STATES; ++i) {
31843:         globalStates[i].globalShape = -1;
61053:         globalStates[i].globalSlots = new (*dataAlloc) SlotList(dataAlloc);
61053:     }
61053: 
61053:     assembler = new (*dataAlloc) Assembler(*codeAlloc, *dataAlloc, *dataAlloc, core,
61053:                                            &LogController, avmplus::AvmCore::config);
32784:     verbose_only( branches = NULL; )
31843: 
40229:     PodArrayZero(vmfragments);
58041:     tracedScripts.clear();
32767: 
31843:     needFlush = JS_FALSE;
31843: }
31843: 
54718: inline bool
60258: IsShapeAboutToBeFinalized(JSContext *cx, const js::Shape *shape)
60258: {
60258:     JSRuntime *rt = cx->runtime;
60258:     if (rt->gcCurrentCompartment != NULL)
60258:         return false;
60258: 
60258:     return !shape->marked();
60258: }
60258: 
60258: inline bool
60258: HasUnreachableGCThings(JSContext *cx, TreeFragment *f)
54718: {
54718:     /*
54718:      * We do not check here for dead scripts as JSScript is not a GC thing.
54718:      * Instead PurgeScriptFragments is used to remove dead script fragments.
54718:      * See bug 584860.
54718:      */
60258:     if (IsAboutToBeFinalized(cx, f->globalObj))
54718:         return true;
48470:     Value* vp = f->gcthings.data();
54718:     for (unsigned len = f->gcthings.length(); len; --len) {
48470:         Value &v = *vp++;
48470:         JS_ASSERT(v.isMarkable());
60258:         if (IsAboutToBeFinalized(cx, v.toGCThing()))
54718:             return true;
31843:     }
52503:     const Shape** shapep = f->shapes.data();
54718:     for (unsigned len = f->shapes.length(); len; --len) {
52503:         const Shape* shape = *shapep++;
60258:         if (IsShapeAboutToBeFinalized(cx, shape))
54718:             return true;
54718:     }
54718:     return false;
31843: }
33595: 
33595: void
60258: TraceMonitor::sweep(JSContext *cx)
54718: {
54718:     JS_ASSERT(!ontrace());
54718:     debug_only_print0(LC_TMTracer, "Purging fragments with dead things");
54718: 
59248:     bool shouldAbortRecording = false;
59248:     TreeFragment *recorderTree = NULL;
59248:     if (recorder) {
59248:         recorderTree = recorder->getTree();
60258:         shouldAbortRecording = HasUnreachableGCThings(cx, recorderTree);
59248:     }
59248:         
33595:     for (size_t i = 0; i < FRAGMENT_TABLE_SIZE; ++i) {
54718:         TreeFragment** fragp = &vmfragments[i];
54718:         while (TreeFragment* frag = *fragp) {
54718:             TreeFragment* peer = frag;
54718:             do {
60258:                 if (HasUnreachableGCThings(cx, peer))
54718:                     break;
35044:                 peer = peer->peer;
54718:             } while (peer);
54718:             if (peer) {
54718:                 debug_only_printf(LC_TMTracer,
54718:                                   "TreeFragment peer %p has dead gc thing."
54718:                                   "Disconnecting tree %p with ip %p\n",
54718:                                   (void *) peer, (void *) frag, frag->ip);
54718:                 JS_ASSERT(frag->root == frag);
54718:                 *fragp = frag->next;
54718:                 do {
59248:                     verbose_only( FragProfiling_FragFinalizer(frag, this); );
59248:                     if (recorderTree == frag)
59248:                         shouldAbortRecording = true;
54718:                     TrashTree(frag);
54718:                     frag = frag->peer;
54718:                 } while (frag);
54718:             } else {
54718:                 fragp = &frag->next;
54718:             }
54718:         }
54718:     }
54718: 
59248:     if (shouldAbortRecording)
54718:         recorder->finishAbort("dead GC things");
31843: }
31843: 
60574: void
60574: TraceMonitor::mark(JSTracer *trc)
60574: {
60574:     TracerState* state = tracerState;
60574:     while (state) {
60574:         if (state->nativeVp)
60574:             MarkValueRange(trc, state->nativeVpLen, state->nativeVp, "nativeVp");
60574:         state = state->prev;
60574:     }
60574: }
60574: 
30860: /*
48470:  * Box a value from the native stack back into the Value format.
48470:  */
48470: static inline void
48470: NativeToValue(JSContext* cx, Value& v, JSValueType type, double* slot)
48470: {
48470:     if (type == JSVAL_TYPE_DOUBLE) {
48470:         v.setNumber(*slot);
48470:     } else if (JS_LIKELY(type <= JSVAL_UPPER_INCL_TYPE_OF_BOXABLE_SET)) {
48470:         v.boxNonDoubleFrom(type, (uint64 *)slot);
48470:     } else if (type == JSVAL_TYPE_STRORNULL) {
48470:         JSString *str = *(JSString **)slot;
48470:         v = str ? StringValue(str) : NullValue();
48470:     } else if (type == JSVAL_TYPE_OBJORNULL) {
48470:         JSObject *obj = *(JSObject **)slot;
48470:         v = obj ? ObjectValue(*obj) : NullValue();
48470:     } else {
48470:         JS_ASSERT(type == JSVAL_TYPE_BOXED);
48470:         JS_STATIC_ASSERT(sizeof(Value) == sizeof(double));
48470:         v = *(Value *)slot;
48470:     }
48470: 
48470: #ifdef DEBUG
17494:     switch (type) {
48470:       case JSVAL_TYPE_NONFUNOBJ:
48470:         JS_ASSERT(!IsFunctionObject(v));
29883:         debug_only_printf(LC_TMTracer,
48470:                           "object<%p:%s> ",
48470:                           (void*) &v.toObject(),
48470:                           v.toObject().getClass()->name);
48470:         break;
48470:       case JSVAL_TYPE_INT32:
48470:         debug_only_printf(LC_TMTracer, "int<%d> ", v.toInt32());
48470:         break;
48470:       case JSVAL_TYPE_DOUBLE:
48470:         debug_only_printf(LC_TMTracer, "double<%g> ", v.toNumber());
48470:         break;
48470:       case JSVAL_TYPE_STRING:
48470:         debug_only_printf(LC_TMTracer, "string<%p> ", (void*)v.toString());
48470:         break;
48470:       case JSVAL_TYPE_NULL:
50496:         JS_ASSERT(v.isNull());
48470:         debug_only_print0(LC_TMTracer, "null ");
48470:         break;
48470:       case JSVAL_TYPE_BOOLEAN:
48470:         debug_only_printf(LC_TMTracer, "bool<%d> ", v.toBoolean());
48470:         break;
48470:       case JSVAL_TYPE_UNDEFINED:
50496:         JS_ASSERT(v.isUndefined());
40307:         debug_only_print0(LC_TMTracer, "undefined ");
27542:         break;
48470:       case JSVAL_TYPE_MAGIC:
48470:         debug_only_printf(LC_TMTracer, "magic<%d> ", v.whyMagic());
48470:         break;
57721:       case JSVAL_TYPE_FUNOBJ:
48470:         JS_ASSERT(IsFunctionObject(v));
57721: #if defined JS_JIT_SPEW
57721:         if (LogController.lcbits & LC_TMTracer) {
48470:             JSFunction* fun = GET_FUNCTION_PRIVATE(cx, &v.toObject());
57721:             char funName[40];
57721:             if (fun->atom)
59889:                 JS_PutEscapedFlatString(funName, sizeof funName, ATOM_TO_STRING(fun->atom), 0);
57721:             else
57721:                 strcpy(funName, "unnamed");
57721:             LogController.printf("function<%p:%s> ", (void*) &v.toObject(), funName);
57721:         }
57721: #endif
57721:         break;
48470:       case JSVAL_TYPE_STRORNULL:
48470:         debug_only_printf(LC_TMTracer, "nullablestr<%p> ", v.isNull() ? NULL : (void *)v.toString());
48470:         break;
48470:       case JSVAL_TYPE_OBJORNULL:
48470:         debug_only_printf(LC_TMTracer, "nullablestr<%p> ", v.isNull() ? NULL : (void *)&v.toObject());
48470:         break;
48470:       case JSVAL_TYPE_BOXED:
48621:         debug_only_printf(LC_TMTracer, "box<%llx> ", (long long unsigned int)v.asRawBits());
48470:         break;
36401:       default:
36401:         JS_NOT_REACHED("unexpected type");
36401:         break;
17393:     }
48470: #endif
48470: }
48470: 
48470: void
48470: ExternNativeToValue(JSContext* cx, Value& v, JSValueType type, double* slot)
48470: {
48470:     return NativeToValue(cx, v, type, slot);
32709: }
32709: 
29882: class BuildNativeFrameVisitor : public SlotVisitorBase
29882: {
29880:     JSContext *mCx;
48470:     JSValueType *mTypeMap;
29880:     double *mGlobal;
29880:     double *mStack;
29880: public:
29880:     BuildNativeFrameVisitor(JSContext *cx,
48470:                             JSValueType *typemap,
29880:                             double *global,
29880:                             double *stack) :
29880:         mCx(cx),
29880:         mTypeMap(typemap),
29880:         mGlobal(global),
29880:         mStack(stack)
29880:     {}
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE void
48470:     visitGlobalSlot(Value *vp, unsigned n, unsigned slot) {
29883:         debug_only_printf(LC_TMTracer, "global%d: ", n);
48470:         ValueToNative(*vp, *mTypeMap++, &mGlobal[slot]);
29880:     }
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
48470:     visitStackSlots(Value *vp, int count, JSStackFrame* fp) {
29880:         for (int i = 0; i < count; ++i) {
29883:             debug_only_printf(LC_TMTracer, "%s%d: ", stackSlotKind(), i);
48470:             ValueToNative(*vp++, *mTypeMap++, mStack++);
48470:         }
48470:         return true;
48470:     }
48470: 
48470:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
53840:     visitFrameObjPtr(void* p, JSStackFrame* fp) {
48470:         debug_only_printf(LC_TMTracer, "%s%d: ", stackSlotKind(), 0);
53840:         if (p == fp->addressOfScopeChain())
53840:             *(JSObject **)mStack = &fp->scopeChain();
53840:         else
53840:             *(JSObject **)mStack = fp->hasArgsObj() ? &fp->argsObj() : NULL;
48470: #ifdef DEBUG
48470:         if (*mTypeMap == JSVAL_TYPE_NULL) {
53840:             JS_ASSERT(*(JSObject **)mStack == NULL);
48470:             debug_only_print0(LC_TMTracer, "null ");
48470:         } else {
48470:             JS_ASSERT(*mTypeMap == JSVAL_TYPE_NONFUNOBJ);
53840:             JS_ASSERT(!(*(JSObject **)p)->isFunction());
48470:             debug_only_printf(LC_TMTracer,
53840:                               "object<%p:%s> ", *(void **)p,
53840:                               (*(JSObject **)p)->getClass()->name);
48470:         }
48470: #endif
48470:         mTypeMap++;
48470:         mStack++;
29880:         return true;
29880:     }
29880: };
29880: 
22652: static JS_REQUIRES_STACK void
29880: BuildNativeFrame(JSContext *cx, JSObject *globalObj, unsigned callDepth,
29880:                  unsigned ngslots, uint16 *gslots,
48470:                  JSValueType *typeMap, double *global, double *stack)
29880: {
29880:     BuildNativeFrameVisitor visitor(cx, typeMap, global, stack);
29882:     VisitSlots(visitor, cx, globalObj, callDepth, ngslots, gslots);
29883:     debug_only_print0(LC_TMTracer, "\n");
21433: }
21433: 
29882: class FlushNativeGlobalFrameVisitor : public SlotVisitorBase
29882: {
29880:     JSContext *mCx;
48470:     JSValueType *mTypeMap;
29880:     double *mGlobal;
29880: public:
29880:     FlushNativeGlobalFrameVisitor(JSContext *cx,
48470:                                   JSValueType *typeMap,
29880:                                   double *global) :
29880:         mCx(cx),
29880:         mTypeMap(typeMap),
29880:         mGlobal(global)
29880:     {}
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE void
48470:     visitGlobalSlot(Value *vp, unsigned n, unsigned slot) {
29883:         debug_only_printf(LC_TMTracer, "global%d=", n);
48619:         JS_ASSERT(JS_THREAD_DATA(mCx)->waiveGCQuota);
48470:         NativeToValue(mCx, *vp, *mTypeMap++, &mGlobal[slot]);
29880:     }
29880: };
29880: 
29882: class FlushNativeStackFrameVisitor : public SlotVisitorBase
29882: {
29880:     JSContext *mCx;
48470:     const JSValueType *mInitTypeMap;
48470:     const JSValueType *mTypeMap;
29880:     double *mStack;
29880: public:
29880:     FlushNativeStackFrameVisitor(JSContext *cx,
48470:                                  const JSValueType *typeMap,
53840:                                  double *stack) :
29880:         mCx(cx),
33564:         mInitTypeMap(typeMap),
29880:         mTypeMap(typeMap),
53840:         mStack(stack)
29880:     {}
29880: 
48470:     const JSValueType* getTypeMap()
29880:     {
29880:         return mTypeMap;
29880:     }
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
48470:     visitStackSlots(Value *vp, size_t count, JSStackFrame* fp) {
48619:         JS_ASSERT(JS_THREAD_DATA(mCx)->waiveGCQuota);
29880:         for (size_t i = 0; i < count; ++i) {
29883:             debug_only_printf(LC_TMTracer, "%s%u=", stackSlotKind(), unsigned(i));
48470:             NativeToValue(mCx, *vp, *mTypeMap, mStack);
33564:             vp++;
33564:             mTypeMap++;
33564:             mStack++;
29880:         }
29880:         return true;
29880:     }
48470: 
48470:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
53840:     visitFrameObjPtr(void* p, JSStackFrame* fp) {
48619:         JS_ASSERT(JS_THREAD_DATA(mCx)->waiveGCQuota);
48470:         debug_only_printf(LC_TMTracer, "%s%u=", stackSlotKind(), 0);
53840:         JSObject *frameobj = *(JSObject **)mStack;
53840:         JS_ASSERT((frameobj == NULL) == (*mTypeMap == JSVAL_TYPE_NULL));
53840:         if (p == fp->addressOfArgs()) {
53840:             if (frameobj) {
53840:                 JS_ASSERT_IF(fp->hasArgsObj(), frameobj == &fp->argsObj());
53840:                 fp->setArgsObj(*frameobj);
60143:                 JS_ASSERT(frameobj->isArguments());
59965:                 if (frameobj->isNormalArguments())
53840:                     frameobj->setPrivate(fp);
59965:                 else
60143:                     JS_ASSERT(!frameobj->getPrivate());
53840:                 debug_only_printf(LC_TMTracer,
53840:                                   "argsobj<%p> ",
53840:                                   (void *)frameobj);
53840:             } else {
53840:                 JS_ASSERT(!fp->hasArgsObj());
53840:                 debug_only_print0(LC_TMTracer,
53840:                                   "argsobj<null> ");
53840:             }
53840:             /* else, SynthesizeFrame has initialized fp->args.nactual */
53840:         } else {
53840:             JS_ASSERT(p == fp->addressOfScopeChain());
53840:             if (frameobj->isCall() &&
53840:                 !frameobj->getPrivate() &&
60146:                 fp->maybeCallee() == frameobj->getCallObjCallee())
53840:             {
53840:                 JS_ASSERT(&fp->scopeChain() == JSStackFrame::sInvalidScopeChain);
53840:                 frameobj->setPrivate(fp);
53840:                 fp->setScopeChainAndCallObj(*frameobj);
53840:             } else {
53840:                 fp->setScopeChainNoCallObj(*frameobj);
53840:             }
53840:             debug_only_printf(LC_TMTracer,
53840:                               "scopechain<%p> ",
53840:                               (void *)frameobj);
53840:         }
48470: #ifdef DEBUG
48470:         JSValueType type = *mTypeMap;
48470:         if (type == JSVAL_TYPE_NULL) {
48470:             debug_only_print0(LC_TMTracer, "null ");
48470:         } else {
48470:             JS_ASSERT(type == JSVAL_TYPE_NONFUNOBJ);
53840:             JS_ASSERT(!frameobj->isFunction());
48470:             debug_only_printf(LC_TMTracer,
48470:                               "object<%p:%s> ",
53840:                               *(void **)p,
53840:                               frameobj->getClass()->name);
48470:         }
48470: #endif
48470:         mTypeMap++;
48470:         mStack++;
48470:         return true;
48470:     }
29880: };
29880: 
29880: /* Box the given native frame into a JS frame. This is infallible. */
22652: static JS_REQUIRES_STACK void
35083: FlushNativeGlobalFrame(JSContext *cx, JSObject *globalObj, double *global, unsigned ngslots,
48470:                        uint16 *gslots, JSValueType *typemap)
29880: {
29880:     FlushNativeGlobalFrameVisitor visitor(cx, typemap, global);
29882:     VisitGlobalSlots(visitor, cx, globalObj, ngslots, gslots);
29883:     debug_only_print0(LC_TMTracer, "\n");
17857: }
17857: 
28268: /*
31924:  * Returns the number of values on the native stack, excluding the innermost
31924:  * frame. This walks all FrameInfos on the native frame stack and sums the
31924:  * slot usage of each frame.
31924:  */
31924: static int32
41276: StackDepthFromCallStack(TracerState* state, uint32 callDepth)
31924: {
31924:     int32 nativeStackFramePos = 0;
31924: 
31924:     // Duplicate native stack layout computation: see VisitFrameSlots header comment.
31924:     for (FrameInfo** fip = state->callstackBase; fip < state->rp + callDepth; fip++)
31924:         nativeStackFramePos += (*fip)->callerHeight;
31924:     return nativeStackFramePos;
31924: }
31924: 
31924: /*
30660:  * Generic function to read upvars on trace from slots of active frames.
29021:  *     T   Traits type parameter. Must provide static functions:
29021:  *             interp_get(fp, slot)     Read the value out of an interpreter frame.
29021:  *             native_slot(argc, slot)  Return the position of the desired value in the on-trace
29021:  *                                      stack frame (with position 0 being callee).
29021:  *
30660:  *     upvarLevel  Static level of the function containing the upvar definition
29021:  *     slot        Identifies the value to get. The meaning is defined by the traits type.
29021:  *     callDepth   Call depth of current point relative to trace entry
29021:  */
29021: template<typename T>
48470: inline JSValueType
30860: GetUpvarOnTrace(JSContext* cx, uint32 upvarLevel, int32 slot, uint32 callDepth, double* result)
29021: {
60574:     TracerState* state = JS_TRACE_MONITOR(cx).tracerState;
28738:     FrameInfo** fip = state->rp + callDepth;
28738: 
28738:     /*
31924:      * First search the FrameInfo call stack for an entry containing our
31924:      * upvar, namely one with level == upvarLevel. The first FrameInfo is a
31924:      * transition from the entry frame to some callee. However, it is not
31924:      * known (from looking at the FrameInfo) whether the entry frame had a
31924:      * callee. Rather than special-case this or insert more logic into the
31924:      * loop, instead just stop before that FrameInfo (i.e. |> base| instead of
31924:      * |>= base|), and let the code after the loop handle it.
31924:      */
31924:     int32 stackOffset = StackDepthFromCallStack(state, callDepth);
31924:     while (--fip > state->callstackBase) {
28738:         FrameInfo* fi = *fip;
31924: 
31924:         /*
31924:          * The loop starts aligned to the top of the stack, so move down to the first meaningful
31924:          * callee. Then read the callee directly from the frame.
31924:          */
31924:         stackOffset -= fi->callerHeight;
31924:         JSObject* callee = *(JSObject**)(&state->stackBase[stackOffset]);
31924:         JSFunction* fun = GET_FUNCTION_PRIVATE(cx, callee);
28738:         uintN calleeLevel = fun->u.i.script->staticLevel;
28738:         if (calleeLevel == upvarLevel) {
28738:             /*
31924:              * Now find the upvar's value in the native stack. stackOffset is
31924:              * the offset of the start of the activation record corresponding
31924:              * to *fip in the native stack.
31924:              */
31924:             uint32 native_slot = T::native_slot(fi->callerArgc, slot);
31924:             *result = state->stackBase[stackOffset + native_slot];
29021:             return fi->get_typemap()[native_slot];
29021:         }
29021:     }
29021: 
29021:     // Next search the trace entry frame, which is not in the FrameInfo stack.
28911:     if (state->outermostTree->script->staticLevel == upvarLevel) {
36361:         uint32 argc = state->outermostTree->argc;
29021:         uint32 native_slot = T::native_slot(argc, slot);
29021:         *result = state->stackBase[native_slot];
29021:         return state->callstackBase[0]->get_typemap()[native_slot];
28738:     }
28738: 
28738:     /*
28738:      * If we did not find the upvar in the frames for the active traces,
28268:      * then we simply get the value from the interpreter state.
28268:      */
48582:     JS_ASSERT(upvarLevel < UpvarCookie::UPVAR_LEVEL_LIMIT);
48582:     JSStackFrame* fp = cx->findFrameAtLevel(upvarLevel);
48470:     Value v = T::interp_get(fp, slot);
48470:     JSValueType type = getCoercedType(v);
48470:     ValueToNative(v, type, result);
28268:     return type;
28268: }
28268: 
29021: // For this traits type, 'slot' is the argument index, which may be -2 for callee.
29021: struct UpvarArgTraits {
48470:     static Value interp_get(JSStackFrame* fp, int32 slot) {
53840:         return fp->formalArg(slot);
29021:     }
29021: 
29021:     static uint32 native_slot(uint32 argc, int32 slot) {
29021:         return 2 /*callee,this*/ + slot;
29021:     }
29021: };
29021: 
29021: uint32 JS_FASTCALL
30860: GetUpvarArgOnTrace(JSContext* cx, uint32 upvarLevel, int32 slot, uint32 callDepth, double* result)
30860: {
30860:     return GetUpvarOnTrace<UpvarArgTraits>(cx, upvarLevel, slot, callDepth, result);
29021: }
29021: 
29021: // For this traits type, 'slot' is an index into the local slots array.
29021: struct UpvarVarTraits {
48470:     static Value interp_get(JSStackFrame* fp, int32 slot) {
42714:         return fp->slots()[slot];
29021:     }
29021: 
29021:     static uint32 native_slot(uint32 argc, int32 slot) {
37694:         return 4 /*callee,this,arguments,scopeChain*/ + argc + slot;
29021:     }
29021: };
29021: 
29021: uint32 JS_FASTCALL
30860: GetUpvarVarOnTrace(JSContext* cx, uint32 upvarLevel, int32 slot, uint32 callDepth, double* result)
30860: {
30860:     return GetUpvarOnTrace<UpvarVarTraits>(cx, upvarLevel, slot, callDepth, result);
30860: }
30860: 
30860: /*
30860:  * For this traits type, 'slot' is an index into the stack area (within slots,
30860:  * after nfixed) of a frame with no function. (On trace, the top-level frame is
30860:  * the only one that can have no function.)
29022:  */
29022: struct UpvarStackTraits {
48470:     static Value interp_get(JSStackFrame* fp, int32 slot) {
53840:         return fp->slots()[slot + fp->numFixed()];
29022:     }
29022: 
29022:     static uint32 native_slot(uint32 argc, int32 slot) {
29022:         /*
30860:          * Locals are not imported by the tracer when the frame has no
51056:          * function, so we do not add fp->getFixedCount().
29022:          */
29022:         JS_ASSERT(argc == 0);
29022:         return slot;
29022:     }
29022: };
29022: 
29022: uint32 JS_FASTCALL
31924: GetUpvarStackOnTrace(JSContext* cx, uint32 upvarLevel, int32 slot, uint32 callDepth,
31924:                      double* result)
30860: {
30860:     return GetUpvarOnTrace<UpvarStackTraits>(cx, upvarLevel, slot, callDepth, result);
29022: }
29022: 
32593: // Parameters needed to access a value from a closure on trace.
32593: struct ClosureVarInfo
32593: {
32593:     uint32   slot;
35479: #ifdef DEBUG
32593:     uint32   callDepth;
35479: #endif
32593: };
32593: 
30660: /*
30660:  * Generic function to read upvars from Call objects of active heavyweight functions.
32593:  *     call       Callee Function object in which the upvar is accessed.
30660:  */
30647: template<typename T>
30860: inline uint32
32593: GetFromClosure(JSContext* cx, JSObject* call, const ClosureVarInfo* cv, double* result)
32593: {
52503:     JS_ASSERT(call->isCall());
30647: 
53840: #ifdef DEBUG
60574:     TracerState* state = JS_TRACE_MONITOR(cx).tracerState;
37694:     FrameInfo** fip = state->rp + cv->callDepth;
33607:     int32 stackOffset = StackDepthFromCallStack(state, cv->callDepth);
31924:     while (--fip > state->callstackBase) {
30647:         FrameInfo* fi = *fip;
37694: 
37694:         /*
37694:          * The loop starts aligned to the top of the stack, so move down to the first meaningful
37694:          * callee. Then read the callee directly from the frame.
37694:          */
37694:         stackOffset -= fi->callerHeight;
31924:         JSObject* callee = *(JSObject**)(&state->stackBase[stackOffset]);
31924:         if (callee == call) {
37725:             // This is not reachable as long as the tracer guards on the identity of the callee's
37725:             // parent when making a call:
37725:             //
37725:             // - We can only reach this point if we execute JSOP_LAMBDA on trace, then call the
37725:             //   function created by the lambda, and then execute a JSOP_NAME on trace.
37725:             // - Each time we execute JSOP_LAMBDA we get a function with a different parent.
37725:             // - When we execute the call to the new function, we exit trace because the parent
37725:             //   is different.
30647:             JS_NOT_REACHED("JSOP_NAME variable found in outer trace");
30647:         }
31924:     }
31924: #endif
30647: 
53840:     // We already guarded on trace that we aren't touching an outer tree's entry frame
30647:     VOUCH_DOES_NOT_REQUIRE_STACK();
31452:     JSStackFrame* fp = (JSStackFrame*) call->getPrivate();
53840:     JS_ASSERT(fp != cx->fp());
53840: 
48470:     Value v;
32593:     if (fp) {
53840:         v = T::get_slot(fp, cv->slot);
32593:     } else {
35479:         /*
35479:          * Get the value from the object. We know we have a Call object, and
35479:          * that our slot index is fine, so don't monkey around with calling the
35479:          * property getter (which just looks in the slot) or calling
35479:          * js_GetReservedSlot. Just get the slot directly. Note the static
56180:          * asserts in jsfun.cpp which make sure Call objects use slots.
35479:          */
53840:         JS_ASSERT(cv->slot < T::slot_count(call));
53840:         v = T::get_slot(call, cv->slot);
32593:     }
48470:     JSValueType type = getCoercedType(v);
48470:     ValueToNative(v, type, result);
30647:     return type;
30647: }
30647: 
30647: struct ArgClosureTraits
30647: {
35479:     // Get the right frame slots to use our slot index with.
35475:     // See also UpvarArgTraits.
52503:     static inline Value get_slot(JSStackFrame* fp, unsigned slot) {
52503:         JS_ASSERT(slot < fp->numFormalArgs());
53840:         return fp->formalArg(slot);
52503:     }
35479: 
35479:     // Get the right object slots to use our slot index with.
52503:     static inline Value get_slot(JSObject* obj, unsigned slot) {
52503:         return obj->getSlot(slot_offset(obj) + slot);
52503:     }
52503: 
56180:     // Get the offset of our object slots from the object's slots pointer.
35479:     static inline uint32 slot_offset(JSObject* obj) {
55746:         return JSObject::CALL_RESERVED_SLOTS;
52503:     }
52503: 
35479:     // Get the maximum slot index of this type that should be allowed
35479:     static inline uint16 slot_count(JSObject* obj) {
53840:         return obj->getCallObjCalleeFunction()->nargs;
35479:     }
52503: 
30647: private:
30647:     ArgClosureTraits();
30647: };
30647: 
30647: uint32 JS_FASTCALL
32593: GetClosureArg(JSContext* cx, JSObject* callee, const ClosureVarInfo* cv, double* result)
32593: {
32593:     return GetFromClosure<ArgClosureTraits>(cx, callee, cv, result);
30647: }
30647: 
30647: struct VarClosureTraits
30647: {
35476:     // See also UpvarVarTraits.
52503:     static inline Value get_slot(JSStackFrame* fp, unsigned slot) {
59968:         JS_ASSERT(slot < fp->fun()->script()->bindings.countVars());
52503:         return fp->slots()[slot];
52503:     }
52503: 
52503:     static inline Value get_slot(JSObject* obj, unsigned slot) {
52503:         return obj->getSlot(slot_offset(obj) + slot);
52503:     }
52503: 
35479:     static inline uint32 slot_offset(JSObject* obj) {
55746:         return JSObject::CALL_RESERVED_SLOTS +
53840:                obj->getCallObjCalleeFunction()->nargs;
35479:     }
52503: 
35479:     static inline uint16 slot_count(JSObject* obj) {
59968:         return obj->getCallObjCalleeFunction()->script()->bindings.countVars();
35479:     }
52503: 
30647: private:
30647:     VarClosureTraits();
30647: };
30647: 
30647: uint32 JS_FASTCALL
32593: GetClosureVar(JSContext* cx, JSObject* callee, const ClosureVarInfo* cv, double* result)
32593: {
32593:     return GetFromClosure<VarClosureTraits>(cx, callee, cv, result);
30647: }
30647: 
19076: /**
23446:  * Box the given native stack frame into the virtual machine stack. This
23446:  * is infallible.
19076:  *
19076:  * @param callDepth the distance between the entry frame into our trace and
51446:  *                  cx->fp() when we make this call.  If this is not called as a
19076:  *                  result of a nested exit, callDepth is 0.
48470:  * @param mp an array of JSValueType that indicate what the types of the things
29896:  *           on the stack are.
19076:  * @param np pointer to the native stack.  We want to copy values from here to
19076:  *           the JS stack as needed.
19076:  * @return the number of things we popped off of np.
19076:  */
22652: static JS_REQUIRES_STACK int
53840: FlushNativeStackFrame(JSContext* cx, unsigned callDepth, const JSValueType* mp, double* np)
53840: {
17857:     /* Root all string and object references first (we don't need to call the GC for this). */
53840:     FlushNativeStackFrameVisitor visitor(cx, mp, np);
29882:     VisitStackSlots(visitor, cx, callDepth);
29880: 
29883:     debug_only_print0(LC_TMTracer, "\n");
29880:     return visitor.getTypeMap() - mp;
17361: }
17361: 
17363: /* Emit load instructions onto the trace that read the initial stack state. */
22652: JS_REQUIRES_STACK void
56750: TraceRecorder::importImpl(Address addr, const void* p, JSValueType t,
56750:                           const char *prefix, uintN index, JSStackFrame *fp)
17319: {
17480:     LIns* ins;
48470:     if (t == JSVAL_TYPE_INT32) { /* demoted */
48470:         JS_ASSERT(hasInt32Repr(*(const Value *)p));
30860: 
30860:         /*
30860:          * Ok, we have a valid demotion attempt pending, so insert an integer
30860:          * read and promote it to double since all arithmetic operations expect
30860:          * to see doubles on entry. The first op to use this slot will emit a
41265:          * d2i cast which will cancel out the i2d we insert here.
30860:          */
56750:         ins = w.ldi(addr);
56750:         ins = w.i2d(ins);
17480:     } else {
53840:         JS_ASSERT_IF(t != JSVAL_TYPE_BOXED && !isFrameObjPtrTraceType(t),
48470:                      ((const Value *)p)->isNumber() == (t == JSVAL_TYPE_DOUBLE));
48470:         if (t == JSVAL_TYPE_DOUBLE) {
56750:             ins = w.ldd(addr);
48470:         } else if (t == JSVAL_TYPE_BOOLEAN) {
56750:             ins = w.ldi(addr);
48470:         } else if (t == JSVAL_TYPE_UNDEFINED) {
56750:             ins = w.immiUndefined();
48470:         } else if (t == JSVAL_TYPE_MAGIC) {
56750:             ins = w.ldi(addr);
56750:         } else {
56750:             ins = w.ldp(addr);
18232:         }
17480:     }
24381:     checkForGlobalObjectReallocation();
17372:     tracker.set(p, ins);
26972: 
17372: #ifdef DEBUG
17737:     char name[64];
37694:     JS_ASSERT(strlen(prefix) < 11);
17925:     void* mark = NULL;
17925:     jsuword* localNames = NULL;
18011:     const char* funName = NULL;
57812:     JSAutoByteString funNameBytes;
17925:     if (*prefix == 'a' || *prefix == 'v') {
17925:         mark = JS_ARENA_MARK(&cx->tempPool);
59968:         JSFunction *fun = fp->fun();
59968:         Bindings &bindings = fun->script()->bindings;
59968:         if (bindings.hasLocalNames())
59968:             localNames = bindings.getLocalNameArray(cx, &cx->tempPool);
59968:         funName = fun->atom
59968:                   ? js_AtomToPrintableString(cx, fun->atom, &funNameBytes)
51056:                   : "<anonymous>";
18011:     }
17737:     if (!strcmp(prefix, "argv")) {
51120:         if (index < fp->numFormalArgs()) {
17737:             JSAtom *atom = JS_LOCAL_NAME_TO_ATOM(localNames[index]);
57812:             JSAutoByteString atomBytes;
57812:             JS_snprintf(name, sizeof name, "$%s.%s", funName,
57812:                         js_AtomToPrintableString(cx, atom, &atomBytes));
18011:         } else {
18011:             JS_snprintf(name, sizeof name, "$%s.<arg%d>", funName, index);
18011:         }
17737:     } else if (!strcmp(prefix, "vars")) {
51120:         JSAtom *atom = JS_LOCAL_NAME_TO_ATOM(localNames[fp->numFormalArgs() + index]);
57812:         JSAutoByteString atomBytes;
57812:         JS_snprintf(name, sizeof name, "$%s.%s", funName,
57812:                     js_AtomToPrintableString(cx, atom, &atomBytes));
17737:     } else {
17379:         JS_snprintf(name, sizeof name, "$%s%d", prefix, index);
17737:     }
17925: 
17925:     if (mark)
17925:         JS_ARENA_RELEASE(&cx->tempPool, mark);
56750:     w.name(ins, name);
17737: 
48470:     debug_only_printf(LC_TMTracer, "import vp=%p name=%s type=%c\n",
48470:                       p, name, TypeToChar(t));
48470: #endif
48470: }
48470: 
48470: JS_REQUIRES_STACK void
56750: TraceRecorder::import(Address addr, const Value* p, JSValueType t,
48470:                       const char *prefix, uintN index, JSStackFrame *fp)
48470: {
56750:     return importImpl(addr, p, t, prefix, index, fp);
17317: }
17317: 
29882: class ImportBoxedStackSlotVisitor : public SlotVisitorBase
29882: {
29880:     TraceRecorder &mRecorder;
29880:     LIns *mBase;
29880:     ptrdiff_t mStackOffset;
48470:     JSValueType *mTypemap;
29880:     JSStackFrame *mFp;
29880: public:
29880:     ImportBoxedStackSlotVisitor(TraceRecorder &recorder,
29880:                                 LIns *base,
29880:                                 ptrdiff_t stackOffset,
48470:                                 JSValueType *typemap) :
29880:         mRecorder(recorder),
29880:         mBase(base),
29880:         mStackOffset(stackOffset),
29880:         mTypemap(typemap)
29880:     {}
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
48470:     visitStackSlots(Value *vp, size_t count, JSStackFrame* fp) {
29880:         for (size_t i = 0; i < count; ++i) {
48470:             if (*mTypemap == JSVAL_TYPE_BOXED) {
56750:                 mRecorder.import(StackAddress(mBase, mStackOffset), vp, JSVAL_TYPE_BOXED,
29896:                                  "jsval", i, fp);
56750:                 LIns *vp_ins = mRecorder.unbox_value(*vp,
56750:                                                      StackAddress(mBase, mStackOffset),
29880:                                                      mRecorder.copy(mRecorder.anchor));
29880:                 mRecorder.set(vp, vp_ins);
29880:             }
29880:             vp++;
29880:             mTypemap++;
29880:             mStackOffset += sizeof(double);
29880:         }
29880:         return true;
29880:     }
48470: 
48470:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
53840:     visitFrameObjPtr(void* p, JSStackFrame *fp) {
48470:         JS_ASSERT(*mTypemap != JSVAL_TYPE_BOXED);
48470:         mTypemap++;
48470:         mStackOffset += sizeof(double);
48470:         return true;
48470:     }
29880: };
29880: 
22652: JS_REQUIRES_STACK void
36361: TraceRecorder::import(TreeFragment* tree, LIns* sp, unsigned stackSlots, unsigned ngslots,
48470:                       unsigned callDepth, JSValueType* typeMap)
17997: {
30860:     /*
30860:      * If we get a partial list that doesn't have all the types (i.e. recording
30860:      * from a side exit that was recorded but we added more global slots
30860:      * later), merge the missing types from the entry type map. This is safe
30860:      * because at the loop edge we verify that we have compatible types for all
30860:      * globals (entry type and loop edge type match). While a different trace
30860:      * of the tree might have had a guard with a different type map for these
30860:      * slots we just filled in here (the guard we continue from didn't know
30860:      * about them), since we didn't take that particular guard the only way we
30860:      * could have ended up here is if that other trace had at its end a
30860:      * compatible type distribution with the entry map. Since that's exactly
30860:      * what we used to fill in the types our current side exit didn't provide,
30860:      * this is always safe to do.
30860:      */
28956: 
48470:     JSValueType* globalTypeMap = typeMap + stackSlots;
36361:     unsigned length = tree->nGlobalTypes();
28956: 
28956:     /*
30860:      * This is potentially the typemap of the side exit and thus shorter than
30860:      * the tree's global type map.
28956:      */
28956:     if (ngslots < length) {
30860:         MergeTypeMaps(&globalTypeMap /* out param */, &ngslots /* out param */,
36361:                       tree->globalTypeMap(), length,
48470:                       (JSValueType*)alloca(sizeof(JSValueType) * length));
28956:     }
36361:     JS_ASSERT(ngslots == tree->nGlobalTypes());
29880: 
29880:     /*
30860:      * Check whether there are any values on the stack we have to unbox and do
30860:      * that first before we waste any time fetching the state from the stack.
29880:      */
36401:     ImportBoxedStackSlotVisitor boxedStackVisitor(*this, sp, -tree->nativeStackBase, typeMap);
29882:     VisitStackSlots(boxedStackVisitor, cx, callDepth);
36401: 
36401:     /*
36401:      * Remember the import type map so we can lazily import later whatever
36401:      * we need.
36401:      */
36401:     importTypeMap.set(importStackSlots = stackSlots,
36401:                       importGlobalSlots = ngslots,
36401:                       typeMap, globalTypeMap);
17997: }
17997: 
25938: JS_REQUIRES_STACK bool
52503: TraceRecorder::isValidSlot(JSObject *obj, const Shape* shape)
25938: {
42717:     uint32 setflags = (js_CodeSpec[*cx->regs->pc].format & (JOF_SET | JOF_INCDEC | JOF_FOR));
25938: 
25938:     if (setflags) {
52503:         if (!shape->hasDefaultSetter())
33542:             RETURN_VALUE("non-stub setter", false);
52503:         if (!shape->writable())
33542:             RETURN_VALUE("writing to a read-only property", false);
25938:     }
30860: 
25938:     /* This check applies even when setflags == 0. */
52503:     if (setflags != JOF_SET && !shape->hasDefaultGetter()) {
52503:         JS_ASSERT(!shape->isMethod());
33542:         RETURN_VALUE("non-stub getter", false);
32658:     }
25938: 
52503:     if (!obj->containsSlot(shape->slot))
40265:         RETURN_VALUE("invalid-slot obj property", false);
25938: 
25938:     return true;
25938: }
25938: 
17894: /* Lazily import a global slot if we don't already have it in the tracker. */
36401: JS_REQUIRES_STACK void
36401: TraceRecorder::importGlobalSlot(unsigned slot)
36401: {
36401:     JS_ASSERT(slot == uint16(slot));
40410:     JS_ASSERT(globalObj->numSlots() <= MAX_GLOBAL_SLOTS);
40410: 
48470:     Value* vp = &globalObj->getSlotRef(slot);
36401:     JS_ASSERT(!known(vp));
36401: 
36401:     /* Add the slot to the list of interned global slots. */
48470:     JSValueType type;
39928:     int index = tree->globalSlots->offsetOf(uint16(slot));
36401:     if (index == -1) {
36401:         type = getCoercedType(*vp);
48470:         if (type == JSVAL_TYPE_INT32 && (!oracle || oracle->isGlobalSlotUndemotable(cx, slot)))
48470:             type = JSVAL_TYPE_DOUBLE;
36401:         index = (int)tree->globalSlots->length();
39928:         tree->globalSlots->add(uint16(slot));
36401:         tree->typeMap.add(type);
36401:         SpecializeTreesToMissingGlobals(cx, globalObj, tree);
36401:         JS_ASSERT(tree->nGlobalTypes() == tree->globalSlots->length());
36401:     } else {
36401:         type = importTypeMap[importStackSlots + index];
36401:     }
56750:     import(EosAddress(eos_ins, slot * sizeof(double)), vp, type, "global", index, NULL);
36401: }
36401: 
36401: /* Lazily import a global slot if we don't already have it in the tracker. */
22652: JS_REQUIRES_STACK bool
17892: TraceRecorder::lazilyImportGlobalSlot(unsigned slot)
17891: {
18712:     if (slot != uint16(slot)) /* we use a table of 16-bit ints, bail out if that's not enough */
17891:         return false;
30860:     /*
30860:      * If the global object grows too large, alloca in ExecuteTree might fail,
30860:      * so abort tracing on global objects with unreasonably many slots.
27065:      */
40410:     if (globalObj->numSlots() > MAX_GLOBAL_SLOTS)
40410:         return false;
48470:     Value* vp = &globalObj->getSlotRef(slot);
24381:     if (known(vp))
17891:         return true; /* we already have it */
36401:     importGlobalSlot(slot);
17891:     return true;
17891: }
17891: 
18197: /* Write back a value onto the stack or global frames. */
18197: LIns*
59993: TraceRecorder::writeBack(LIns* ins, LIns* base, ptrdiff_t offset, bool shouldDemoteToInt32)
18197: {
30860:     /*
30860:      * Sink all type casts targeting the stack into the side exit by simply storing the original
30860:      * (uncasted) value. Each guard generates the side exit map based on the types of the
30860:      * last stores to every stack location, so it's safe to not perform them on-trace.
30860:      */
56180:     JS_ASSERT(base == lirbuf->sp || base == eos_ins);
59993:     if (shouldDemoteToInt32 && IsPromotedInt32(ins))
59993:         ins = w.demoteToInt32(ins);
59878: 
59878:     Address addr;
59878:     if (base == lirbuf->sp) {
59878:         addr = StackAddress(base, offset);
59878:     } else {
59878:         addr = EosAddress(base, offset);
60559:         unsigned slot = unsigned(offset / sizeof(double));
60559:         (void)pendingGlobalSlotsToSet.append(slot);  /* OOM is safe. */
59878:     }
56750:     return w.st(ins, addr);
18197: }
18197: 
18197: /* Update the tracker, then issue a write back store. */
22652: JS_REQUIRES_STACK void
59993: TraceRecorder::setImpl(void* p, LIns* i, bool shouldDemoteToInt32)
18197: {
26018:     JS_ASSERT(i != NULL);
24381:     checkForGlobalObjectReallocation();
18197:     tracker.set(p, i);
30860: 
30860:     /*
30860:      * If we are writing to this location for the first time, calculate the
30860:      * offset into the native frame manually. Otherwise just look up the last
30860:      * load or store associated with the same source address (p) and use the
30860:      * same offset/base.
30860:      */
19068:     LIns* x = nativeFrameTracker.get(p);
19068:     if (!x) {
48470:         if (isVoidPtrGlobal(p))
59993:             x = writeBack(i, eos_ins, nativeGlobalOffset((Value *)p), shouldDemoteToInt32);
18197:         else
59993:             x = writeBack(i, lirbuf->sp, nativespOffsetImpl(p), shouldDemoteToInt32);
17962:         nativeFrameTracker.set(p, x);
17803:     } else {
38567: #if defined NANOJIT_64BIT
42688:         JS_ASSERT( x->isop(LIR_stq) || x->isop(LIR_sti) || x->isop(LIR_std));
38567: #else
42688:         JS_ASSERT( x->isop(LIR_sti) || x->isop(LIR_std));
38567: #endif
34324: 
53611:         ptrdiff_t disp;
34324:         LIns *base = x->oprnd2();
53611:         if (base->isop(LIR_addp) && base->oprnd2()->isImmP()) {
53611:             disp = ptrdiff_t(base->oprnd2()->immP());
34324:             base = base->oprnd1();
53611:         } else {
34324:             disp = x->disp();
53611:         }
34324: 
34572:         JS_ASSERT(base == lirbuf->sp || base == eos_ins);
35083:         JS_ASSERT(disp == ((base == lirbuf->sp)
48470:                             ? nativespOffsetImpl(p)
48470:                             : nativeGlobalOffset((Value *)p)));
34324: 
59993:         writeBack(i, base, disp, shouldDemoteToInt32);
34324:     }
17320: }
17320: 
48470: JS_REQUIRES_STACK inline void
59993: TraceRecorder::set(Value* p, LIns* i, bool shouldDemoteToInt32)
59993: {
59993:     return setImpl(p, i, shouldDemoteToInt32);
48470: }
48470: 
48470: JS_REQUIRES_STACK void
59993: TraceRecorder::setFrameObjPtr(void* p, LIns* i, bool shouldDemoteToInt32)
48470: {
48470:     JS_ASSERT(isValidFrameObjPtr(p));
59993:     return setImpl(p, i, shouldDemoteToInt32);
48470: }
48470: 
22652: JS_REQUIRES_STACK LIns*
48470: TraceRecorder::attemptImport(const Value* p)
36662: {
38532:     if (LIns* i = getFromTracker(p))
36662:         return i;
36662: 
36662:     /* If the variable was not known, it could require a lazy import. */
36662:     CountSlotsVisitor countVisitor(p);
36662:     VisitStackSlots(countVisitor, cx, callDepth);
36662: 
53840:     if (countVisitor.stopped() || size_t(p - cx->fp()->slots()) < cx->fp()->numSlots())
36662:         return get(p);
36662: 
36662:     return NULL;
36662: }
36662: 
48470: inline nanojit::LIns*
48470: TraceRecorder::getFromTrackerImpl(const void* p)
38532: {
38532:     checkForGlobalObjectReallocation();
38532:     return tracker.get(p);
38532: }
38532: 
48470: inline nanojit::LIns*
48470: TraceRecorder::getFromTracker(const Value* p)
48470: {
48470:     return getFromTrackerImpl(p);
48470: }
48470: 
36662: JS_REQUIRES_STACK LIns*
48470: TraceRecorder::getImpl(const void *p)
48470: {
48470:     LIns* x = getFromTrackerImpl(p);
36401:     if (x)
36401:         return x;
48470:     if (isVoidPtrGlobal(p)) {
48470:         unsigned slot = nativeGlobalSlot((const Value *)p);
39928:         JS_ASSERT(tree->globalSlots->offsetOf(uint16(slot)) != -1);
36401:         importGlobalSlot(slot);
36401:     } else {
48470:         unsigned slot = nativeStackSlotImpl(p);
48470:         JSValueType type = importTypeMap[slot];
56750:         importImpl(StackAddress(lirbuf->sp, -tree->nativeStackBase + slot * sizeof(jsdouble)),
51446:                    p, type, "stack", slot, cx->fp());
36401:     }
48470:     JS_ASSERT(knownImpl(p));
17320:     return tracker.get(p);
17320: }
17320: 
31444: JS_REQUIRES_STACK LIns*
48470: TraceRecorder::get(const Value *p)
48470: {
48470:     return getImpl(p);
48470: }
48470: 
48470: #ifdef DEBUG
48470: bool
53840: TraceRecorder::isValidFrameObjPtr(void *p)
48470: {
51446:     JSStackFrame *fp = cx->fp();
53840:     for (; fp; fp = fp->prev()) {
53840:         if (fp->addressOfScopeChain() == p || fp->addressOfArgs() == p)
48470:             return true;
48470:     }
48470:     return false;
48470: }
48470: #endif
48470: 
48470: JS_REQUIRES_STACK LIns*
53840: TraceRecorder::getFrameObjPtr(void *p)
48470: {
48470:     JS_ASSERT(isValidFrameObjPtr(p));
48470:     return getImpl(p);
48470: }
48470: 
48470: JS_REQUIRES_STACK LIns*
48470: TraceRecorder::addr(Value* p)
31444: {
31444:     return isGlobal(p)
56750:            ? w.addp(eos_ins, w.nameImmw(nativeGlobalOffset(p)))
56750:            : w.addp(lirbuf->sp, w.nameImmw(nativespOffset(p)));
31444: }
31444: 
48470: JS_REQUIRES_STACK inline bool
48470: TraceRecorder::knownImpl(const void* p)
24381: {
24381:     checkForGlobalObjectReallocation();
24381:     return tracker.has(p);
24381: }
24381: 
48470: JS_REQUIRES_STACK inline bool
48470: TraceRecorder::known(const Value* vp)
48470: {
48470:     return knownImpl(vp);
48470: }
48470: 
48470: JS_REQUIRES_STACK inline bool
48470: TraceRecorder::known(JSObject** p)
48470: {
48470:     return knownImpl(p);
48470: }
48470: 
24381: /*
55746:  * The slots of the global object are sometimes reallocated by the interpreter.
24381:  * This function check for that condition and re-maps the entries of the tracker
24381:  * accordingly.
24381:  */
24381: JS_REQUIRES_STACK void
53561: TraceRecorder::checkForGlobalObjectReallocationHelper()
53561: {
56180:     debug_only_print0(LC_TMTracer, "globalObj->slots relocated, updating tracker\n");
56180:     Value* src = global_slots;
55746:     Value* dst = globalObj->getSlots();
55746:     jsuint length = globalObj->capacity;
27062:     LIns** map = (LIns**)alloca(sizeof(LIns*) * length);
24381:     for (jsuint n = 0; n < length; ++n) {
24381:         map[n] = tracker.get(src);
24381:         tracker.set(src++, NULL);
24381:     }
24381:     for (jsuint n = 0; n < length; ++n)
24381:         tracker.set(dst++, map[n]);
56180:     global_slots = globalObj->getSlots();
24381: }
24381: 
20416: /* Determine whether the current branch is a loop edge (taken or not taken). */
22652: static JS_REQUIRES_STACK bool
30860: IsLoopEdge(jsbytecode* pc, jsbytecode* header)
20416: {
20416:     switch (*pc) {
20416:       case JSOP_IFEQ:
20416:       case JSOP_IFNE:
20416:         return ((pc + GET_JUMP_OFFSET(pc)) == header);
20416:       case JSOP_IFEQX:
20416:       case JSOP_IFNEX:
20416:         return ((pc + GET_JUMPX_OFFSET(pc)) == header);
20416:       default:
20416:         JS_ASSERT((*pc == JSOP_AND) || (*pc == JSOP_ANDX) ||
20416:                   (*pc == JSOP_OR) || (*pc == JSOP_ORX));
20416:     }
20416:     return false;
20416: }
20416: 
29882: class AdjustCallerGlobalTypesVisitor : public SlotVisitorBase
29882: {
29880:     TraceRecorder &mRecorder;
29880:     JSContext *mCx;
29880:     nanojit::LirBuffer *mLirbuf;
48470:     JSValueType *mTypeMap;
29880: public:
29880:     AdjustCallerGlobalTypesVisitor(TraceRecorder &recorder,
48470:                                    JSValueType *typeMap) :
29880:         mRecorder(recorder),
29880:         mCx(mRecorder.cx),
29880:         mLirbuf(mRecorder.lirbuf),
29880:         mTypeMap(typeMap)
29880:     {}
29880: 
48470:     JSValueType* getTypeMap()
29880:     {
29880:         return mTypeMap;
29880:     }
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE void
48470:     visitGlobalSlot(Value *vp, unsigned n, unsigned slot) {
29880:         LIns *ins = mRecorder.get(vp);
59993:         bool isPromote = IsPromotedInt32(ins);
48470:         if (isPromote && *mTypeMap == JSVAL_TYPE_DOUBLE) {
56750:             mRecorder.w.st(mRecorder.get(vp),
56750:                            EosAddress(mRecorder.eos_ins, mRecorder.nativeGlobalOffset(vp)));
30860:             /*
30860:              * Aggressively undo speculation so the inner tree will compile
30860:              * if this fails.
30860:              */
46181:             JS_TRACE_MONITOR(mCx).oracle->markGlobalSlotUndemotable(mCx, slot);
29880:         }
48470:         JS_ASSERT(!(!isPromote && *mTypeMap == JSVAL_TYPE_INT32));
29880:         ++mTypeMap;
29880:     }
29880: };
29880: 
29882: class AdjustCallerStackTypesVisitor : public SlotVisitorBase
29882: {
29880:     TraceRecorder &mRecorder;
29880:     JSContext *mCx;
29880:     nanojit::LirBuffer *mLirbuf;
29880:     unsigned mSlotnum;
48470:     JSValueType *mTypeMap;
29880: public:
29880:     AdjustCallerStackTypesVisitor(TraceRecorder &recorder,
48470:                                   JSValueType *typeMap) :
29880:         mRecorder(recorder),
29880:         mCx(mRecorder.cx),
29880:         mLirbuf(mRecorder.lirbuf),
29880:         mSlotnum(0),
29880:         mTypeMap(typeMap)
29880:     {}
29880: 
48470:     JSValueType* getTypeMap()
29880:     {
29880:         return mTypeMap;
29880:     }
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
48470:     visitStackSlots(Value *vp, size_t count, JSStackFrame* fp) {
48470:         /* N.B. vp may actually point to a JSObject*. */
29880:         for (size_t i = 0; i < count; ++i) {
29880:             LIns *ins = mRecorder.get(vp);
59993:             bool isPromote = IsPromotedInt32(ins);
48470:             if (isPromote && *mTypeMap == JSVAL_TYPE_DOUBLE) {
56750:                 mRecorder.w.st(ins, StackAddress(mLirbuf->sp, mRecorder.nativespOffset(vp)));
30860:                 /*
30860:                  * Aggressively undo speculation so the inner tree will compile
30860:                  * if this fails.
30860:                  */
46181:                 JS_TRACE_MONITOR(mCx).oracle->markStackSlotUndemotable(mCx, mSlotnum);
29880:             }
48470:             JS_ASSERT(!(!isPromote && *mTypeMap == JSVAL_TYPE_INT32));
29880:             ++vp;
29880:             ++mTypeMap;
29880:             ++mSlotnum;
29880:         }
29880:         return true;
29880:     }
48470: 
48470:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
53840:     visitFrameObjPtr(void* p, JSStackFrame* fp) {
48470:         JS_ASSERT(*mTypeMap != JSVAL_TYPE_BOXED);
48470:         ++mTypeMap;
48470:         ++mSlotnum;
48470:         return true;
48470:     }
29880: };
29880: 
28239: /*
30860:  * Promote slots if necessary to match the called tree's type map. This
30860:  * function is infallible and must only be called if we are certain that it is
30860:  * possible to reconcile the types for each slot in the inner and outer trees.
28239:  */
28239: JS_REQUIRES_STACK void
35044: TraceRecorder::adjustCallerTypes(TreeFragment* f)
35044: {
36361:     AdjustCallerGlobalTypesVisitor globalVisitor(*this, f->globalTypeMap());
36361:     VisitGlobalSlots(globalVisitor, cx, *tree->globalSlots);
36361: 
36361:     AdjustCallerStackTypesVisitor stackVisitor(*this, f->stackTypeMap());
29882:     VisitStackSlots(stackVisitor, cx, 0);
29880: 
18650:     JS_ASSERT(f == f->root);
18250: }
18250: 
48470: JS_REQUIRES_STACK inline JSValueType
48470: TraceRecorder::determineSlotType(Value* vp)
48470: {
48470:     if (vp->isNumber()) {
38532:         LIns *i = getFromTracker(vp);
48470:         JSValueType t;
36401:         if (i) {
59993:             t = IsPromotedInt32(i) ? JSVAL_TYPE_INT32 : JSVAL_TYPE_DOUBLE;
36401:         } else if (isGlobal(vp)) {
39928:             int offset = tree->globalSlots->offsetOf(uint16(nativeGlobalSlot(vp)));
36401:             JS_ASSERT(offset != -1);
48470:             t = importTypeMap[importStackSlots + offset];
48470:         } else {
48470:             t = importTypeMap[nativeStackSlot(vp)];
48470:         }
48470:         JS_ASSERT_IF(t == JSVAL_TYPE_INT32, hasInt32Repr(*vp));
48470:         return t;
48470:     }
48470: 
48470:     if (vp->isObject())
48470:         return vp->toObject().isFunction() ? JSVAL_TYPE_FUNOBJ : JSVAL_TYPE_NONFUNOBJ;
48470:     return vp->extractNonDoubleObjectTraceType();
19084: }
19084: 
29882: class DetermineTypesVisitor : public SlotVisitorBase
29882: {
29880:     TraceRecorder &mRecorder;
48470:     JSValueType *mTypeMap;
29880: public:
29880:     DetermineTypesVisitor(TraceRecorder &recorder,
48470:                           JSValueType *typeMap) :
29880:         mRecorder(recorder),
29880:         mTypeMap(typeMap)
29880:     {}
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE void
48470:     visitGlobalSlot(Value *vp, unsigned n, unsigned slot) {
29880:         *mTypeMap++ = mRecorder.determineSlotType(vp);
29880:     }
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
48470:     visitStackSlots(Value *vp, size_t count, JSStackFrame* fp) {
29880:         for (size_t i = 0; i < count; ++i)
29880:             *mTypeMap++ = mRecorder.determineSlotType(vp++);
29880:         return true;
29880:     }
29880: 
48470:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
53840:     visitFrameObjPtr(void* p, JSStackFrame* fp) {
53840:         *mTypeMap++ = getFrameObjPtrTraceType(p, fp);
48470:         return true;
48470:     }
48470: 
48470:     JSValueType* getTypeMap()
29880:     {
29880:         return mTypeMap;
29880:     }
29880: };
29880: 
31903: #if defined JS_JIT_SPEW
31903: JS_REQUIRES_STACK static void
31903: TreevisLogExit(JSContext* cx, VMSideExit* exit)
31903: {
31903:     debug_only_printf(LC_TMTreeVis, "TREEVIS ADDEXIT EXIT=%p TYPE=%s FRAG=%p PC=%p FILE=\"%s\""
31937:                       " LINE=%d OFFS=%d", (void*)exit, getExitName(exit->exitType),
53840:                       (void*)exit->from, (void*)cx->regs->pc, cx->fp()->script()->filename,
51446:                       js_FramePCToLineNumber(cx, cx->fp()), FramePCOffset(cx, cx->fp()));
31903:     debug_only_print0(LC_TMTreeVis, " STACK=\"");
31903:     for (unsigned i = 0; i < exit->numStackSlots; i++)
48470:         debug_only_printf(LC_TMTreeVis, "%c", TypeToChar(exit->stackTypeMap()[i]));
31903:     debug_only_print0(LC_TMTreeVis, "\" GLOBALS=\"");
31903:     for (unsigned i = 0; i < exit->numGlobalSlots; i++)
48470:         debug_only_printf(LC_TMTreeVis, "%c", TypeToChar(exit->globalTypeMap()[i]));
31903:     debug_only_print0(LC_TMTreeVis, "\"\n");
31903: }
31903: #endif
31903: 
27540: JS_REQUIRES_STACK VMSideExit*
17850: TraceRecorder::snapshot(ExitType exitType)
17850: {
51446:     JSStackFrame* const fp = cx->fp();
42717:     JSFrameRegs* const regs = cx->regs;
20969:     jsbytecode* pc = regs->pc;
20969: 
30860:     /*
30860:      * Check for a return-value opcode that needs to restart at the next
30860:      * instruction.
30860:      */
20969:     const JSCodeSpec& cs = js_CodeSpec[*pc];
20969: 
26972:     /*
26972:      * When calling a _FAIL native, make the snapshot's pc point to the next
30860:      * instruction after the CALL or APPLY. Even on failure, a _FAIL native
30860:      * must not be called again from the interpreter.
26972:      */
32669:     bool resumeAfter = (pendingSpecializedNative &&
32669:                         JSTN_ERRTYPE(pendingSpecializedNative) == FAIL_STATUS);
20969:     if (resumeAfter) {
57712:         JS_ASSERT(*pc == JSOP_CALL || *pc == JSOP_FUNAPPLY || *pc == JSOP_FUNCALL ||
57712:                   *pc == JSOP_NEW || *pc == JSOP_SETPROP || *pc == JSOP_SETNAME);
20969:         pc += cs.length;
20969:         regs->pc = pc;
21476:         MUST_FLOW_THROUGH("restore_pc");
20969:     }
20969: 
30860:     /*
30860:      * Generate the entry map for the (possibly advanced) pc and stash it in
30860:      * the trace.
30860:      */
30860:     unsigned stackSlots = NativeStackSlots(cx, callDepth);
30860: 
30860:     /*
30860:      * It's sufficient to track the native stack use here since all stores
30860:      * above the stack watermark defined by guards are killed.
30860:      */
17962:     trackNativeStackUse(stackSlots + 1);
20969: 
20947:     /* Capture the type map into a temporary location. */
36361:     unsigned ngslots = tree->globalSlots->length();
48470:     unsigned typemap_size = (stackSlots + ngslots) * sizeof(JSValueType);
33930: 
33930:     /* Use the recorder-local temporary type map. */
48470:     JSValueType* typemap = NULL;
33930:     if (tempTypeMap.resize(typemap_size))
33930:         typemap = tempTypeMap.begin(); /* crash if resize() fails. */
29880: 
29880:     /*
29880:      * Determine the type of a store by looking at the current type of the
30860:      * actual value the interpreter is using. For numbers we have to check what
30860:      * kind of store we used last (integer or double) to figure out what the
30860:      * side exit show reflect in its typemap.
29880:      */
29880:     DetermineTypesVisitor detVisitor(*this, typemap);
29882:     VisitSlots(detVisitor, cx, callDepth, ngslots,
36361:                tree->globalSlots->data());
29880:     JS_ASSERT(unsigned(detVisitor.getTypeMap() - typemap) ==
29880:               ngslots + stackSlots);
20969: 
26972:     /*
48470:      * If this snapshot is for a side exit that leaves a boxed Value result on
31444:      * the stack, make a note of this in the typemap. Examples include the
31444:      * builtinStatus guard after calling a _FAIL builtin, a JSFastNative, or
48470:      * GetPropertyByName; and the type guard in unbox_value after such a call
31444:      * (also at the beginning of a trace branched from such a type guard).
31444:      */
31444:     if (pendingUnboxSlot ||
32669:         (pendingSpecializedNative && (pendingSpecializedNative->flags & JSTN_UNBOX_AFTER))) {
31444:         unsigned pos = stackSlots - 1;
42717:         if (pendingUnboxSlot == cx->regs->sp - 2)
31444:             pos = stackSlots - 2;
48470:         typemap[pos] = JSVAL_TYPE_BOXED;
48470:     } else if (pendingSpecializedNative &&
48470:                (pendingSpecializedNative->flags & JSTN_RETURN_NULLABLE_STR)) {
48470:         typemap[stackSlots - 1] = JSVAL_TYPE_STRORNULL;
48470:     } else if (pendingSpecializedNative &&
48470:                (pendingSpecializedNative->flags & JSTN_RETURN_NULLABLE_OBJ)) {
48470:         typemap[stackSlots - 1] = JSVAL_TYPE_OBJORNULL;
31444:     } 
26972: 
26972:     /* Now restore the the original pc (after which early returns are ok). */
26286:     if (resumeAfter) {
20969:         MUST_FLOW_LABEL(restore_pc);
20969:         regs->pc = pc - cs.length;
20969:     } else {
30860:         /*
30860:          * If we take a snapshot on a goto, advance to the target address. This
30860:          * avoids inner trees returning on a break goto, which the outer
30860:          * recorder then would confuse with a break in the outer tree.
30860:          */
20947:         if (*pc == JSOP_GOTO)
20947:             pc += GET_JUMP_OFFSET(pc);
20947:         else if (*pc == JSOP_GOTOX)
20947:             pc += GET_JUMPX_OFFSET(pc);
20969:     }
20969: 
26972:     /*
27540:      * Check if we already have a matching side exit; if so we can return that
27540:      * side exit instead of creating a new one.
26972:      */
36361:     VMSideExit** exits = tree->sideExits.data();
36361:     unsigned nexits = tree->sideExits.length();
20957:     if (exitType == LOOP_EXIT) {
20947:         for (unsigned n = 0; n < nexits; ++n) {
21521:             VMSideExit* e = exits[n];
53840:             if (e->pc == pc && (e->imacpc == fp->maybeImacropc()) &&
28996:                 ngslots == e->numGlobalSlots &&
31495:                 !memcmp(exits[n]->fullTypeMap(), typemap, typemap_size)) {
20947:                 AUDIT(mergedLoopExits);
31903: #if defined JS_JIT_SPEW
31903:                 TreevisLogExit(cx, e);
31903: #endif
27540:                 return e;
27540:             }
27540:         }
27540:     }
27540: 
27540:     /* We couldn't find a matching side exit, so create a new one. */
33161:     VMSideExit* exit = (VMSideExit*)
48470:         traceAlloc().alloc(sizeof(VMSideExit) + (stackSlots + ngslots) * sizeof(JSValueType));
26972: 
26972:     /* Setup side exit structure. */
20931:     exit->from = fragment;
20931:     exit->calldepth = callDepth;
20931:     exit->numGlobalSlots = ngslots;
20931:     exit->numStackSlots = stackSlots;
53840:     exit->numStackSlotsBelowCurrentFrame = cx->fp()->isFunctionFrame() ?
53840:                                            nativeStackOffset(&cx->fp()->calleeValue()) / sizeof(double) :
31939:                                            0;
20931:     exit->exitType = exitType;
25111:     exit->pc = pc;
53840:     exit->imacpc = fp->maybeImacropc();
36361:     exit->sp_adj = (stackSlots * sizeof(double)) - tree->nativeStackBase;
23262:     exit->rp_adj = exit->calldepth * sizeof(FrameInfo*);
30034:     exit->lookupFlags = js_InferFlags(cx, 0);
31495:     memcpy(exit->fullTypeMap(), typemap, typemap_size);
30435: 
31903: #if defined JS_JIT_SPEW
31903:     TreevisLogExit(cx, exit);
31903: #endif
27540:     return exit;
17381: }
17381: 
33161: JS_REQUIRES_STACK GuardRecord*
27540: TraceRecorder::createGuardRecord(VMSideExit* exit)
27540: {
40355: #ifdef JS_JIT_SPEW
40355:     // For debug builds, place the guard records in a longer lasting
40355:     // pool.  This is because the fragment profiler will look at them
40355:     // relatively late in the day, after they would have been freed,
40355:     // in some cases, had they been allocated in traceAlloc().
40355:     GuardRecord* gr = new (dataAlloc()) GuardRecord();
40355: #else
40355:     // The standard place (for production builds).
34351:     GuardRecord* gr = new (traceAlloc()) GuardRecord();
40355: #endif
27540: 
27540:     gr->exit = exit;
27540:     exit->addGuard(gr);
27540: 
33186:     // gr->profCount is calloc'd to zero
32784:     verbose_only(
32784:         gr->profGuardID = fragment->guardNumberer++;
32784:         gr->nextInFrag = fragment->guardsForFrag;
32784:         fragment->guardsForFrag = gr;
32784:     )
32784: 
33161:     return gr;
27540: }
27540: 
56222: /* Test if 'ins' is in a form that can be used as a guard/branch condition. */
56222: static bool
56222: isCond(LIns* ins)
56222: {
56222:     return ins->isCmp() || ins->isImmI(0) || ins->isImmI(1);
56222: }
56222: 
56222: /* Ensure 'ins' is in a form suitable for a guard/branch condition. */
56222: void
56222: TraceRecorder::ensureCond(LIns** ins, bool* cond)
56222: {
56222:     if (!isCond(*ins)) {
56222:         *cond = !*cond;
56750:         *ins = (*ins)->isI() ? w.eqi0(*ins) : w.eqp0(*ins);
56222:     }
56222: }
56222: 
27540: /*
27540:  * Emit a guard for condition (cond), expecting to evaluate to boolean result
55556:  * (expected) and using the supplied side exit if the condition doesn't hold.
55556:  *
55556:  * Callers shouldn't generate guards that always exit (which can occur due to
55556:  * optimization of the guard condition) because it's bad for both compile-time
55556:  * speed (all the code generated after the guard is dead) and run-time speed
55556:  * (fragment that always exit are slow).  This function has two modes for
55556:  * handling an always-exit guard;  which mode is used depends on the value of
55556:  * abortIfAlwaysExits:
55556:  *
55556:  * - abortIfAlwaysExits == false:  This is the default mode.  If the guard
55556:  *   will always exit, we assert (in debug builds) as a signal that we are
55556:  *   generating bad traces.  (In optimized builds that lack assertions the
55556:  *   guard will be generated correctly, so the code will be slow but safe.) In
55556:  *   this mode, the caller is responsible for not generating an always-exit
55556:  *   guard.  The return value will always be RECORD_CONTINUE, so the caller
55556:  *   need not check it.
55556:  *
55556:  * - abortIfAlwaysExits == true:  If the guard will always exit, we abort
55556:  *   recording and return RECORD_STOP;  otherwise we generate the guard
55556:  *   normally and return RECORD_CONTINUE.  This mode can be used when the
55556:  *   caller doesn't know ahead of time whether the guard will always exit.  In
55556:  *   this mode, the caller must check the return value.
55556:  */
55556: JS_REQUIRES_STACK RecordingStatus
55556: TraceRecorder::guard(bool expected, LIns* cond, VMSideExit* exit,
55556:                      bool abortIfAlwaysExits/* = false */)
55556: {
26972:     if (exit->exitType == LOOP_EXIT)
36361:         tree->sideExits.add(exit);
27540: 
56222:     JS_ASSERT(isCond(cond));
54427: 
55556:     if ((cond->isImmI(0) && expected) || (cond->isImmI(1) && !expected)) {
55556:         if (abortIfAlwaysExits) {
55556:             /* The guard always exits, the caller must check for an abort. */
55556:             RETURN_STOP("Constantly false guard detected");
55556:         }
55556:         /*
59251:          * If you hit this assertion, first decide if you want recording to
55556:          * abort in the case where the guard always exits.  If not, find a way
55556:          * to detect that case and avoid calling guard().  Otherwise, change
55556:          * the invocation of guard() so it passes in abortIfAlwaysExits=true,
55556:          * and have the caller check the return value, eg. using
55556:          * CHECK_STATUS().  (In optimized builds, we'll fall through to the
55556:          * insGuard() below and an always-exits guard will be inserted, which
55556:          * is correct but sub-optimal.)
55556:          */
59251:         JS_NOT_REACHED("unexpected constantly false guard detected");
55556:     }
55556: 
55556:     /*
55556:      * Nb: if the guard is never taken, no instruction will be created and
55556:      * insGuard() will return NULL.  This is a good thing.
56750:      */
56750:     GuardRecord* guardRec = createGuardRecord(exit);
56750:     expected ? w.xf(cond, guardRec) : w.xt(cond, guardRec);
55556:     return RECORD_CONTINUE;
55556: }
55556: 
55556: /*
55556:  * Emit a guard for condition (cond), expecting to evaluate to boolean result
55556:  * (expected) and generate a side exit with type exitType to jump to if the
55556:  * condition does not hold.
55556:  */
55556: JS_REQUIRES_STACK RecordingStatus
55556: TraceRecorder::guard(bool expected, LIns* cond, ExitType exitType,
55556:                      bool abortIfAlwaysExits/* = false */)
55556: {
55556:     return guard(expected, cond, snapshot(exitType), abortIfAlwaysExits);
27540: }
27540: 
38603: /*
38603:  * Emit a guard a 32-bit integer arithmetic operation op(d0, d1) and
38603:  * using the supplied side exit if it overflows.
38603:  */
38603: JS_REQUIRES_STACK LIns*
38603: TraceRecorder::guard_xov(LOpcode op, LIns* d0, LIns* d1, VMSideExit* exit)
38603: {
56750:     JS_ASSERT(exit->exitType == OVERFLOW_EXIT);
38603: 
38603:     GuardRecord* guardRec = createGuardRecord(exit);
38603:     switch (op) {
42688:       case LIR_addi:
56750:         return w.addxovi(d0, d1, guardRec);
42688:       case LIR_subi:
56750:         return w.subxovi(d0, d1, guardRec);
42688:       case LIR_muli:
56750:         return w.mulxovi(d0, d1, guardRec);
38603:       default:
56750:         break;
56750:     }
56750:     JS_NOT_REACHED("unexpected opcode");
56750:     return NULL;
38603: }
38603: 
27540: JS_REQUIRES_STACK VMSideExit*
27540: TraceRecorder::copy(VMSideExit* copy)
27540: {
27540:     size_t typemap_size = copy->numGlobalSlots + copy->numStackSlots;
33161:     VMSideExit* exit = (VMSideExit*)
48470:         traceAlloc().alloc(sizeof(VMSideExit) + typemap_size * sizeof(JSValueType));
27540: 
27540:     /* Copy side exit structure. */
48470:     memcpy(exit, copy, sizeof(VMSideExit) + typemap_size * sizeof(JSValueType));
27540:     exit->guards = NULL;
27540:     exit->from = fragment;
27540:     exit->target = NULL;
27540: 
27540:     if (exit->exitType == LOOP_EXIT)
36361:         tree->sideExits.add(exit);
31904: #if defined JS_JIT_SPEW
31903:     TreevisLogExit(cx, exit);
31904: #endif
27540:     return exit;
21083: }
21083: 
30860: /*
31526:  * Determine whether any context associated with the same thread as cx is
31526:  * executing native code.
31526:  */
31526: static inline bool
60574: ProhibitFlush(TraceMonitor *tm)
60574: {
60574:     return !!tm->tracerState; // don't flush if we're running a trace
31526: }
31526: 
36712: static void
32748: ResetJITImpl(JSContext* cx)
27493: {
54560:     if (!cx->traceJitEnabled)
27493:         return;
37741:     TraceMonitor* tm = &JS_TRACE_MONITOR(cx);
29883:     debug_only_print0(LC_TMTracer, "Flushing cache.\n");
36712:     if (tm->recorder) {
36712:         JS_ASSERT_NOT_ON_TRACE(cx);
37741:         AbortRecording(cx, "flush cache");
36712:     }
59713: #if JS_METHODJIT
59713:     if (tm->profile)
59713:         AbortProfiling(cx);
59713: #endif
60574:     if (ProhibitFlush(tm)) {
31920:         debug_only_print0(LC_TMTracer, "Deferring JIT flush due to deep bail.\n");
27493:         tm->needFlush = JS_TRUE;
27493:         return;
27493:     }
31843:     tm->flush();
27493: }
27493: 
18606: /* Compile the current fragment. */
33542: JS_REQUIRES_STACK AbortableRecordingStatus
35083: TraceRecorder::compile()
24307: {
29368: #ifdef MOZ_TRACEVIS
31063:     TraceVisStateObj tvso(cx, S_COMPILE);
29368: #endif
29368: 
35083:     if (traceMonitor->needFlush) {
32748:         ResetJIT(cx, FR_DEEP_BAIL);
33542:         return ARECORD_ABORTED;
27493:     }
36361:     if (tree->maxNativeStackSlots >= MAX_NATIVE_STACK_SLOTS) {
29883:         debug_only_print0(LC_TMTracer, "Blacklist: excessive stack use.\n");
40251:         Blacklist((jsbytecode*)tree->ip);
33542:         return ARECORD_STOP;
18118:     }
25099:     if (anchor && anchor->exitType != CASE_EXIT)
40251:         ++tree->branchCount;
33159:     if (outOfMemory())
33542:         return ARECORD_STOP;
31475: 
35377:     /* :TODO: windows support */
35377: #if defined DEBUG && !defined WIN32
35377:     /* Associate a filename and line number with the fragment. */
53840:     const char* filename = cx->fp()->script()->filename;
35377:     char* label = (char*)js_malloc((filename ? strlen(filename) : 7) + 16);
56017:     if (label) {
35377:         sprintf(label, "%s:%u", filename ? filename : "<stdin>",
51446:                 js_FramePCToLineNumber(cx, cx->fp()));
40300:         lirbuf->printer->addrNameMap->addAddrRange(fragment, sizeof(Fragment), 0, label);
35377:         js_free(label);
56017:     }
35377: #endif
35377: 
35083:     Assembler *assm = traceMonitor->assembler;
61053:     JS_ASSERT(!assm->error());
40300:     assm->compile(fragment, tempAlloc(), /*optimize*/true verbose_only(, lirbuf->printer));
31475: 
61053:     if (assm->error()) {
35083:         assm->setError(nanojit::None);
29883:         debug_only_print0(LC_TMTracer, "Blacklisted: error during compilation\n");
40251:         Blacklist((jsbytecode*)tree->ip);
33542:         return ARECORD_STOP;
22633:     }
35083: 
35083:     if (outOfMemory())
35083:         return ARECORD_STOP;
30860:     ResetRecordingAttempts(cx, (jsbytecode*)fragment->ip);
40251:     ResetRecordingAttempts(cx, (jsbytecode*)tree->ip);
25099:     if (anchor) {
25099: #ifdef NANOJIT_IA32
25099:         if (anchor->exitType == CASE_EXIT)
31475:             assm->patch(anchor, anchor->switchInfo);
25099:         else
25099: #endif
31475:             assm->patch(anchor);
25099:     }
18211:     JS_ASSERT(fragment->code());
36361:     JS_ASSERT_IF(fragment == fragment->root, fragment->root == tree);
30860: 
33542:     return ARECORD_CONTINUE;
18334: }
18334: 
31495: static void
35044: JoinPeers(Assembler* assm, VMSideExit* exit, TreeFragment* target)
31495: {
31495:     exit->target = target;
31495:     assm->patch(exit);
31495: 
31937:     debug_only_printf(LC_TMTreeVis, "TREEVIS JOIN ANCHOR=%p FRAG=%p\n", (void*)exit, (void*)target);
31903: 
31495:     if (exit->root() == target)
31495:         return;
31495: 
36361:     target->dependentTrees.addUnique(exit->root());
36361:     exit->root()->linkedTrees.addUnique(target);
31495: }
31495: 
31473: /* Results of trying to connect an arbitrary type A with arbitrary type B */
31473: enum TypeCheckResult
31473: {
31473:     TypeCheck_Okay,         /* Okay: same type */
41265:     TypeCheck_Promote,      /* Okay: Type A needs d2i() */
41265:     TypeCheck_Demote,       /* Okay: Type A needs i2d() */
31473:     TypeCheck_Undemote,     /* Bad: Slot is undemotable */
31473:     TypeCheck_Bad           /* Bad: incompatible types */
31473: };
31473: 
31473: class SlotMap : public SlotVisitorBase
31473: {
31473:   public:
31473:     struct SlotInfo
31473:     {
31920:         SlotInfo()
59993:           : vp(NULL), isPromotedInt32(false), lastCheck(TypeCheck_Bad)
31920:         {}
59993:         SlotInfo(Value* vp, bool isPromotedInt32)
59993:           : vp(vp), isPromotedInt32(isPromotedInt32), lastCheck(TypeCheck_Bad),
59993:             type(getCoercedType(*vp))
33564:         {}
53840:         SlotInfo(JSValueType t)
59993:           : vp(NULL), isPromotedInt32(false), lastCheck(TypeCheck_Bad), type(t)
31920:         {}
48470:         SlotInfo(Value* vp, JSValueType t)
59993:           : vp(vp), isPromotedInt32(t == JSVAL_TYPE_INT32), lastCheck(TypeCheck_Bad), type(t)
48470:         {}
48470:         void            *vp;
59993:         bool            isPromotedInt32;
31473:         TypeCheckResult lastCheck;
48470:         JSValueType     type;
31473:     };
31473: 
33564:     SlotMap(TraceRecorder& rec)
31920:         : mRecorder(rec),
31920:           mCx(rec.cx),
33564:           slots(NULL)
31473:     {
31473:     }
31473: 
39928:     virtual ~SlotMap()
39928:     {
39928:     }
39928: 
31473:     JS_REQUIRES_STACK JS_ALWAYS_INLINE void
48470:     visitGlobalSlot(Value *vp, unsigned n, unsigned slot)
31473:     {
31473:         addSlot(vp);
31473:     }
31473: 
31473:     JS_ALWAYS_INLINE SlotMap::SlotInfo&
31473:     operator [](unsigned i)
31473:     {
31473:         return slots[i];
31473:     }
31473: 
31473:     JS_ALWAYS_INLINE SlotMap::SlotInfo&
31473:     get(unsigned i)
31473:     {
31473:         return slots[i];
31473:     }
31473: 
31473:     JS_ALWAYS_INLINE unsigned
31473:     length()
31473:     {
31473:         return slots.length();
31473:     }
31473: 
31473:     /**
31473:      * Possible return states:
31473:      *
31473:      * TypeConsensus_Okay:      All types are compatible. Caller must go through slot list and handle
31473:      *                          promote/demotes.
31473:      * TypeConsensus_Bad:       Types are not compatible. Individual type check results are undefined.
31473:      * TypeConsensus_Undemotes: Types would be compatible if slots were marked as undemotable
31473:      *                          before recording began. Caller can go through slot list and mark
31473:      *                          such slots as undemotable.
31473:      */
31473:     JS_REQUIRES_STACK TypeConsensus
36361:     checkTypes(LinkableFragment* f)
36361:     {
36361:         if (length() != f->typeMap.length())
31473:             return TypeConsensus_Bad;
31473: 
31473:         bool has_undemotes = false;
31473:         for (unsigned i = 0; i < length(); i++) {
36361:             TypeCheckResult result = checkType(i, f->typeMap[i]);
31473:             if (result == TypeCheck_Bad)
31473:                 return TypeConsensus_Bad;
31473:             if (result == TypeCheck_Undemote)
31473:                 has_undemotes = true;
31473:             slots[i].lastCheck = result;
31473:         }
31473:         if (has_undemotes)
31473:             return TypeConsensus_Undemotes;
31473:         return TypeConsensus_Okay;
31473:     }
31473: 
31473:     JS_REQUIRES_STACK JS_ALWAYS_INLINE void
48470:     addSlot(Value* vp)
31473:     {
59993:         bool isPromotedInt32 = false;
48470:         if (vp->isNumber()) {
38532:             if (LIns* i = mRecorder.getFromTracker(vp)) {
59993:                 isPromotedInt32 = IsPromotedInt32(i);
36401:             } else if (mRecorder.isGlobal(vp)) {
39928:                 int offset = mRecorder.tree->globalSlots->offsetOf(uint16(mRecorder.nativeGlobalSlot(vp)));
36401:                 JS_ASSERT(offset != -1);
59993:                 isPromotedInt32 = mRecorder.importTypeMap[mRecorder.importStackSlots + offset] ==
48470:                                   JSVAL_TYPE_INT32;
36401:             } else {
59993:                 isPromotedInt32 = mRecorder.importTypeMap[mRecorder.nativeStackSlot(vp)] ==
48470:                                   JSVAL_TYPE_INT32;
36401:             }
36401:         }
59993:         slots.add(SlotInfo(vp, isPromotedInt32));
33564:     }
33564: 
33564:     JS_REQUIRES_STACK JS_ALWAYS_INLINE void
48470:     addSlot(JSValueType t)
33564:     {
34332:         slots.add(SlotInfo(NULL, t));
34332:     }
34332: 
34332:     JS_REQUIRES_STACK JS_ALWAYS_INLINE void
48470:     addSlot(Value *vp, JSValueType t)
34332:     {
34332:         slots.add(SlotInfo(vp, t));
31473:     }
31473: 
30295:     JS_REQUIRES_STACK void
31473:     markUndemotes()
31473:     {
31473:         for (unsigned i = 0; i < length(); i++) {
31473:             if (get(i).lastCheck == TypeCheck_Undemote)
41802:                 mRecorder.markSlotUndemotable(mRecorder.tree, i);
31473:         }
31473:     }
31473: 
31473:     JS_REQUIRES_STACK virtual void
31473:     adjustTypes()
31473:     {
34319:         for (unsigned i = 0; i < length(); i++)
34319:             adjustType(get(i));
34319:     }
34319: 
34319:   protected:
34319:     JS_REQUIRES_STACK virtual void
34319:     adjustType(SlotInfo& info) {
31473:         JS_ASSERT(info.lastCheck != TypeCheck_Undemote && info.lastCheck != TypeCheck_Bad);
47493: #ifdef DEBUG
31473:         if (info.lastCheck == TypeCheck_Promote) {
48470:             JS_ASSERT(info.type == JSVAL_TYPE_INT32 || info.type == JSVAL_TYPE_DOUBLE);
47493:             /*
47493:              * This should only happen if the slot has a trivial conversion, i.e.
59993:              * IsPromotedInt32() is true.  We check this.  
47493:              *
47493:              * Note that getFromTracker() will return NULL if the slot was
47493:              * never used, in which case we don't do the check.  We could
47493:              * instead called mRecorder.get(info.vp) and always check, but
47493:              * get() has side-effects, which is not good in an assertion.
47493:              * Not checking unused slots isn't so bad.
47493:              */
48470:             LIns* ins = mRecorder.getFromTrackerImpl(info.vp);
59993:             JS_ASSERT_IF(ins, IsPromotedInt32(ins));
47493:         } else 
47493: #endif
47493:         if (info.lastCheck == TypeCheck_Demote) {
48470:             JS_ASSERT(info.type == JSVAL_TYPE_INT32 || info.type == JSVAL_TYPE_DOUBLE);
48470:             JS_ASSERT(mRecorder.getImpl(info.vp)->isD());
41265: 
41265:             /* Never demote this final i2d. */
48470:             mRecorder.setImpl(info.vp, mRecorder.getImpl(info.vp), false);
31473:         }
31473:     }
34319: 
31473:   private:
31473:     TypeCheckResult
48470:     checkType(unsigned i, JSValueType t)
31473:     {
31473:         debug_only_printf(LC_TMTracer,
59993:                           "checkType slot %d: interp=%c typemap=%c isNum=%d isPromotedInt32=%d\n",
31473:                           i,
48470:                           TypeToChar(slots[i].type),
48470:                           TypeToChar(t),
48470:                           slots[i].type == JSVAL_TYPE_INT32 || slots[i].type == JSVAL_TYPE_DOUBLE,
59993:                           slots[i].isPromotedInt32);
31473:         switch (t) {
48470:           case JSVAL_TYPE_INT32:
48470:             if (slots[i].type != JSVAL_TYPE_INT32 && slots[i].type != JSVAL_TYPE_DOUBLE)
31473:                 return TypeCheck_Bad; /* Not a number? Type mismatch. */
31473:             /* This is always a type mismatch, we can't close a double to an int. */
59993:             if (!slots[i].isPromotedInt32)
31473:                 return TypeCheck_Undemote;
31473:             /* Looks good, slot is an int32, the last instruction should be promotable. */
59993:             JS_ASSERT_IF(slots[i].vp,
59993:                          hasInt32Repr(*(const Value *)slots[i].vp) && slots[i].isPromotedInt32);
34332:             return slots[i].vp ? TypeCheck_Promote : TypeCheck_Okay;
48470:           case JSVAL_TYPE_DOUBLE:
48470:             if (slots[i].type != JSVAL_TYPE_INT32 && slots[i].type != JSVAL_TYPE_DOUBLE)
31473:                 return TypeCheck_Bad; /* Not a number? Type mismatch. */
59993:             if (slots[i].isPromotedInt32)
34332:                 return slots[i].vp ? TypeCheck_Demote : TypeCheck_Bad;
31473:             return TypeCheck_Okay;
31473:           default:
33564:             return slots[i].type == t ? TypeCheck_Okay : TypeCheck_Bad;
31473:         }
31473:         JS_NOT_REACHED("shouldn't fall through type check switch");
31473:     }
31473:   protected:
31473:     TraceRecorder& mRecorder;
31473:     JSContext* mCx;
31473:     Queue<SlotInfo> slots;
31473: };
31473: 
31473: class DefaultSlotMap : public SlotMap
31473: {
31473:   public:
33564:     DefaultSlotMap(TraceRecorder& tr) : SlotMap(tr)
31473:     {
31473:     }
31473:     
39928:     virtual ~DefaultSlotMap()
39928:     {
39928:     }
39928: 
31473:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
48470:     visitStackSlots(Value *vp, size_t count, JSStackFrame* fp)
31473:     {
31473:         for (size_t i = 0; i < count; i++)
31473:             addSlot(&vp[i]);
31473:         return true;
31473:     }
48470: 
48470:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
53840:     visitFrameObjPtr(void* p, JSStackFrame* fp)
53840:     {
53840:         addSlot(getFrameObjPtrTraceType(p, fp));
48470:         return true;
48470:     }
31473: };
31473: 
31473: JS_REQUIRES_STACK TypeConsensus
31473: TraceRecorder::selfTypeStability(SlotMap& slotMap)
31473: {
31473:     debug_only_printf(LC_TMTracer, "Checking type stability against self=%p\n", (void*)fragment);
36361:     TypeConsensus consensus = slotMap.checkTypes(tree);
31473: 
31473:     /* Best case: loop jumps back to its own header */
31473:     if (consensus == TypeConsensus_Okay)
31473:         return TypeConsensus_Okay;
31473: 
57803:     /*
57803:      * If the only thing keeping this loop from being stable is undemotions, then mark relevant
31473:      * slots as undemotable.
31473:      */
31473:     if (consensus == TypeConsensus_Undemotes)
31473:         slotMap.markUndemotes();
31473: 
31473:     return consensus;
31473: }
31473: 
31473: JS_REQUIRES_STACK TypeConsensus
35044: TraceRecorder::peerTypeStability(SlotMap& slotMap, const void* ip, TreeFragment** pPeer)
31473: {
57803:     JS_ASSERT(tree->first == LookupLoop(traceMonitor, ip, tree->globalObj, tree->globalShape, tree->argc));
57803: 
31473:     /* See if there are any peers that would make this stable */
31473:     bool onlyUndemotes = false;
57803:     for (TreeFragment *peer = tree->first; peer != NULL; peer = peer->peer) {
36361:         if (!peer->code() || peer == fragment)
31473:             continue;
31473:         debug_only_printf(LC_TMTracer, "Checking type stability against peer=%p\n", (void*)peer);
36361:         TypeConsensus consensus = slotMap.checkTypes(peer);
31473:         if (consensus == TypeConsensus_Okay) {
31473:             *pPeer = peer;
57803:             /*
57803:              * Return this even though there will be linkage; the trace itself is not stable.
31473:              * Caller should inspect ppeer to check for a compatible peer.
31473:              */
31473:             return TypeConsensus_Okay;
31473:         }
31473:         if (consensus == TypeConsensus_Undemotes)
31473:             onlyUndemotes = true;
31473:     }
31473: 
31473:     return onlyUndemotes ? TypeConsensus_Undemotes : TypeConsensus_Bad;
31473: }
31473: 
33542: /*
33542:  * Complete and compile a trace and link it to the existing tree if
33542:  * appropriate.  Returns ARECORD_ABORTED or ARECORD_STOP, depending on whether
33542:  * the recorder was deleted. Outparam is always set.
33542:  */
33542: JS_REQUIRES_STACK AbortableRecordingStatus
57803: TraceRecorder::closeLoop()
57803: {
57803:     VMSideExit *exit = snapshot(UNSTABLE_LOOP_EXIT);
57803: 
57803:     DefaultSlotMap slotMap(*this);
57803:     VisitSlots(slotMap, cx, 0, *tree->globalSlots);
57803: 
26557:     /*
30860:      * We should have arrived back at the loop header, and hence we don't want
32776:      * to be in an imacro here and the opcode should be either JSOP_TRACE or, in
56217:      * case this loop was blacklisted in the meantime, JSOP_NOTRACE.
56217:      */
57803:     JS_ASSERT(*cx->regs->pc == JSOP_TRACE || *cx->regs->pc == JSOP_NOTRACE);
57803:     JS_ASSERT(!cx->fp()->hasImacropc());
26557: 
21684:     if (callDepth != 0) {
29883:         debug_only_print0(LC_TMTracer,
29883:                           "Blacklisted: stack depth mismatch, possible recursion.\n");
40251:         Blacklist((jsbytecode*)tree->ip);
22609:         trashSelf = true;
33564:         return ARECORD_STOP;
33564:     }
33564: 
57803:     JS_ASSERT(exit->numStackSlots == tree->nStackTypes);
40226:     JS_ASSERT(fragment->root == tree);
57803:     JS_ASSERT(!trashSelf);
24491: 
35044:     TreeFragment* peer = NULL;
31473: 
57803:     TypeConsensus consensus = selfTypeStability(slotMap);
31473:     if (consensus != TypeConsensus_Okay) {
53524:         TypeConsensus peerConsensus = peerTypeStability(slotMap, tree->ip, &peer);
31473:         /* If there was a semblance of a stable peer (even if not linkable), keep the result. */
31473:         if (peerConsensus != TypeConsensus_Bad)
31473:             consensus = peerConsensus;
31473:     }
21433: 
21433: #if DEBUG
31473:     if (consensus != TypeConsensus_Okay || peer)
18606:         AUDIT(unstableLoopVariable);
21433: #endif
21433: 
34319:     /*
34319:      * This exit is indeed linkable to something now. Process any promote or
34319:      * demotes that are pending in the slot map.
31473:      */
31473:     if (consensus == TypeConsensus_Okay)
31473:         slotMap.adjustTypes();
31473: 
31473:     if (consensus != TypeConsensus_Okay || peer) {
56750:         fragment->lastIns = w.x(createGuardRecord(exit));
21433: 
31473:         /* If there is a peer, there must have been an "Okay" consensus. */
31473:         JS_ASSERT_IF(peer, consensus == TypeConsensus_Okay);
31473: 
31473:         /* Compile as a type-unstable loop, and hope for a connection later. */
21433:         if (!peer) {
21685:             /*
21685:              * If such a fragment does not exist, let's compile the loop ahead
21685:              * of time anyway.  Later, if the loop becomes type stable, we will
21685:              * connect these two fragments together.
21433:              */
29883:             debug_only_print0(LC_TMTracer,
29883:                               "Trace has unstable loop variable with no stable peer, "
29883:                               "compiling anyway.\n");
34351:             UnstableExit* uexit = new (traceAlloc()) UnstableExit;
21433:             uexit->fragment = fragment;
21433:             uexit->exit = exit;
36361:             uexit->next = tree->unstableExits;
36361:             tree->unstableExits = uexit;
21433:         } else {
21433:             JS_ASSERT(peer->code());
21433:             exit->target = peer;
29883:             debug_only_printf(LC_TMTracer,
29883:                               "Joining type-unstable trace to target fragment %p.\n",
29883:                               (void*)peer);
40251:             peer->dependentTrees.addUnique(tree);
36361:             tree->linkedTrees.addUnique(peer);
21433:         }
21433:     } else {
31903:         exit->exitType = LOOP_EXIT;
31937:         debug_only_printf(LC_TMTreeVis, "TREEVIS CHANGEEXIT EXIT=%p TYPE=%s\n", (void*)exit,
31903:                           getExitName(LOOP_EXIT));
32600: 
32600:         JS_ASSERT((fragment == fragment->root) == !!loopLabel);
32600:         if (loopLabel) {
56750:             w.j(loopLabel);
56750:             w.comment("end-loop");
56750:             w.livep(lirbuf->state);
48798:         }
48798: 
48798:         exit->target = tree;
48798:         /*
48798:          * This guard is dead code.  However, it must be present because it
48798:          * can keep alive values on the stack.  Without it, StackFilter can
48798:          * remove some stack stores that it shouldn't.  See bug 582766 comment
48798:          * 19.
48798:          */
56750:         fragment->lastIns = w.x(createGuardRecord(exit));
31473:     }
33542: 
35083:     CHECK_STATUS_A(compile());
31473: 
31937:     debug_only_printf(LC_TMTreeVis, "TREEVIS CLOSELOOP EXIT=%p PEER=%p\n", (void*)exit, (void*)peer);
31903: 
40226:     JS_ASSERT(LookupLoop(traceMonitor, tree->ip, tree->globalObj, tree->globalShape, tree->argc) ==
40226:               tree->first);
40226:     JS_ASSERT(tree->first);
40226: 
40226:     peer = tree->first;
31920:     joinEdgesToEntry(peer);
19588: 
31851:     debug_only_stmt(DumpPeerStability(traceMonitor, peer->ip, peer->globalObj,
31851:                                       peer->globalShape, peer->argc);)
31851: 
29883:     debug_only_print0(LC_TMTracer,
29883:                       "updating specializations on dependent and linked trees\n");
40226:     if (tree->code())
40251:         SpecializeTreesToMissingGlobals(cx, globalObj, tree);
25491: 
25627:     /*
25627:      * If this is a newly formed tree, and the outer tree has not been compiled yet, we
25627:      * should try to compile the outer tree again.
25627:      */
56217:     if (outerPC)
56217:         AttemptCompilation(cx, globalObj, outerScript, outerPC, outerArgc);
29883: #ifdef JS_JIT_SPEW
29883:     debug_only_printf(LC_TMMinimal,
32784:                       "Recording completed at  %s:%u@%u via closeLoop (FragID=%06u)\n",
53840:                       cx->fp()->script()->filename,
51446:                       js_FramePCToLineNumber(cx, cx->fp()),
51446:                       FramePCOffset(cx, cx->fp()),
32784:                       fragment->profFragID);
29883:     debug_only_print0(LC_TMMinimal, "\n");
29883: #endif
31473: 
35083:     return finishSuccessfully();
21433: }
21433: 
31495: static void
31495: FullMapFromExit(TypeMap& typeMap, VMSideExit* exit)
31495: {
31495:     typeMap.setLength(0);
31495:     typeMap.fromRaw(exit->stackTypeMap(), exit->numStackSlots);
31495:     typeMap.fromRaw(exit->globalTypeMap(), exit->numGlobalSlots);
31495:     /* Include globals that were later specialized at the root of the tree. */
36361:     if (exit->numGlobalSlots < exit->root()->nGlobalTypes()) {
36361:         typeMap.fromRaw(exit->root()->globalTypeMap() + exit->numGlobalSlots,
36361:                         exit->root()->nGlobalTypes() - exit->numGlobalSlots);
31495:     }
31495: }
31495: 
31496: static JS_REQUIRES_STACK TypeConsensus
35044: TypeMapLinkability(JSContext* cx, const TypeMap& typeMap, TreeFragment* peer)
35044: {
36361:     const TypeMap& peerMap = peer->typeMap;
31495:     unsigned minSlots = JS_MIN(typeMap.length(), peerMap.length());
31495:     TypeConsensus consensus = TypeConsensus_Okay;
31495:     for (unsigned i = 0; i < minSlots; i++) {
31495:         if (typeMap[i] == peerMap[i])
31495:             continue;
48470:         if (typeMap[i] == JSVAL_TYPE_INT32 && peerMap[i] == JSVAL_TYPE_DOUBLE &&
41802:             IsSlotUndemotable(JS_TRACE_MONITOR(cx).oracle, cx, peer, i, peer->ip)) {
31495:             consensus = TypeConsensus_Undemotes;
31495:         } else {
31495:             return TypeConsensus_Bad;
31495:         }
31495:     }
31495:     return consensus;
31495: }
31495: 
41802: JS_REQUIRES_STACK unsigned
41802: TraceRecorder::findUndemotesInTypemaps(const TypeMap& typeMap, LinkableFragment* f,
31851:                         Queue<unsigned>& undemotes)
31851: {
31851:     undemotes.setLength(0);
36361:     unsigned minSlots = JS_MIN(typeMap.length(), f->typeMap.length());
31851:     for (unsigned i = 0; i < minSlots; i++) {
48470:         if (typeMap[i] == JSVAL_TYPE_INT32 && f->typeMap[i] == JSVAL_TYPE_DOUBLE) {
31851:             undemotes.add(i);
36361:         } else if (typeMap[i] != f->typeMap[i]) {
31851:             return 0;
31851:         }
31851:     }
31851:     for (unsigned i = 0; i < undemotes.length(); i++)
41802:         markSlotUndemotable(f, undemotes[i]);
31851:     return undemotes.length();
31851: }
31851: 
22652: JS_REQUIRES_STACK void
35044: TraceRecorder::joinEdgesToEntry(TreeFragment* peer_root)
21433: {
32604:     if (fragment->root != fragment)
31851:         return;
31851: 
31920:     TypeMap typeMap(NULL);
31920:     Queue<unsigned> undemotes(NULL);
31851: 
35044:     for (TreeFragment* peer = peer_root; peer; peer = peer->peer) {
36361:         if (!peer->code())
21433:             continue;
36361:         UnstableExit* uexit = peer->unstableExits;
21433:         while (uexit != NULL) {
31851:             /* Build the full typemap for this unstable exit */
31851:             FullMapFromExit(typeMap, uexit->exit);
31851:             /* Check its compatibility against this tree */
40251:             TypeConsensus consensus = TypeMapLinkability(cx, typeMap, tree);
31851:             JS_ASSERT_IF(consensus == TypeConsensus_Okay, peer != fragment);
31851:             if (consensus == TypeConsensus_Okay) {
29883:                 debug_only_printf(LC_TMTracer,
29883:                                   "Joining type-stable trace to target exit %p->%p.\n",
29883:                                   (void*)uexit->fragment, (void*)uexit->exit);
36397: 
36397:                 /*
36397:                  * See bug 531513. Before linking these trees, make sure the
36397:                  * peer's dependency graph is up to date.
36397:                  */
36397:                 TreeFragment* from = uexit->exit->root();
36397:                 if (from->nGlobalTypes() < tree->nGlobalTypes()) {
36397:                     SpecializeTreesToLateGlobals(cx, from, tree->globalTypeMap(),
36397:                                                  tree->nGlobalTypes());
36397:                 }
36397: 
31851:                 /* It's okay! Link together and remove the unstable exit. */
36397:                 JS_ASSERT(tree == fragment);
36397:                 JoinPeers(traceMonitor->assembler, uexit->exit, tree);
36361:                 uexit = peer->removeUnstableExit(uexit->exit);
31851:             } else {
31851:                 /* Check for int32->double slots that suggest trashing. */
41802:                 if (findUndemotesInTypemaps(typeMap, tree, undemotes)) {
22609:                     JS_ASSERT(peer == uexit->fragment->root);
22609:                     if (fragment == peer)
22609:                         trashSelf = true;
22609:                     else
22609:                         whichTreesToTrash.addUnique(uexit->fragment->root);
57804:                     break;
31851:                 }
21433:                 uexit = uexit->next;
21433:             }
21433:         }
21433:     }
21433: }
21433: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
31473: TraceRecorder::endLoop()
31473: {
33542:     return endLoop(snapshot(LOOP_EXIT));
31473: }
31473: 
18606: /* Emit an always-exit guard and compile the tree (used for break statements. */
33542: JS_REQUIRES_STACK AbortableRecordingStatus
31473: TraceRecorder::endLoop(VMSideExit* exit)
18606: {
40226:     JS_ASSERT(fragment->root == tree);
40226: 
21684:     if (callDepth != 0) {
29883:         debug_only_print0(LC_TMTracer, "Blacklisted: stack depth mismatch, possible recursion.\n");
40226:         Blacklist((jsbytecode*)tree->ip);
22609:         trashSelf = true;
33542:         return ARECORD_STOP;
21684:     }
21684: 
56750:     fragment->lastIns = w.x(createGuardRecord(exit));
33542: 
35083:     CHECK_STATUS_A(compile());
21483: 
31937:     debug_only_printf(LC_TMTreeVis, "TREEVIS ENDLOOP EXIT=%p\n", (void*)exit);
31903: 
40226:     JS_ASSERT(LookupLoop(traceMonitor, tree->ip, tree->globalObj, tree->globalShape, tree->argc) ==
40226:               tree->first);
40226: 
40226:     joinEdgesToEntry(tree->first);
40226: 
40226:     debug_only_stmt(DumpPeerStability(traceMonitor, tree->ip, tree->globalObj,
40226:                                       tree->globalShape, tree->argc);)
21433: 
30860:     /*
30860:      * Note: this must always be done, in case we added new globals on trace
30860:      * and haven't yet propagated those to linked and dependent trees.
30860:      */
29883:     debug_only_print0(LC_TMTracer,
29883:                       "updating specializations on dependent and linked trees\n");
40226:     if (tree->code())
36361:         SpecializeTreesToMissingGlobals(cx, globalObj, fragment->root);
30860: 
30860:     /*
30860:      * If this is a newly formed tree, and the outer tree has not been compiled
30860:      * yet, we should try to compile the outer tree again.
25627:      */
56217:     if (outerPC)
56217:         AttemptCompilation(cx, globalObj, outerScript, outerPC, outerArgc);
29883: #ifdef JS_JIT_SPEW
29883:     debug_only_printf(LC_TMMinimal,
32784:                       "Recording completed at  %s:%u@%u via endLoop (FragID=%06u)\n",
53840:                       cx->fp()->script()->filename,
51446:                       js_FramePCToLineNumber(cx, cx->fp()),
51446:                       FramePCOffset(cx, cx->fp()),
32784:                       fragment->profFragID);
29883:     debug_only_print0(LC_TMTracer, "\n");
29883: #endif
33542: 
35083:     return finishSuccessfully();
17334: }
17334: 
18241: /* Emit code to adjust the stack to match the inner tree's stack expectations. */
22652: JS_REQUIRES_STACK void
39937: TraceRecorder::prepareTreeCall(TreeFragment* inner)
39937: {
31943:     VMSideExit* exit = snapshot(OOM_EXIT);
30860: 
30860:     /*
30860:      * The inner tree expects to be called from the current frame. If the outer
30860:      * tree (this trace) is currently inside a function inlining code
30860:      * (calldepth > 0), we have to advance the native stack pointer such that
30860:      * we match what the inner trace expects to see. We move it back when we
30860:      * come out of the inner tree call.
30860:      */
18007:     if (callDepth > 0) {
30860:         /*
30860:          * Calculate the amount we have to lift the native stack pointer by to
30860:          * compensate for any outer frames that the inner tree doesn't expect
30860:          * but the outer tree has.
30860:          */
53840:         ptrdiff_t sp_adj = nativeStackOffset(&cx->fp()->calleeValue());
30860: 
30860:         /* Calculate the amount we have to lift the call stack by. */
23262:         ptrdiff_t rp_adj = callDepth * sizeof(FrameInfo*);
30860: 
30860:         /*
30860:          * Guard that we have enough stack space for the tree we are trying to
30860:          * call on top of the new value for sp.
30860:          */
29883:         debug_only_printf(LC_TMTracer,
33132:                           "sp_adj=%lld outer=%lld inner=%lld\n",
33132:                           (long long int)sp_adj,
36361:                           (long long int)tree->nativeStackBase,
36361:                           (long long int)inner->nativeStackBase);
32746:         ptrdiff_t sp_offset =
36361:                 - tree->nativeStackBase /* rebase sp to beginning of outer tree's stack */
18184:                 + sp_adj /* adjust for stack in outer frame inner tree can't see */
36361:                 + inner->maxNativeStackSlots * sizeof(double); /* plus the inner tree's stack */
56750:         LIns* sp_top = w.addp(lirbuf->sp, w.nameImmw(sp_offset));
56750:         guard(true, w.ltp(sp_top, eos_ins), exit);
30860: 
18133:         /* Guard that we have enough call stack space. */
36361:         ptrdiff_t rp_offset = rp_adj + inner->maxCallDepth * sizeof(FrameInfo*);
56750:         LIns* rp_top = w.addp(lirbuf->rp, w.nameImmw(rp_offset));
56750:         guard(true, w.ltp(rp_top, eor_ins), exit);
32746: 
32746:         sp_offset =
36361:                 - tree->nativeStackBase /* rebase sp to beginning of outer tree's stack */
18184:                 + sp_adj /* adjust for stack in outer frame inner tree can't see */
36361:                 + inner->nativeStackBase; /* plus the inner tree's stack base */
32746:         /* We have enough space, so adjust sp and rp to their new level. */
56750:         w.stStateField(w.addp(lirbuf->sp, w.nameImmw(sp_offset)), sp);
56750:         w.stStateField(w.addp(lirbuf->rp, w.nameImmw(rp_adj)), rp);
18007:     }
31943: 
31943:     /*
31943:      * The inner tree will probably access stack slots. So tell nanojit not to
35344:      * discard or defer stack writes before emitting the call tree code.
31943:      *
31943:      * (The ExitType of this snapshot is nugatory. The exit can't be taken.)
31943:      */
56750:     w.xbarrier(createGuardRecord(exit));
18241: }
18241: 
60244: class ClearSlotsVisitor : public SlotVisitorBase
60244: {
60244:     Tracker &tracker;
60244:   public:
60244:     ClearSlotsVisitor(Tracker &tracker)
60244:       : tracker(tracker)
60244:     {}
60244: 
60244:     JS_ALWAYS_INLINE bool
60244:     visitStackSlots(Value *vp, size_t count, JSStackFrame *) {
60244:         for (Value *vpend = vp + count; vp != vpend; ++vp)
60244:             tracker.set(vp, NULL);
60244:         return true;
60244:     }
60244: 
60244:     JS_ALWAYS_INLINE bool
60244:     visitFrameObjPtr(void *p, JSStackFrame *) {
60244:         tracker.set(p, NULL);
60244:         return true;
60244:     }
60244: };
60244: 
31913: static unsigned
48470: BuildGlobalTypeMapFromInnerTree(Queue<JSValueType>& typeMap, VMSideExit* inner)
31913: {
31913: #if defined DEBUG
31913:     unsigned initialSlots = typeMap.length();
31913: #endif
31913:     /* First, use the innermost exit's global typemap. */
31913:     typeMap.add(inner->globalTypeMap(), inner->numGlobalSlots);
31913: 
31913:     /* Add missing global types from the innermost exit's tree. */
36361:     TreeFragment* innerFrag = inner->root();
31913:     unsigned slots = inner->numGlobalSlots;
36361:     if (slots < innerFrag->nGlobalTypes()) {
36361:         typeMap.add(innerFrag->globalTypeMap() + slots, innerFrag->nGlobalTypes() - slots);
36361:         slots = innerFrag->nGlobalTypes();
31913:     }
31913:     JS_ASSERT(typeMap.length() - initialSlots == slots);
31913:     return slots;
31913: }
31913: 
18241: /* Record a call to an inner tree. */
22652: JS_REQUIRES_STACK void
39937: TraceRecorder::emitTreeCall(TreeFragment* inner, VMSideExit* exit)
35044: {
18007:     /* Invoke the inner tree. */
35344:     LIns* args[] = { lirbuf->state }; /* reverse order */
35344:     /* Construct a call info structure for the target tree. */
35344:     CallInfo* ci = new (traceAlloc()) CallInfo();
35344:     ci->_address = uintptr_t(inner->code());
35344:     JS_ASSERT(ci->_address);
47594:     ci->_typesig = CallInfo::typeSig1(ARGTYPE_P, ARGTYPE_P);
39910:     ci->_isPure = 0;
48613:     ci->_storeAccSet = ACCSET_STORE_ANY;
35344:     ci->_abi = ABI_FASTCALL;
35344: #ifdef DEBUG
35344:     ci->_name = "fragment";
35344: #endif
56750:     LIns* rec = w.call(ci, args);
56750:     LIns* lr = w.ldpGuardRecordExit(rec);
56750:     LIns* nested = w.jtUnoptimizable(w.eqiN(w.ldiVMSideExitField(lr, exitType), NESTED_EXIT));
56750: 
35344:     /*
35344:      * If the tree exits on a regular (non-nested) guard, keep updating lastTreeExitGuard
35344:      * with that guard. If we mismatch on a tree call guard, this will contain the last
35344:      * non-nested guard we encountered, which is the innermost loop or branch guard.
35344:      */
56750:     w.stStateField(lr, lastTreeExitGuard);
56750:     LIns* done1 = w.j(NULL);
35344: 
35344:     /*
35344:      * The tree exited on a nested guard. This only occurs once a tree call guard mismatches
35344:      * and we unwind the tree call stack. We store the first (innermost) tree call guard in state
35344:      * and we will try to grow the outer tree the failing call was in starting at that guard.
35344:      */
56750:     w.label(nested);
56750:     LIns* done2 = w.jfUnoptimizable(w.eqp0(w.ldpStateField(lastTreeCallGuard)));
56750:     w.stStateField(lr, lastTreeCallGuard);
56750:     w.stStateField(w.addp(w.ldpStateField(rp),
56750:                           w.i2p(w.lshiN(w.ldiVMSideExitField(lr, calldepth),
35345:                                         sizeof(void*) == 4 ? 2 : 3))),
56180:                    rpAtLastTreeCall);
56750:     w.label(done1, done2);
35344: 
35344:     /*
41276:      * Keep updating outermostTreeExit so that TracerState always contains the most recent
35344:      * side exit.
35344:      */
56750:     w.stStateField(lr, outermostTreeExitGuard);
26972: 
18116:     /* Read back all registers, in case the called tree changed any of them. */
29896: #ifdef DEBUG
48470:     JSValueType* map;
29896:     size_t i;
31495:     map = exit->globalTypeMap();
29896:     for (i = 0; i < exit->numGlobalSlots; i++)
48470:         JS_ASSERT(map[i] != JSVAL_TYPE_BOXED);
31495:     map = exit->stackTypeMap();
29896:     for (i = 0; i < exit->numStackSlots; i++)
48470:         JS_ASSERT(map[i] != JSVAL_TYPE_BOXED);
29896: #endif
35344: 
60244:     /* The inner tree may modify currently-tracked upvars, so flush everything. */
60244:     ClearSlotsVisitor visitor(tracker);
60244:     VisitStackSlots(visitor, cx, callDepth);
36401:     SlotList& gslots = *tree->globalSlots;
36401:     for (unsigned i = 0; i < gslots.length(); i++) {
36401:         unsigned slot = gslots[i];
48470:         Value* vp = &globalObj->getSlotRef(slot);
36401:         tracker.set(vp, NULL);
36401:     }
36401: 
37009:     /* Set stack slots from the innermost frame. */
37009:     importTypeMap.setLength(NativeStackSlots(cx, callDepth));
37009:     unsigned startOfInnerFrame = importTypeMap.length() - exit->numStackSlots;
36401:     for (unsigned i = 0; i < exit->numStackSlots; i++)
36401:         importTypeMap[startOfInnerFrame + i] = exit->stackTypeMap()[i];
37009:     importStackSlots = importTypeMap.length();
37009:     JS_ASSERT(importStackSlots == NativeStackSlots(cx, callDepth));
36401: 
36401:     /*
30860:      * Bug 502604 - It is illegal to extend from the outer typemap without
30860:      * first extending from the inner. Make a new typemap here.
30246:      */
36401:     BuildGlobalTypeMapFromInnerTree(importTypeMap, exit);
36401: 
36401:     importGlobalSlots = importTypeMap.length() - importStackSlots;
36401:     JS_ASSERT(importGlobalSlots == tree->globalSlots->length());
26972: 
18159:     /* Restore sp and rp to their original values (we still have them in a register). */
18159:     if (callDepth > 0) {
56750:         w.stStateField(lirbuf->sp, sp);
56750:         w.stStateField(lirbuf->rp, rp);
18159:     }
26972: 
26972:     /*
26972:      * Guard that we come out of the inner tree along the same side exit we came out when
26972:      * we called the inner tree at recording time.
26972:      */
35344:     VMSideExit* nestedExit = snapshot(NESTED_EXIT);
35083:     JS_ASSERT(exit->exitType == LOOP_EXIT);
56750:     guard(true, w.eqp(lr, w.nameImmpNonGC(exit)), nestedExit);
31937:     debug_only_printf(LC_TMTreeVis, "TREEVIS TREECALL INNER=%p EXIT=%p GUARD=%p\n", (void*)inner,
35344:                       (void*)nestedExit, (void*)exit);
30860: 
18650:     /* Register us as a dependent tree of the inner tree. */
36361:     inner->dependentTrees.addUnique(fragment->root);
36361:     tree->linkedTrees.addUnique(inner);
18334: }
18334: 
18694: /* Add a if/if-else control-flow merge point to the list of known merge points. */
22652: JS_REQUIRES_STACK void
18694: TraceRecorder::trackCfgMerges(jsbytecode* pc)
18694: {
18694:     /* If we hit the beginning of an if/if-else, then keep track of the merge point after it. */
18694:     JS_ASSERT((*pc == JSOP_IFEQ) || (*pc == JSOP_IFEQX));
53840:     jssrcnote* sn = js_GetSrcNote(cx->fp()->script(), pc);
18694:     if (sn != NULL) {
18694:         if (SN_TYPE(sn) == SRC_IF) {
18694:             cfgMerges.add((*pc == JSOP_IFEQ)
18694:                           ? pc + GET_JUMP_OFFSET(pc)
18694:                           : pc + GET_JUMPX_OFFSET(pc));
18694:         } else if (SN_TYPE(sn) == SRC_IF_ELSE)
18694:             cfgMerges.add(pc + js_GetSrcNoteOffset(sn, 0));
18694:     }
18694: }
18694: 
30860: /*
30860:  * Invert the direction of the guard if this is a loop edge that is not
30860:  * taken (thin loop).
30860:  */
22652: JS_REQUIRES_STACK void
26557: TraceRecorder::emitIf(jsbytecode* pc, bool cond, LIns* x)
26557: {
26557:     ExitType exitType;
57700:     JS_ASSERT(isCond(x));
40251:     if (IsLoopEdge(pc, (jsbytecode*)tree->ip)) {
26557:         exitType = LOOP_EXIT;
26557: 
26557:         /*
30860:          * If we are about to walk out of the loop, generate code for the
30860:          * inverse loop condition, pretending we recorded the case that stays
30860:          * on trace.
26557:          */
26557:         if ((*pc == JSOP_IFEQ || *pc == JSOP_IFEQX) == cond) {
26557:             JS_ASSERT(*pc == JSOP_IFNE || *pc == JSOP_IFNEX || *pc == JSOP_IFEQ || *pc == JSOP_IFEQX);
29883:             debug_only_print0(LC_TMTracer,
29883:                               "Walking out of the loop, terminating it anyway.\n");
20416:             cond = !cond;
26557:         }
26557: 
26557:         /*
30860:          * Conditional guards do not have to be emitted if the condition is
30860:          * constant. We make a note whether the loop condition is true or false
30860:          * here, so we later know whether to emit a loop edge or a loop end.
21433:          */
41265:         if (x->isImmI()) {
41265:             pendingLoop = (x->immI() == int32(cond));
26557:             return;
26557:         }
26557:     } else {
26557:         exitType = BRANCH_EXIT;
26557:     }
57700:     if (!x->isImmI())
26557:         guard(cond, x, exitType);
20416: }
20416: 
18694: /* Emit code for a fused IFEQ/IFNE. */
22652: JS_REQUIRES_STACK void
18694: TraceRecorder::fuseIf(jsbytecode* pc, bool cond, LIns* x)
18694: {
26557:     if (*pc == JSOP_IFEQ || *pc == JSOP_IFNE) {
26557:         emitIf(pc, cond, x);
26557:         if (*pc == JSOP_IFEQ)
18694:             trackCfgMerges(pc);
26557:     }
26557: }
26557: 
26557: /* Check whether we have reached the end of the trace. */
33542: JS_REQUIRES_STACK AbortableRecordingStatus
26557: TraceRecorder::checkTraceEnd(jsbytecode *pc)
26557: {
40251:     if (IsLoopEdge(pc, (jsbytecode*)tree->ip)) {
30860:         /*
30860:          * If we compile a loop, the trace should have a zero stack balance at
30860:          * the loop edge. Currently we are parked on a comparison op or
30860:          * IFNE/IFEQ, so advance pc to the loop header and adjust the stack
30860:          * pointer and pretend we have reached the loop header.
26557:          */
34351:         if (pendingLoop) {
53840:             JS_ASSERT(!cx->fp()->hasImacropc() && (pc == cx->regs->pc || pc == cx->regs->pc + 1));
42717:             JSFrameRegs orig = *cx->regs;
42717: 
42717:             cx->regs->pc = (jsbytecode*)tree->ip;
51446:             cx->regs->sp = cx->fp()->base() + tree->spOffsetAtEntry;
26557: 
33755:             JSContext* localcx = cx;
33564:             AbortableRecordingStatus ars = closeLoop();
42717:             *localcx->regs = orig;
33542:             return ars;
42641:         }
42641: 
33542:         return endLoop();
33542:     }
42641:     return ARECORD_CONTINUE;
21685: }
21685: 
33748: /*
33748:  * Check whether the shape of the global object has changed. The return value
35048:  * indicates whether the recorder is still active.  If 'false', any active
35048:  * recording has been aborted and the JIT may have been reset.
33748:  */
27493: static JS_REQUIRES_STACK bool
37741: CheckGlobalObjectShape(JSContext* cx, TraceMonitor* tm, JSObject* globalObj,
24491:                        uint32 *shape = NULL, SlotList** slots = NULL)
24491: {
27493:     if (tm->needFlush) {
32748:         ResetJIT(cx, FR_DEEP_BAIL);
27493:         return false;
27493:     }
24491: 
40410:     if (globalObj->numSlots() > MAX_GLOBAL_SLOTS) {
35048:         if (tm->recorder)
37741:             AbortRecording(cx, "too many slots in global object");
33748:         return false;
33748:     }
27889: 
40871:     /*
40871:      * The global object must have a unique shape. That way, if an operand
40871:      * isn't the global at record time, a shape guard suffices to ensure
40871:      * that it isn't the global at run time.
40871:      */
52503:     if (!globalObj->hasOwnShape()) {
56567:         if (!globalObj->globalObjectOwnShapeChange(cx)) {
40871:             debug_only_print0(LC_TMTracer,
40871:                               "Can't record: failed to give globalObj a unique shape.\n");
40871:             return false;
40871:         }
40871:     }
40871: 
40847:     uint32 globalShape = globalObj->shape();
24491: 
24491:     if (tm->recorder) {
35044:         TreeFragment* root = tm->recorder->getFragment()->root;
30860: 
24491:         /* Check the global shape matches the recorder's treeinfo's shape. */
26819:         if (globalObj != root->globalObj || globalShape != root->globalShape) {
21514:             AUDIT(globalShapeMismatchAtEntry);
29883:             debug_only_printf(LC_TMTracer,
29883:                               "Global object/shape mismatch (%p/%u vs. %p/%u), flushing cache.\n",
26823:                               (void*)globalObj, globalShape, (void*)root->globalObj,
29883:                               root->globalShape);
30860:             Backoff(cx, (jsbytecode*) root->ip);
32748:             ResetJIT(cx, FR_GLOBAL_SHAPE_MISMATCH);
24491:             return false;
24491:         }
24491:         if (shape)
24491:             *shape = globalShape;
24491:         if (slots)
36361:             *slots = root->globalSlots;
24491:         return true;
24491:     }
24491: 
24491:     /* No recorder, search for a tracked global-state (or allocate one). */
24491:     for (size_t i = 0; i < MONITOR_N_GLOBAL_STATES; ++i) {
24491:         GlobalState &state = tm->globalStates[i];
24491: 
27012:         if (state.globalShape == uint32(-1)) {
26819:             state.globalObj = globalObj;
24491:             state.globalShape = globalShape;
24491:             JS_ASSERT(state.globalSlots);
24491:             JS_ASSERT(state.globalSlots->length() == 0);
24491:         }
24491: 
26819:         if (state.globalObj == globalObj && state.globalShape == globalShape) {
24491:             if (shape)
24491:                 *shape = globalShape;
24491:             if (slots)
24491:                 *slots = state.globalSlots;
24491:             return true;
24491:         }
24491:     }
24491: 
24491:     /* No currently-tracked-global found and no room to allocate, abort. */
24491:     AUDIT(globalShapeMismatchAtEntry);
29883:     debug_only_printf(LC_TMTracer,
29883:                       "No global slotlist for global shape %u, flushing cache.\n",
29883:                       globalShape);
32748:     ResetJIT(cx, FR_GLOBALS_FULL);
24491:     return false;
21514: }
21514: 
35083: /*
35083:  * Return whether or not the recorder could be started. If 'false', the JIT has
35083:  * been reset in response to an OOM.
35083:  */
35083: bool JS_REQUIRES_STACK
35083: TraceRecorder::startRecorder(JSContext* cx, VMSideExit* anchor, VMFragment* f,
36361:                              unsigned stackSlots, unsigned ngslots,
48470:                              JSValueType* typeMap, VMSideExit* expectedInnerExit,
56217:                              JSScript* outerScript, jsbytecode* outerPC, uint32 outerArgc,
56217:                              bool speculate)
17731: {
37741:     TraceMonitor *tm = &JS_TRACE_MONITOR(cx);
60157:     JS_ASSERT(!tm->profile);
35083:     JS_ASSERT(!tm->needFlush);
53840:     JS_ASSERT_IF(cx->fp()->hasImacropc(), f->root != f);
35083: 
60811:     /* We can't (easily) use js_new() here because the constructor is private. */
60811:     void *memory = js_malloc(sizeof(TraceRecorder));
60811:     tm->recorder = memory
60811:                  ? new(memory) TraceRecorder(cx, anchor, f, stackSlots, ngslots, typeMap,
56217:                                              expectedInnerExit, outerScript, outerPC, outerArgc,
60811:                                              speculate)
60811:                  : NULL;
35083: 
59733:     if (!tm->recorder || tm->outOfMemory() || OverfullJITCache(cx, tm)) {
35083:         ResetJIT(cx, FR_OOM);
35083:         return false;
35083:     }
35083: 
17731:     return true;
17731: }
17731: 
17853: static void
54718: TrashTree(TreeFragment* f)
35044: {
18650:     JS_ASSERT(f == f->root);
31937:     debug_only_printf(LC_TMTreeVis, "TREEVIS TRASH FRAG=%p\n", (void*)f);
32784: 
18211:     if (!f->code())
18211:         return;
17889:     AUDIT(treesTrashed);
29883:     debug_only_print0(LC_TMTracer, "Trashing tree info.\n");
31920:     f->setCode(NULL);
36361:     TreeFragment** data = f->dependentTrees.data();
36361:     unsigned length = f->dependentTrees.length();
18650:     for (unsigned n = 0; n < length; ++n)
54718:         TrashTree(data[n]);
36361:     data = f->linkedTrees.data();
36361:     length = f->linkedTrees.length();
30458:     for (unsigned n = 0; n < length; ++n)
54718:         TrashTree(data[n]);
17853: }
17853: 
53840: static void
31924: SynthesizeFrame(JSContext* cx, const FrameInfo& fi, JSObject* callee)
17923: {
22652:     VOUCH_DOES_NOT_REQUIRE_STACK();
22652: 
51446:     /* Assert that we have a correct sp distance from cx->fp()->slots in fi. */
51446:     JSStackFrame* const fp = cx->fp();
25111:     JS_ASSERT_IF(!fi.imacpc,
53840:                  js_ReconstructStackDepth(cx, fp->script(), fi.pc) ==
53840:                  uintN(fi.spdist - fp->numFixed()));
53840: 
53840:     /* Use the just-flushed prev-frame to get the callee function. */
53840:     JSFunction* newfun = callee->getFunctionPrivate();
53840:     JSScript* newscript = newfun->script();
53840: 
53840:     /* Fill in the prev-frame's sp. */
53840:     JSFrameRegs *regs = cx->regs;
53840:     regs->sp = fp->slots() + fi.spdist;
53840:     regs->pc = fi.pc;
53840:     if (fi.imacpc)
53840:         fp->setImacropc(fi.imacpc);
53840: 
53840:     /* Set argc/flags then mimic JSOP_CALL. */
28949:     uintN argc = fi.get_argc();
53840:     uint32 flags = fi.is_constructing ()
53840:                    ? JSFRAME_CONSTRUCTING | JSFRAME_CONSTRUCTING
53840:                    : 0;
53840: 
53840:     /* Get pointer to new/frame/slots, prepare arguments. */
42714:     StackSpace &stack = cx->stack();
53840:     JSStackFrame *newfp = stack.getInlineFrame(cx, regs->sp, argc, newfun,
53840:                                                newscript, &flags);
53840: 
53840:     /* Initialize frame; do not need to initialize locals. */
53840:     newfp->initCallFrame(cx, *callee, newfun, argc, flags);
53840: 
19591: #ifdef DEBUG
53840:     /* The stack is conservatively marked, so we can leave non-canonical args uninitialized. */
53840:     if (newfp->hasOverflowArgs()) {
53840:         Value *beg = newfp->actualArgs() - 2;
53840:         Value *end = newfp->actualArgs() + newfp->numFormalArgs();
53840:         for (Value *p = beg; p != end; ++p)
53840:             p->setMagic(JS_ARG_POISON);
53840:     }
53840: 
53840:     /* These should be initialized by FlushNativeStackFrame. */
53840:     newfp->thisValue().setMagic(JS_THIS_POISON);
53840:     newfp->setScopeChainNoCallObj(*JSStackFrame::sInvalidScopeChain);
53840: #endif
17923: 
53840:     /* Officially push the frame. */
53840:     stack.pushInlineFrame(cx, newscript, newfp, cx->regs);
53840: 
53840:     /* Call object will be set by FlushNativeStackFrame. */
53840: 
53840:     /* Call the debugger hook if present. */
21141:     JSInterpreterHook hook = cx->debugHooks->callHook;
21141:     if (hook) {
53840:         newfp->setHookData(hook(cx, newfp, JS_TRUE, 0,
53840:                                 cx->debugHooks->callHookData));
53840:     }
42714: }
42714: 
35083: static JS_REQUIRES_STACK bool
56217: RecordTree(JSContext* cx, TreeFragment* first, JSScript* outerScript, jsbytecode* outerPC,
53524:            uint32 outerArgc, SlotList* globalSlots)
40226: {
40226:     TraceMonitor* tm = &JS_TRACE_MONITOR(cx);
40226: 
35083:     /* Try to find an unused peer fragment, or allocate a new one. */
46181:     JS_ASSERT(first->first == first);
46181:     TreeFragment* f = NULL;
46181:     size_t count = 0;
46181:     for (TreeFragment* peer = first; peer; peer = peer->peer, ++count) {
46181:         if (!peer->code())
46181:             f = peer;
46181:     }
46181:     if (!f)
46181:         f = AddNewPeerToPeerList(tm, first);
35083:     JS_ASSERT(f->root == f);
35083: 
46181:     /* Disable speculation if we are starting to accumulate a lot of trees. */
46181:     bool speculate = count < MAXPEERS-1;
46181: 
35083:     /* save a local copy for use after JIT flush */
35083:     const void* localRootIP = f->root->ip;
35083: 
35083:     /* Make sure the global type map didn't change on us. */
40226:     if (!CheckGlobalObjectShape(cx, tm, f->globalObj)) {
35083:         Backoff(cx, (jsbytecode*) localRootIP);
35083:         return false;
35083:     }
35083: 
35083:     AUDIT(recorderStarted);
35083: 
58041:     if (tm->outOfMemory() ||
59733:         OverfullJITCache(cx, tm) ||
61053:         !tm->tracedScripts.put(cx->fp()->script()))
61053:     {
59733:         if (!OverfullJITCache(cx, tm))
58041:             js_ReportOutOfMemory(cx);
35083:         Backoff(cx, (jsbytecode*) f->root->ip);
35083:         ResetJIT(cx, FR_OOM);
35083:         debug_only_print0(LC_TMTracer,
35083:                           "Out of memory recording new tree, flushing cache.\n");
35083:         return false;
35083:     }
35083: 
36361:     JS_ASSERT(!f->code());
36361: 
46181:     f->initialize(cx, globalSlots, speculate);
36361: 
36361: #ifdef DEBUG
36361:     AssertTreeIsUnique(tm, f);
36361: #endif
36361: #ifdef JS_JIT_SPEW
36361:     debug_only_printf(LC_TMTreeVis, "TREEVIS CREATETREE ROOT=%p PC=%p FILE=\"%s\" LINE=%d OFFS=%d",
36361:                       (void*)f, f->ip, f->treeFileName, f->treeLineNumber,
51446:                       FramePCOffset(cx, cx->fp()));
36361:     debug_only_print0(LC_TMTreeVis, " STACK=\"");
36361:     for (unsigned i = 0; i < f->nStackTypes; i++)
48470:         debug_only_printf(LC_TMTreeVis, "%c", TypeToChar(f->typeMap[i]));
36361:     debug_only_print0(LC_TMTreeVis, "\" GLOBALS=\"");
36361:     for (unsigned i = 0; i < f->nGlobalTypes(); i++)
48470:         debug_only_printf(LC_TMTreeVis, "%c", TypeToChar(f->typeMap[f->nStackTypes + i]));
36361:     debug_only_print0(LC_TMTreeVis, "\"\n");
36361: #endif
35083: 
30860:     /* Recording primary trace. */
36361:     return TraceRecorder::startRecorder(cx, NULL, f, f->nStackTypes,
36361:                                         f->globalSlots->length(),
36361:                                         f->typeMap.data(), NULL,
56217:                                         outerScript, outerPC, outerArgc, speculate);
23918: }
23918: 
31496: static JS_REQUIRES_STACK TypeConsensus
35044: FindLoopEdgeTarget(JSContext* cx, VMSideExit* exit, TreeFragment** peerp)
35044: {
35044:     TreeFragment* from = exit->root();
31495: 
31495:     JS_ASSERT(from->code());
41802:     Oracle* oracle = JS_TRACE_MONITOR(cx).oracle;
31495: 
31920:     TypeMap typeMap(NULL);
31495:     FullMapFromExit(typeMap, exit);
36361:     JS_ASSERT(typeMap.length() - exit->numStackSlots == from->nGlobalTypes());
31495: 
31495:     /* Mark all double slots as undemotable */
36361:     uint16* gslots = from->globalSlots->data();
31495:     for (unsigned i = 0; i < typeMap.length(); i++) {
48470:         if (typeMap[i] == JSVAL_TYPE_DOUBLE) {
36361:             if (i < from->nStackTypes)
41802:                 oracle->markStackSlotUndemotable(cx, i, from->ip);
33564:             else if (i >= exit->numStackSlots)
41802:                 oracle->markGlobalSlotUndemotable(cx, gslots[i - exit->numStackSlots]);
33564:         }
33564:     }
33564: 
53524:     JS_ASSERT(exit->exitType == UNSTABLE_LOOP_EXIT);
53524: 
53524:     TreeFragment* firstPeer = from->first;
33564: 
35044:     for (TreeFragment* peer = firstPeer; peer; peer = peer->peer) {
36361:         if (!peer->code())
31495:             continue;
31495:         JS_ASSERT(peer->argc == from->argc);
36361:         JS_ASSERT(exit->numStackSlots == peer->nStackTypes);
31495:         TypeConsensus consensus = TypeMapLinkability(cx, typeMap, peer);
31499:         if (consensus == TypeConsensus_Okay || consensus == TypeConsensus_Undemotes) {
31495:             *peerp = peer;
31495:             return consensus;
31495:         }
31495:     }
31495: 
31495:     return TypeConsensus_Bad;
31495: }
31495: 
30860: static JS_REQUIRES_STACK bool
56217: AttemptToStabilizeTree(JSContext* cx, JSObject* globalObj, VMSideExit* exit,
56217:                        JSScript* outerScript, jsbytecode* outerPC, uint32 outerArgc)
21433: {
37741:     TraceMonitor* tm = &JS_TRACE_MONITOR(cx);
28308:     if (tm->needFlush) {
32748:         ResetJIT(cx, FR_DEEP_BAIL);
28308:         return false;
28308:     }
28308: 
35044:     TreeFragment* from = exit->root();
35044: 
35044:     TreeFragment* peer = NULL;
31495:     TypeConsensus consensus = FindLoopEdgeTarget(cx, exit, &peer);
31495:     if (consensus == TypeConsensus_Okay) {
36361:         JS_ASSERT(from->globalSlots == peer->globalSlots);
33564:         JS_ASSERT_IF(exit->exitType == UNSTABLE_LOOP_EXIT,
36361:                      from->nStackTypes == peer->nStackTypes);
36361:         JS_ASSERT(exit->numStackSlots == peer->nStackTypes);
31495:         /* Patch this exit to its peer */
31495:         JoinPeers(tm->assembler, exit, peer);
31495:         /*
31495:          * Update peer global types. The |from| fragment should already be updated because it on
31495:          * the execution path, and somehow connected to the entry trace.
31495:          */
36361:         if (peer->nGlobalTypes() < peer->globalSlots->length())
36361:             SpecializeTreesToMissingGlobals(cx, globalObj, peer);
36361:         JS_ASSERT(from->nGlobalTypes() == from->globalSlots->length());
31495:         /* This exit is no longer unstable, so remove it. */
33564:         if (exit->exitType == UNSTABLE_LOOP_EXIT)
36361:             from->removeUnstableExit(exit);
36361:         debug_only_stmt(DumpPeerStability(tm, peer->ip, globalObj, from->globalShape, from->argc);)
31495:         return false;
31495:     } else if (consensus == TypeConsensus_Undemotes) {
24295:         /* The original tree is unconnectable, so trash it. */
54718:         TrashTree(peer);
24295:         return false;
24246:     }
31495: 
36361:     SlotList *globalSlots = from->globalSlots;
36361: 
33564:     JS_ASSERT(from == from->root);
33564: 
33564:     /* If this tree has been blacklisted, don't try to record a new one. */
56217:     if (*(jsbytecode*)from->ip == JSOP_NOTRACE)
33564:         return false;
33564: 
56217:     return RecordTree(cx, from->first, outerScript, outerPC, outerArgc, globalSlots);
21433: }
21433: 
35083: static JS_REQUIRES_STACK VMFragment*
35083: CreateBranchFragment(JSContext* cx, TreeFragment* root, VMSideExit* anchor)
35083: {
37741:     TraceMonitor* tm = &JS_TRACE_MONITOR(cx);
35083: 
35083:     verbose_only(
37741:     uint32_t profFragID = (LogController.lcbits & LC_FragProfile)
35083:                           ? (++(tm->lastFragID)) : 0;
35083:     )
35083: 
42717:     VMFragment* f = new (*tm->dataAlloc) VMFragment(cx->regs->pc verbose_only(, profFragID));
35083: 
35083:     debug_only_printf(LC_TMTreeVis, "TREEVIS CREATEBRANCH ROOT=%p FRAG=%p PC=%p FILE=\"%s\""
35083:                       " LINE=%d ANCHOR=%p OFFS=%d\n",
53840:                       (void*)root, (void*)f, (void*)cx->regs->pc, cx->fp()->script()->filename,
51446:                       js_FramePCToLineNumber(cx, cx->fp()), (void*)anchor,
51446:                       FramePCOffset(cx, cx->fp()));
35083:     verbose_only( tm->branches = new (*tm->dataAlloc) Seq<Fragment*>(f, tm->branches); )
35083: 
35083:     f->root = root;
35083:     if (anchor)
35083:         anchor->target = f;
35083:     return f;
35083: }
35083: 
22652: static JS_REQUIRES_STACK bool
56217: AttemptToExtendTree(JSContext* cx, VMSideExit* anchor, VMSideExit* exitedFrom,
56217:                     JSScript *outerScript, jsbytecode* outerPC
29368: #ifdef MOZ_TRACEVIS
29368:     , TraceVisStateObj* tvso = NULL
29368: #endif
29368:     )
18620: {
37741:     TraceMonitor* tm = &JS_TRACE_MONITOR(cx);
35083:     JS_ASSERT(!tm->recorder);
35083: 
28308:     if (tm->needFlush) {
32748:         ResetJIT(cx, FR_DEEP_BAIL);
29368: #ifdef MOZ_TRACEVIS
29368:         if (tvso) tvso->r = R_FAIL_EXTEND_FLUSH;
29368: #endif
28308:         return false;
28308:     }
28308: 
35044:     TreeFragment* f = anchor->root();
36361:     JS_ASSERT(f->code());
18781: 
30860:     /*
30860:      * Don't grow trees above a certain size to avoid code explosion due to
30860:      * tail duplication.
30860:      */
35044:     if (f->branchCount >= MAX_BRANCHES) {
56551: #ifdef JS_METHODJIT
56551:         if (cx->methodJitEnabled && cx->profilingEnabled)
56551:             Blacklist((jsbytecode *)f->ip);
56551: #endif
29368: #ifdef MOZ_TRACEVIS
29368:         if (tvso) tvso->r = R_FAIL_EXTEND_MAX_BRANCHES;
29368: #endif
18781:         return false;
29368:     }
17939: 
35083:     VMFragment* c = (VMFragment*)anchor->target;
35083:     if (!c) {
35083:         c = CreateBranchFragment(cx, f, anchor);
35083:     } else {
27290:         /*
30860:          * If we are recycling a fragment, it might have a different ip so reset it
30860:          * here. This can happen when attaching a branch to a NESTED_EXIT, which
30860:          * might extend along separate paths (i.e. after the loop edge, and after a
30860:          * return statement).
27290:          */
42717:         c->ip = cx->regs->pc;
35083:         JS_ASSERT(c->root == f);
35083:     }
27290: 
29883:     debug_only_printf(LC_TMTracer,
29883:                       "trying to attach another branch to the tree (hits = %d)\n", c->hits());
25627: 
25627:     int32_t& hits = c->hits();
36395:     int32_t maxHits = HOTEXIT + MAXEXIT;
36395:     if (anchor->exitType == CASE_EXIT)
36395:         maxHits *= anchor->switchInfo->count;
56217:     if (outerPC || (hits++ >= HOTEXIT && hits <= maxHits)) {
17939:         /* start tracing secondary trace from this point */
24246:         unsigned stackSlots;
18621:         unsigned ngslots;
48470:         JSValueType* typeMap;
31920:         TypeMap fullMap(NULL);
35044:         if (!exitedFrom) {
30860:             /*
30860:              * If we are coming straight from a simple side exit, just use that
30860:              * exit's type map as starting point.
30860:              */
20931:             ngslots = anchor->numGlobalSlots;
24246:             stackSlots = anchor->numStackSlots;
31495:             typeMap = anchor->fullTypeMap();
18621:         } else {
30860:             /*
30860:              * If we side-exited on a loop exit and continue on a nesting
30860:              * guard, the nesting guard (anchor) has the type information for
30860:              * everything below the current scope, and the actual guard we
30860:              * exited from has the types for everything in the current scope
30860:              * (and whatever it inlined). We have to merge those maps here.
30860:              */
21521:             VMSideExit* e1 = anchor;
21521:             VMSideExit* e2 = exitedFrom;
31495:             fullMap.add(e1->stackTypeMap(), e1->numStackSlotsBelowCurrentFrame);
31495:             fullMap.add(e2->stackTypeMap(), e2->numStackSlots);
24246:             stackSlots = fullMap.length();
31913:             ngslots = BuildGlobalTypeMapFromInnerTree(fullMap, e2);
31913:             JS_ASSERT(ngslots >= e1->numGlobalSlots); // inner tree must have all globals
31913:             JS_ASSERT(ngslots == fullMap.length() - stackSlots);
24246:             typeMap = fullMap.data();
24246:         }
28960:         JS_ASSERT(ngslots >= anchor->numGlobalSlots);
36361:         bool rv = TraceRecorder::startRecorder(cx, anchor, c, stackSlots, ngslots, typeMap,
56551:                                                exitedFrom, outerScript, outerPC, f->argc,
53524:                                                hits < maxHits);
29368: #ifdef MOZ_TRACEVIS
29368:         if (!rv && tvso)
29368:             tvso->r = R_FAIL_EXTEND_START;
29368: #endif
29368:         return rv;
29368:     }
29368: #ifdef MOZ_TRACEVIS
29368:     if (tvso) tvso->r = R_FAIL_EXTEND_COLD;
29368: #endif
17939:     return false;
17939: }
17939: 
41777: static JS_REQUIRES_STACK bool
35044: ExecuteTree(JSContext* cx, TreeFragment* f, uintN& inlineCallCount,
41777:             VMSideExit** innermostNestedGuardp, VMSideExit** lrp);
41777: 
41777: static inline MonitorResult
41777: RecordingIfTrue(bool b)
41777: {
41777:     return b ? MONITOR_RECORDING : MONITOR_NOT_RECORDING;
41777: }
41777: 
41777: /*
41777:  * A postcondition of recordLoopEdge is that if recordLoopEdge does not return
41777:  * MONITOR_RECORDING, the recording has been aborted.
41777:  */
41777: JS_REQUIRES_STACK MonitorResult
34351: TraceRecorder::recordLoopEdge(JSContext* cx, TraceRecorder* r, uintN& inlineCallCount)
17939: {
37741:     TraceMonitor* tm = &JS_TRACE_MONITOR(cx);
26557: 
27493:     /* Process needFlush and deep abort requests. */
27493:     if (tm->needFlush) {
32748:         ResetJIT(cx, FR_DEEP_BAIL);
41777:         return MONITOR_NOT_RECORDING;
27493:     }
26557: 
34351:     JS_ASSERT(r->fragment && !r->fragment->lastIns);
35044:     TreeFragment* root = r->fragment->root;
42717:     TreeFragment* first = LookupOrAddLoop(tm, cx->regs->pc, root->globalObj,
53840:                                           root->globalShape, entryFrameArgc(cx));
21514: 
30860:     /*
30860:      * Make sure the shape of the global object still matches (this might flush
30860:      * the JIT cache).
30860:      */
53840:     JSObject* globalObj = cx->fp()->scopeChain().getGlobal();
24491:     uint32 globalShape = -1;
24491:     SlotList* globalSlots = NULL;
33748:     if (!CheckGlobalObjectShape(cx, tm, globalObj, &globalShape, &globalSlots)) {
33748:         JS_ASSERT(!tm->recorder);
41777:         return MONITOR_NOT_RECORDING;
33748:     }
21433: 
29883:     debug_only_printf(LC_TMTracer,
29883:                       "Looking for type-compatible peer (%s:%d@%d)\n",
53840:                       cx->fp()->script()->filename,
51446:                       js_FramePCToLineNumber(cx, cx->fp()),
51446:                       FramePCOffset(cx, cx->fp()));
21433: 
28239:     // Find a matching inner tree. If none can be found, compile one.
35044:     TreeFragment* f = r->findNestedCompatiblePeer(first);
28239:     if (!f || !f->code()) {
21433:         AUDIT(noCompatInnerTrees);
25937: 
35083:         TreeFragment* outerFragment = root;
56217:         JSScript* outerScript = outerFragment->script;
56217:         jsbytecode* outerPC = (jsbytecode*) outerFragment->ip;
28244:         uint32 outerArgc = outerFragment->argc;
53840:         JS_ASSERT(entryFrameArgc(cx) == first->argc);
54835: 
54835:         if (AbortRecording(cx, "No compatible inner tree") == JIT_RESET)
54835:             return MONITOR_NOT_RECORDING;
54427: 
56217:         return RecordingIfTrue(RecordTree(cx, first, outerScript, outerPC, outerArgc, globalSlots));
41777:     }
41777: 
41777:     AbortableRecordingStatus status = r->attemptTreeCall(f, inlineCallCount);
41777:     if (status == ARECORD_CONTINUE)
41777:         return MONITOR_RECORDING;
41777:     if (status == ARECORD_ERROR) {
41777:         if (TRACE_RECORDER(cx))
41777:             AbortRecording(cx, "Error returned while recording loop edge");
41777:         return MONITOR_ERROR;
41777:     }
41777:     JS_ASSERT(status == ARECORD_ABORTED && !tm->recorder);
41777:     return MONITOR_NOT_RECORDING;
33564: }
33564: 
33564: JS_REQUIRES_STACK AbortableRecordingStatus
35044: TraceRecorder::attemptTreeCall(TreeFragment* f, uintN& inlineCallCount)
33564: {
33564:     adjustCallerTypes(f);
39937:     prepareTreeCall(f);
33564: 
33564: #ifdef DEBUG
53139:     uintN oldInlineCallCount = inlineCallCount;
33564: #endif
33171: 
33599:     JSContext *localCx = cx;
33599: 
37009:     // Refresh the import type map so the tracker can reimport values after the
37009:     // call with their correct types. The inner tree must not change the type of
37009:     // any variable in a frame above the current one (i.e., upvars).
37009:     //
37009:     // Note that DetermineTypesVisitor may call determineSlotType, which may
37009:     // read from the (current, stale) import type map, but this is safe here.
37009:     // The reason is that determineSlotType will read the import type map only
37009:     // if there is not a tracker instruction for that value, which means that
37009:     // value has not been written yet, so that type map entry is up to date.
37009:     importTypeMap.setLength(NativeStackSlots(cx, callDepth));
37009:     DetermineTypesVisitor visitor(*this, importTypeMap.data());
37009:     VisitStackSlots(visitor, cx, callDepth);
37009: 
21521:     VMSideExit* innermostNestedGuard = NULL;
41777:     VMSideExit* lr;
41777:     bool ok = ExecuteTree(cx, f, inlineCallCount, &innermostNestedGuard, &lr);
41777: 
41777:     /*
41777:      * If ExecuteTree reentered the interpreter, it may have killed |this|
41777:      * and/or caused an error, which must be propagated.
41777:      */
41777:     JS_ASSERT_IF(TRACE_RECORDER(localCx), TRACE_RECORDER(localCx) == this);
41777:     if (!ok)
41777:         return ARECORD_ERROR;
33599:     if (!TRACE_RECORDER(localCx))
33564:         return ARECORD_ABORTED;
33171: 
33171:     if (!lr) {
37741:         AbortRecording(cx, "Couldn't call inner tree");
33564:         return ARECORD_ABORTED;
33564:     }
33564: 
40251:     TreeFragment* outerFragment = tree;
56217:     JSScript* outerScript = outerFragment->script;
56217:     jsbytecode* outerPC = (jsbytecode*) outerFragment->ip;
20931:     switch (lr->exitType) {
18051:       case LOOP_EXIT:
18284:         /* If the inner tree exited on an unknown loop exit, grow the tree around it. */
18284:         if (innermostNestedGuard) {
54835:             if (AbortRecording(cx, "Inner tree took different side exit, abort current "
54835:                                    "recording and grow nesting tree") == JIT_RESET) {
54835:                 return ARECORD_ABORTED;
54835:             }
56217:             return AttemptToExtendTree(localCx, innermostNestedGuard, lr, outerScript, outerPC)
54835:                    ? ARECORD_CONTINUE
54835:                    : ARECORD_ABORTED;
33564:         }
33564: 
33564:         JS_ASSERT(oldInlineCallCount == inlineCallCount);
30860: 
30860:         /* Emit a call to the inner tree and continue recording the outer tree trace. */
39937:         emitTreeCall(f, lr);
33564:         return ARECORD_CONTINUE;
30860: 
21433:       case UNSTABLE_LOOP_EXIT:
33564:       {
30860:         /* Abort recording so the inner loop can become type stable. */
33564:         JSObject* _globalObj = globalObj;
54835:         if (AbortRecording(cx, "Inner tree is trying to stabilize, "
54835:                                "abort outer recording") == JIT_RESET) {
54835:             return ARECORD_ABORTED;
54835:         }
56217:         return AttemptToStabilizeTree(localCx, _globalObj, lr, outerScript, outerPC,
56217:                                       outerFragment->argc)
54835:                ? ARECORD_CONTINUE
54835:                : ARECORD_ABORTED;
54427:       }
54427: 
56185:       case MUL_ZERO_EXIT:
29354:       case OVERFLOW_EXIT:
56185:         if (lr->exitType == MUL_ZERO_EXIT)
56185:             traceMonitor->oracle->markInstructionSlowZeroTest(cx->regs->pc);
56185:         else
46181:             traceMonitor->oracle->markInstructionUndemotable(cx->regs->pc);
30860:         /* FALL THROUGH */
18051:       case BRANCH_EXIT:
54835:       case CASE_EXIT:
30860:         /* Abort recording the outer tree, extend the inner tree. */
54835:         if (AbortRecording(cx, "Inner tree is trying to grow, "
54835:                                "abort outer recording") == JIT_RESET) {
54835:             return ARECORD_ABORTED;
54835:         }
56217:         return AttemptToExtendTree(localCx, lr, NULL, outerScript, outerPC)
54835:                ? ARECORD_CONTINUE
54835:                : ARECORD_ABORTED;
30860: 
35083:       case NESTED_EXIT:
35083:         JS_NOT_REACHED("NESTED_EXIT should be replaced by innermost side exit");
18051:       default:
29894:         debug_only_printf(LC_TMTracer, "exit_type=%s\n", getExitName(lr->exitType));
37741:         AbortRecording(cx, "Inner tree not suitable for calling");
33564:         return ARECORD_ABORTED;
18051:     }
17988: }
21433: 
48470: static inline bool
48470: IsEntryTypeCompatible(const Value &v, JSValueType type)
48470: {
53571:     bool ok;
53571: 
53571:     JS_ASSERT(type <= JSVAL_UPPER_INCL_TYPE_OF_BOXABLE_SET);
53571:     JS_ASSERT(type != JSVAL_TYPE_OBJECT);   /* JSVAL_TYPE_OBJECT does not belong in a type map */
53571: 
53571:     if (v.isInt32()) {
53571:         ok = (type == JSVAL_TYPE_INT32 || type == JSVAL_TYPE_DOUBLE);
53571: 
53571:     } else if (v.isDouble()) {
53571:         int32_t _;
53571:         ok = (type == JSVAL_TYPE_DOUBLE) || 
53571:              (type == JSVAL_TYPE_INT32 && JSDOUBLE_IS_INT32(v.toDouble(), &_));
53571: 
53571:     } else if (v.isObject()) {
53571:         ok = v.toObject().isFunction()
53571:            ? type == JSVAL_TYPE_FUNOBJ
53571:            : type == JSVAL_TYPE_NONFUNOBJ;
53571: 
53571:     } else {
53571:         ok = v.extractNonDoubleObjectTraceType() == type;
53571:     }
48470: #ifdef DEBUG
53571:     char ttag = TypeToChar(type);
53571:     char vtag = ValueToTypeChar(v);
53571:     debug_only_printf(LC_TMTracer, "%c/%c ", vtag, ttag);
53571:     if (!ok)
53571:         debug_only_printf(LC_TMTracer, "%s", "(incompatible types)");
53571: #endif
53571:     return ok;
21433: }
21433: 
53840: static inline bool
53840: IsFrameObjPtrTypeCompatible(void *p, JSStackFrame *fp, JSValueType type)
53840: {
53840:     if (p == fp->addressOfScopeChain())
53840:         return type == JSVAL_TYPE_NONFUNOBJ;
53840:     JS_ASSERT(p == fp->addressOfArgs());
53840:     JS_ASSERT(type == JSVAL_TYPE_NONFUNOBJ || type == JSVAL_TYPE_NULL);
53840:     return fp->hasArgsObj() == (type == JSVAL_TYPE_NONFUNOBJ);
53840: }
53840: 
29882: class TypeCompatibilityVisitor : public SlotVisitorBase
29882: {
29880:     TraceRecorder &mRecorder;
29880:     JSContext *mCx;
46181:     Oracle *mOracle;
48470:     JSValueType *mTypeMap;
29880:     unsigned mStackSlotNum;
29880:     bool mOk;
29880: public:
29880:     TypeCompatibilityVisitor (TraceRecorder &recorder,
48470:                               JSValueType *typeMap) :
29880:         mRecorder(recorder),
29880:         mCx(mRecorder.cx),
46181:         mOracle(JS_TRACE_MONITOR(mCx).oracle),
29880:         mTypeMap(typeMap),
29880:         mStackSlotNum(0),
29880:         mOk(true)
29880:     {}
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE void
48470:     visitGlobalSlot(Value *vp, unsigned n, unsigned slot) {
29883:         debug_only_printf(LC_TMTracer, "global%d=", n);
48470:         if (!IsEntryTypeCompatible(*vp, *mTypeMap)) {
29880:             mOk = false;
59993:         } else if (!IsPromotedInt32(mRecorder.get(vp)) && *mTypeMap == JSVAL_TYPE_INT32) {
46181:             mOracle->markGlobalSlotUndemotable(mCx, slot);
29880:             mOk = false;
48470:         } else if (vp->isInt32() && *mTypeMap == JSVAL_TYPE_DOUBLE) {
46181:             mOracle->markGlobalSlotUndemotable(mCx, slot);
29880:         }
29880:         mTypeMap++;
29880:     }
29880: 
53840:     /*
53840:      * For the below two methods, one may be inclined to 'return false' early
53840:      * when mOk is set to 'false'. Don't do that. It is very important to run
53840:      * through the whole list to let all mis-matching slots get marked
53840:      * undemotable in the oracle.
53840:      */
53840: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
48470:     visitStackSlots(Value *vp, size_t count, JSStackFrame* fp) {
29880:         for (size_t i = 0; i < count; ++i) {
29883:             debug_only_printf(LC_TMTracer, "%s%u=", stackSlotKind(), unsigned(i));
48470:             if (!IsEntryTypeCompatible(*vp, *mTypeMap)) {
29880:                 mOk = false;
59993:             } else if (!IsPromotedInt32(mRecorder.get(vp)) && *mTypeMap == JSVAL_TYPE_INT32) {
46181:                 mOracle->markStackSlotUndemotable(mCx, mStackSlotNum);
29880:                 mOk = false;
48470:             } else if (vp->isInt32() && *mTypeMap == JSVAL_TYPE_DOUBLE) {
46181:                 mOracle->markStackSlotUndemotable(mCx, mStackSlotNum);
29880:             }
29880:             vp++;
29880:             mTypeMap++;
29880:             mStackSlotNum++;
29880:         }
29880:         return true;
29880:     }
29880: 
48470:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
53840:     visitFrameObjPtr(void* p, JSStackFrame* fp) {
48470:         debug_only_printf(LC_TMTracer, "%s%u=", stackSlotKind(), 0);
53840:         if (!IsFrameObjPtrTypeCompatible(p, fp, *mTypeMap))
48470:             mOk = false;
48470:         mTypeMap++;
48470:         mStackSlotNum++;
48470:         return true;
48470:     }
48470: 
29880:     bool isOk() {
29880:         return mOk;
29880:     }
29880: };
29880: 
35044: JS_REQUIRES_STACK TreeFragment*
35044: TraceRecorder::findNestedCompatiblePeer(TreeFragment* f)
28239: {
37741:     TraceMonitor* tm;
21433: 
21433:     tm = &JS_TRACE_MONITOR(cx);
36361:     unsigned int ngslots = tree->globalSlots->length();
36361: 
35044:     for (; f != NULL; f = f->peer) {
28239:         if (!f->code())
21433:             continue;
28239: 
29883:         debug_only_printf(LC_TMTracer, "checking nested types %p: ", (void*)f);
21433: 
36361:         if (ngslots > f->nGlobalTypes())
36361:             SpecializeTreesToMissingGlobals(cx, globalObj, f);
30860: 
30860:         /*
30860:          * Determine whether the typemap of the inner tree matches the outer
30860:          * tree's current state. If the inner tree expects an integer, but the
30860:          * outer tree doesn't guarantee an integer for that slot, we mark the
30860:          * slot undemotable and mismatch here. This will force a new tree to be
30860:          * compiled that accepts a double for the slot. If the inner tree
30860:          * expects a double, but the outer tree has an integer, we can proceed,
30860:          * but we mark the location undemotable.
28239:          */
36361:         TypeCompatibilityVisitor visitor(*this, f->typeMap.data());
36361:         VisitSlots(visitor, cx, 0, *tree->globalSlots);
29880: 
29883:         debug_only_printf(LC_TMTracer, " %s\n", visitor.isOk() ? "match" : "");
29880:         if (visitor.isOk())
21433:             return f;
21433:     }
21433: 
21433:     return NULL;
21433: }
21433: 
29882: class CheckEntryTypeVisitor : public SlotVisitorBase
29882: {
29880:     bool mOk;
48470:     JSValueType *mTypeMap;
29880: public:
48470:     CheckEntryTypeVisitor(JSValueType *typeMap) :
29880:         mOk(true),
29880:         mTypeMap(typeMap)
29880:     {}
29880: 
48470:     JS_ALWAYS_INLINE void checkSlot(const Value &v, char const *name, int i) {
29883:         debug_only_printf(LC_TMTracer, "%s%d=", name, i);
29896:         JS_ASSERT(*(uint8_t*)mTypeMap != 0xCD);
48470:         mOk = IsEntryTypeCompatible(v, *mTypeMap++);
29880:     }
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE void
48470:     visitGlobalSlot(Value *vp, unsigned n, unsigned slot) {
29880:         if (mOk)
48470:             checkSlot(*vp, "global", n);
29880:     }
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
48470:     visitStackSlots(Value *vp, size_t count, JSStackFrame* fp) {
29880:         for (size_t i = 0; i < count; ++i) {
29880:             if (!mOk)
29880:                 break;
48470:             checkSlot(*vp++, stackSlotKind(), i);
48470:         }
48470:         return mOk;
48470:     }
48470: 
48470:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
53840:     visitFrameObjPtr(void* p, JSStackFrame *fp) {
53840:         debug_only_printf(LC_TMTracer, "%s%d=", stackSlotKind(), 0);
53840:         JS_ASSERT(*(uint8_t*)mTypeMap != 0xCD);
53840:         return mOk = IsFrameObjPtrTypeCompatible(p, fp, *mTypeMap++);
29880:     }
29880: 
29880:     bool isOk() {
29880:         return mOk;
29880:     }
29880: };
29880: 
21433: /**
21433:  * Check if types are usable for trace execution.
21433:  *
21433:  * @param cx            Context.
36361:  * @param f             Tree of peer we're testing.
21433:  * @return              True if compatible (with or without demotions), false otherwise.
21433:  */
22652: static JS_REQUIRES_STACK bool
36361: CheckEntryTypes(JSContext* cx, JSObject* globalObj, TreeFragment* f)
36361: {
36361:     unsigned int ngslots = f->globalSlots->length();
36361: 
36361:     JS_ASSERT(f->nStackTypes == NativeStackSlots(cx, 0));
36361: 
36361:     if (ngslots > f->nGlobalTypes())
36361:         SpecializeTreesToMissingGlobals(cx, globalObj, f);
36361: 
36361:     JS_ASSERT(f->typeMap.length() == NativeStackSlots(cx, 0) + ngslots);
36361:     JS_ASSERT(f->typeMap.length() == f->nStackTypes + ngslots);
36361:     JS_ASSERT(f->nGlobalTypes() == ngslots);
36361: 
36361:     CheckEntryTypeVisitor visitor(f->typeMap.data());
36361:     VisitSlots(visitor, cx, 0, *f->globalSlots);
21433: 
29883:     debug_only_print0(LC_TMTracer, "\n");
29880:     return visitor.isOk();
21433: }
21433: 
21433: /**
21433:  * Find an acceptable entry tree given a PC.
21433:  *
21433:  * @param cx            Context.
29880:  * @param globalObj     Global object.
21433:  * @param f             First peer fragment.
21433:  * @param nodemote      If true, will try to find a peer that does not require demotion.
25627:  * @out   count         Number of fragments consulted.
21433:  */
35044: static JS_REQUIRES_STACK TreeFragment*
35044: FindVMCompatiblePeer(JSContext* cx, JSObject* globalObj, TreeFragment* f, uintN& count)
25627: {
25627:     count = 0;
21433:     for (; f != NULL; f = f->peer) {
36361:         if (!f->code())
21433:             continue;
29883:         debug_only_printf(LC_TMTracer,
29883:                           "checking vm types %p (ip: %p): ", (void*)f, f->ip);
36361:         if (CheckEntryTypes(cx, globalObj, f))
21433:             return f;
25627:         ++count;
21433:     }
21433:     return NULL;
21433: }
21433: 
35083: /*
35083:  * For the native stacks and global frame, reuse the storage in |tm->storage|.
35083:  * This reuse depends on the invariant that only one trace uses |tm->storage| at
35083:  * a time. This is subtley correct in lieu of deep bail; see comment for
37741:  * |deepBailSp| in DeepBail.
35083:  */
35083: JS_ALWAYS_INLINE
41276: TracerState::TracerState(JSContext* cx, TraceMonitor* tm, TreeFragment* f,
35083:                          uintN& inlineCallCount, VMSideExit** innermostNestedGuardp)
35083:   : cx(cx),
37037:     stackBase(tm->storage->stack()),
36361:     sp(stackBase + f->nativeStackBase / sizeof(double)),
37037:     eos(tm->storage->global()),
37037:     callstackBase(tm->storage->callstack()),
35083:     sor(callstackBase),
35083:     rp(callstackBase),
35083:     eor(callstackBase + JS_MIN(MAX_CALL_STACK_ENTRIES,
35083:                                JS_MAX_INLINE_CALL_COUNT - inlineCallCount)),
35083:     lastTreeExitGuard(NULL),
35083:     lastTreeCallGuard(NULL),
35083:     rpAtLastTreeCall(NULL),
36361:     outermostTree(f),
35083:     inlineCallCountp(&inlineCallCount),
35083:     innermostNestedGuardp(innermostNestedGuardp),
35083: #ifdef EXECUTE_TREE_TIMER
35083:     startTime(rdtsc()),
35083: #endif
35083:     builtinStatus(0),
56212:     nativeVp(NULL)
35083: {
60574:     JS_ASSERT(tm == &JS_TRACE_MONITOR(cx));
35083:     JS_ASSERT(!tm->tracecx);
35083:     tm->tracecx = cx;
60574:     prev = tm->tracerState;
60574:     tm->tracerState = this;
35083: 
35083:     JS_ASSERT(eos == stackBase + MAX_NATIVE_STACK_SLOTS);
35083:     JS_ASSERT(sp < eos);
35083: 
35083:     /*
35083:      * inlineCallCount has already been incremented, if being invoked from
35083:      * EnterFrame. It is okay to have a 0-frame restriction since the JIT
35083:      * might not need any frames.
35083:      */
35083:     JS_ASSERT(inlineCallCount <= JS_MAX_INLINE_CALL_COUNT);
35083: 
35083: #ifdef DEBUG
35083:     /*
35083:      * Cannot 0xCD-fill global frame since it may overwrite a bailed outer
35083:      * ExecuteTree's 0xdeadbeefdeadbeef marker.
35083:      */
37037:     memset(tm->storage->stack(), 0xCD, MAX_NATIVE_STACK_SLOTS * sizeof(double));
37037:     memset(tm->storage->callstack(), 0xCD, MAX_CALL_STACK_ENTRIES * sizeof(FrameInfo*));
35083: #endif
35083: }
35083: 
35083: JS_ALWAYS_INLINE
41276: TracerState::~TracerState()
35083: {
35083:     JS_ASSERT(!nativeVp);
35083: 
60574:     TraceMonitor *tm = &JS_TRACE_MONITOR(cx);
60574:     tm->tracerState = prev;
60574:     tm->tracecx = NULL;
35083: }
35083: 
35083: /* Call |f|, return the exit taken. */
35083: static JS_ALWAYS_INLINE VMSideExit*
41276: ExecuteTrace(JSContext* cx, Fragment* f, TracerState& state)
35083: {
60574:     JS_ASSERT(!JS_TRACE_MONITOR(cx).bailExit);
60157: #ifdef JS_METHODJIT
60157:     JS_ASSERT(!TRACE_PROFILER(cx));
60157: #endif
41276:     union { NIns *code; GuardRecord* (FASTCALL *func)(TracerState*); } u;
35083:     u.code = f->code();
35083:     GuardRecord* rec;
35083: #if defined(JS_NO_FASTCALL) && defined(NANOJIT_IA32)
35083:     SIMULATE_FASTCALL(rec, state, NULL, u.func);
35083: #else
35343:     rec = u.func(&state);
35083: #endif
60574:     JS_ASSERT(!JS_TRACE_MONITOR(cx).bailExit);
35083:     return (VMSideExit*)rec->exit;
35083: }
35083: 
35083: /* Check whether our assumptions about the incoming scope-chain are upheld. */
35084: static JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
36361: ScopeChainCheck(JSContext* cx, TreeFragment* f)
36361: {
53840:     JS_ASSERT(f->globalObj == cx->fp()->scopeChain().getGlobal());
21433: 
31467:     /*
31467:      * The JIT records and expects to execute with two scope-chain
31467:      * assumptions baked-in:
31467:      *
31467:      *   1. That the bottom of the scope chain is global, in the sense of
31467:      *      JSCLASS_IS_GLOBAL.
31467:      *
31467:      *   2. That the scope chain between fp and the global is free of
31467:      *      "unusual" native objects such as HTML forms or other funny
31467:      *      things.
31467:      *
31467:      * #2 is checked here while following the scope-chain links, via
31467:      * js_IsCacheableNonGlobalScope, which consults a whitelist of known
31467:      * class types; once a global is found, it's checked for #1. Failing
31467:      * either check causes an early return from execution.
31467:      */
53840:     JSObject* child = &cx->fp()->scopeChain();
39930:     while (JSObject* parent = child->getParent()) {
31467:         if (!js_IsCacheableNonGlobalScope(child)) {
31467:             debug_only_print0(LC_TMTracer,"Blacklist: non-cacheable object on scope chain.\n");
31467:             Blacklist((jsbytecode*) f->root->ip);
35083:             return false;
31467:         }
31467:         child = parent;
31467:     }
35083:     JS_ASSERT(child == f->globalObj);
35083: 
60566:     if (!f->globalObj->isGlobal()) {
31467:         debug_only_print0(LC_TMTracer, "Blacklist: non-global at root of scope chain.\n");
31467:         Blacklist((jsbytecode*) f->root->ip);
35083:         return false;
35083:     }
35083: 
41111:     return true;
41111: }
41111: 
60576: enum LEAVE_TREE_STATUS {
60576:   NO_DEEP_BAIL = 0,
60576:   DEEP_BAILED = 1
60576: };
60576: 
60576: static LEAVE_TREE_STATUS
41276: LeaveTree(TraceMonitor *tm, TracerState&, VMSideExit *lr);
41111: 
41777: /* Return false if the interpreter should goto error. */
41777: static JS_REQUIRES_STACK bool
41111: ExecuteTree(JSContext* cx, TreeFragment* f, uintN& inlineCallCount,
41777:             VMSideExit** innermostNestedGuardp, VMSideExit **lrp)
41111: {
41111: #ifdef MOZ_TRACEVIS
41111:     TraceVisStateObj tvso(cx, S_EXECUTE);
41111: #endif
41111:     JS_ASSERT(f->root == f && f->code());
41111:     TraceMonitor* tm = &JS_TRACE_MONITOR(cx);
41111: 
60157:     JS_ASSERT(!tm->profile);
60157: 
42732:     if (!ScopeChainCheck(cx, f) || !cx->stack().ensureEnoughSpaceToEnterTrace() ||
42732:         inlineCallCount + f->maxCallDepth > JS_MAX_INLINE_CALL_COUNT) {
41777:         *lrp = NULL;
41777:         return true;
41777:     }
41111: 
35083:     /* Make sure the global object is sane. */
40410:     JS_ASSERT(f->globalObj->numSlots() <= MAX_GLOBAL_SLOTS);
36361:     JS_ASSERT(f->nGlobalTypes() == f->globalSlots->length());
36361:     JS_ASSERT_IF(f->globalSlots->length() != 0,
40847:                  f->globalObj->shape() == f->globalShape);
35083: 
35083:     /* Initialize trace state. */
41276:     TracerState state(cx, tm, f, inlineCallCount, innermostNestedGuardp);
37037:     double* stack = tm->storage->stack();
37037:     double* global = tm->storage->global();
35083:     JSObject* globalObj = f->globalObj;
36361:     unsigned ngslots = f->globalSlots->length();
36361:     uint16* gslots = f->globalSlots->data();
35083: 
35083:     BuildNativeFrame(cx, globalObj, 0 /* callDepth */, ngslots, gslots,
36361:                      f->typeMap.data(), global, stack);
35083: 
35083:     AUDIT(traceTriggered);
53324:     debug_only_printf(LC_TMTracer, "entering trace at %s:%u@%u, execs: %u code: %p\n",
53840:                       cx->fp()->script()->filename,
51446:                       js_FramePCToLineNumber(cx, cx->fp()),
51446:                       FramePCOffset(cx, cx->fp()),
53324:            f->execs,
29883:            f->code());
18210: 
40410:     debug_only_stmt(uint32 globalSlots = globalObj->numSlots();)
37037:     debug_only_stmt(*(uint64*)&tm->storage->global()[globalSlots] = 0xdeadbeefdeadbeefLL;)
35083: 
35083:     /* Execute trace. */
59736:     tm->iterationCounter = 0;
56551:     debug_only(int64 t0 = PRMJ_Now();)
29368: #ifdef MOZ_TRACEVIS
35083:     VMSideExit* lr = (TraceVisStateObj(cx, S_NATIVE), ExecuteTrace(cx, f, state));
18723: #else
35083:     VMSideExit* lr = ExecuteTrace(cx, f, state);
35083: #endif
56551:     debug_only(int64 t1 = PRMJ_Now();)
35083: 
35083:     JS_ASSERT_IF(lr->exitType == LOOP_EXIT, !lr->calldepth);
35083: 
35083:     /* Restore interpreter state. */
60576: #ifdef DEBUG
60576:     LEAVE_TREE_STATUS lts = 
60576: #endif
37694:         LeaveTree(tm, state, lr);
60576: #ifdef DEBUG
60576:     JS_ASSERT_IF(lts == NO_DEEP_BAIL,
60576:                  *(uint64*)&tm->storage->global()[globalSlots] == 0xdeadbeefdeadbeefLL);
60576: #endif
41777: 
41777:     *lrp = state.innermost;
41777:     bool ok = !(state.builtinStatus & BUILTIN_ERROR);
60211:     JS_ASSERT_IF(cx->isExceptionPending(), !ok);
53578: 
59736:     size_t iters = tm->iterationCounter;
56551: 
56551:     f->execs++;
56551:     f->iters += iters;
56551: 
56551: #ifdef DEBUG
56551:     JSStackFrame *fp = cx->fp();
56551:     const char *prefix = "";
56551:     if (iters == LOOP_COUNT_MAX)
56551:         prefix = ">";
56551:     debug_only_printf(LC_TMMinimal, "  [%.3f ms] Tree at line %u executed for %s%u iterations;"
56551:                       " executed %u times; leave for %s at %s:%u (%s)\n",
56551:                       double(t1-t0) / PRMJ_USEC_PER_MSEC,
56551:                       f->treeLineNumber, prefix, (uintN)iters, f->execs,
56551:                       getExitName(lr->exitType),
56551:                       fp->script()->filename,
56551:                       js_FramePCToLineNumber(cx, fp),
56551:                       js_CodeName[fp->hasImacropc() ? *fp->imacropc() : *cx->regs->pc]);
56551: #endif
56551:     
54560: #ifdef JS_METHODJIT
57802:     if (cx->methodJitEnabled) {
56551:         if (lr->exitType == LOOP_EXIT && f->iters < MIN_LOOP_ITERS
56551:             && f->execs >= LOOP_CHECK_ITERS)
56551:         {
56551:             debug_only_printf(LC_TMMinimal, "  Blacklisting at line %u (executed only %d iters)\n",
56551:                               f->treeLineNumber, f->iters);
53578:             Blacklist((jsbytecode *)f->ip);
53578:         }
53579:     }
54560: #endif
41777:     return ok;
24612: }
24612: 
48619: class Guardian {
48619:     bool *flagp;
48619: public:
48619:     Guardian(bool *flagp) {
48619:         this->flagp = flagp;
48619:         JS_ASSERT(!*flagp);
48619:         *flagp = true;
48619:     }
48619: 
48619:     ~Guardian() {
48619:         JS_ASSERT(*flagp);
48619:         *flagp = false;
48619:     }
48619: };
48619: 
60576: static JS_FORCES_STACK LEAVE_TREE_STATUS
41276: LeaveTree(TraceMonitor *tm, TracerState& state, VMSideExit* lr)
24612: {
24612:     VOUCH_DOES_NOT_REQUIRE_STACK();
24612: 
24612:     JSContext* cx = state.cx;
35078: 
48619:     /* Temporary waive the soft GC quota to make sure LeaveTree() doesn't fail. */
48619:     Guardian waiver(&JS_THREAD_DATA(cx)->waiveGCQuota);
48619: 
24612:     FrameInfo** callstack = state.callstackBase;
24612:     double* stack = state.stackBase;
17923: 
30860:     /*
30860:      * Except if we find that this is a nested bailout, the guard the call
30860:      * returned is the one we have to use to adjust pc and sp.
30860:      */
21521:     VMSideExit* innermost = lr;
20429: 
30860:     /*
30860:      * While executing a tree we do not update state.sp and state.rp even if
30860:      * they grow. Instead, guards tell us by how much sp and rp should be
30860:      * incremented in case of a side exit. When calling a nested tree, however,
30860:      * we actively adjust sp and rp. If we have such frames from outer trees on
30860:      * the stack, then rp will have been adjusted. Before we can process the
30860:      * stack of the frames of the tree we directly exited from, we have to
30860:      * first work our way through the outer frames and generate interpreter
30860:      * frames for them. Once the call stack (rp) is empty, we can process the
30860:      * final frames (which again are not directly visible and only the guard we
30860:      * exited on will tells us about).
30860:      */
23262:     FrameInfo** rp = (FrameInfo**)state.rp;
20931:     if (lr->exitType == NESTED_EXIT) {
21521:         VMSideExit* nested = state.lastTreeCallGuard;
20429:         if (!nested) {
30860:             /*
30860:              * If lastTreeCallGuard is not set in state, we only have a single
30860:              * level of nesting in this exit, so lr itself is the innermost and
30860:              * outermost nested guard, and hence we set nested to lr. The
30860:              * calldepth of the innermost guard is not added to state.rp, so we
30860:              * do it here manually. For a nesting depth greater than 1 the
35344:              * call tree code already added the innermost guard's calldepth
30860:              * to state.rpAtLastTreeCall.
30860:              */
20429:             nested = lr;
20429:             rp += lr->calldepth;
20429:         } else {
30860:             /*
30860:              * During unwinding state.rp gets overwritten at every step and we
30860:              * restore it here to its state at the innermost nested guard. The
30860:              * builtin already added the calldepth of that innermost guard to
30860:              * rpAtLastTreeCall.
30860:              */
23262:             rp = (FrameInfo**)state.rpAtLastTreeCall;
20429:         }
20429:         innermost = state.lastTreeExitGuard;
24612:         if (state.innermostNestedGuardp)
24612:             *state.innermostNestedGuardp = nested;
20429:         JS_ASSERT(nested);
20931:         JS_ASSERT(nested->exitType == NESTED_EXIT);
20429:         JS_ASSERT(state.lastTreeExitGuard);
20931:         JS_ASSERT(state.lastTreeExitGuard->exitType != NESTED_EXIT);
19590:     }
21433: 
27166:     int32_t bs = state.builtinStatus;
37741:     bool bailed = innermost->exitType == STATUS_EXIT && (bs & BUILTIN_BAILED);
24870:     if (bailed) {
24612:         /*
24612:          * Deep-bail case.
24612:          *
24612:          * A _FAIL native already called LeaveTree. We already reconstructed
24612:          * the interpreter stack, in pre-call state, with pc pointing to the
24612:          * CALL/APPLY op, for correctness. Then we continued in native code.
53557:          */
59879:         if (!(bs & BUILTIN_ERROR)) {
24870:             /*
31444:              * The builtin or native deep-bailed but finished successfully
31444:              * (no exception or error).
31444:              *
31444:              * After it returned, the JIT code stored the results of the
31444:              * builtin or native at the top of the native stack and then
31444:              * immediately flunked the guard on state->builtinStatus.
24612:              *
24612:              * Now LeaveTree has been called again from the tail of
30860:              * ExecuteTree. We are about to return to the interpreter. Adjust
24612:              * the top stack frame to resume on the next op.
24612:              */
42717:             JSFrameRegs* regs = cx->regs;
31444:             JSOp op = (JSOp) *regs->pc;
57712:             JS_ASSERT(op == JSOP_CALL || op == JSOP_FUNAPPLY || op == JSOP_FUNCALL || op == JSOP_NEW ||
32557:                       op == JSOP_GETPROP || op == JSOP_GETTHISPROP || op == JSOP_GETARGPROP ||
32557:                       op == JSOP_GETLOCALPROP || op == JSOP_LENGTH ||
37688:                       op == JSOP_GETELEM || op == JSOP_CALLELEM || op == JSOP_CALLPROP ||
32658:                       op == JSOP_SETPROP || op == JSOP_SETNAME || op == JSOP_SETMETHOD ||
35477:                       op == JSOP_SETELEM || op == JSOP_INITELEM || op == JSOP_ENUMELEM ||
42641:                       op == JSOP_INSTANCEOF ||
42641:                       op == JSOP_ITER || op == JSOP_MOREITER || op == JSOP_ENDITER ||
42641:                       op == JSOP_FORARG || op == JSOP_FORLOCAL ||
43262:                       op == JSOP_FORNAME || op == JSOP_FORPROP || op == JSOP_FORELEM ||
43262:                       op == JSOP_DELPROP || op == JSOP_DELELEM);
34339: 
34339:             /*
34339:              * JSOP_SETELEM can be coalesced with a JSOP_POP in the interpeter.
34339:              * Since this doesn't re-enter the recorder, the post-state snapshot
34339:              * is invalid. Fix it up here.
34339:              */
34339:             if (op == JSOP_SETELEM && JSOp(regs->pc[JSOP_SETELEM_LENGTH]) == JSOP_POP) {
34339:                 regs->sp -= js_CodeSpec[JSOP_SETELEM].nuses;
34339:                 regs->sp += js_CodeSpec[JSOP_SETELEM].ndefs;
34339:                 regs->pc += JSOP_SETELEM_LENGTH;
34339:                 op = JSOP_POP;
34339:             }
34339: 
31444:             const JSCodeSpec& cs = js_CodeSpec[op];
31444:             regs->sp -= (cs.format & JOF_INVOKE) ? GET_ARGC(regs->pc) + 2 : cs.nuses;
31444:             regs->sp += cs.ndefs;
31444:             regs->pc += cs.length;
53840:             JS_ASSERT_IF(!cx->fp()->hasImacropc(),
53840:                          cx->fp()->slots() + cx->fp()->numFixed() +
53840:                          js_ReconstructStackDepth(cx, cx->fp()->script(), regs->pc) ==
31444:                          regs->sp);
30287: 
30287:             /*
30287:              * If there's a tree call around the point that we deep exited at,
30287:              * then state.sp and state.rp were restored to their original
30287:              * values before the tree call and sp might be less than deepBailSp,
30287:              * which we sampled when we were told to deep bail.
30287:              */
30287:             JS_ASSERT(state.deepBailSp >= state.stackBase && state.sp <= state.deepBailSp);
31444: 
31444:             /*
31444:              * As explained above, the JIT code stored a result value or values
31444:              * on the native stack. Transfer them to the interpreter stack now.
31444:              * (Some opcodes, like JSOP_CALLELEM, produce two values, hence the
31444:              * loop.)
31444:              */
48470:             JSValueType* typeMap = innermost->stackTypeMap();
31444:             for (int i = 1; i <= cs.ndefs; i++) {
48470:                 NativeToValue(cx,
31444:                               regs->sp[-i],
31444:                               typeMap[innermost->numStackSlots - i],
31444:                               (jsdouble *) state.deepBailSp
48470:                               + innermost->sp_adj / sizeof(jsdouble) - i);
31444:             }
24870:         }
60576:         return DEEP_BAILED;
24612:     }
24612: 
19588:     while (callstack < rp) {
31924:         FrameInfo* fi = *callstack;
53840:         /* Peek at the callee native slot in the not-yet-synthesized prev frame. */
31924:         JSObject* callee = *(JSObject**)&stack[fi->callerHeight];
31924: 
30860:         /*
53840:          * Flush the slots for cx->fp() (which will become cx->fp()->prev after
53840:          * SynthesizeFrame). Since a frame's arguments (including callee
53840:          * and thisv) are part of the frame, we only want to flush up to the
53840:          * next frame's arguments, so set cx->regs->sp to to not include said
53840:          * arguments. The upcoming call to SynthesizeFrame will reset regs->sp
53840:          * to its correct value.
53840:          */
53840:         cx->regs->sp = cx->fp()->slots() + (fi->spdist - (2 + fi->get_argc()));
53840:         int slots = FlushNativeStackFrame(cx, 0 /* callDepth */, fi->get_typemap(), stack);
53840: 
53840:         /* Finish initializing cx->fp() and push a new cx->fp(). */
31924:         SynthesizeFrame(cx, *fi, callee);
19588: #ifdef DEBUG
51446:         JSStackFrame* fp = cx->fp();
29883:         debug_only_printf(LC_TMTracer,
33564:                           "synthesized deep frame for %s:%u@%u, slots=%d, fi=%p\n",
53840:                           fp->script()->filename,
21685:                           js_FramePCToLineNumber(cx, fp),
42717:                           FramePCOffset(cx, fp),
33564:                           slots,
33567:                           (void*)*callstack);
19588: #endif
30860:         /*
30860:          * Keep track of the additional frames we put on the interpreter stack
30860:          * and the native stack slots we consumed.
30860:          */
24612:         ++*state.inlineCallCountp;
19588:         ++callstack;
18170:         stack += slots;
18164:     }
19588: 
30860:     /*
30860:      * We already synthesized the frames around the innermost guard. Here we
30860:      * just deal with additional frames inside the tree we are bailing out
30860:      * from.
30860:      */
19588:     JS_ASSERT(rp == callstack);
20429:     unsigned calldepth = innermost->calldepth;
31924:     unsigned calleeOffset = 0;
19588:     for (unsigned n = 0; n < calldepth; ++n) {
53840:         /* Peek at the callee native slot in the not-yet-synthesized prev frame. */
31924:         calleeOffset += callstack[n]->callerHeight;
31924:         JSObject* callee = *(JSObject**)&stack[calleeOffset];
31924: 
31924:         /* Reconstruct the frame. */
53840:         SynthesizeFrame(cx, *callstack[n], callee);
24612:         ++*state.inlineCallCountp;
19588: #ifdef DEBUG
51446:         JSStackFrame* fp = cx->fp();
29883:         debug_only_printf(LC_TMTracer,
29883:                           "synthesized shallow frame for %s:%u@%u\n",
53840:                           fp->script()->filename, js_FramePCToLineNumber(cx, fp),
42717:                           FramePCOffset(cx, fp));
19588: #endif
19588:     }
17923: 
30860:     /*
30860:      * Adjust sp and pc relative to the tree we exited from (not the tree we
30860:      * entered into).  These are our final values for sp and pc since
54855:      * SynthesizeFrame has already taken care of all frames in between.
30860:      */
51446:     JSStackFrame* const fp = cx->fp();
18226: 
30860:     /*
30860:      * If we are not exiting from an inlined frame, the state->sp is spbase.
30860:      * Otherwise spbase is whatever slots frames around us consume.
30860:      */
42717:     cx->regs->pc = innermost->pc;
51055:     if (innermost->imacpc)
53840:         fp->setImacropc(innermost->imacpc);
51055:     else
53840:         fp->clearImacropc();
53840: 
53840:     /*
53840:      * Set cx->regs->regs for the top frame. Since the top frame does not have a
53840:      * FrameInfo (a FrameInfo is only pushed for calls), we basically need to
53840:      * compute the offset from fp->slots() to the top of the stack based on the
53840:      * number of native slots allocated for this function.
53840:      *
53840:      * Duplicate native stack layout computation: see VisitFrameSlots header comment.
53840:      */
53840:     uintN slotOffset = innermost->numStackSlots - innermost->numStackSlotsBelowCurrentFrame;
53840:     if (fp->isGlobalFrame()) {
53840:         /* Global nfixed slots are not kept on the native stack, so add them back. */
53840:         slotOffset += fp->globalScript()->nfixed;
53840:     } else {
53840:         /* A frame's native slots includes args and frame ptrs, so strip them off. */
53840:         slotOffset -= NumSlotsBeforeFixed(fp);
53840:     }
53840:     cx->regs->sp = fp->slots() + slotOffset;
53840: 
53840:     /* Assert that we computed sp correctly. */
53840:     JS_ASSERT_IF(!fp->hasImacropc(),
53840:                  fp->slots() + fp->numFixed() +
53840:                  js_ReconstructStackDepth(cx, fp->script(), cx->regs->pc) == cx->regs->sp);
17923: 
24612: #ifdef EXECUTE_TREE_TIMER
24612:     uint64 cycles = rdtsc() - state.startTime;
21459: #elif defined(JS_JIT_SPEW)
19040:     uint64 cycles = 0;
18788: #endif
29883:     debug_only_printf(LC_TMTracer,
33132:                       "leaving trace at %s:%u@%u, op=%s, lr=%p, exitType=%s, sp=%lld, "
19588:                       "calldepth=%d, cycles=%llu\n",
53840:                       fp->script()->filename,
21685:                       js_FramePCToLineNumber(cx, fp),
42717:                       FramePCOffset(cx, fp),
53840:                       js_CodeName[fp->hasImacropc() ? *fp->imacropc() : *cx->regs->pc],
25469:                       (void*)lr,
29894:                       getExitName(lr->exitType),
48470:                       (long long int)(cx->regs->sp - fp->base()),
19588:                       calldepth,
33132:                       (unsigned long long int)cycles);
17923: 
53840: #ifdef DEBUG
53840:     int slots =
53840: #endif
53840:         FlushNativeStackFrame(cx, innermost->calldepth, innermost->stackTypeMap(), stack);
53840:     JS_ASSERT(unsigned(slots) == innermost->numStackSlots);
53840: 
30860:     /*
30860:      * If this trace is part of a tree, later branches might have added
30860:      * additional globals for which we don't have any type information
30860:      * available in the side exit. We merge in this information from the entry
30860:      * type-map. See also the comment in the constructor of TraceRecorder
30860:      * regarding why this is always safe to do.
30860:      */
36361:     TreeFragment* outermostTree = state.outermostTree;
24612:     uint16* gslots = outermostTree->globalSlots->data();
24612:     unsigned ngslots = outermostTree->globalSlots->length();
24612:     JS_ASSERT(ngslots == outermostTree->nGlobalTypes());
48470:     JSValueType* globalTypeMap;
25491: 
30860:     /* Are there enough globals? */
37694:     TypeMap& typeMap = *tm->cachedTempTypeMap;
37694:     typeMap.clear();
25491:     if (innermost->numGlobalSlots == ngslots) {
30860:         /* Yes. This is the ideal fast path. */
31495:         globalTypeMap = innermost->globalTypeMap();
25491:     } else {
30860:         /*
30860:          * No. Merge the typemap of the innermost entry and exit together. This
30860:          * should always work because it is invalid for nested trees or linked
30860:          * trees to have incompatible types. Thus, whenever a new global type
30860:          * is lazily added into a tree, all dependent and linked trees are
30860:          * immediately specialized (see bug 476653).
30860:          */
36361:         JS_ASSERT(innermost->root()->nGlobalTypes() == ngslots);
36361:         JS_ASSERT(innermost->root()->nGlobalTypes() > innermost->numGlobalSlots);
31913:         typeMap.ensure(ngslots);
31913: #ifdef DEBUG
31913:         unsigned check_ngslots =
31913: #endif
31913:         BuildGlobalTypeMapFromInnerTree(typeMap, innermost);
31913:         JS_ASSERT(check_ngslots == ngslots);
31913:         globalTypeMap = typeMap.data();
25491:     }
18200: 
30860:     /* Write back interned globals. */
34572:     JS_ASSERT(state.eos == state.stackBase + MAX_NATIVE_STACK_SLOTS);
36361:     JSObject* globalObj = outermostTree->globalObj;
35083:     FlushNativeGlobalFrame(cx, globalObj, state.eos, ngslots, gslots, globalTypeMap);
53840: 
23706: #ifdef JS_JIT_SPEW
23706:     if (innermost->exitType != TIMEOUT_EXIT)
17726:         AUDIT(sideExitIntoInterpreter);
23706:     else
23706:         AUDIT(timeoutIntoInterpreter);
23706: #endif
17397: 
24612:     state.innermost = innermost;
60576:     return NO_DEEP_BAIL;
17772: }
17376: 
57807: static jsbytecode *
57807: GetLoopBottom(JSContext *cx, jsbytecode *pc)
57807: {
57807:     JS_ASSERT(*pc == JSOP_TRACE || *pc == JSOP_NOTRACE);
57807:     JSScript *script = cx->fp()->script();
57807:     jssrcnote *sn = js_GetSrcNote(script, pc);
57807:     if (!sn)
57807:         return NULL;
57807:     return pc + js_GetSrcNoteOffset(sn, 0);
57807: }
57807: 
57807: JS_ALWAYS_INLINE void
57807: TraceRecorder::assertInsideLoop()
57807: {
57807: #ifdef DEBUG
57807:     /* Asserts at callDepth == 0 will catch problems at the call op. */
57807:     if (callDepth > 0)
57807:         return;
57807: 
57807:     jsbytecode *pc = cx->regs->fp->hasImacropc() ? cx->regs->fp->imacropc() : cx->regs->pc;
57807:     jsbytecode *beg = (jsbytecode *)tree->ip;
57807:     jsbytecode *end = GetLoopBottom(cx, beg);
57807: 
57807:     /*
57807:      * In some cases (continue in a while loop), we jump to the goto
57807:      * immediately preceeding a loop (the one that jumps to the loop
57807:      * condition).
57807:      */
57807:     JS_ASSERT(pc >= beg - JSOP_GOTO_LENGTH && pc <= end);
57807: #endif
57807: }
57807: 
41777: JS_REQUIRES_STACK MonitorResult
57880: RecordLoopEdge(JSContext* cx, uintN& inlineCallCount)
17939: {
29368: #ifdef MOZ_TRACEVIS
31063:     TraceVisStateObj tvso(cx, S_MONITOR);
29368: #endif
29368: 
37741:     TraceMonitor* tm = &JS_TRACE_MONITOR(cx);
17939: 
60157:     JS_ASSERT(!tm->profile);
60157: 
18672:     /* Is the recorder currently active? */
17954:     if (tm->recorder) {
57807:         tm->recorder->assertInsideLoop();
42717:         jsbytecode* pc = cx->regs->pc;
36380:         if (pc == tm->recorder->tree->ip) {
36380:             tm->recorder->closeLoop();
36380:         } else {
41777:             MonitorResult r = TraceRecorder::recordLoopEdge(cx, tm->recorder, inlineCallCount);
41777:             JS_ASSERT((r == MONITOR_RECORDING) == (TRACE_RECORDER(cx) != NULL));
41777:             if (r == MONITOR_RECORDING || r == MONITOR_ERROR)
41777:                 return r;
41777: 
24493:             /*
34351:              * recordLoopEdge will invoke an inner tree if we have a matching
30860:              * one. If we arrive here, that tree didn't run to completion and
30860:              * instead we mis-matched or the inner tree took a side exit other than
30860:              * the loop exit. We are thus no longer guaranteed to be parked on the
56551:              * same loop header RecordLoopEdge was called for. In fact, this
30860:              * might not even be a loop header at all. Hence if the program counter
30860:              * no longer hovers over the inner loop header, return to the
30860:              * interpreter and do not attempt to trigger or record a new tree at
30860:              * this location.
24493:              */
42717:             if (pc != cx->regs->pc) {
29368: #ifdef MOZ_TRACEVIS
29368:                 tvso.r = R_INNER_SIDE_EXIT;
29368: #endif
41777:                 return MONITOR_NOT_RECORDING;
17954:             }
29368:         }
36380:     }
18317:     JS_ASSERT(!tm->recorder);
17939: 
30860:     /*
30860:      * Make sure the shape of the global object still matches (this might flush
30860:      * the JIT cache).
30860:      */
53840:     JSObject* globalObj = cx->fp()->scopeChain().getGlobal();
24491:     uint32 globalShape = -1;
24491:     SlotList* globalSlots = NULL;
24491: 
28105:     if (!CheckGlobalObjectShape(cx, tm, globalObj, &globalShape, &globalSlots)) {
42717:         Backoff(cx, cx->regs->pc);
41777:         return MONITOR_NOT_RECORDING;
28105:     }
21514: 
25087:     /* Do not enter the JIT code with a pending operation callback. */
53161:     if (JS_THREAD_DATA(cx)->interruptFlags) {
29368: #ifdef MOZ_TRACEVIS
29368:         tvso.r = R_CALLBACK_PENDING;
29368: #endif
41777:         return MONITOR_NOT_RECORDING;
29368:     }
25087: 
42717:     jsbytecode* pc = cx->regs->pc;
53840:     uint32 argc = entryFrameArgc(cx);
28244: 
35044:     TreeFragment* f = LookupOrAddLoop(tm, pc, globalObj, globalShape, argc);
21433: 
30860:     /*
30860:      * If we have no code in the anchor and no peers, we definitively won't be
30860:      * able to activate any trees, so start compiling.
30860:      */
21433:     if (!f->code() && !f->peer) {
25627:     record:
29368:         if (++f->hits() < HOTLOOP) {
29368: #ifdef MOZ_TRACEVIS
29368:             tvso.r = f->hits() < 1 ? R_BACKED_OFF : R_COLD;
29368: #endif
41777:             return MONITOR_NOT_RECORDING;
29368:         }
30860: 
41111:         if (!ScopeChainCheck(cx, f)) {
41111: #ifdef MOZ_TRACEVIS
41111:             tvso.r = R_FAIL_SCOPE_CHAIN_CHECK;
41111: #endif
41777:             return MONITOR_NOT_RECORDING;
41111:         }
41111: 
30860:         /*
30860:          * We can give RecordTree the root peer. If that peer is already taken,
30860:          * it will walk the peer list and find us a free slot or allocate a new
30860:          * tree if needed.
30860:          */
58070:         bool rv = RecordTree(cx, f->first, NULL, NULL, 0, globalSlots);
29368: #ifdef MOZ_TRACEVIS
29368:         if (!rv)
29368:             tvso.r = R_FAIL_RECORD_TREE;
29368: #endif
41777:         return RecordingIfTrue(rv);
21433:     }
25627: 
29883:     debug_only_printf(LC_TMTracer,
29883:                       "Looking for compat peer %d@%d, from %p (ip: %p)\n",
51446:                       js_FramePCToLineNumber(cx, cx->fp()),
51446:                       FramePCOffset(cx, cx->fp()), (void*)f, f->ip);
25627: 
25627:     uintN count;
35044:     TreeFragment* match = FindVMCompatiblePeer(cx, globalObj, f, count);
25627:     if (!match) {
25627:         if (count < MAXPEERS)
25627:             goto record;
30860: 
30860:         /*
30860:          * If we hit the max peers ceiling, don't try to lookup fragments all
30860:          * the time. That's expensive. This must be a rather type-unstable loop.
30860:          */
29883:         debug_only_print0(LC_TMTracer, "Blacklisted: too many peer trees.\n");
30860:         Blacklist((jsbytecode*) f->root->ip);
29368: #ifdef MOZ_TRACEVIS
29368:         tvso.r = R_MAX_PEERS;
29368: #endif
41777:         return MONITOR_NOT_RECORDING;
25627:     }
21433: 
21521:     VMSideExit* lr = NULL;
21521:     VMSideExit* innermostNestedGuard = NULL;
21433: 
41777:     if (!ExecuteTree(cx, match, inlineCallCount, &innermostNestedGuard, &lr))
41777:         return MONITOR_ERROR;
41777: 
29368:     if (!lr) {
29368: #ifdef MOZ_TRACEVIS
29368:         tvso.r = R_FAIL_EXECUTE_TREE;
29368: #endif
41777:         return MONITOR_NOT_RECORDING;
29368:     }
21433: 
30860:     /*
30860:      * If we exit on a branch, or on a tree call guard, try to grow the inner
30860:      * tree (in case of a branch exit), or the tree nested around the tree we
30860:      * exited from (in case of the tree call guard).
30860:      */
29368:     bool rv;
20931:     switch (lr->exitType) {
21433:       case UNSTABLE_LOOP_EXIT:
58070:         rv = AttemptToStabilizeTree(cx, globalObj, lr, NULL, NULL, 0);
29368: #ifdef MOZ_TRACEVIS
29368:         if (!rv)
29368:             tvso.r = R_FAIL_STABILIZE;
29368: #endif
41777:         return RecordingIfTrue(rv);
30860: 
56185:       case MUL_ZERO_EXIT:
29354:       case OVERFLOW_EXIT:
56185:         if (lr->exitType == MUL_ZERO_EXIT)
56185:             tm->oracle->markInstructionSlowZeroTest(cx->regs->pc);
56185:         else
42717:             tm->oracle->markInstructionUndemotable(cx->regs->pc);
30860:         /* FALL THROUGH */
18284:       case BRANCH_EXIT:
25099:       case CASE_EXIT:
56217:         rv = AttemptToExtendTree(cx, lr, NULL, NULL, NULL
29368: #ifdef MOZ_TRACEVIS
29368:                                                    , &tvso
29368: #endif
29368:                                  );
41777:         return RecordingIfTrue(rv);
30860: 
18284:       case LOOP_EXIT:
41777:         if (innermostNestedGuard) {
56217:             rv = AttemptToExtendTree(cx, innermostNestedGuard, lr, NULL, NULL
29368: #ifdef MOZ_TRACEVIS
29368:                                                                        , &tvso
29368: #endif
29368:                                      );
41777:             return RecordingIfTrue(rv);
41777:         }
29368: #ifdef MOZ_TRACEVIS
29368:         tvso.r = R_NO_EXTEND_OUTER;
29368: #endif
41777:         return MONITOR_NOT_RECORDING;
30860: 
29368: #ifdef MOZ_TRACEVIS
43049:       case MISMATCH_EXIT:
43049:         tvso.r = R_MISMATCH_EXIT;
43049:         return MONITOR_NOT_RECORDING;
43049:       case OOM_EXIT:
43049:         tvso.r = R_OOM_EXIT;
43049:         return MONITOR_NOT_RECORDING;
43049:       case TIMEOUT_EXIT:
43049:         tvso.r = R_TIMEOUT_EXIT;
43049:         return MONITOR_NOT_RECORDING;
43049:       case DEEP_BAIL_EXIT:
43049:         tvso.r = R_DEEP_BAIL_EXIT;
43049:         return MONITOR_NOT_RECORDING;
43049:       case STATUS_EXIT:
43049:         tvso.r = R_STATUS_EXIT;
43049:         return MONITOR_NOT_RECORDING;
29368: #endif
30860: 
18284:       default:
30860:         /*
30860:          * No, this was an unusual exit (i.e. out of memory/GC), so just resume
30860:          * interpretation.
30860:          */
29368: #ifdef MOZ_TRACEVIS
29368:         tvso.r = R_OTHER_EXIT;
29368: #endif
41777:         return MONITOR_NOT_RECORDING;
18284:     }
17939: }
17939: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
35083: TraceRecorder::monitorRecording(JSOp op)
35083: {
60780:     JS_ASSERT(!addPropShapeBefore);
60780: 
37741:     TraceMonitor &localtm = JS_TRACE_MONITOR(cx);
35115:     debug_only_stmt( JSContext *localcx = cx; )
57807:     assertInsideLoop();
60157:     JS_ASSERT(!localtm.profile);
33171: 
33171:     /* Process needFlush requests now. */
33171:     if (localtm.needFlush) {
32748:         ResetJIT(cx, FR_DEEP_BAIL);
33542:         return ARECORD_ABORTED;
27493:     }
35083:     JS_ASSERT(!fragment->lastIns);
26559: 
26557:     /*
26557:      * Clear one-shot state used to communicate between record_JSOP_CALL and post-
28086:      * opcode-case-guts record hook (record_NativeCallComplete).
26557:      */
35083:     pendingSpecializedNative = NULL;
35083:     newobj_ins = NULL;
60559:     pendingGlobalSlotsToSet.clear();
19068: 
33180:     /* Handle one-shot request from finishGetProp or INSTANCEOF to snapshot post-op state and guard. */
35083:     if (pendingGuardCondition) {
56222:         LIns* cond = pendingGuardCondition;
56222:         bool expected = true;
56222:         /* Put 'cond' in a form suitable for a guard/branch condition if it's not already. */
56222:         ensureCond(&cond, &expected);
56222:         guard(expected, cond, STATUS_EXIT);
35083:         pendingGuardCondition = NULL;
31444:     }
31444: 
60155:     /* Handle one-shot request to unbox the result of a property get or ObjectToIterator. */
35083:     if (pendingUnboxSlot) {
35083:         LIns* val_ins = get(pendingUnboxSlot);
48470:         /*
48470:          * We need to know from where to unbox the value. Since pendingUnboxSlot
48470:          * is only set in finishGetProp, we can depend on LIns* tracked for
48470:          * pendingUnboxSlot to have this information.
48470:          */
48470:         LIns* unboxed_ins = unbox_value(*pendingUnboxSlot,
56750:                                         AnyAddress(val_ins->oprnd1(), val_ins->disp()),
48470:                                         snapshot(BRANCH_EXIT));
48470:         set(pendingUnboxSlot, unboxed_ins);
35083:         pendingUnboxSlot = 0;
31444:     }
31444: 
29883:     debug_only_stmt(
37741:         if (LogController.lcbits & LC_TMRecorder) {
56733:             debug_only_print0(LC_TMRecorder, "\n");
53840:             js_Disassemble1(cx, cx->fp()->script(), cx->regs->pc,
53840:                             cx->fp()->hasImacropc()
53840:                                 ? 0 : cx->regs->pc - cx->fp()->script()->code,
53840:                             !cx->fp()->hasImacropc(), stdout);
29883:         }
29883:     )
19075: 
30860:     /*
30860:      * If op is not a break or a return from a loop, continue recording and
30860:      * follow the trace. We check for imacro-calling bytecodes inside each
30860:      * switch case to resolve the if (JSOP_IS_IMACOP(x)) conditions at compile
30860:      * time.
30860:      */
26011: 
33542:     AbortableRecordingStatus status;
26563: #ifdef DEBUG
53840:     bool wasInImacro = (cx->fp()->hasImacropc());
26563: #endif
23111:     switch (op) {
28008:       default:
41777:           AbortRecording(cx, "unsupported opcode");
33542:           status = ARECORD_ERROR;
41777:           break;
56741: # define OPDEF(op,val,name,token,length,nuses,ndefs,prec,format)              \
56741:       case op:                                                                \
56750:         w.comment(#op);                                                       \
56741:         status = this->record_##op();                                         \
23111:         break;
23111: # include "jsopcode.tbl"
23111: # undef OPDEF
23111:     }
23111: 
41777:     /* N.B. |this| may have been deleted. */
41777: 
41777:     if (!JSOP_IS_IMACOP(op)) {
35083:         JS_ASSERT(status != ARECORD_IMACRO);
53840:         JS_ASSERT_IF(!wasInImacro, !localcx->fp()->hasImacropc());
41777:     }
41777: 
41777:     if (localtm.recorder) {
41777:         JS_ASSERT(status != ARECORD_ABORTED);
41777:         JS_ASSERT(localtm.recorder == this);
41777: 
41777:         /* |this| recorder completed, but a new one started; keep recording. */
41777:         if (status == ARECORD_COMPLETED)
41777:             return ARECORD_CONTINUE;
41777: 
41777:         /* Handle lazy aborts; propagate the 'error' status. */
41777:         if (StatusAbortsRecorderIfActive(status)) {
41777:             AbortRecording(cx, js_CodeName[op]);
41777:             return status == ARECORD_ERROR ? ARECORD_ERROR : ARECORD_ABORTED;
41777:         }
41777: 
59733:         if (outOfMemory() || OverfullJITCache(cx, &localtm)) {
32748:             ResetJIT(cx, FR_OOM);
43801: 
43801:             /*
43801:              * If the status returned was ARECORD_IMACRO, then we just
43801:              * changed cx->regs, we need to tell the interpreter to sync
43801:              * its local variables.
43801:              */
43801:             return status == ARECORD_IMACRO ? ARECORD_IMACRO_ABORTED : ARECORD_ABORTED;
27933:         }
41777:     } else {
41777:         JS_ASSERT(status == ARECORD_COMPLETED ||
41777:                   status == ARECORD_ABORTED ||
41777:                   status == ARECORD_ERROR);
41777:     }
35083:     return status;
35083: }
35083: 
54835: JS_REQUIRES_STACK TraceRecorder::AbortResult
37741: AbortRecording(JSContext* cx, const char* reason)
17350: {
25627: #ifdef DEBUG
35083:     JS_ASSERT(TRACE_RECORDER(cx));
54835:     return TRACE_RECORDER(cx)->finishAbort(reason);
35083: #else
54835:     return TRACE_RECORDER(cx)->finishAbort("[no reason]");
35083: #endif
17350: }
17350: 
36489: #if defined NANOJIT_IA32
36489: static bool
36489: CheckForSSE2()
36489: {
36489:     char *c = getenv("X86_FORCE_SSE2");
36489:     if (c)
36489:         return (!strcmp(c, "true") ||
36489:                 !strcmp(c, "1") ||
36489:                 !strcmp(c, "yes"));
36489: 
36489:     int features = 0;
36489: #if defined _MSC_VER
36489:     __asm
36489:     {
36489:         pushad
36489:         mov eax, 1
36489:         cpuid
36489:         mov features, edx
36489:         popad
36489:     }
36489: #elif defined __GNUC__
36489:     asm("xchg %%esi, %%ebx\n" /* we can't clobber ebx on gcc (PIC register) */
36489:         "mov $0x01, %%eax\n"
36489:         "cpuid\n"
36489:         "mov %%edx, %0\n"
36489:         "xchg %%esi, %%ebx\n"
36489:         : "=m" (features)
36489:         : /* We have no inputs */
36489:         : "%eax", "%esi", "%ecx", "%edx"
36489:        );
36489: #elif defined __SUNPRO_C || defined __SUNPRO_CC
36489:     asm("push %%ebx\n"
36489:         "mov $0x01, %%eax\n"
36489:         "cpuid\n"
36489:         "pop %%ebx\n"
36489:         : "=d" (features)
36489:         : /* We have no inputs */
36489:         : "%eax", "%ecx"
36489:        );
36489: #endif
36489:     return (features & (1<<26)) != 0;
36489: }
36489: #endif
36489: 
26545: #if defined(NANOJIT_ARM)
26545: 
26545: #if defined(_MSC_VER) && defined(WINCE)
26545: 
26545: // these come in from jswince.asm
28185: extern "C" int js_arm_try_armv5_op();
28185: extern "C" int js_arm_try_armv6_op();
28185: extern "C" int js_arm_try_armv7_op();
26545: extern "C" int js_arm_try_vfp_op();
26545: 
28185: static unsigned int
37741: arm_check_arch()
37741: {
28185:     unsigned int arch = 4;
28185:     __try {
28185:         js_arm_try_armv5_op();
28185:         arch = 5;
28185:         js_arm_try_armv6_op();
28185:         arch = 6;
28185:         js_arm_try_armv7_op();
28185:         arch = 7;
28185:     } __except(GetExceptionCode() == EXCEPTION_ILLEGAL_INSTRUCTION) {
28185:     }
28185:     return arch;
28185: }
28185: 
26545: static bool
37741: arm_check_vfp()
37741: {
33060: #ifdef WINCE_WINDOWS_MOBILE
33060:     return false;
33060: #else
26545:     bool ret = false;
26545:     __try {
26545:         js_arm_try_vfp_op();
26545:         ret = true;
26545:     } __except(GetExceptionCode() == EXCEPTION_ILLEGAL_INSTRUCTION) {
26545:         ret = false;
26545:     }
26545:     return ret;
33060: #endif
26545: }
26545: 
31723: #define HAVE_ENABLE_DISABLE_DEBUGGER_EXCEPTIONS 1
31723: 
31723: /* See "Suppressing Exception Notifications while Debugging", at
31723:  * http://msdn.microsoft.com/en-us/library/ms924252.aspx
31723:  */
31723: static void
37741: disable_debugger_exceptions()
32777: {
31723:     // 2 == TLSSLOT_KERNEL
31723:     DWORD kctrl = (DWORD) TlsGetValue(2);
31723:     // 0x12 = TLSKERN_NOFAULT | TLSKERN_NOFAULTMSG
31723:     kctrl |= 0x12;
31723:     TlsSetValue(2, (LPVOID) kctrl);
31723: }
31723: 
31723: static void
37741: enable_debugger_exceptions()
32777: {
31723:     // 2 == TLSSLOT_KERNEL
31723:     DWORD kctrl = (DWORD) TlsGetValue(2);
31723:     // 0x12 = TLSKERN_NOFAULT | TLSKERN_NOFAULTMSG
31723:     kctrl &= ~0x12;
31723:     TlsSetValue(2, (LPVOID) kctrl);
31723: }
31723: 
26545: #elif defined(__GNUC__) && defined(AVMPLUS_LINUX)
26545: 
28185: // Assume ARMv4 by default.
28185: static unsigned int arm_arch = 4;
26545: static bool arm_has_vfp = false;
26545: static bool arm_has_neon = false;
26545: static bool arm_has_iwmmxt = false;
26545: static bool arm_tests_initialized = false;
26545: 
40165: #ifdef ANDROID
43029: // we're actually reading /proc/cpuinfo, but oh well
43029: static void
43029: arm_read_auxv()
43029: {
43029:   char buf[1024];
43029:   char* pos;
43029:   const char* ver_token = "CPU architecture: ";
43029:   FILE* f = fopen("/proc/cpuinfo", "r");
43029:   fread(buf, sizeof(char), 1024, f);
43029:   fclose(f);
43029:   pos = strstr(buf, ver_token);
43029:   if (pos) {
43029:     int ver = *(pos + strlen(ver_token)) - '0';
43029:     arm_arch = ver;
43029:   }
43029:   arm_has_neon = strstr(buf, "neon") != NULL;
43029:   arm_has_vfp = strstr(buf, "vfp") != NULL;
43029:   arm_has_iwmmxt = strstr(buf, "iwmmxt") != NULL;
43029:   arm_tests_initialized = true;
43029: }
43029: 
43029: #else
40165: 
26545: static void
37741: arm_read_auxv()
37741: {
26545:     int fd;
26545:     Elf32_auxv_t aux;
26545: 
26545:     fd = open("/proc/self/auxv", O_RDONLY);
26545:     if (fd > 0) {
26545:         while (read(fd, &aux, sizeof(Elf32_auxv_t))) {
26545:             if (aux.a_type == AT_HWCAP) {
26545:                 uint32_t hwcap = aux.a_un.a_val;
26545:                 if (getenv("ARM_FORCE_HWCAP"))
26545:                     hwcap = strtoul(getenv("ARM_FORCE_HWCAP"), NULL, 0);
41218:                 else if (getenv("_SBOX_DIR"))
41218:                     continue;  // Ignore the rest, if we're running in scratchbox
26545:                 // hardcode these values to avoid depending on specific versions
26545:                 // of the hwcap header, e.g. HWCAP_NEON
26545:                 arm_has_vfp = (hwcap & 64) != 0;
26545:                 arm_has_iwmmxt = (hwcap & 512) != 0;
26545:                 // this flag is only present on kernel 2.6.29
26545:                 arm_has_neon = (hwcap & 4096) != 0;
26545:             } else if (aux.a_type == AT_PLATFORM) {
26545:                 const char *plat = (const char*) aux.a_un.a_val;
26545:                 if (getenv("ARM_FORCE_PLATFORM"))
26545:                     plat = getenv("ARM_FORCE_PLATFORM");
41218:                 else if (getenv("_SBOX_DIR"))
41218:                     continue;  // Ignore the rest, if we're running in scratchbox
28185:                 // The platform string has the form "v[0-9][lb]". The "l" or "b" indicate little-
28185:                 // or big-endian variants and the digit indicates the version of the platform.
28185:                 // We can only accept ARMv4 and above, but allow anything up to ARMv9 for future
28185:                 // processors. Architectures newer than ARMv7 are assumed to be
28185:                 // backwards-compatible with ARMv7.
28185:                 if ((plat[0] == 'v') &&
28185:                     (plat[1] >= '4') && (plat[1] <= '9') &&
28185:                     ((plat[2] == 'l') || (plat[2] == 'b')))
28185:                 {
28185:                     arm_arch = plat[1] - '0';
28185:                 }
26545:             }
26545:         }
26545:         close (fd);
26545: 
26545:         // if we don't have 2.6.29, we have to do this hack; set
26545:         // the env var to trust HWCAP.
28185:         if (!getenv("ARM_TRUST_HWCAP") && (arm_arch >= 7))
26545:             arm_has_neon = true;
26545:     }
26545: 
26545:     arm_tests_initialized = true;
26545: }
26545: 
43029: #endif
43029: 
38525: static unsigned int
38525: arm_check_arch()
37741: {
26545:     if (!arm_tests_initialized)
26545:         arm_read_auxv();
26545: 
38525:     return arm_arch;
28185: }
28185: 
28185: static bool
38525: arm_check_vfp()
37741: {
28185:     if (!arm_tests_initialized)
28185:         arm_read_auxv();
28185: 
26545:     return arm_has_vfp;
26545: }
26545: 
26545: #else
28185: #warning Not sure how to check for architecture variant on your platform. Assuming ARMv4.
28185: static unsigned int
37741: arm_check_arch() { return 4; }
26548: static bool
37741: arm_check_vfp() { return false; }
26545: #endif
26545: 
31723: #ifndef HAVE_ENABLE_DISABLE_DEBUGGER_EXCEPTIONS
31723: static void
37741: enable_debugger_exceptions() { }
31723: static void
37741: disable_debugger_exceptions() { }
31723: #endif
31723: 
26545: #endif /* NANOJIT_ARM */
26545: 
27884: #define K *1024
27884: #define M K K
27884: #define G K M
27884: 
27884: void
37741: SetMaxCodeCacheBytes(JSContext* cx, uint32 bytes)
37741: {
27884:     if (bytes > 1 G)
27884:         bytes = 1 G;
27884:     if (bytes < 128 K)
27884:         bytes = 128 K;
59733:     JS_THREAD_DATA(cx)->maxCodeCacheBytes = bytes;
27884: }
27884: 
58041: bool
37741: InitJIT(TraceMonitor *tm)
18068: {
59733:     // InitJIT expects this area to be zero'd
59733:     memset(tm, 0, sizeof(*tm));
59733: 
29883: #if defined JS_JIT_SPEW
32784:     tm->profAlloc = NULL;
30860:     /* Set up debug logging. */
29883:     if (!did_we_set_up_debug_logging) {
30860:         InitJITLogController();
29883:         did_we_set_up_debug_logging = true;
29883:     }
32784:     /* Set up fragprofiling, if required. */
37741:     if (LogController.lcbits & LC_FragProfile) {
61053:         tm->profAlloc = js_new<VMAllocator>((char*)NULL, 0); /* no reserve needed in debug builds */
60811:         JS_ASSERT(tm->profAlloc);
32784:         tm->profTab = new (*tm->profAlloc) FragStatsMap(*tm->profAlloc);
32784:     }
32784:     tm->lastFragID = 0;
29883: #else
40229:     PodZero(&LogController);
29883: #endif
29883: 
26545:     if (!did_we_check_processor_features) {
18333: #if defined NANOJIT_IA32
38583:         avmplus::AvmCore::config.i386_use_cmov =
38583:             avmplus::AvmCore::config.i386_sse2 = CheckForSSE2();
38583:         avmplus::AvmCore::config.i386_fixed_esp = true;
18333: #endif
26545: #if defined NANOJIT_ARM
31723: 
37742:         disable_debugger_exceptions();
37742: 
37742:         bool            arm_vfp     = arm_check_vfp();
37742:         unsigned int    arm_arch    = arm_check_arch();
37742: 
37742:         enable_debugger_exceptions();
37038: 
37773:         avmplus::AvmCore::config.arm_vfp        = arm_vfp;
28185:         avmplus::AvmCore::config.soft_float     = !arm_vfp;
37773:         avmplus::AvmCore::config.arm_arch       = arm_arch;
28185: 
28185:         // Sanity-check the configuration detection.
28185:         //  * We don't understand architectures prior to ARMv4.
28185:         JS_ASSERT(arm_arch >= 4);
26545: #endif
26545:         did_we_check_processor_features = true;
26545:     }
26545: 
60811:     #define CHECK_ALLOC(lhs, rhs) \
60811:         do { lhs = (rhs); if (!lhs) return false; } while (0)
60811: 
60811:     CHECK_ALLOC(tm->oracle, js_new<Oracle>());
41802: 
56551:     tm->profile = NULL;
56551:     
60811:     CHECK_ALLOC(tm->recordAttempts, js_new<RecordAttemptMap>());
38568:     if (!tm->recordAttempts->init(PC_HASH_COUNT))
60811:         return false;
60811: 
60811:     CHECK_ALLOC(tm->loopProfiles, js_new<LoopProfileMap>());
56551:     if (!tm->loopProfiles->init(PC_HASH_COUNT))
60811:         return false;
56551: 
56551:     tm->flushEpoch = 0;
56551:     
61053:     char *dataReserve, *traceReserve, *tempReserve;
61053:     CHECK_ALLOC(dataReserve, (char *)js_malloc(DataReserveSize));
61053:     CHECK_ALLOC(traceReserve, (char *)js_malloc(TraceReserveSize));
61053:     CHECK_ALLOC(tempReserve, (char *)js_malloc(TempReserveSize));
61053:     CHECK_ALLOC(tm->dataAlloc, js_new<VMAllocator>(dataReserve, DataReserveSize));
61053:     CHECK_ALLOC(tm->traceAlloc, js_new<VMAllocator>(traceReserve, TraceReserveSize));
61053:     CHECK_ALLOC(tm->tempAlloc, js_new<VMAllocator>(tempReserve, TempReserveSize));
60811:     CHECK_ALLOC(tm->codeAlloc, js_new<CodeAlloc>());
60811:     CHECK_ALLOC(tm->frameCache, js_new<FrameInfoCache>(tm->dataAlloc));
60811:     CHECK_ALLOC(tm->storage, js_new<TraceNativeStorage>());
60811:     CHECK_ALLOC(tm->cachedTempTypeMap, js_new<TypeMap>((Allocator*)NULL));
32767:     tm->flush();
32784:     verbose_only( tm->branches = NULL; )
31920: 
17884: #if !defined XP_WIN
40230:     debug_only(PodZero(&jitstats));
17884: #endif
33126: 
33126: #ifdef JS_JIT_SPEW
33126:     /* Architecture properties used by test cases. */
33126:     jitstats.archIsIA32 = 0;
33126:     jitstats.archIs64BIT = 0;
33126:     jitstats.archIsARM = 0;
33126:     jitstats.archIsSPARC = 0;
33126:     jitstats.archIsPPC = 0;
33126: #if defined NANOJIT_IA32
33126:     jitstats.archIsIA32 = 1;
33126: #endif
33126: #if defined NANOJIT_64BIT
33126:     jitstats.archIs64BIT = 1;
33126: #endif
33126: #if defined NANOJIT_ARM
33126:     jitstats.archIsARM = 1;
33126: #endif
33126: #if defined NANOJIT_SPARC
33126:     jitstats.archIsSPARC = 1;
33126: #endif
33126: #if defined NANOJIT_PPC
33126:     jitstats.archIsPPC = 1;
33126: #endif
33126: #if defined NANOJIT_X64
33126:     jitstats.archIsAMD64 = 1;
33126: #endif
33126: #endif
58041: 
58041:     if (!tm->tracedScripts.init())
58041:         return false;
58041:     return true;
17726: }
17726: 
25940: void
37741: FinishJIT(TraceMonitor *tm)
18068: {
35083:     JS_ASSERT(!tm->recorder);
60157:     JS_ASSERT(!tm->profile);
35083: 
21459: #ifdef JS_JIT_SPEW
29883:     if (jitstats.recorderStarted) {
34295:         char sep = ':';
34295:         debug_only_print0(LC_TMStats, "recorder");
34295: #define RECORDER_JITSTAT(_ident, _name)                             \
34295:         debug_only_printf(LC_TMStats, "%c " _name "(%llu)", sep,    \
34295:                           (unsigned long long int)jitstats._ident); \
34295:         sep = ',';
34295: #define JITSTAT(x) /* nothing */
34295: #include "jitstats.tbl"
34295: #undef JITSTAT
34295: #undef RECORDER_JITSTAT
34295:         debug_only_print0(LC_TMStats, "\n");
34295: 
34295:         sep = ':';
34295:         debug_only_print0(LC_TMStats, "monitor");
34295: #define MONITOR_JITSTAT(_ident, _name)                              \
34295:         debug_only_printf(LC_TMStats, "%c " _name "(%llu)", sep,    \
34295:                           (unsigned long long int)jitstats._ident); \
34295:         sep = ',';
34295: #define JITSTAT(x) /* nothing */
34295: #include "jitstats.tbl"
34295: #undef JITSTAT
34295: #undef MONITOR_JITSTAT
34295:         debug_only_print0(LC_TMStats, "\n");
22616:     }
17726: #endif
31920: 
60811:     js_delete(tm->recordAttempts);
60811:     js_delete(tm->loopProfiles);
60811:     js_delete(tm->oracle);
28105: 
32784: #ifdef DEBUG
32784:     // Recover profiling data from expiring Fragments, and display
32784:     // final results.
37741:     if (LogController.lcbits & LC_FragProfile) {
32784:         for (Seq<Fragment*>* f = tm->branches; f; f = f->tail) {
37741:             FragProfiling_FragFinalizer(f->head, tm);
32784:         }
32784:         for (size_t i = 0; i < FRAGMENT_TABLE_SIZE; ++i) {
35044:             for (TreeFragment *f = tm->vmfragments[i]; f; f = f->next) {
32784:                 JS_ASSERT(f->root == f);
35044:                 for (TreeFragment *p = f; p; p = p->peer)
37741:                     FragProfiling_FragFinalizer(p, tm);
32784:             }
32784:         }
37741:         FragProfiling_showResults(tm);
60811:         js_delete(tm->profAlloc);
32784: 
32784:     } else {
32784:         NanoAssert(!tm->profTab);
32784:         NanoAssert(!tm->profAlloc);
32784:     }
32784: #endif
32784: 
40229:     PodArrayZero(tm->vmfragments);
32767: 
60811:     js_delete(tm->frameCache);
33563:     tm->frameCache = NULL;
60811: 
60811:     js_delete(tm->codeAlloc);
32767:     tm->codeAlloc = NULL;
60811: 
60811:     js_delete(tm->dataAlloc);
33159:     tm->dataAlloc = NULL;
60811: 
60811:     js_delete(tm->traceAlloc);
33545:     tm->traceAlloc = NULL;
60811: 
60811:     js_delete(tm->tempAlloc);
33167:     tm->tempAlloc = NULL;
60811: 
60811:     js_delete(tm->storage);
37037:     tm->storage = NULL;
60811: 
60811:     js_delete(tm->cachedTempTypeMap);
37694:     tm->cachedTempTypeMap = NULL;
17442: }
17442: 
31920: JS_REQUIRES_STACK void
58041: PurgeScriptFragments(TraceMonitor* tm, JSScript* script)
31920: {
31920:     debug_only_printf(LC_TMTracer,
31920:                       "Purging fragments for JSScript %p.\n", (void*)script);
31920: 
54718:     /* A recorder script is being evaluated and can not be destroyed or GC-ed. */
54718:     JS_ASSERT_IF(tm->recorder, 
54718:                  JS_UPTRDIFF(tm->recorder->getTree()->ip, script->code) >= script->length);
54718: 
60567:     for (LoopProfileMap::Enum e(*tm->loopProfiles); !e.empty(); e.popFront()) {
60567:         if (JS_UPTRDIFF(e.front().key, script->code) < script->length)
60567:             e.removeFront();
60567:     }
60567: 
58041:     TracedScriptSet::Ptr found = tm->tracedScripts.lookup(script);
58041:     if (!found)
58041:         return;
58041:     tm->tracedScripts.remove(found);
58041: 
24879:     for (size_t i = 0; i < FRAGMENT_TABLE_SIZE; ++i) {
35044:         TreeFragment** fragp = &tm->vmfragments[i];
35044:         while (TreeFragment* frag = *fragp) {
28311:             if (JS_UPTRDIFF(frag->ip, script->code) < script->length) {
30860:                 /* This fragment is associated with the script. */
29883:                 debug_only_printf(LC_TMTracer,
35044:                                   "Disconnecting TreeFragment %p "
25089:                                   "with ip %p, in range [%p,%p).\n",
28311:                                   (void*)frag, frag->ip, script->code,
29883:                                   script->code + script->length);
31920: 
31920:                 JS_ASSERT(frag->root == frag);
32777:                 *fragp = frag->next;
32777:                 do {
37741:                     verbose_only( FragProfiling_FragFinalizer(frag, tm); )
54718:                     TrashTree(frag);
32777:                 } while ((frag = frag->peer) != NULL);
32777:                 continue;
32777:             }
32777:             fragp = &frag->next;
32777:         }
32777:     }
32777: 
38568:     RecordAttemptMap &table = *tm->recordAttempts;
39896:     for (RecordAttemptMap::Enum e(table); !e.empty(); e.popFront()) {
38568:         if (JS_UPTRDIFF(e.front().key, script->code) < script->length)
38568:             e.removeFront();
38568:     }
24879: }
24879: 
26826: bool
59733: OverfullJITCache(JSContext *cx, TraceMonitor* tm)
26826: {
26826:     /*
31475:      * You might imagine the outOfMemory flag on the allocator is sufficient
26826:      * to model the notion of "running out of memory", but there are actually
26826:      * two separate issues involved:
26826:      *
26826:      *  1. The process truly running out of memory: malloc() or mmap()
26826:      *     failed.
26826:      *
26826:      *  2. The limit we put on the "intended size" of the tracemonkey code
26826:      *     cache, in pages, has been exceeded.
26826:      *
26826:      * Condition 1 doesn't happen very often, but we're obliged to try to
26826:      * safely shut down and signal the rest of spidermonkey when it
26826:      * does. Condition 2 happens quite regularly.
26826:      *
31475:      * Presently, the code in this file doesn't check the outOfMemory condition
26826:      * often enough, and frequently misuses the unchecked results of
61053:      * lirbuffer insertions on the assumption that it will notice the
31475:      * outOfMemory flag "soon enough" when it returns to the monitorRecording
31475:      * function. This turns out to be a false assumption if we use outOfMemory
26826:      * to signal condition 2: we regularly provoke "passing our intended
26826:      * size" and regularly fail to notice it in time to prevent writing
26826:      * over the end of an artificially self-limited LIR buffer.
26826:      *
26826:      * To mitigate, though not completely solve, this problem, we're
26826:      * modeling the two forms of memory exhaustion *separately* for the
31475:      * time being: condition 1 is handled by the outOfMemory flag inside
26826:      * nanojit, and condition 2 is being handled independently *here*. So
31920:      * we construct our allocators to use all available memory they like,
31475:      * and only report outOfMemory to us when there is literally no OS memory
26826:      * left. Merely purging our cache when we hit our highwater mark is
26826:      * handled by the (few) callers of this function.
26826:      *
26826:      */
59733:     jsuint maxsz = JS_THREAD_DATA(cx)->maxCodeCacheBytes;
61053:     return (tm->codeAlloc->size() + tm->dataAlloc->size() + tm->traceAlloc->size() > maxsz);
26826: }
26826: 
25214: JS_FORCES_STACK JS_FRIEND_API(void)
37741: DeepBail(JSContext *cx)
25214: {
25214:     JS_ASSERT(JS_ON_TRACE(cx));
25214: 
27577:     /*
27577:      * Exactly one context on the current thread is on trace. Find out which
27577:      * one. (Most callers cannot guarantee that it's cx.)
27577:      */
37741:     TraceMonitor *tm = &JS_TRACE_MONITOR(cx);
27577: 
24612:     /* It's a bug if a non-FAIL_STATUS builtin gets here. */
60574:     JS_ASSERT(tm->bailExit);
27577: 
27882:     tm->tracecx = NULL;
29883:     debug_only_print0(LC_TMTracer, "Deep bail.\n");
60574:     LeaveTree(tm, *tm->tracerState, tm->bailExit);
60574:     tm->bailExit = NULL;
60574: 
60574:     TracerState* state = tm->tracerState;
37741:     state->builtinStatus |= BUILTIN_BAILED;
34572: 
34572:     /*
34572:      * Between now and the LeaveTree in ExecuteTree, |tm->storage| may be reused
34572:      * if another trace executes before the currently executing native returns.
34572:      * However, all such traces will complete by the time the currently
34572:      * executing native returns and the return value is written to the native
34572:      * stack. After that point, no traces may execute until the LeaveTree in
34572:      * ExecuteTree, hence the invariant is maintained that only one trace uses
34572:      * |tm->storage| at a time.
34572:      */
30287:     state->deepBailSp = state->sp;
22652: }
22652: 
48470: JS_REQUIRES_STACK Value&
17412: TraceRecorder::argval(unsigned n) const
17412: {
51446:     JS_ASSERT(n < cx->fp()->numFormalArgs());
53840:     return cx->fp()->formalArg(n);
17412: }
17412: 
48470: JS_REQUIRES_STACK Value&
17412: TraceRecorder::varval(unsigned n) const
17412: {
53840:     JS_ASSERT(n < cx->fp()->numSlots());
51446:     return cx->fp()->slots()[n];
17412: }
17412: 
48470: JS_REQUIRES_STACK Value&
17412: TraceRecorder::stackval(int n) const
17412: {
42717:     return cx->regs->sp[n];
17412: }
17412: 
48470: JS_REQUIRES_STACK void
48470: TraceRecorder::updateAtoms()
48470: {
54169:     JSScript *script = cx->fp()->script();
51446:     atoms = FrameAtomBase(cx, cx->fp());
59221:     consts = (cx->fp()->hasImacropc() || !JSScript::isValidOffset(script->constOffset))
48470:              ? 0
54169:              : script->consts()->vector;
56750:     strictModeCode_ins = w.name(w.immi(script->strictModeCode), "strict");
48470: }
48470: 
48470: JS_REQUIRES_STACK void
48470: TraceRecorder::updateAtoms(JSScript *script)
48470: {
48470:     atoms = script->atomMap.vector;
59221:     consts = JSScript::isValidOffset(script->constOffset) ? script->consts()->vector : 0;
56750:     strictModeCode_ins = w.name(w.immi(script->strictModeCode), "strict");
48470: }
48470: 
37694: /*
37694:  * Generate LIR to compute the scope chain.
37694:  */
22652: JS_REQUIRES_STACK LIns*
37694: TraceRecorder::scopeChain()
37694: {
53840:     return cx->fp()->isFunctionFrame()
53840:            ? getFrameObjPtr(cx->fp()->addressOfScopeChain())
37694:            : entryScopeChain();
37694: }
37694: 
37694: /*
37694:  * Generate LIR to compute the scope chain on entry to the trace. This is
37694:  * generally useful only for getting to the global object, because only
37694:  * the global object is guaranteed to be present.
37694:  */
37694: JS_REQUIRES_STACK LIns*
37694: TraceRecorder::entryScopeChain() const
18286: {
56750:     return w.ldpStackFrameScopeChain(entryFrameIns());
18286: }
18286: 
30248: /*
51446:  * Generate LIR to compute the stack frame on entry to the trace.
51446:  */
51446: JS_REQUIRES_STACK LIns*
51446: TraceRecorder::entryFrameIns() const
51446: {
56750:     return w.ldpFrameFp(w.ldpContextField(regs));
51446: }
51446: 
51446: /*
30860:  * Return the frame of a call object if that frame is part of the current
30860:  * trace. |depthp| is an optional outparam: if it is non-null, it will be
51446:  * filled in with the depth of the call object's frame relevant to cx->fp().
30248:  */
30248: JS_REQUIRES_STACK JSStackFrame*
30248: TraceRecorder::frameIfInRange(JSObject* obj, unsigned* depthp) const
30248: {
31452:     JSStackFrame* ofp = (JSStackFrame*) obj->getPrivate();
51446:     JSStackFrame* fp = cx->fp();
30248:     for (unsigned depth = 0; depth <= callDepth; ++depth) {
30248:         if (fp == ofp) {
30248:             if (depthp)
30248:                 *depthp = depth;
30248:             return ofp;
30248:         }
53840:         if (!(fp = fp->prev()))
30248:             break;
30248:     }
30248:     return NULL;
18286: }
18286: 
48613: JS_DEFINE_CALLINFO_4(extern, UINT32, GetClosureVar, CONTEXT, OBJECT, CVIPTR, DOUBLEPTR,
48613:                      0, ACCSET_STORE_ANY)
48613: JS_DEFINE_CALLINFO_4(extern, UINT32, GetClosureArg, CONTEXT, OBJECT, CVIPTR, DOUBLEPTR,
48613:                      0, ACCSET_STORE_ANY)
30647: 
30647: /*
30860:  * Search the scope chain for a property lookup operation at the current PC and
33542:  * generate LIR to access the given property. Return RECORD_CONTINUE on success,
33542:  * otherwise abort and return RECORD_STOP. There are 3 outparams:
30647:  *
30647:  *     vp           the address of the current property value
30647:  *     ins          LIR instruction representing the property value on trace
31075:  *     NameResult   describes how to look up name; see comment for NameResult in jstracer.h
31075:  */
33542: JS_REQUIRES_STACK AbortableRecordingStatus
48470: TraceRecorder::scopeChainProp(JSObject* chainHead, Value*& vp, LIns*& ins, NameResult& nr)
35370: {
53840:     JS_ASSERT(chainHead == &cx->fp()->scopeChain());
35370:     JS_ASSERT(chainHead != globalObj);
18286: 
37741:     TraceMonitor &localtm = *traceMonitor;
33171: 
42717:     JSAtom* atom = atoms[GET_INDEX(cx->regs->pc)];
18286:     JSObject* obj2;
18286:     JSProperty* prop;
35370:     JSObject *obj = chainHead;
41777:     if (!js_FindProperty(cx, ATOM_TO_JSID(atom), &obj, &obj2, &prop))
41777:         RETURN_ERROR_A("error in js_FindProperty");
33171: 
33171:     /* js_FindProperty can reenter the interpreter and kill |this|. */
33171:     if (!localtm.recorder)
33542:         return ARECORD_ABORTED;
33171: 
27933:     if (!prop)
33542:         RETURN_STOP_A("failed to find name in non-global scope chain");
18286: 
18286:     if (obj == globalObj) {
32593:         // Even if the property is on the global object, we must guard against
32593:         // the creation of properties that shadow the property in the middle
35370:         // of the scope chain.
35370:         LIns *head_ins;
53840:         if (cx->fp()->isFunctionFrame()) {
35370:             // Skip any Call object when inside a function. Any reference to a
35370:             // Call name the compiler resolves statically and we do not need
35370:             // to match shapes of the Call objects.
53840:             chainHead = cx->fp()->callee().getParent();
56750:             head_ins = w.ldpObjParent(get(&cx->fp()->calleeValue()));
35370:         } else {
35370:             head_ins = scopeChain();
35370:         }
32593:         LIns *obj_ins;
35370:         CHECK_STATUS_A(traverseScopeChain(chainHead, head_ins, obj, obj_ins));
32593: 
56567:         if (obj2 != obj)
33542:             RETURN_STOP_A("prototype property");
43290: 
52503:         Shape* shape = (Shape*) prop;
56567:         if (!isValidSlot(obj, shape))
56567:             return ARECORD_STOP;
56567:         if (!lazilyImportGlobalSlot(shape->slot))
33542:             RETURN_STOP_A("lazy import of global slot failed");
52503:         vp = &obj->getSlotRef(shape->slot);
30647:         ins = get(vp);
31075:         nr.tracked = true;
33542:         return ARECORD_CONTINUE;
18286:     }
18286: 
52503:     if (obj == obj2 && obj->isCall()) {
35479:         AbortableRecordingStatus status =
35479:             InjectStatus(callProp(obj, prop, ATOM_TO_JSID(atom), vp, ins, nr));
35479:         return status;
35479:     }
32593: 
33542:     RETURN_STOP_A("fp->scopeChain is not global or active call object");
32593: }
32593: 
32593: /*
32593:  * Generate LIR to access a property of a Call object.
32593:  */
33542: JS_REQUIRES_STACK RecordingStatus
48470: TraceRecorder::callProp(JSObject* obj, JSProperty* prop, jsid id, Value*& vp,
32593:                         LIns*& ins, NameResult& nr)
32593: {
52503:     Shape *shape = (Shape*) prop;
29359: 
42717:     JSOp op = JSOp(*cx->regs->pc);
32658:     uint32 setflags = (js_CodeSpec[op].format & (JOF_SET | JOF_INCDEC | JOF_FOR));
52503:     if (setflags && !shape->writable())
33542:         RETURN_STOP("writing to a read-only property");
29359: 
52503:     uintN slot = uint16(shape->shortid);
18286: 
18426:     vp = NULL;
32593:     JSStackFrame* cfp = (JSStackFrame*) obj->getPrivate();
32593:     if (cfp) {
58293:         if (shape->getterOp() == GetCallArg) {
51120:             JS_ASSERT(slot < cfp->numFormalArgs());
53840:             vp = &cfp->formalArg(slot);
32593:             nr.v = *vp;
58293:         } else if (shape->getterOp() == GetCallVar ||
58293:                    shape->getterOp() == GetCallVarChecked) {
53840:             JS_ASSERT(slot < cfp->numSlots());
42714:             vp = &cfp->slots()[slot];
32593:             nr.v = *vp;
32593:         } else {
33542:             RETURN_STOP("dynamic property of Call object");
31840:         }
30647: 
52503:         // Now assert that our use of shape->shortid was in fact kosher.
52503:         JS_ASSERT(shape->hasShortID());
35483: 
30647:         if (frameIfInRange(obj)) {
32593:             // At this point we are guaranteed to be looking at an active call oject
30647:             // whose properties are stored in the corresponding JSStackFrame.
30647:             ins = get(vp);
31075:             nr.tracked = true;
33542:             return RECORD_CONTINUE;
31449:         }
32593:     } else {
52503:         // Call objects do not yet have shape->isMethod() properties, but they
32658:         // should. See bug 514046, for which this code is future-proof. Remove
32658:         // this comment when that bug is fixed (so, FIXME: 514046).
32593: #ifdef DEBUG
32593:         JSBool rv =
32593: #endif
52503:             js_GetPropertyHelper(cx, obj, shape->id,
32658:                                  (op == JSOP_CALLNAME)
32658:                                  ? JSGET_NO_METHOD_BARRIER
32658:                                  : JSGET_METHOD_BARRIER,
32658:                                  &nr.v);
32593:         JS_ASSERT(rv);
32593:     }
32593: 
32593:     LIns* obj_ins;
53840:     JSObject* parent = cx->fp()->callee().getParent();
56750:     LIns* parent_ins = w.ldpObjParent(get(&cx->fp()->calleeValue()));
32593:     CHECK_STATUS(traverseScopeChain(parent, parent_ins, obj, obj_ins));
32593: 
35479:     if (!cfp) {
35479:         // Because the parent guard in guardCallee ensures this Call object
35479:         // will be the same object now and on trace, and because once a Call
35479:         // object loses its frame it never regains one, on trace we will also
35479:         // have a null private in the Call object. So all we need to do is
35479:         // write the value to the Call object's slot.
58293:         if (shape->getterOp() == GetCallArg) {
52503:             JS_ASSERT(slot < ArgClosureTraits::slot_count(obj));
52503:             slot += ArgClosureTraits::slot_offset(obj);
58293:         } else if (shape->getterOp() == GetCallVar ||
58293:                    shape->getterOp() == GetCallVarChecked) {
52503:             JS_ASSERT(slot < VarClosureTraits::slot_count(obj));
52503:             slot += VarClosureTraits::slot_offset(obj);
35479:         } else {
35479:             RETURN_STOP("dynamic property of Call object");
35479:         }
35479: 
52503:         // Now assert that our use of shape->shortid was in fact kosher.
52503:         JS_ASSERT(shape->hasShortID());
52503: 
52503:         ins = unbox_slot(obj, obj_ins, slot, snapshot(BRANCH_EXIT));
35483:     } else {
34351:         ClosureVarInfo* cv = new (traceAlloc()) ClosureVarInfo();
32593:         cv->slot = slot;
35479: #ifdef DEBUG
32593:         cv->callDepth = callDepth;
35479: #endif
32593: 
53840:         // Even though the frame is out of range, later we might be called as an
53840:         // inner trace such that the target variable is defined in the outer trace
53840:         // entry frame. For simplicity, we just fall off trace.
53840:         guard(false,
56750:               w.eqp(entryFrameIns(), w.ldpObjPrivate(obj_ins)),
53840:               MISMATCH_EXIT);
53840: 
56750:         LIns* outp = w.allocp(sizeof(double));
30647:         LIns* args[] = {
30647:             outp,
56750:             w.nameImmpNonGC(cv),
32593:             obj_ins,
30647:             cx_ins
30647:         };
30647:         const CallInfo* ci;
58293:         if (shape->getterOp() == GetCallArg) {
30860:             ci = &GetClosureArg_ci;
58293:         } else if (shape->getterOp() == GetCallVar ||
58293:                    shape->getterOp() == GetCallVarChecked) {
30860:             ci = &GetClosureVar_ci;
35479:         } else {
35479:             RETURN_STOP("dynamic property of Call object");
35479:         }
35479: 
52503:         // Now assert that our use of shape->shortid was in fact kosher.
52503:         JS_ASSERT(shape->hasShortID());
52503: 
56750:         LIns* call_ins = w.call(ci, args);
35479: 
48470:         JSValueType type = getCoercedType(nr.v);
30647:         guard(true,
56750:               w.name(w.eqi(call_ins, w.immi(type)), "guard(type-stable name access)"),
30647:               BRANCH_EXIT);
56750:         ins = stackLoad(AllocSlotsAddress(outp), type);
35479:     }
31075:     nr.tracked = false;
31075:     nr.obj = obj;
32593:     nr.obj_ins = obj_ins;
52503:     nr.shape = shape;
33542:     return RECORD_CONTINUE;
30647: }
18286: 
22652: JS_REQUIRES_STACK LIns*
17412: TraceRecorder::arg(unsigned n)
17412: {
17412:     return get(&argval(n));
17412: }
17412: 
22652: JS_REQUIRES_STACK void
17415: TraceRecorder::arg(unsigned n, LIns* i)
17415: {
17415:     set(&argval(n), i);
17415: }
17415: 
22652: JS_REQUIRES_STACK LIns*
17412: TraceRecorder::var(unsigned n)
17412: {
17412:     return get(&varval(n));
17412: }
17412: 
22652: JS_REQUIRES_STACK void
17415: TraceRecorder::var(unsigned n, LIns* i)
17415: {
17415:     set(&varval(n), i);
17415: }
17415: 
22652: JS_REQUIRES_STACK LIns*
17412: TraceRecorder::stack(int n)
17412: {
17412:     return get(&stackval(n));
17412: }
17412: 
22652: JS_REQUIRES_STACK void
17412: TraceRecorder::stack(int n, LIns* i)
17412: {
39913:     set(&stackval(n), i);
17412: }
17412: 
56185: /* Leave trace iff one operand is negative and the other is non-negative. */
56185: JS_REQUIRES_STACK void
56185: TraceRecorder::guardNonNeg(LIns* d0, LIns* d1, VMSideExit* exit)
56185: {
56185:     if (d0->isImmI())
56185:         JS_ASSERT(d0->immI() >= 0);
56185:     else
56750:         guard(false, w.ltiN(d0, 0), exit);
56185: 
56185:     if (d1->isImmI())
56185:         JS_ASSERT(d1->immI() >= 0);
56185:     else
56750:         guard(false, w.ltiN(d1, 0), exit);
56185: }
56185: 
23456: JS_REQUIRES_STACK LIns*
21799: TraceRecorder::alu(LOpcode v, jsdouble v0, jsdouble v1, LIns* s0, LIns* s1)
21799: {
29354:     /*
29354:      * To even consider this operation for demotion, both operands have to be
29354:      * integers and the oracle must not give us a negative hint for the
29354:      * instruction.
29354:      */
46181:     if (!oracle || oracle->isInstructionUndemotable(cx->regs->pc) ||
59993:         !IsPromotedInt32(s0) || !IsPromotedInt32(s1)) {
29354:     out:
42688:         if (v == LIR_modd) {
29354:             LIns* args[] = { s1, s0 };
56750:             return w.call(&js_dmod_ci, args);
56750:         }
56750:         LIns* result = w.ins2(v, s0, s1);
41265:         JS_ASSERT_IF(s0->isImmD() && s1->isImmD(), result->isImmD());
29354:         return result;
29354:     }
29354: 
21799:     jsdouble r;
29354:     switch (v) {
42688:     case LIR_addd:
21799:         r = v0 + v1;
29354:         break;
42688:     case LIR_subd:
21799:         r = v0 - v1;
29354:         break;
42688:     case LIR_muld:
29354:         r = v0 * v1;
56185:         if (r == 0.0 && (v0 < 0.0 || v1 < 0.0))
29373:             goto out;
29354:         break;
33095: #if defined NANOJIT_IA32 || defined NANOJIT_X64
42688:     case LIR_divd:
29354:         if (v1 == 0)
29354:             goto out;
29354:         r = v0 / v1;
29354:         break;
42688:     case LIR_modd:
41265:         if (v0 < 0 || v1 == 0 || (s1->isImmD() && v1 < 0))
29354:             goto out;
29354:         r = js_dmod(v0, v1);
29354:         break;
29354: #endif
29354:     default:
29354:         goto out;
29354:     }
29354: 
29354:     /*
29354:      * The result must be an integer at record time, otherwise there is no
29354:      * point in trying to demote it.
29354:      */
29354:     if (jsint(r) != r || JSDOUBLE_IS_NEGZERO(r))
29354:         goto out;
29354: 
59993:     LIns* d0 = w.demoteToInt32(s0);
59993:     LIns* d1 = w.demoteToInt32(s1);
29354: 
29354:     /*
29354:      * Speculatively emit an integer operation, betting that at runtime we
29354:      * will get integer results again.
29354:      */
54554:     VMSideExit* exit = NULL;
29354:     LIns* result;
29354:     switch (v) {
33095: #if defined NANOJIT_IA32 || defined NANOJIT_X64
42688:       case LIR_divd:
41265:         if (d0->isImmI() && d1->isImmI())
56750:             return w.i2d(w.immi(jsint(r)));
29354: 
29354:         exit = snapshot(OVERFLOW_EXIT);
29354: 
32569:         /*
32569:          * If the divisor is greater than zero its always safe to execute
32569:          * the division. If not, we have to make sure we are not running
32569:          * into -2147483648 / -1, because it can raise an overflow exception.
32569:          */
41265:         if (!d1->isImmI()) {
56750:             if (MaybeBranch mbr = w.jt(w.gtiN(d1, 0))) {
56750:                 guard(false, w.eqi0(d1), exit);
56750:                 guard(true,  w.eqi0(w.andi(w.eqiN(d0, 0x80000000),
56750:                                            w.eqiN(d1, -1))), exit);
56750:                 w.label(mbr);
55749:             }
32569:         } else {
41265:             if (d1->immI() == -1)
56750:                 guard(false, w.eqiN(d0, 0x80000000), exit);
56750:         }
56750:         v = LIR_divi;
56750:         result = w.divi(d0, d1);
29354: 
38497:         /* As long as the modulus is zero, the result is an integer. */
56750:         guard(true, w.eqi0(w.modi(result)), exit);
30860: 
30860:         /* Don't lose a -0. */
56750:         guard(false, w.eqi0(result), exit);
29354:         break;
30860: 
42688:       case LIR_modd: {
41265:         if (d0->isImmI() && d1->isImmI())
56750:             return w.i2d(w.immi(jsint(r)));
29354: 
29354:         exit = snapshot(OVERFLOW_EXIT);
29354: 
30860:         /* Make sure we don't trigger division by zero at runtime. */
41265:         if (!d1->isImmI())
56750:             guard(false, w.eqi0(d1), exit);
56750:         v = LIR_modi;
56750:         result = w.modi(w.divi(d0, d1));
29354: 
30860:         /* If the result is not 0, it is always within the integer domain. */
56750:         if (MaybeBranch mbr = w.jf(w.eqi0(result))) {
29354:             /*
29354:              * If the result is zero, we must exit if the lhs is negative since
29354:              * the result is -0 in this case, which is not in the integer domain.
29354:              */
56750:             guard(false, w.ltiN(d0, 0), exit);
56750:             w.label(mbr);
55749:         }
29354:         break;
29354:       }
29354: #endif
30860: 
29354:       default:
41265:         v = arithOpcodeD2I(v);
42688:         JS_ASSERT(v == LIR_addi || v == LIR_muli || v == LIR_subi);
38603: 
38603:         /*
38603:          * If the operands guarantee that the result will be an integer (e.g.
38603:          * z = x * y with 0 <= (x|y) <= 0xffff guarantees z <= fffe0001), we
29354:          * don't have to guard against an overflow. Otherwise we emit a guard
29354:          * that will inform the oracle and cause a non-demoted trace to be
29354:          * attached that uses floating-point math for this operation.
29354:          */
54554:         bool needsOverflowCheck = true, needsNegZeroCheck = true;
54554:         ChecksRequired(v, d0, d1, &needsOverflowCheck, &needsNegZeroCheck);
54554:         if (needsOverflowCheck) {
29373:             exit = snapshot(OVERFLOW_EXIT);
38603:             result = guard_xov(v, d0, d1, exit);
38603:         } else {
56750:             result = w.ins2(v, d0, d1);
29373:         }
54554:         if (needsNegZeroCheck) {
54554:             JS_ASSERT(v == LIR_muli);
56185:             /*
56185:              * Make sure we don't lose a -0. We exit if the result is zero and if
56185:              * either operand is negative. We start out using a weaker guard, checking
56185:              * if either argument is negative. If this ever fails, we recompile with
56185:              * a stronger, but slower, guard.
56185:              */
56185:             if (v0 < 0.0 || v1 < 0.0
56185:                 || !oracle || oracle->isInstructionSlowZeroTest(cx->regs->pc))
56185:             {
54554:                 if (!exit)
54554:                     exit = snapshot(OVERFLOW_EXIT);
56185: 
56185:                 guard(true,
56750:                       w.eqi0(w.andi(w.eqi0(result),
56750:                                     w.ori(w.ltiN(d0, 0),
56750:                                           w.ltiN(d1, 0)))),
56185:                       exit);
56185:             } else {
56185:                 guardNonNeg(d0, d1, snapshot(MUL_ZERO_EXIT));
56185:             }
54427:         }
29354:         break;
29354:     }
56750:     JS_ASSERT_IF(d0->isImmI() && d1->isImmI(), result->isImmI(jsint(r)));
56750:     return w.i2d(result);
56750: }
56750: 
55749: LIns*
56750: TraceRecorder::d2i(LIns* d, bool resultCanBeImpreciseIfFractional)
56750: {
56750:     if (d->isImmD())
56750:         return w.immi(js_DoubleToECMAInt32(d->immD()));
59993:     if (d->isop(LIR_i2d) || d->isop(LIR_ui2d)) {
59993:         // The d2i(i2d(i)) case is obviously a no-op.  (Unlike i2d(d2i(d))!)
59993:         // The d2i(ui2d(ui)) case is less obvious, but it is also a no-op.
59993:         // For example, 4294967295U has the bit pattern 0xffffffff, and
59993:         // d2i(ui2d(4294967295U)) is -1, which also has the bit pattern
59993:         // 0xffffffff.  Another way to think about it:  d2i(ui2d(ui)) is
59993:         // equivalent to ui2i(ui);  ui2i doesn't exist, but it would be a
59993:         // no-op if it did.
59993:         // (Note that the above reasoning depends on the fact that d2i()
59993:         // always succeeds, ie. it never aborts).
56750:         return d->oprnd1();
59993:     }
56750:     if (d->isop(LIR_addd) || d->isop(LIR_subd)) {
59993:         // If 'i32ad' and 'i32bd' are integral doubles that fit in int32s, and
59993:         // 'i32ai' and 'i32bi' are int32s with the equivalent values, then
59993:         // this is true:
59993:         //
59993:         //   d2i(addd(i32ad, i32bd)) == addi(i32ai, i32bi)
59993:         //
59993:         // If the RHS doesn't overflow, this is obvious.  If it does overflow,
59993:         // the result will truncate.  And the LHS will truncate in exactly the
59993:         // same way.  So they're always equal.
56750:         LIns* lhs = d->oprnd1();
56750:         LIns* rhs = d->oprnd2();
59993:         if (IsPromotedInt32(lhs) && IsPromotedInt32(rhs))
59993:             return w.ins2(arithOpcodeD2I(d->opcode()), w.demoteToInt32(lhs), w.demoteToInt32(rhs));
56750:     }
56750:     if (d->isCall()) {
56750:         const CallInfo* ci = d->callInfo();
36402:         if (ci == &js_UnboxDouble_ci) {
48470: #if JS_BITS_PER_WORD == 32
56750:             LIns *tag_ins = d->callArgN(0);
56750:             LIns *payload_ins = d->callArgN(1);
48470:             LIns* args[] = { payload_ins, tag_ins };
56750:             return w.call(&js_UnboxInt32_ci, args);
48470: #else
56750:             LIns* val_ins = d->callArgN(0);
48470:             LIns* args[] = { val_ins };
56750:             return w.call(&js_UnboxInt32_ci, args);
48470: #endif
36403:         }
36403:         if (ci == &js_StringToNumber_ci) {
59890:             LIns* ok_ins = w.allocp(sizeof(JSBool));
59890:             LIns* args[] = { ok_ins, d->callArgN(1), d->callArgN(0) };
59890:             LIns* ret_ins = w.call(&js_StringToInt32_ci, args);
59890:             guard(false,
59890:                   w.name(w.eqi0(w.ldiAlloc(ok_ins)), "guard(oom)"),
59890:                   OOM_EXIT);
59890:             return ret_ins;
36403:         }
36402:     }
41987:     return resultCanBeImpreciseIfFractional
56750:          ? w.rawD2i(d)
56750:          : w.call(&js_DoubleToInt32_ci, &d);
17469: }
17469: 
36402: LIns*
56750: TraceRecorder::d2u(LIns* d)
56750: {
56750:     if (d->isImmD())
56750:         return w.immi(js_DoubleToECMAUint32(d->immD()));
56750:     if (d->isop(LIR_i2d) || d->isop(LIR_ui2d))
56750:         return d->oprnd1();
56750:     return w.call(&js_DoubleToUint32_ci, &d);
36402: }
36402: 
55556: JS_REQUIRES_STACK RecordingStatus
55556: TraceRecorder::makeNumberInt32(LIns* d, LIns** out)
55556: {
55556:     JS_ASSERT(d->isD());
59993:     if (IsPromotedInt32(d)) {
59993:         *out = w.demoteToInt32(d);
55556:         return RECORD_CONTINUE;
55556:     }
55556: 
41987:     // This means "convert double to int if it's integral, otherwise
41987:     // exit".  We first convert the double to an int, then convert it back
55556:     // and exit if the two doubles don't match.  If 'f' is a non-integral
55556:     // immediate we'll end up aborting.
55556:     *out = d2i(d, /* resultCanBeImpreciseIfFractional = */true);
56750:     return guard(true, w.eqd(d, w.i2d(*out)), MISMATCH_EXIT, /* abortIfAlwaysExits = */true);
19979: }
19979: 
59913: JS_REQUIRES_STACK RecordingStatus
59913: TraceRecorder::makeNumberUint32(LIns* d, LIns** out)
59913: {
59913:     JS_ASSERT(d->isD());
59993:     if (IsPromotedUint32(d)) {
59993:         *out = w.demoteToUint32(d);
59913:         return RECORD_CONTINUE;
59913:     }
59913: 
59913:     // This means "convert double to uint if it's integral, otherwise
59913:     // exit".  We first convert the double to an unsigned int, then
59913:     // convert it back and exit if the two doubles don't match.  If
59913:     // 'f' is a non-integral immediate we'll end up aborting.
59913:     *out = d2u(d);
59913:     return guard(true, w.eqd(d, w.ui2d(*out)), MISMATCH_EXIT, /* abortIfAlwaysExits = */true);
59913: }
59913: 
23456: JS_REQUIRES_STACK LIns*
48470: TraceRecorder::stringify(const Value& v)
21685: {
21685:     LIns* v_ins = get(&v);
48470:     if (v.isString())
21447:         return v_ins;
21447: 
21447:     LIns* args[] = { v_ins, cx_ins };
21447:     const CallInfo* ci;
48470:     if (v.isNumber()) {
21447:         ci = &js_NumberToString_ci;
48470:     } else if (v.isUndefined()) {
59932:         return w.immpAtomGC(cx->runtime->atomState.typeAtoms[JSTYPE_VOID]);
48470:     } else if (v.isBoolean()) {
40307:         ci = &js_BooleanIntToString_ci;
21447:     } else {
26036:         /*
26036:          * Callers must deal with non-primitive (non-null object) values by
26036:          * calling an imacro. We don't try to guess about which imacro, with
26036:          * what valueOf hint, here.
26036:          */
48470:         JS_ASSERT(v.isNull());
56750:         return w.immpAtomGC(cx->runtime->atomState.nullAtom);
56750:     }
56750: 
56750:     v_ins = w.call(ci, args);
56750:     guard(false, w.eqp0(v_ins), OOM_EXIT);
21447:     return v_ins;
21447: }
21447: 
41290: JS_REQUIRES_STACK bool
41290: TraceRecorder::canCallImacro() const
41290: {
41290:     /* We cannot nest imacros. */
53840:     return !cx->fp()->hasImacropc();
41290: }
41290: 
33542: JS_REQUIRES_STACK RecordingStatus
41290: TraceRecorder::callImacro(jsbytecode* imacro)
41290: {
41290:     return canCallImacro() ? callImacroInfallibly(imacro) : RECORD_STOP;
41290: }
41290: 
41290: JS_REQUIRES_STACK RecordingStatus
41290: TraceRecorder::callImacroInfallibly(jsbytecode* imacro)
21685: {
51446:     JSStackFrame* fp = cx->fp();
53840:     JS_ASSERT(!fp->hasImacropc());
42717:     JSFrameRegs* regs = cx->regs;
53840:     fp->setImacropc(regs->pc);
21685:     regs->pc = imacro;
48470:     updateAtoms();
33542:     return RECORD_IMACRO;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
20416: TraceRecorder::ifop()
17452: {
48470:     Value& v = stackval(-1);
19604:     LIns* v_ins = get(&v);
20410:     bool cond;
20410:     LIns* x;
23075: 
48470:     if (v.isNull() || v.isUndefined()) {
26753:         cond = false;
56750:         x = w.immi(0);
48470:     } else if (!v.isPrimitive()) {
26557:         cond = true;
56750:         x = w.immi(1);
48470:     } else if (v.isBoolean()) {
23075:         /* Test for boolean is true, negate later if we are testing for false. */
48470:         cond = v.isTrue();
56750:         x = w.eqiN(v_ins, 1);
48470:     } else if (v.isNumber()) {
48470:         jsdouble d = v.toNumber();
20411:         cond = !JSDOUBLE_IS_NaN(d) && d;
57700:         x = w.eqi0(w.eqi0(w.andi(w.eqd(v_ins, v_ins), w.eqi0(w.eqd0(v_ins)))));
48470:     } else if (v.isString()) {
48470:         cond = v.toString()->length() != 0;
57700:         x = w.eqi0(w.eqp0(w.getStringLength(v_ins)));
17452:     } else {
17927:         JS_NOT_REACHED("ifop");
33542:         return ARECORD_STOP;
20410:     }
26557: 
42717:     jsbytecode* pc = cx->regs->pc;
26557:     emitIf(pc, cond, x);
26557:     return checkTraceEnd(pc);
17452: }
17452: 
25099: #ifdef NANOJIT_IA32
30860: /*
30860:  * Record LIR for a tableswitch or tableswitchx op. We record LIR only the
30860:  * "first" time we hit the op. Later, when we start traces after exiting that
30860:  * trace, we just patch.
30860:  */
33542: JS_REQUIRES_STACK AbortableRecordingStatus
25099: TraceRecorder::tableswitch()
25099: {
48470:     Value& v = stackval(-1);
31060: 
31060:     /* No need to guard if the condition can't match any of the cases. */
48470:     if (!v.isNumber())
33542:         return ARECORD_CONTINUE;
25107: 
30860:     /* No need to guard if the condition is constant. */
41265:     LIns* v_ins = d2i(get(&v));
41265:     if (v_ins->isImmI())
33542:         return ARECORD_CONTINUE;
25099: 
42717:     jsbytecode* pc = cx->regs->pc;
25099:     /* Starting a new trace after exiting a trace via switch. */
26011:     if (anchor &&
26011:         (anchor->exitType == CASE_EXIT || anchor->exitType == DEFAULT_EXIT) &&
26011:         fragment->ip == pc) {
33542:         return ARECORD_CONTINUE;
26011:     }
25099: 
25099:     /* Decode jsop. */
25099:     jsint low, high;
25099:     if (*pc == JSOP_TABLESWITCH) {
25099:         pc += JUMP_OFFSET_LEN;
25099:         low = GET_JUMP_OFFSET(pc);
25099:         pc += JUMP_OFFSET_LEN;
25099:         high = GET_JUMP_OFFSET(pc);
25099:     } else {
25099:         pc += JUMPX_OFFSET_LEN;
56001:         low = GET_JUMP_OFFSET(pc);
56001:         pc += JUMP_OFFSET_LEN;
56001:         high = GET_JUMP_OFFSET(pc);
25099:     }
25099: 
50709:     /* 
50709:      * If there are no cases, this is a no-op. The default case immediately
50709:      * follows in the bytecode and is always taken, so we need no special
50709:      * action to handle it.
50709:      */
50709:     int count = high + 1 - low;
56001:     JS_ASSERT(count >= 0);
50709:     if (count == 0)
50709:         return ARECORD_CONTINUE;
50709: 
34322:     /* Cap maximum table-switch size for modesty. */
50709:     if (count > MAX_TABLE_SWITCH)
33542:         return InjectStatus(switchop());
25099: 
25099:     /* Generate switch LIR. */
34351:     SwitchInfo* si = new (traceAlloc()) SwitchInfo();
50709:     si->count = count;
25099:     si->table = 0;
25099:     si->index = (uint32) -1;
56750:     LIns* diff = w.subi(v_ins, w.immi(low));
56750:     LIns* cmp = w.ltui(diff, w.immi(si->count));
56750:     guard(true, cmp, DEFAULT_EXIT);
56750:     // We use AnyAddress;  it's imprecise but this case is rare and not worth its
56750:     // own access region.
56750:     w.st(diff, AnyAddress(w.immpNonGC(&si->index)));
27540:     VMSideExit* exit = snapshot(CASE_EXIT);
27540:     exit->switchInfo = si;
56750:     LIns* guardIns = w.xtbl(diff, createGuardRecord(exit));
31060:     fragment->lastIns = guardIns;
35083:     CHECK_STATUS_A(compile());
35083:     return finishSuccessfully();
25099: }
25099: #endif
25099: 
33542: JS_REQUIRES_STACK RecordingStatus
18687: TraceRecorder::switchop()
18687: {
48470:     Value& v = stackval(-1);
19604:     LIns* v_ins = get(&v);
30860: 
30860:     /* No need to guard if the condition is constant. */
40387:     if (v_ins->isImmAny())
33542:         return RECORD_CONTINUE;
48470:     if (v.isNumber()) {
48470:         jsdouble d = v.toNumber();
18687:         guard(true,
56750:               w.name(w.eqd(v_ins, w.immd(d)), "guard(switch on numeric)"),
18687:               BRANCH_EXIT);
48470:     } else if (v.isString()) {
59890:         LIns* args[] = { w.immpStrGC(v.toString()), v_ins, cx_ins };
59890:         LIns* equal_rval = w.call(&js_EqualStringsOnTrace_ci, args);
59890:         guard(false,
59890:               w.name(w.eqiN(equal_rval, JS_NEITHER), "guard(oom)"),
59890:               OOM_EXIT);
59890:         guard(false,
59890:               w.name(w.eqi0(equal_rval), "guard(switch on string)"),
18687:               BRANCH_EXIT);
48470:     } else if (v.isBoolean()) {
18687:         guard(true,
56750:               w.name(w.eqi(v_ins, w.immi(v.isTrue())), "guard(switch on boolean)"),
18687:               BRANCH_EXIT);
48470:     } else if (v.isUndefined()) {
48470:         // This is a unit type, so no guard is needed.
18687:     } else {
33542:         RETURN_STOP("switch on object or null");
33542:     }
33542:     return RECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK RecordingStatus
48470: TraceRecorder::inc(Value& v, jsint incr, bool pre)
17412: {
17782:     LIns* v_ins = get(&v);
59903:     Value dummy;
59903:     CHECK_STATUS(inc(v, v_ins, dummy, incr, pre));
17782:     set(&v, v_ins);
33542:     return RECORD_CONTINUE;
17782: }
17782: 
17782: /*
30860:  * On exit, v_ins is the incremented unboxed value, and the appropriate value
59903:  * (pre- or post-increment as described by pre) is stacked.  v_out is set to
59903:  * the value corresponding to v_ins.
17782:  */
33542: JS_REQUIRES_STACK RecordingStatus
59903: TraceRecorder::inc(const Value &v, LIns*& v_ins, Value &v_out, jsint incr, bool pre)
31480: {
31480:     LIns* v_after;
59903:     CHECK_STATUS(incHelper(v, v_ins, v_out, v_after, incr));
17544: 
42717:     const JSCodeSpec& cs = js_CodeSpec[*cx->regs->pc];
17544:     JS_ASSERT(cs.ndefs == 1);
17782:     stack(-cs.nuses, pre ? v_after : v_ins);
17782:     v_ins = v_after;
33542:     return RECORD_CONTINUE;
27933: }
27933: 
31480: /*
31480:  * Do an increment operation without storing anything to the stack.
59903:  *
59903:  * v_after is an out param whose value corresponds to the instruction the
59903:  * v_ins_after out param gets set to.
31480:  */
33542: JS_REQUIRES_STACK RecordingStatus
59903: TraceRecorder::incHelper(const Value &v, LIns*& v_ins, Value &v_after,
59903:                          LIns*& v_ins_after, jsint incr)
48470: {
57711:     // FIXME: Bug 606071 on making this work for objects.
57711:     if (!v.isPrimitive())
57711:         RETURN_STOP("can inc primitives only");
57711: 
58282:     // We need to modify |v_ins| the same way relational() modifies
58282:     // its RHS and LHS.
57711:     if (v.isUndefined()) {
59903:         v_ins_after = w.immd(js_NaN);
59903:         v_after.setDouble(js_NaN);
58282:         v_ins = w.immd(js_NaN);
57711:     } else if (v.isNull()) {
59903:         v_ins_after = w.immd(incr);
59903:         v_after.setDouble(incr);
58282:         v_ins = w.immd(0.0);
57711:     } else {
57711:         if (v.isBoolean()) {
57711:             v_ins = w.i2d(v_ins);
57711:         } else if (v.isString()) {
59890:             LIns* ok_ins = w.allocp(sizeof(JSBool));
59890:             LIns* args[] = { ok_ins, v_ins, cx_ins };
57711:             v_ins = w.call(&js_StringToNumber_ci, args);
59890:             guard(false,
59890:                   w.name(w.eqi0(w.ldiAlloc(ok_ins)), "guard(oom)"),
59890:                   OOM_EXIT);
57711:         } else {
57711:             JS_ASSERT(v.isNumber());
57711:         }
57711: 
57711:         jsdouble num;
57711:         AutoValueRooter tvr(cx);
57711:         *tvr.addr() = v;
57711:         ValueToNumber(cx, tvr.value(), &num);
59903:         v_ins_after = alu(LIR_addd, num, incr, v_ins, w.immd(incr));
59903:         v_after.setDouble(num + incr);
57711:     }
57711: 
33542:     return RECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17758: TraceRecorder::incProp(jsint incr, bool pre)
17758: {
48470:     Value& l = stackval(-1);
48470:     if (l.isPrimitive())
33542:         RETURN_STOP_A("incProp on primitive");
17758: 
48470:     JSObject* obj = &l.toObject();
17758:     LIns* obj_ins = get(&l);
17758: 
17761:     uint32 slot;
17758:     LIns* v_ins;
33542:     CHECK_STATUS_A(prop(obj, obj_ins, &slot, &v_ins, NULL));
17761: 
52503:     if (slot == SHAPE_INVALID_SLOT)
33542:         RETURN_STOP_A("incProp on invalid slot");
18666: 
48470:     Value& v = obj->getSlotRef(slot);
59903:     Value v_after;
59903:     CHECK_STATUS_A(inc(v, v_ins, v_after, incr, pre));
17761: 
56180:     LIns* slots_ins = NULL;
59903:     stobj_set_slot(obj, obj_ins, slot, slots_ins, v_after, v_ins);
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK RecordingStatus
17758: TraceRecorder::incElem(jsint incr, bool pre)
17758: {
48470:     Value& r = stackval(-1);
48470:     Value& l = stackval(-2);
48470:     Value* vp;
17758:     LIns* v_ins;
17782:     LIns* addr_ins;
28411: 
48470:     if (!l.isPrimitive() && l.toObject().isDenseArray() && r.isInt32()) {
42749:         guardDenseArray(get(&l), MISMATCH_EXIT);
53615:         CHECK_STATUS(denseArrayElement(l, r, vp, v_ins, addr_ins, snapshot(BRANCH_EXIT)));
20972:         if (!addr_ins) // if we read a hole, abort
33542:             return RECORD_STOP;
59903:         Value v_after;
59903:         CHECK_STATUS(inc(*vp, v_ins, v_after, incr, pre));
59903:         box_value_into(v_after, v_ins, DSlotsAddress(addr_ins));
33542:         return RECORD_CONTINUE;
17412:     }
17412: 
41877:     return callImacro((incr == 1)
41877:                       ? pre ? incelem_imacros.incelem : incelem_imacros.eleminc
41877:                       : pre ? decelem_imacros.decelem : decelem_imacros.elemdec);
41877: }
41877: 
19576: static bool
30860: EvalCmp(LOpcode op, double l, double r)
19576: {
18017:     bool cond;
18017:     switch (op) {
42688:       case LIR_eqd:
30859:         cond = (l == r);
19576:         break;
42688:       case LIR_ltd:
30859:         cond = l < r;
18017:         break;
42688:       case LIR_gtd:
30859:         cond = l > r;
18017:         break;
42688:       case LIR_led:
30859:         cond = l <= r;
18017:         break;
42688:       case LIR_ged:
30859:         cond = l >= r;
18017:         break;
18017:       default:
19576:         JS_NOT_REACHED("unexpected comparison op");
19576:         return false;
19576:     }
19576:     return cond;
19576: }
19576: 
19576: static bool
59890: EvalCmp(JSContext *cx, LOpcode op, JSString* l, JSString* r, JSBool *ret)
19576: {
42688:     if (op == LIR_eqd)
59890:         return EqualStrings(cx, l, r, ret);
59890:     JSBool cmp;
59890:     if (!CompareStrings(cx, l, r, &cmp))
59890:         return false;
59890:     *ret = EvalCmp(op, cmp, 0);
59890:     return true;
59890: }
59890: 
59890: JS_REQUIRES_STACK RecordingStatus
23093: TraceRecorder::strictEquality(bool equal, bool cmpCase)
22705: {
48470:     Value& r = stackval(-1);
48470:     Value& l = stackval(-2);
22705:     LIns* l_ins = get(&l);
22705:     LIns* r_ins = get(&r);
23115:     LIns* x;
59890:     JSBool cond;
22705: 
48490:     JSValueType ltag = getPromotedType(l);
48490:     if (ltag != getPromotedType(r)) {
23115:         cond = !equal;
56750:         x = w.immi(cond);
48470:     } else if (ltag == JSVAL_TYPE_STRING) {
59890:         LIns* args[] = { r_ins, l_ins, cx_ins };
59890:         LIns* equal_ins = w.call(&js_EqualStringsOnTrace_ci, args);
59890:         guard(false,
59890:               w.name(w.eqiN(equal_ins, JS_NEITHER), "guard(oom)"),
59890:               OOM_EXIT);
59890:         x = w.eqiN(equal_ins, equal);
59890:         if (!EqualStrings(cx, l.toString(), r.toString(), &cond))
59890:             RETURN_ERROR("oom");
22705:     } else {
48470:         if (ltag == JSVAL_TYPE_DOUBLE)
56750:             x = w.eqd(l_ins, r_ins);
48470:         else if (ltag == JSVAL_TYPE_NULL || ltag == JSVAL_TYPE_NONFUNOBJ || ltag == JSVAL_TYPE_FUNOBJ)
56750:             x = w.eqp(l_ins, r_ins);
32746:         else
56750:             x = w.eqi(l_ins, r_ins);
22705:         if (!equal)
56750:             x = w.eqi0(x);
48470:         cond = (ltag == JSVAL_TYPE_DOUBLE)
48470:                ? l.toNumber() == r.toNumber()
23117:                : l == r;
23093:     }
59908:     cond = (!!cond == equal);
23093: 
23093:     if (cmpCase) {
23093:         /* Only guard if the same path may not always be taken. */
41265:         if (!x->isImmI())
23093:             guard(cond, x, BRANCH_EXIT);
59890:         return RECORD_CONTINUE;
22705:     }
22705: 
22705:     set(&l, x);
59890:     return RECORD_CONTINUE;
22705: }
22705: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
23093: TraceRecorder::equality(bool negate, bool tryBranchAfterCond)
19576: {
48470:     Value& rval = stackval(-1);
48470:     Value& lval = stackval(-2);
23223:     LIns* l_ins = get(&lval);
23223:     LIns* r_ins = get(&rval);
23223: 
23223:     return equalityHelper(lval, rval, l_ins, r_ins, negate, tryBranchAfterCond, lval);
23223: }
23223: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
48470: TraceRecorder::equalityHelper(Value& l, Value& r, LIns* l_ins, LIns* r_ins,
23223:                               bool negate, bool tryBranchAfterCond,
48470:                               Value& rval)
23223: {
42688:     LOpcode op = LIR_eqi;
59890:     JSBool cond;
59890:     LIns* args[] = { NULL, NULL, NULL };
23223: 
23223:     /*
23223:      * The if chain below closely mirrors that found in 11.9.3, in general
23223:      * deviating from that ordering of ifs only to account for SpiderMonkey's
23223:      * conflation of booleans and undefined and for the possibility of
23223:      * confusing objects and null.  Note carefully the spec-mandated recursion
23223:      * in the final else clause, which terminates because Number == T recurs
23223:      * only if T is Object, but that must recur again to convert Object to
23223:      * primitive, and ToPrimitive throws if the object cannot be converted to
23223:      * a primitive value (which would terminate recursion).
23223:      */
23223: 
48490:     if (getPromotedType(l) == getPromotedType(r)) {
48470:         if (l.isUndefined() || l.isNull()) {
40307:             cond = true;
48470:             if (l.isNull())
42688:                 op = LIR_eqp;
48470:         } else if (l.isObject()) {
48622:             if (l.toObject().getClass()->ext.equality)
35473:                 RETURN_STOP_A("Can't trace extended class equality operator");
56750:             LIns* flags_ins = w.ldiObjFlags(l_ins);
56750:             LIns* flag_ins = w.andi(flags_ins, w.nameImmui(JSObject::HAS_EQUALITY));
56750:             guard(true, w.eqi0(flag_ins), BRANCH_EXIT);
56192: 
42688:             op = LIR_eqp;
23223:             cond = (l == r);
48470:         } else if (l.isBoolean()) {
48470:             JS_ASSERT(r.isBoolean());
40307:             cond = (l == r);
48470:         } else if (l.isString()) {
49111:             JSString *l_str = l.toString();
49111:             JSString *r_str = r.toString();
49111:             if (!l_str->isRope() && !r_str->isRope() && l_str->length() == 1 && r_str->length() == 1) {
49111:                 VMSideExit *exit = snapshot(BRANCH_EXIT);
57719:                 LIns *c = w.immw(1);
56750:                 guard(true, w.eqp(w.getStringLength(l_ins), c), exit);
56750:                 guard(true, w.eqp(w.getStringLength(r_ins), c), exit);
57719:                 l_ins = w.getStringChar(l_ins, w.immpNonGC(0));
57719:                 r_ins = w.getStringChar(r_ins, w.immpNonGC(0));
49111:             } else {
59890:                 args[0] = r_ins, args[1] = l_ins, args[2] = cx_ins;
59890:                 LIns *equal_ins = w.call(&js_EqualStringsOnTrace_ci, args);
59890:                 guard(false,
59890:                       w.name(w.eqiN(equal_ins, JS_NEITHER), "guard(oom)"),
59890:                       OOM_EXIT);
59890:                 l_ins = equal_ins;
56750:                 r_ins = w.immi(1);
49111:             }
59890:             if (!EqualStrings(cx, l.toString(), r.toString(), &cond))
59890:                 RETURN_ERROR_A("oom");
48470:         } else {
48470:             JS_ASSERT(l.isNumber() && r.isNumber());
48470:             cond = (l.toNumber() == r.toNumber());
42688:             op = LIR_eqd;
23223:         }
48470:     } else if (l.isNull() && r.isUndefined()) {
56750:         l_ins = w.immiUndefined();
40307:         cond = true;
48470:     } else if (l.isUndefined() && r.isNull()) {
56750:         r_ins = w.immiUndefined();
40307:         cond = true;
48470:     } else if (l.isNumber() && r.isString()) {
59890:         LIns* ok_ins = w.allocp(sizeof(JSBool));
59890:         args[0] = ok_ins, args[1] = r_ins, args[2] = cx_ins;
56750:         r_ins = w.call(&js_StringToNumber_ci, args);
59890:         guard(false,
59890:               w.name(w.eqi0(w.ldiAlloc(ok_ins)), "guard(oom)"),
59890:               OOM_EXIT);
59890:         JSBool ok;
59890:         double d = js_StringToNumber(cx, r.toString(), &ok);
59890:         if (!ok)
59890:             RETURN_ERROR_A("oom");
59890:         cond = (l.toNumber() == d);
42688:         op = LIR_eqd;
48470:     } else if (l.isString() && r.isNumber()) {
59890:         LIns* ok_ins = w.allocp(sizeof(JSBool));
59890:         args[0] = ok_ins, args[1] = l_ins, args[2] = cx_ins;
56750:         l_ins = w.call(&js_StringToNumber_ci, args);
59890:         guard(false,
59890:               w.name(w.eqi0(w.ldiAlloc(ok_ins)), "guard(oom)"),
59890:               OOM_EXIT);
59890:         JSBool ok;
59890:         double d = js_StringToNumber(cx, l.toString(), &ok);
59890:         if (!ok)
59890:             RETURN_ERROR_A("oom");
59890:         cond = (d == r.toNumber());
42688:         op = LIR_eqd;
23223:     } else {
40896:         // Below we may assign to l or r, which modifies the interpreter state.
40896:         // This is fine as long as we also update the tracker.
48470:         if (l.isBoolean()) {
56750:             l_ins = w.i2d(l_ins);
40896:             set(&l, l_ins);
48470:             l.setInt32(l.isTrue());
23223:             return equalityHelper(l, r, l_ins, r_ins, negate,
23223:                                   tryBranchAfterCond, rval);
23223:         }
48470:         if (r.isBoolean()) {
56750:             r_ins = w.i2d(r_ins);
40896:             set(&r, r_ins);
48470:             r.setInt32(r.isTrue());
23223:             return equalityHelper(l, r, l_ins, r_ins, negate,
23223:                                   tryBranchAfterCond, rval);
23223:         }
48470:         if ((l.isString() || l.isNumber()) && !r.isPrimitive()) {
40896:             CHECK_STATUS_A(guardNativeConversion(r));
41290:             return InjectStatus(callImacro(equality_imacros.any_obj));
28175:         }
48470:         if (!l.isPrimitive() && (r.isString() || r.isNumber())) {
40896:             CHECK_STATUS_A(guardNativeConversion(l));
41290:             return InjectStatus(callImacro(equality_imacros.obj_any));
41134:         }
41134: 
56750:         l_ins = w.immi(0);
56750:         r_ins = w.immi(1);
23223:         cond = false;
23223:     }
23223: 
23223:     /* If the operands aren't numbers, compare them as integers. */
56750:     LIns* x = w.ins2(op, l_ins, r_ins);
22705:     if (negate) {
56750:         x = w.eqi0(x);
22705:         cond = !cond;
22705:     }
22705: 
42717:     jsbytecode* pc = cx->regs->pc;
26557: 
22705:     /*
26118:      * Don't guard if the same path is always taken.  If it isn't, we have to
26118:      * fuse comparisons and the following branch, because the interpreter does
26118:      * that.
22705:      */
26557:     if (tryBranchAfterCond)
26557:         fuseIf(pc + 1, cond, x);
26557: 
26557:     /*
26557:      * There is no need to write out the result of this comparison if the trace
26557:      * ends on this operation.
26557:      */
27933:     if (pc[1] == JSOP_IFNE || pc[1] == JSOP_IFEQ)
33542:         CHECK_STATUS_A(checkTraceEnd(pc + 1));
22705: 
22705:     /*
22705:      * We update the stack after the guard. This is safe since the guard bails
22705:      * out at the comparison and the interpreter will therefore re-execute the
22705:      * comparison. This way the value of the condition doesn't have to be
22705:      * calculated and saved on the stack in most cases.
22705:      */
23223:     set(&rval, x);
26557: 
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
23093: TraceRecorder::relational(LOpcode op, bool tryBranchAfterCond)
22705: {
48470:     Value& r = stackval(-1);
48470:     Value& l = stackval(-2);
22705:     LIns* x = NULL;
59890:     JSBool cond;
22705:     LIns* l_ins = get(&l);
22705:     LIns* r_ins = get(&r);
22705:     bool fp = false;
22705:     jsdouble lnum, rnum;
22705: 
22705:     /*
22705:      * 11.8.5 if either argument is an object with a function-valued valueOf
22705:      * property; if both arguments are objects with non-function-valued valueOf
22705:      * properties, abort.
22705:      */
48470:     if (!l.isPrimitive()) {
40896:         CHECK_STATUS_A(guardNativeConversion(l));
48470:         if (!r.isPrimitive()) {
40896:             CHECK_STATUS_A(guardNativeConversion(r));
41290:             return InjectStatus(callImacro(binary_imacros.obj_obj));
41290:         }
41290:         return InjectStatus(callImacro(binary_imacros.obj_any));
22705:     }
48470:     if (!r.isPrimitive()) {
40896:         CHECK_STATUS_A(guardNativeConversion(r));
41290:         return InjectStatus(callImacro(binary_imacros.any_obj));
28175:     }
22705: 
22705:     /* 11.8.5 steps 3, 16-21. */
48470:     if (l.isString() && r.isString()) {
59890:         LIns* args[] = { r_ins, l_ins, cx_ins };
59890:         LIns* result_ins = w.call(&js_CompareStringsOnTrace_ci, args);
59890:         guard(false,
59890:               w.name(w.eqiN(result_ins, INT32_MIN), "guard(oom)"),
59890:               OOM_EXIT);
59890:         l_ins = result_ins;
56750:         r_ins = w.immi(0);
59890:         if (!EvalCmp(cx, op, l.toString(), r.toString(), &cond))
59890:             RETURN_ERROR_A("oom");
22705:         goto do_comparison;
22705:     }
22705: 
22705:     /* 11.8.5 steps 4-5. */
48470:     if (!l.isNumber()) {
48470:         if (l.isBoolean()) {
56750:             l_ins = w.i2d(l_ins);
48470:         } else if (l.isUndefined()) {
56750:             l_ins = w.immd(js_NaN);
48470:         } else if (l.isString()) {
59890:             LIns* ok_ins = w.allocp(sizeof(JSBool));
59890:             LIns* args[] = { ok_ins, l_ins, cx_ins };
56750:             l_ins = w.call(&js_StringToNumber_ci, args);
59890:             guard(false,
59890:                   w.name(w.eqi0(w.ldiAlloc(ok_ins)), "guard(oom)"),
59890:                   OOM_EXIT);
48470:         } else if (l.isNull()) {
56750:             l_ins = w.immd(0.0);
48470:         } else {
22705:             JS_NOT_REACHED("JSVAL_IS_NUMBER if int/double, objects should "
22705:                            "have been handled at start of method");
33542:             RETURN_STOP_A("safety belt");
22705:         }
22705:     }
48470:     if (!r.isNumber()) {
48470:         if (r.isBoolean()) {
56750:             r_ins = w.i2d(r_ins);
48470:         } else if (r.isUndefined()) {
56750:             r_ins = w.immd(js_NaN);
48470:         } else if (r.isString()) {
59890:             LIns* ok_ins = w.allocp(sizeof(JSBool));
59890:             LIns* args[] = { ok_ins, r_ins, cx_ins };
56750:             r_ins = w.call(&js_StringToNumber_ci, args);
59890:             guard(false,
59890:                   w.name(w.eqi0(w.ldiAlloc(ok_ins)), "guard(oom)"),
59890:                   OOM_EXIT);
48470:         } else if (r.isNull()) {
56750:             r_ins = w.immd(0.0);
48470:         } else {
22705:             JS_NOT_REACHED("JSVAL_IS_NUMBER if int/double, objects should "
22705:                            "have been handled at start of method");
33542:             RETURN_STOP_A("safety belt");
22705:         }
22705:     }
22705:     {
48470:         AutoValueRooter tvr(cx);
40403:         *tvr.addr() = l;
40828:         ValueToNumber(cx, tvr.value(), &lnum);
40403:         *tvr.addr() = r;
40828:         ValueToNumber(cx, tvr.value(), &rnum);
22705:     }
30860:     cond = EvalCmp(op, lnum, rnum);
22705:     fp = true;
22705: 
22705:     /* 11.8.5 steps 6-15. */
22705:   do_comparison:
30860:     /*
30860:      * If the result is not a number or it's not a quad, we must use an integer
30860:      * compare.
30860:      */
22651:     if (!fp) {
41265:         JS_ASSERT(isCmpDOpcode(op));
41265:         op = cmpOpcodeD2I(op);
22651:     }
56750:     x = w.ins2(op, l_ins, r_ins);
22705: 
42717:     jsbytecode* pc = cx->regs->pc;
26557: 
22705:     /*
26118:      * Don't guard if the same path is always taken.  If it isn't, we have to
26118:      * fuse comparisons and the following branch, because the interpreter does
26118:      * that.
22705:      */
26557:     if (tryBranchAfterCond)
26557:         fuseIf(pc + 1, cond, x);
26557: 
26557:     /*
26557:      * There is no need to write out the result of this comparison if the trace
26557:      * ends on this operation.
26557:      */
27933:     if (pc[1] == JSOP_IFNE || pc[1] == JSOP_IFEQ)
33542:         CHECK_STATUS_A(checkTraceEnd(pc + 1));
22705: 
22705:     /*
22705:      * We update the stack after the guard. This is safe since the guard bails
22705:      * out at the comparison and the interpreter will therefore re-execute the
22705:      * comparison. This way the value of the condition doesn't have to be
22705:      * calculated and saved on the stack in most cases.
22705:      */
17413:     set(&l, x);
26557: 
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK RecordingStatus
17467: TraceRecorder::unary(LOpcode op)
17467: {
48470:     Value& v = stackval(-1);
41265:     bool intop = retTypes[op] == LTy_I;
48470:     if (v.isNumber()) {
17467:         LIns* a = get(&v);
17467:         if (intop)
41265:             a = d2i(a);
56750:         a = w.ins1(op, a);
17467:         if (intop)
56750:             a = w.i2d(a);
17467:         set(&v, a);
33542:         return RECORD_CONTINUE;
33542:     }
33542:     return RECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK RecordingStatus
17467: TraceRecorder::binary(LOpcode op)
17467: {
48470:     Value& r = stackval(-1);
48470:     Value& l = stackval(-2);
48470: 
48470:     if (!l.isPrimitive()) {
40896:         CHECK_STATUS(guardNativeConversion(l));
48470:         if (!r.isPrimitive()) {
40896:             CHECK_STATUS(guardNativeConversion(r));
41290:             return callImacro(binary_imacros.obj_obj);
41290:         }
41290:         return callImacro(binary_imacros.obj_any);
21685:     }
48470:     if (!r.isPrimitive()) {
40896:         CHECK_STATUS(guardNativeConversion(r));
41290:         return callImacro(binary_imacros.any_obj);
41134:     }
41134: 
41265:     bool intop = retTypes[op] == LTy_I;
17467:     LIns* a = get(&l);
17467:     LIns* b = get(&r);
21799: 
48470:     bool leftIsNumber = l.isNumber();
48470:     jsdouble lnum = leftIsNumber ? l.toNumber() : 0;
48470: 
48470:     bool rightIsNumber = r.isNumber();
48470:     jsdouble rnum = rightIsNumber ? r.toNumber() : 0;
48470: 
48470:     if (l.isString()) {
42688:         NanoAssert(op != LIR_addd); // LIR_addd/IS_STRING case handled by record_JSOP_ADD()
59890:         LIns* ok_ins = w.allocp(sizeof(JSBool));
59890:         LIns* args[] = { ok_ins, a, cx_ins };
56750:         a = w.call(&js_StringToNumber_ci, args);
59890:         guard(false,
59890:               w.name(w.eqi0(w.ldiAlloc(ok_ins)), "guard(oom)"),
59890:               OOM_EXIT);
59890:         JSBool ok;
59890:         lnum = js_StringToNumber(cx, l.toString(), &ok);
59890:         if (!ok)
59890:             RETURN_ERROR("oom");
21799:         leftIsNumber = true;
17910:     }
48470:     if (r.isString()) {
42688:         NanoAssert(op != LIR_addd); // LIR_addd/IS_STRING case handled by record_JSOP_ADD()
59890:         LIns* ok_ins = w.allocp(sizeof(JSBool));
59890:         LIns* args[] = { ok_ins, b, cx_ins };
56750:         b = w.call(&js_StringToNumber_ci, args);
59890:         guard(false,
59890:               w.name(w.eqi0(w.ldiAlloc(ok_ins)), "guard(oom)"),
59890:               OOM_EXIT);
59890:         JSBool ok;
59890:         rnum = js_StringToNumber(cx, r.toString(), &ok);
59890:         if (!ok)
59890:             RETURN_ERROR("oom");
21799:         rightIsNumber = true;
17910:     }
48470:     if (l.isBoolean()) {
56750:         a = w.i2d(a);
48470:         lnum = l.toBoolean();
48470:         leftIsNumber = true;
48470:     } else if (l.isUndefined()) {
56750:         a = w.immd(js_NaN);
40307:         lnum = js_NaN;
21799:         leftIsNumber = true;
20972:     }
48470:     if (r.isBoolean()) {
56750:         b = w.i2d(b);
48470:         rnum = r.toBoolean();
48470:         rightIsNumber = true;
48470:     } else if (r.isUndefined()) {
56750:         b = w.immd(js_NaN);
40307:         rnum = js_NaN;
21799:         rightIsNumber = true;
21799:     }
21799:     if (leftIsNumber && rightIsNumber) {
17467:         if (intop) {
56750:             a = (op == LIR_rshui) ? d2u(a) : d2i(a);
41265:             b = d2i(b);
17467:         }
21799:         a = alu(op, lnum, rnum, a, b);
17467:         if (intop)
56750:             a = (op == LIR_rshui) ? w.ui2d(a) : w.i2d(a);
17467:         set(&l, a);
33542:         return RECORD_CONTINUE;
33542:     }
33542:     return RECORD_STOP;
17467: }
17467: 
33560: #if defined DEBUG_notme && defined XP_UNIX
33560: #include <stdio.h>
33560: 
33560: static FILE* shapefp = NULL;
33560: 
33560: static void
33560: DumpShape(JSObject* obj, const char* prefix)
33560: {
33560:     if (!shapefp) {
33560:         shapefp = fopen("/tmp/shapes.dump", "w");
33560:         if (!shapefp)
33560:             return;
33560:     }
33560: 
52503:     fprintf(shapefp, "\n%s: shape %u flags %x\n", prefix, obj->shape(), obj->flags);
52503:     for (Shape::Range r = obj->lastProperty()->all(); !r.empty(); r.popFront()) {
52503:         const Shape &shape = r.front();
52503: 
52503:         if (JSID_IS_ATOM(shape.id)) {
57721:             putc(' ', shapefp);
57721:             JS_PutString(JSID_TO_STRING(shape.id), shapefp);
52503:         } else {
52503:             JS_ASSERT(!JSID_IS_OBJECT(shape.id));
52503:             fprintf(shapefp, " %d", JSID_TO_INT(shape.id));
33560:         }
33560:         fprintf(shapefp, " %u %p %p %x %x %d\n",
52503:                 shape.slot, shape.getter, shape.setter, shape.attrs, shape.flags, shape.shortid);
33560:     }
33560:     fflush(shapefp);
33560: }
33560: 
32777: void
33560: TraceRecorder::dumpGuardedShapes(const char* prefix)
33560: {
38568:     for (GuardedShapeTable::Range r = guardedShapeTable.all(); !r.empty(); r.popFront())
38568:         DumpShape(r.front().value, prefix);
33560: }
33560: #endif /* DEBUG_notme && XP_UNIX */
33560: 
33560: JS_REQUIRES_STACK RecordingStatus
32777: TraceRecorder::guardShape(LIns* obj_ins, JSObject* obj, uint32 shape, const char* guardName,
40393:                           VMSideExit* exit)
32777: {
33560:     // Test (with add if missing) for a remembered guard for (obj_ins, obj).
38568:     GuardedShapeTable::AddPtr p = guardedShapeTable.lookupForAdd(obj_ins);
38568:     if (p) {
38568:         JS_ASSERT(p->value == obj);
38568:         return RECORD_CONTINUE;
48588:     }
38568:     if (!guardedShapeTable.add(p, obj_ins, obj))
33560:         return RECORD_ERROR;
48588: 
48588:     if (obj == globalObj) {
48588:         // In this case checking object identity is equivalent and faster.
48588:         guard(true,
56750:               w.name(w.eqp(obj_ins, w.immpObjGC(globalObj)), "guard_global"),
48588:               exit);
48588:         return RECORD_CONTINUE;
33560:     }
33560: 
33560: #if defined DEBUG_notme && defined XP_UNIX
33560:     DumpShape(obj, "guard");
33560:     fprintf(shapefp, "for obj_ins %p\n", obj_ins);
33560: #endif
33560: 
33560:     // Finally, emit the shape guard.
56750:     guard(true, w.name(w.eqiN(w.ldiObjShape(obj_ins), shape), guardName), exit);
33560:     return RECORD_CONTINUE;
33560: }
33560: 
38568: void
38568: TraceRecorder::forgetGuardedShapesForObject(JSObject* obj)
38568: {
39896:     for (GuardedShapeTable::Enum e(guardedShapeTable); !e.empty(); e.popFront()) {
38568:         if (e.front().value == obj) {
33560: #if defined DEBUG_notme && defined XP_UNIX
33560:             DumpShape(entry->obj, "forget");
33560: #endif
38568:             e.removeFront();
38568:         }
38568:     }
33560: }
33560: 
33560: void
33560: TraceRecorder::forgetGuardedShapes()
33560: {
33560: #if defined DEBUG_notme && defined XP_UNIX
33560:     dumpGuardedShapes("forget-all");
33560: #endif
38568:     guardedShapeTable.clear();
32777: }
32777: 
39939: JS_REQUIRES_STACK AbortableRecordingStatus
40374: TraceRecorder::test_property_cache(JSObject* obj, LIns* obj_ins, JSObject*& obj2, PCVal& pcval)
17459: {
42717:     jsbytecode* pc = cx->regs->pc;
32658:     JS_ASSERT(*pc != JSOP_INITPROP && *pc != JSOP_INITMETHOD &&
32658:               *pc != JSOP_SETNAME && *pc != JSOP_SETPROP && *pc != JSOP_SETMETHOD);
19093: 
18439:     // Mimic the interpreter's special case for dense arrays by skipping up one
18439:     // hop along the proto chain when accessing a named (not indexed) property,
18439:     // typically to find Array.prototype methods.
18026:     JSObject* aobj = obj;
39928:     if (obj->isDenseArray()) {
42749:         guardDenseArray(obj_ins, BRANCH_EXIT);
39928:         aobj = obj->getProto();
56750:         obj_ins = w.ldpObjProto(obj_ins);
18026:     }
18026: 
42765:     if (!aobj->isNative())
42764:         RETURN_STOP_A("non-native object");
42764: 
17459:     JSAtom* atom;
40362:     PropertyCacheEntry* entry;
40362:     JS_PROPERTY_CACHE(cx).test(cx, pc, aobj, obj2, entry, atom);
38504:     if (atom) {
18439:         // Miss: pre-fill the cache for the interpreter, as well as for our needs.
49113:         // FIXME: bug 458271.
18439:         jsid id = ATOM_TO_JSID(atom);
49113: 
49113:         // The lookup below may change object shapes.
49113:         forgetGuardedShapes();
49113: 
17747:         JSProperty* prop;
19093:         if (JOF_OPMODE(*pc) == JOF_NAME) {
18112:             JS_ASSERT(aobj == obj);
33171: 
37741:             TraceMonitor &localtm = *traceMonitor;
27575:             entry = js_FindPropertyHelper(cx, id, true, &obj, &obj2, &prop);
41777:             if (!entry)
41777:                 RETURN_ERROR_A("error in js_FindPropertyHelper");
27575: 
33171:             /* js_FindPropertyHelper can reenter the interpreter and kill |this|. */
33171:             if (!localtm.recorder)
33542:                 return ARECORD_ABORTED;
33171: 
27933:             if (entry == JS_NO_PROP_CACHE_FILL)
33542:                 RETURN_STOP_A("cannot cache name");
17747:         } else {
37741:             TraceMonitor &localtm = *traceMonitor;
19712:             int protoIndex = js_LookupPropertyWithFlags(cx, aobj, id,
19712:                                                         cx->resolveFlags,
19712:                                                         &obj2, &prop);
27575: 
41777:             if (protoIndex < 0)
41777:                 RETURN_ERROR_A("error in js_LookupPropertyWithFlags");
41777: 
33171:             /* js_LookupPropertyWithFlags can reenter the interpreter and kill |this|. */
56567:             if (!localtm.recorder)
33542:                 return ARECORD_ABORTED;
33171: 
17998:             if (prop) {
43290:                 if (!obj2->isNative())
33542:                     RETURN_STOP_A("property found on non-native object");
40362:                 entry = JS_PROPERTY_CACHE(cx).fill(cx, aobj, 0, protoIndex, obj2,
52503:                                                    (Shape*) prop);
27575:                 JS_ASSERT(entry);
27575:                 if (entry == JS_NO_PROP_CACHE_FILL)
27575:                     entry = NULL;
17998:             }
33171: 
17998:         }
17998: 
17998:         if (!prop) {
17998:             // Propagate obj from js_FindPropertyHelper to record_JSOP_BINDNAME
18712:             // via our obj2 out-parameter. If we are recording JSOP_SETNAME and
18712:             // the global it's assigning does not yet exist, create it.
17998:             obj2 = obj;
19093: 
40374:             // Use a null pcval to return "no such property" to our caller.
40374:             pcval.setNull();
33542:             return ARECORD_CONTINUE;
17998:         }
17998: 
18439:         if (!entry)
33542:             RETURN_STOP_A("failed to fill property cache");
17998:     }
17878: 
18439: #ifdef JS_THREADSAFE
18439:     // There's a potential race in any JS_THREADSAFE embedding that's nuts
18439:     // enough to share mutable objects on the scope or proto chain, but we
18439:     // don't care about such insane embeddings. Anyway, the (scope, proto)
18439:     // entry->vcap coordinates must reach obj2 from aobj at this point.
56559:     JS_ASSERT(cx->thread->data.requestDepth);
18439: #endif
18439: 
40393:     return InjectStatus(guardPropertyCacheHit(obj_ins, aobj, obj2, entry, pcval));
33542: }
33542: 
33542: JS_REQUIRES_STACK RecordingStatus
30847: TraceRecorder::guardPropertyCacheHit(LIns* obj_ins,
30847:                                      JSObject* aobj,
30847:                                      JSObject* obj2,
40362:                                      PropertyCacheEntry* entry,
40374:                                      PCVal& pcval)
30847: {
32761:     VMSideExit* exit = snapshot(BRANCH_EXIT);
32761: 
40362:     uint32 vshape = entry->vshape();
38981: 
38531:     // Special case for the global object, which may be aliased to get a property value.
38531:     // To catch cross-global property accesses we must check against globalObj identity.
38531:     // But a JOF_NAME mode opcode needs no guard, as we ensure the global object's shape
38531:     // never changes, and name ops can't reach across a global object ('with' aborts).
38531:     if (aobj == globalObj) {
38531:         if (entry->adding())
38531:             RETURN_STOP("adding a property to the global object");
38531: 
53840:         JSOp op = js_GetOpcode(cx, cx->fp()->script(), cx->regs->pc);
38531:         if (JOF_OPMODE(op) != JOF_NAME) {
38531:             guard(true,
56750:                   w.name(w.eqp(obj_ins, w.immpObjGC(globalObj)), "guard_global"),
38531:                   exit);
38531:         }
38531:     } else {
40393:         CHECK_STATUS(guardShape(obj_ins, aobj, entry->kshape, "guard_kshape", exit));
38531:     }
30847: 
30847:     if (entry->adding()) {
56750:         LIns *vshape_ins =
57719:             w.ldiRuntimeProtoHazardShape(w.ldpConstContextField(runtime));
40327: 
30847:         guard(true,
56750:               w.name(w.eqiN(vshape_ins, vshape), "guard_protoHazardShape"),
57779:               BRANCH_EXIT);
30847:     }
18439: 
21685:     // For any hit that goes up the scope and/or proto chains, we will need to
18439:     // guard on the shape of the object containing the property.
40362:     if (entry->vcapTag() >= 1) {
40847:         JS_ASSERT(obj2->shape() == vshape);
38531:         if (obj2 == globalObj)
38531:             RETURN_STOP("hitting the global object via a prototype chain");
18439: 
20979:         LIns* obj2_ins;
40362:         if (entry->vcapTag() == 1) {
40362:             // Duplicate the special case in PropertyCache::test.
56750:             obj2_ins = w.ldpObjProto(obj_ins);
56750:             guard(false, w.eqp0(obj2_ins), exit);
56750:         } else {
56750:             obj2_ins = w.immpObjGC(obj2);
20979:         }
40393:         CHECK_STATUS(guardShape(obj2_ins, obj2, vshape, "guard_vshape", exit));
17878:     }
17747: 
18439:     pcval = entry->vword;
33542:     return RECORD_CONTINUE;
17459: }
17459: 
17429: void
48470: TraceRecorder::stobj_set_fslot(LIns *obj_ins, unsigned slot, const Value &v, LIns* v_ins)
48470: {
56750:     box_value_into(v, v_ins, FSlotsAddress(obj_ins, slot));
28554: }
28554: 
28554: void
56180: TraceRecorder::stobj_set_dslot(LIns *obj_ins, unsigned slot, LIns*& slots_ins, 
48470:                                const Value &v, LIns* v_ins)
22626: {
56180:     if (!slots_ins)
57719:         slots_ins = w.ldpObjSlots(obj_ins);
56750:     box_value_into(v, v_ins, DSlotsAddress(slots_ins, slot));
22626: }
22626: 
22626: void
56180: TraceRecorder::stobj_set_slot(JSObject *obj, LIns* obj_ins, unsigned slot, LIns*& slots_ins,
48470:                               const Value &v, LIns* v_ins)
17426: {
55746:     /*
55746:      * A shape guard must have already been generated for obj, which will
55746:      * ensure that future objects have the same number of fixed slots.
55746:      */
55746:     if (!obj->hasSlotsArray()) {
55746:         JS_ASSERT(slot < obj->numSlots());
48470:         stobj_set_fslot(obj_ins, slot, v, v_ins);
55746:     } else {
56180:         stobj_set_dslot(obj_ins, slot, slots_ins, v, v_ins);
55746:     }
48470: }
48470: 
17899: LIns*
48470: TraceRecorder::unbox_slot(JSObject *obj, LIns *obj_ins, uint32 slot, VMSideExit *exit)
48470: {
55746:     /* Same guarantee about fixed slots as stobj_set_slot. */
56750:     Address addr = (!obj->hasSlotsArray())
56750:                  ? (Address)FSlotsAddress(obj_ins, slot)
56750:                  : (Address)DSlotsAddress(w.ldpObjSlots(obj_ins), slot);
56750: 
56750:     return unbox_value(obj->getSlot(slot), addr, exit);
48470: }
48470: 
48470: #if JS_BITS_PER_WORD == 32
48470: 
48470: void
56750: TraceRecorder::box_undefined_into(Address addr)
56750: {
56750:     w.stiValueTag(w.nameImmui(JSVAL_TAG_UNDEFINED), addr);
57719:     w.stiValuePayload(w.immi(0), addr);
48470: }
48470: 
48470: void
56750: TraceRecorder::box_null_into(Address addr)
56750: {
56750:     w.stiValueTag(w.nameImmui(JSVAL_TAG_NULL), addr);
57719:     w.stiValuePayload(w.immi(0), addr);
48470: }
48470: 
48470: inline LIns*
56750: TraceRecorder::unbox_number_as_double(Address addr, LIns *tag_ins, VMSideExit *exit)
56750: {
56750:     guard(true, w.leui(tag_ins, w.nameImmui(JSVAL_UPPER_INCL_TAG_OF_NUMBER_SET)), exit);
56750:     LIns *val_ins = w.ldiValuePayload(addr);
48470:     LIns* args[] = { val_ins, tag_ins };
56750:     return w.call(&js_UnboxDouble_ci, args);
48470: }
48470: 
48470: inline LIns*
56750: TraceRecorder::unbox_non_double_object(Address addr, LIns* tag_ins,
56750:                                        JSValueType type, VMSideExit* exit)
48470: {
48470:     LIns *val_ins;
48470:     if (type == JSVAL_TYPE_UNDEFINED) {
56750:         val_ins = w.immiUndefined();
48470:     } else if (type == JSVAL_TYPE_NULL) {
56750:         val_ins = w.immpNull();
48470:     } else {
48470:         JS_ASSERT(type == JSVAL_TYPE_INT32 || type == JSVAL_TYPE_OBJECT ||
48470:                   type == JSVAL_TYPE_STRING || type == JSVAL_TYPE_BOOLEAN ||
48470:                   type == JSVAL_TYPE_MAGIC);
56750:         val_ins = w.ldiValuePayload(addr);
56750:     }
56750: 
56750:     guard(true, w.eqi(tag_ins, w.nameImmui(JSVAL_TYPE_TO_TAG(type))), exit);
48470:     return val_ins;
48470: }
48470: 
48470: LIns*
56750: TraceRecorder::unbox_object(Address addr, LIns* tag_ins, JSValueType type, VMSideExit* exit)
48470: {
48470:     JS_ASSERT(type == JSVAL_TYPE_FUNOBJ || type == JSVAL_TYPE_NONFUNOBJ);
57719:     guard(true, w.name(w.eqi(tag_ins, w.nameImmui(JSVAL_TAG_OBJECT)), "isObj"), exit);
56750:     LIns *payload_ins = w.ldiValuePayload(addr);
48470:     if (type == JSVAL_TYPE_FUNOBJ)
48613:         guardClass(payload_ins, &js_FunctionClass, exit, LOAD_NORMAL);
48470:     else
48613:         guardNotClass(payload_ins, &js_FunctionClass, exit, LOAD_NORMAL);
48470:     return payload_ins;
48470: }
48470: 
48470: LIns*
56750: TraceRecorder::unbox_value(const Value &v, Address addr, VMSideExit *exit, bool force_double)
56750: {
56750:     LIns *tag_ins = w.ldiValueTag(addr);
48470: 
48470:     if (v.isNumber() && force_double)
56750:         return unbox_number_as_double(addr, tag_ins, exit);
48470: 
48470:     if (v.isInt32()) {
57719:         guard(true, w.name(w.eqi(tag_ins, w.nameImmui(JSVAL_TAG_INT32)), "isInt"), exit);
56750:         return w.i2d(w.ldiValuePayload(addr));
48470:     }
48470: 
48470:     if (v.isDouble()) {
57719:         guard(true, w.name(w.ltui(tag_ins, w.nameImmui(JSVAL_TAG_CLEAR)), "isDouble"), exit);
56750:         return w.ldd(addr);
48470:     }
48470: 
48470:     if (v.isObject()) {
48470:         JSValueType type = v.toObject().isFunction() ? JSVAL_TYPE_FUNOBJ : JSVAL_TYPE_NONFUNOBJ;
56750:         return unbox_object(addr, tag_ins, type, exit);
48470:     }
48470: 
48470:     JSValueType type = v.extractNonDoubleObjectTraceType();
56750:     return unbox_non_double_object(addr, tag_ins, type, exit);
48470: }
48470: 
48470: void
56750: TraceRecorder::unbox_any_object(Address addr, LIns **obj_ins, LIns **is_obj_ins)
56750: {
56750:     LIns *tag_ins = w.ldiValueTag(addr);
56750:     *is_obj_ins = w.eqi(tag_ins, w.nameImmui(JSVAL_TAG_OBJECT));
56750:     *obj_ins = w.ldiValuePayload(addr);
48470: }
48470: 
48470: LIns*
56750: TraceRecorder::is_boxed_true(Address addr)
56750: {
56750:     LIns *tag_ins = w.ldiValueTag(addr);
56750:     LIns *bool_ins = w.eqi(tag_ins, w.nameImmui(JSVAL_TAG_BOOLEAN));
56750:     LIns *payload_ins = w.ldiValuePayload(addr);
59879:     return w.gtiN(w.andi(bool_ins, payload_ins), 0);
48470: }
48470: 
55525: LIns*
56750: TraceRecorder::is_boxed_magic(Address addr, JSWhyMagic why)
56750: {
56750:     LIns *tag_ins = w.ldiValueTag(addr);
56750:     return w.eqi(tag_ins, w.nameImmui(JSVAL_TAG_MAGIC));
55525: }
55525: 
48470: void
56750: TraceRecorder::box_value_into(const Value &v, LIns *v_ins, Address addr)
48470: {
48470:     if (v.isNumber()) {
48470:         JS_ASSERT(v_ins->isD());
48470:         if (fcallinfo(v_ins) == &js_UnboxDouble_ci) {
56750:             w.stiValueTag(v_ins->callArgN(0), addr);
56750:             w.stiValuePayload(v_ins->callArgN(1), addr);
59993:         } else if (IsPromotedInt32(v_ins)) {
59993:             LIns *int_ins = w.demoteToInt32(v_ins);
56750:             w.stiValueTag(w.nameImmui(JSVAL_TAG_INT32), addr);
56750:             w.stiValuePayload(int_ins, addr);
56750:         } else {
56750:             w.std(v_ins, addr);
48470:         }
48470:         return;
48470:     }
48470: 
48470:     if (v.isUndefined()) {
56750:         box_undefined_into(addr);
48470:     } else if (v.isNull()) {
56750:         box_null_into(addr);
48470:     } else {
48470:         JSValueTag tag = v.isObject() ? JSVAL_TAG_OBJECT : v.extractNonDoubleObjectTraceTag();
56750:         w.stiValueTag(w.nameImmui(tag), addr);
56750:         w.stiValuePayload(v_ins, addr);
48470:     }
48470: }
48470: 
48470: LIns*
54427: TraceRecorder::box_value_for_native_call(const Value &v, LIns *v_ins)
54427: {
54427:     return box_value_into_alloc(v, v_ins);
54427: }
54427: 
54427: #elif JS_BITS_PER_WORD == 64
48470: 
48470: void
56750: TraceRecorder::box_undefined_into(Address addr)
56750: {
56750:     w.stq(w.nameImmq(JSVAL_BITS(JSVAL_VOID)), addr);
48470: }
48470: 
48470: inline LIns *
48470: TraceRecorder::non_double_object_value_has_type(LIns *v_ins, JSValueType type)
48470: {
56750:     return w.eqi(w.q2i(w.rshuqN(v_ins, JSVAL_TAG_SHIFT)),
56750:                  w.nameImmui(JSVAL_TYPE_TO_TAG(type)));
48470: }
48470: 
48470: inline LIns *
48470: TraceRecorder::unpack_ptr(LIns *v_ins)
48470: {
56750:     return w.andq(v_ins, w.nameImmq(JSVAL_PAYLOAD_MASK));
48470: }
48470: 
48470: inline LIns *
48470: TraceRecorder::unbox_number_as_double(LIns *v_ins, VMSideExit *exit)
48470: {
48470:     guard(true,
56750:           w.ltuq(v_ins, w.nameImmq(JSVAL_UPPER_EXCL_SHIFTED_TAG_OF_NUMBER_SET)),
48470:           exit);
48470:     LIns* args[] = { v_ins };
56750:     return w.call(&js_UnboxDouble_ci, args);
48470: }
48470: 
48470: inline nanojit::LIns*
48470: TraceRecorder::unbox_non_double_object(LIns* v_ins, JSValueType type, VMSideExit* exit)
48470: {
48470:     JS_ASSERT(type <= JSVAL_UPPER_INCL_TYPE_OF_VALUE_SET);
48470:     LIns *unboxed_ins;
48470:     if (type == JSVAL_TYPE_UNDEFINED) {
56750:         unboxed_ins = w.immiUndefined();
48470:     } else if (type == JSVAL_TYPE_NULL) {
56750:         unboxed_ins = w.immpNull();
53557:     } else if (type >= JSVAL_LOWER_INCL_TYPE_OF_PTR_PAYLOAD_SET) {
48470:         unboxed_ins = unpack_ptr(v_ins);
48470:     } else {
48470:         JS_ASSERT(type == JSVAL_TYPE_INT32 || type == JSVAL_TYPE_BOOLEAN || type == JSVAL_TYPE_MAGIC);
56750:         unboxed_ins = w.q2i(v_ins);
48470:     }
48470: 
48470:     guard(true, non_double_object_value_has_type(v_ins, type), exit);
48470:     return unboxed_ins;
27012: }
27012: 
27012: LIns*
48470: TraceRecorder::unbox_object(LIns* v_ins, JSValueType type, VMSideExit* exit)
48470: {
48470:     JS_STATIC_ASSERT(JSVAL_TYPE_OBJECT == JSVAL_UPPER_INCL_TYPE_OF_VALUE_SET);
48470:     JS_ASSERT(type == JSVAL_TYPE_FUNOBJ || type == JSVAL_TYPE_NONFUNOBJ);
48470:     guard(true,
56750:           w.geuq(v_ins, w.nameImmq(JSVAL_SHIFTED_TAG_OBJECT)),
48470:           exit);
48470:     v_ins = unpack_ptr(v_ins);
48470:     if (type == JSVAL_TYPE_FUNOBJ)
48613:         guardClass(v_ins, &js_FunctionClass, exit, LOAD_NORMAL);
48470:     else
48613:         guardNotClass(v_ins, &js_FunctionClass, exit, LOAD_NORMAL);
48470:     return v_ins;
17426: }
17426: 
47465: LIns*
56750: TraceRecorder::unbox_value(const Value &v, Address addr, VMSideExit *exit, bool force_double)
56750: {
56750:     LIns *v_ins = w.ldq(addr);
48470: 
48470:     if (v.isNumber() && force_double)
48470:         return unbox_number_as_double(v_ins, exit);
48470: 
48470:     if (v.isInt32()) {
48470:         guard(true, non_double_object_value_has_type(v_ins, JSVAL_TYPE_INT32), exit);
56750:         return w.i2d(w.q2i(v_ins));
48470:     }
48470: 
48470:     if (v.isDouble()) {
56750:         guard(true, w.leuq(v_ins, w.nameImmq(JSVAL_SHIFTED_TAG_MAX_DOUBLE)), exit);
56750:         return w.qasd(v_ins);
48470:     }
48470: 
48470:     if (v.isObject()) {
48470:         JSValueType type = v.toObject().isFunction() ? JSVAL_TYPE_FUNOBJ : JSVAL_TYPE_NONFUNOBJ;
48470:         return unbox_object(v_ins, type, exit);
48470:     }
48470: 
48470:     JSValueType type = v.extractNonDoubleObjectTraceType();
48470:     return unbox_non_double_object(v_ins, type, exit);
48470: }
48470: 
48470: void
56750: TraceRecorder::unbox_any_object(Address addr, LIns **obj_ins, LIns **is_obj_ins)
48470: {
48470:     JS_STATIC_ASSERT(JSVAL_TYPE_OBJECT == JSVAL_UPPER_INCL_TYPE_OF_VALUE_SET);
56750:     LIns *v_ins = w.ldq(addr);
56750:     *is_obj_ins = w.geuq(v_ins, w.nameImmq(JSVAL_TYPE_OBJECT));
48470:     *obj_ins = unpack_ptr(v_ins);
47465: }
47465: 
47465: LIns*
56750: TraceRecorder::is_boxed_true(Address addr)
56750: {
56750:     LIns *v_ins = w.ldq(addr);
56750:     return w.eqq(v_ins, w.immq(JSVAL_BITS(JSVAL_TRUE)));
47465: }
47465: 
47465: LIns*
56750: TraceRecorder::is_boxed_magic(Address addr, JSWhyMagic why)
56750: {
56750:     LIns *v_ins = w.ldq(addr);
56750:     return w.eqq(v_ins, w.nameImmq(BUILD_JSVAL(JSVAL_TAG_MAGIC, why)));
55525: }
55525: 
55525: LIns*
48470: TraceRecorder::box_value_for_native_call(const Value &v, LIns *v_ins)
48470: {
48470:     if (v.isNumber()) {
41265:         JS_ASSERT(v_ins->isD());
36402:         if (fcallinfo(v_ins) == &js_UnboxDouble_ci)
56750:             return v_ins->callArgN(0);
59993:         if (IsPromotedInt32(v_ins)) {
59993:             return w.orq(w.ui2uq(w.demoteToInt32(v_ins)),
56750:                          w.nameImmq(JSVAL_SHIFTED_TAG_INT32));
56750:         }
56750:         return w.dasq(v_ins);
48470:     }
48470: 
48470:     if (v.isNull())
56750:         return w.nameImmq(JSVAL_BITS(JSVAL_NULL));
48470:     if (v.isUndefined())
56750:         return w.nameImmq(JSVAL_BITS(JSVAL_VOID));
48470: 
48470:     JSValueTag tag = v.isObject() ? JSVAL_TAG_OBJECT : v.extractNonDoubleObjectTraceTag();
48470:     uint64 shiftedTag = ((uint64)tag) << JSVAL_TAG_SHIFT;
56750:     LIns *shiftedTag_ins = w.nameImmq(shiftedTag);
48470: 
53557:     if (v.hasPtrPayload())
56750:         return w.orq(v_ins, shiftedTag_ins);
56750:     return w.orq(w.ui2uq(v_ins), shiftedTag_ins);
48470: }
48470: 
48470: void
56750: TraceRecorder::box_value_into(const Value &v, LIns *v_ins, Address addr)
48470: {
48470:     LIns *boxed_ins = box_value_for_native_call(v, v_ins);
56750:     w.st(boxed_ins, addr);
48470: }
48470: 
56180: #endif  /* JS_BITS_PER_WORD */
56180: 
48470: LIns*
48470: TraceRecorder::box_value_into_alloc(const Value &v, LIns *v_ins)
48470: {
56750:     LIns *alloc_ins = w.allocp(sizeof(Value));
56750:     box_value_into(v, v_ins, AllocSlotsAddress(alloc_ins));
48470:     return alloc_ins;
48470: }
48470: 
48470: LIns*
48470: TraceRecorder::is_string_id(LIns *id_ins)
48470: {
56750:     return w.eqp0(w.andp(id_ins, w.nameImmw(JSID_TYPE_MASK)));
48470: }
48470: 
48470: LIns *
48470: TraceRecorder::unbox_string_id(LIns *id_ins)
48470: {
48470:     JS_STATIC_ASSERT(JSID_TYPE_STRING == 0);
48470:     return id_ins;
48470: }
48470: 
48470: LIns *
48470: TraceRecorder::unbox_int_id(LIns *id_ins)
48470: {
56750:     return w.rshiN(w.p2i(id_ins), 1);
17470: }
17460: 
33542: JS_REQUIRES_STACK RecordingStatus
17688: TraceRecorder::getThis(LIns*& this_ins)
17688: {
53481:     JSStackFrame *fp = cx->fp();
53840: 
53840:     if (fp->isGlobalFrame()) {
53455:         // Top-level code. It is an invariant of the interpreter that fp->thisv
53455:         // is non-null. Furthermore, we would not be recording if globalObj
53455:         // were not at the end of the scope chain, so `this` can only be one
53455:         // object, which we can burn into the trace.
53840:         JS_ASSERT(!fp->thisValue().isPrimitive());
53455: 
53455: #ifdef DEBUG
53455:         JSObject *obj = globalObj->thisObject(cx);
53455:         if (!obj)
53455:             RETURN_ERROR("thisObject hook failed");
53840:         JS_ASSERT(&fp->thisValue().toObject() == obj);
53840: #endif
53840: 
56750:         this_ins = w.immpObjGC(&fp->thisValue().toObject());
33542:         return RECORD_CONTINUE;
27495:     }
27495: 
53840:     JS_ASSERT(fp->callee().getGlobal() == globalObj);    
55713:     Value& thisv = fp->thisValue();
55713: 
59942:     if (thisv.isObject() || fp->fun()->inStrictMode()) {
55713:         /*
55713:          * fp->thisValue() has already been computed. Since the
55713:          * type-specialization of traces distinguishes between computed and
55713:          * uncomputed |this|, the same will be true at run time (or we
55713:          * won't get this far).
53455:          */
53840:         this_ins = get(&fp->thisValue());
33542:         return RECORD_CONTINUE;
28326:     }
32796: 
55713:     /* Don't bother tracing calls on wrapped primitive |this| values. */
55713:     if (!thisv.isNullOrUndefined())
55713:         RETURN_STOP("wrapping primitive |this|");
55713: 
55713:     /*
55713:      * Compute 'this' now. The result is globalObj->thisObject(), which is
55713:      * trace-constant. getThisObject writes back to fp->thisValue(), so do
55713:      * the same on trace.
55713:      */
55713:     if (!fp->computeThis(cx))
55713:         RETURN_ERROR("computeThis failed");
55713: 
55713:     /* thisv is a reference, so it'll see the newly computed |this|. */
56750:     this_ins = w.immpObjGC(globalObj);
55713:     set(&thisv, this_ins);
33542:     return RECORD_CONTINUE;
17688: }
17688: 
42749: JS_REQUIRES_STACK void
48470: TraceRecorder::guardClassHelper(bool cond, LIns* obj_ins, Class* clasp, VMSideExit* exit,
48613:                                 LoadQual loadQual)
48613: {
56750:     LIns* class_ins = w.ldpObjClasp(obj_ins, loadQual);
37776: 
37776: #ifdef JS_JIT_SPEW
37776:     char namebuf[32];
57719:     JS_snprintf(namebuf, sizeof namebuf, "%s_clasp", clasp->name);
57719:     LIns* clasp_ins = w.name(w.immpNonGC(clasp), namebuf);
37776:     JS_snprintf(namebuf, sizeof namebuf, "guard(class is %s)", clasp->name);
57719:     LIns* cmp_ins = w.name(w.eqp(class_ins, clasp_ins), namebuf);
37776: #else
57719:     LIns* clasp_ins = w.immpNonGC(clasp);
57719:     LIns* cmp_ins = w.eqp(class_ins, clasp_ins);
57719: #endif
57719:     guard(cond, cmp_ins, exit);
42749: }
42749: 
42749: JS_REQUIRES_STACK void
48613: TraceRecorder::guardClass(LIns* obj_ins, Class* clasp, VMSideExit* exit, LoadQual loadQual)
48613: {
48613:     guardClassHelper(true, obj_ins, clasp, exit, loadQual);
42749: }
42749: 
42749: JS_REQUIRES_STACK void
48613: TraceRecorder::guardNotClass(LIns* obj_ins, Class* clasp, VMSideExit* exit, LoadQual loadQual)
48613: {
48613:     guardClassHelper(false, obj_ins, clasp, exit, loadQual);
42749: }
42749: 
42749: JS_REQUIRES_STACK void
42749: TraceRecorder::guardDenseArray(LIns* obj_ins, ExitType exitType)
42749: {
48613:     guardClass(obj_ins, &js_ArrayClass, snapshot(exitType), LOAD_NORMAL);
42749: }
42749: 
42749: JS_REQUIRES_STACK void
42749: TraceRecorder::guardDenseArray(LIns* obj_ins, VMSideExit* exit)
42749: {
48613:     guardClass(obj_ins, &js_ArrayClass, exit, LOAD_NORMAL);
32777: }
32777: 
32777: JS_REQUIRES_STACK bool
29513: TraceRecorder::guardHasPrototype(JSObject* obj, LIns* obj_ins,
29513:                                  JSObject** pobj, LIns** pobj_ins,
29513:                                  VMSideExit* exit)
29513: {
32603:     *pobj = obj->getProto();
56750:     *pobj_ins = w.ldpObjProto(obj_ins);
29513: 
29513:     bool cond = *pobj == NULL;
56750:     guard(cond, w.name(w.eqp0(*pobj_ins), "guard(proto-not-null)"), exit);
29513:     return !cond;
29513: }
29513: 
33542: JS_REQUIRES_STACK RecordingStatus
53614: TraceRecorder::guardPrototypeHasNoIndexedProperties(JSObject* obj, LIns* obj_ins, VMSideExit *exit)
27891: {
27891:     /*
30860:      * Guard that no object along the prototype chain has any indexed
30860:      * properties which might become visible through holes in the array.
27891:      */
27891:     if (js_PrototypeHasIndexedProperties(cx, obj))
33542:         return RECORD_STOP;
27891: 
57756:     JS_ASSERT(obj->isDenseArray());
57756: 
57756:     /*
57756:      * Changing __proto__ on a dense array makes it slow, so we can just bake in
57756:      * the current prototype as the first prototype to test. This avoids an
57756:      * extra load when running the trace.
57756:      */
57756:     obj = obj->getProto();
57756:     JS_ASSERT(obj);
57756: 
57756:     obj_ins = w.immpObjGC(obj);
57756: 
57756:     /*
57756:      * Changing __proto__ on a native object changes its shape, and adding
57756:      * indexed properties changes shapes too.  And non-native objects never pass
57756:      * shape guards.  So it's enough to just guard on shapes up the proto chain;
57756:      * any change to the proto chain length will make us fail a guard before we
57756:      * run off the end of the proto chain.
57756:      */
57756:     do {
40847:         CHECK_STATUS(guardShape(obj_ins, obj, obj->shape(), "guard(shape)", exit));
57756:         obj = obj->getProto();
57756:         obj_ins = w.ldpObjProto(obj_ins);
57756:     } while (obj);
57756: 
33542:     return RECORD_CONTINUE;
33542: }
33542: 
40896: /*
40896:  * Guard that the object stored in v has the ECMA standard [[DefaultValue]]
40896:  * method. Several imacros require this.
40896:  */
40896: JS_REQUIRES_STACK RecordingStatus
48470: TraceRecorder::guardNativeConversion(Value& v)
48470: {
48470:     JSObject* obj = &v.toObject();
40896:     LIns* obj_ins = get(&v);
40896: 
48470:     ConvertOp convert = obj->getClass()->convert;
48470:     if (convert != Valueify(JS_ConvertStub) && convert != js_TryValueOf)
40896:         RETURN_STOP("operand has convert hook");
40896: 
40896:     VMSideExit* exit = snapshot(BRANCH_EXIT);
40896:     if (obj->isNative()) {
40896:         // The common case. Guard on shape rather than class because it'll
40896:         // often be free: we're about to do a shape guard anyway to get the
40896:         // .valueOf property of this object, and this guard will be cached.
40896:         CHECK_STATUS(guardShape(obj_ins, obj, obj->shape(),
40896:                                 "guardNativeConversion", exit));
40896:     } else {
47607:         // We could specialize to guard on just JSClass.convert, but a mere
47607:         // class guard is simpler and slightly faster.
48613:         guardClass(obj_ins, obj->getClass(), snapshot(MISMATCH_EXIT), LOAD_NORMAL);
40896:     }
40896:     return RECORD_CONTINUE;
40896: }
40896: 
53840: JS_REQUIRES_STACK void
60559: TraceRecorder::clearReturningFrameFromNativeTracker()
60244: {
60244:     /*
60244:      * Clear all tracker entries associated with the frame for the same reason
60244:      * described in record_EnterFrame. Reuse the generic visitor to avoid
60244:      * duplicating logic. The generic visitor stops at 'sp', whereas we need to
60244:      * clear up to script->nslots, so finish the job manually.
60244:      */
60244:     ClearSlotsVisitor visitor(nativeFrameTracker);
60244:     VisitStackSlots(visitor, cx, 0);
60244:     Value *vp = cx->regs->sp;
60244:     Value *vpend = cx->fp()->slots() + cx->fp()->script()->nslots;
53840:     for (; vp < vpend; ++vp)
60244:         nativeFrameTracker.set(vp, NULL);
53840: }
53840: 
56018: class BoxArg
56018: {
56018:   public:
56750:     BoxArg(TraceRecorder *tr, Address addr)
56750:         : tr(tr), addr(addr) {}
53840:     TraceRecorder *tr;
56750:     Address addr;
53840:     void operator()(uintN argi, Value *src) {
56750:         tr->box_value_into(*src, tr->get(src), OffsetAddress(addr, argi * sizeof(Value)));
53840:     }
53840: };
37009: 
31847: /*
31847:  * If we have created an |arguments| object for the frame, we must copy the
31847:  * argument values into the object as properties in case it is used after
31847:  * this frame returns.
31847:  */
31847: JS_REQUIRES_STACK void
37694: TraceRecorder::putActivationObjects()
37694: {
53840:     JSStackFrame *const fp = cx->fp();
53840:     bool have_args = fp->hasArgsObj() && !fp->argsObj().isStrictArguments();
53840:     bool have_call = fp->isFunctionFrame() && fp->fun()->isHeavyweight();
37694: 
37694:     if (!have_args && !have_call)
37694:         return;
37694: 
53840:     uintN nformal = fp->numFormalArgs();
53840:     uintN nactual = fp->numActualArgs();
53840:     uintN nargs = have_args && have_call ? Max(nformal, nactual)
53840:                                          : have_args ? nactual : nformal;
37694: 
37694:     LIns *args_ins;
53840:     if (nargs > 0) {
56750:         args_ins = w.allocp(nargs * sizeof(Value));
53840:         /* Don't copy all the actuals if we are only boxing for the callobj. */
56750:         Address addr = AllocSlotsAddress(args_ins);
53840:         if (nargs == nactual)
56750:             fp->forEachCanonicalActualArg(BoxArg(this, addr));
53840:         else
56750:             fp->forEachFormalArg(BoxArg(this, addr));
56750:     } else {
57719:         args_ins = w.immpNonGC(0);
37694:     }
37694: 
37694:     if (have_args) {
53840:         LIns* argsobj_ins = getFrameObjPtr(fp->addressOfArgs());
31847:         LIns* args[] = { args_ins, argsobj_ins, cx_ins };
56750:         w.call(&js_PutArgumentsOnTrace_ci, args);
31847:     }
37694: 
37694:     if (have_call) {
59968:         int nslots = fp->fun()->script()->bindings.countVars();
37694:         LIns* slots_ins;
37694:         if (nslots) {
56750:             slots_ins = w.allocp(sizeof(Value) * nslots);
37694:             for (int i = 0; i < nslots; ++i) {
56750:                 box_value_into(fp->slots()[i], get(&fp->slots()[i]), 
56750:                                AllocSlotsAddress(slots_ins, i));
56750:             }
56750:         } else {
57719:             slots_ins = w.immpNonGC(0);
37694:         }
37694: 
53840:         LIns* scopeChain_ins = getFrameObjPtr(fp->addressOfScopeChain());
56750:         LIns* args[] = { slots_ins, w.nameImmi(nslots), args_ins,
56750:                          w.nameImmi(fp->numFormalArgs()), scopeChain_ins, cx_ins };
56750:         w.call(&js_PutCallObjectOnTrace_ci, args);
37694:     }
31847: }
31847: 
53524: JS_REQUIRES_STACK AbortableRecordingStatus
53524: TraceRecorder::record_EnterFrame()
17818: {
51446:     JSStackFrame* const fp = cx->fp();
19078: 
17852:     if (++callDepth >= MAX_CALLDEPTH)
33542:         RETURN_STOP_A("exceeded maximum call depth");
30860: 
57812:     debug_only_stmt(JSAutoByteString funBytes);
29883:     debug_only_printf(LC_TMTracer, "EnterFrame %s, callDepth=%d\n",
57822:                       cx->fp()->fun()->atom ?
57822:                         js_AtomToPrintableString(cx, cx->fp()->fun()->atom, &funBytes) :
57822:                         "<anonymous>",
29883:                       callDepth);
29883:     debug_only_stmt(
37741:         if (LogController.lcbits & LC_TMRecorder) {
53840:             js_Disassemble(cx, cx->fp()->script(), JS_TRUE, stdout);
29883:             debug_only_print0(LC_TMTracer, "----\n");
29883:         }
29883:     )
56750:     LIns* void_ins = w.immiUndefined();
30248: 
42714:     // Before we enter this frame, we need to clear out any dangling insns left
42714:     // in the tracer. While we also clear when returning from a function, it is
42714:     // possible to have the following sequence of stack usage:
42714:     //
42714:     //  [fp1]*****************   push
42714:     //  [fp1]*****               pop
42714:     //  [fp1]*****[fp2]          call
42714:     //  [fp1]*****[fp2]***       push
42714:     //
30248:     // Duplicate native stack layout computation: see VisitFrameSlots header comment.
53840: 
53840:     // args: carefully copy stack layout
53840:     uintN nactual = fp->numActualArgs();
53840:     uintN nformal = fp->numFormalArgs();
53840:     if (nactual < nformal) {
53840:         // Fill in missing with void.
53840:         JS_ASSERT(fp->actualArgs() == fp->formalArgs());
53840:         Value *beg = fp->formalArgs() + nactual;
53840:         Value *end = fp->formalArgsEnd();
53840:         for (Value *vp = beg; vp != end; ++vp) {
53840:             nativeFrameTracker.set(vp, NULL);
53840:             set(vp, void_ins);
53840:         }
53840:     } else if (nactual > nformal) {
53840:         // Although the VM clones the formal args to the top of the stack, due
53840:         // to the fact that we only track the canonical arguments (in argument
53840:         // order), the native stack offset of the arguments doesn't change. The
53840:         // only thing that changes is which js::Value* in the tracker maps to
53840:         // that slot. Thus, all we need to do here is fixup the trackers, not
53840:         // emit any actual copying on trace.
53840:         JS_ASSERT(fp->actualArgs() != fp->formalArgs());
53840:         JS_ASSERT(fp->hasOverflowArgs());
53840:         Value *srcbeg = fp->actualArgs() - 2;
53840:         Value *srcend = fp->actualArgs() + nformal;
53840:         Value *dstbeg = fp->formalArgs() - 2;
53840:         for (Value *src = srcbeg, *dst = dstbeg; src != srcend; ++src, ++dst) {
53840:             nativeFrameTracker.set(dst, NULL);
53840:             tracker.set(dst, tracker.get(src));
53840:             nativeFrameTracker.set(src, NULL);
53840:             tracker.set(src, NULL);
53840:         }
53840:     }
53840: 
53840:     // argsObj: clear and set to null
53840:     nativeFrameTracker.set(fp->addressOfArgs(), NULL);
56750:     setFrameObjPtr(fp->addressOfArgs(), w.immpNull());
53840: 
53840:     // scopeChain: clear, initialize before snapshot, set below
53840:     nativeFrameTracker.set(fp->addressOfScopeChain(), NULL);
56750:     setFrameObjPtr(fp->addressOfScopeChain(), w.immpNull());
53840: 
53840:     // nfixed: clear and set to undefined
53840:     Value *vp = fp->slots();
53840:     Value *vpstop = vp + fp->numFixed();
42714:     for (; vp < vpstop; ++vp) {
42714:         nativeFrameTracker.set(vp, NULL);
42714:         set(vp, void_ins);
42714:     }
42714: 
53840:     // nfixed to nslots: clear
53840:     vp = fp->base();
53840:     vpstop = fp->slots() + fp->numSlots();
42714:     for (; vp < vpstop; ++vp)
42714:         nativeFrameTracker.set(vp, NULL);
33564: 
53840:     LIns* callee_ins = get(&cx->fp()->calleeValue());
56750:     LIns* scopeChain_ins = w.ldpObjParent(callee_ins);
37694: 
53840:     // set scopeChain for real
53840:     if (cx->fp()->fun()->isHeavyweight()) {
53840:         if (js_IsNamedLambda(cx->fp()->fun()))
37694:             RETURN_STOP_A("can't call named lambda heavyweight on trace");
37694: 
56750:         LIns* fun_ins = w.nameImmpNonGC(cx->fp()->fun());
37694: 
37694:         LIns* args[] = { scopeChain_ins, callee_ins, fun_ins, cx_ins };
56750:         LIns* call_ins = w.call(&js_CreateCallObjectOnTrace_ci, args);
56750:         guard(false, w.eqp0(call_ins), OOM_EXIT);
37694: 
53840:         setFrameObjPtr(fp->addressOfScopeChain(), call_ins);
53840:     } else {
53840:         setFrameObjPtr(fp->addressOfScopeChain(), scopeChain_ins);
37694:     }
37694: 
33564:     /* Try inlining one level in case this recursion doesn't go too deep. */
53840:     if (fp->script() == fp->prev()->script() &&
53840:         fp->prev()->prev() && fp->prev()->prev()->script() == fp->script()) {
33564:         RETURN_STOP_A("recursion started inlining");
33564:     }
33564: 
56201:     if (fp->isConstructing()) {
56750:         LIns* args[] = { callee_ins, w.nameImmpNonGC(&js_ObjectClass), cx_ins };
56750:         LIns* tv_ins = w.call(&js_CreateThisFromTrace_ci, args);
56750:         guard(false, w.eqp0(tv_ins), OOM_EXIT);
56201:         set(&fp->thisValue(), tv_ins);
56201:     }
56201: 
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17818: TraceRecorder::record_LeaveFrame()
17818: {
55536:     debug_only_stmt(JSStackFrame *fp = cx->fp();)
55536: 
55525:     JS_ASSERT(js_CodeSpec[js_GetOpcode(cx, fp->script(),
42717:               cx->regs->pc)].length == JSOP_CALL_LENGTH);
33564: 
26118:     if (callDepth-- <= 0)
33542:         RETURN_STOP_A("returned out of a loop we started tracing");
18001: 
18001:     // LeaveFrame gets called after the interpreter popped the frame and
53840:     // stored rval, so cx->fp() not cx->fp()->prev, and -1 not 0.
48470:     updateAtoms();
39913:     set(&stackval(-1), rval_ins);
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_PUSH()
17409: {
56750:     stack(0, w.immiUndefined());
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_POPV()
17926: {
48470:     Value& rval = stackval(-1);
20907: 
51446:     // Store it in cx->fp()->rval. NB: Tricky dependencies. cx->fp() is the right
20907:     // frame because POPV appears only in global and eval code and we don't
20907:     // trace JSOP_EVAL or leaving the frame where tracing started.
51446:     LIns *fp_ins = entryFrameIns();
56750:     box_value_into(rval, get(&rval), StackFrameAddress(fp_ins,
56750:                                                        JSStackFrame::offsetOfReturnValue()));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
21685: TraceRecorder::record_JSOP_ENTERWITH()
21685: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
21685: TraceRecorder::record_JSOP_LEAVEWITH()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
57757: static JSBool JS_FASTCALL
57757: functionProbe(JSContext *cx, JSFunction *fun, int enter)
57757: {
50455: #ifdef MOZ_TRACE_JSCALLS
57757:     JSScript *script = fun ? FUN_SCRIPT(fun) : NULL;
57757:     if (enter > 0)
57757:         Probes::enterJSFun(cx, fun, script, enter);
57757:     else
57757:         Probes::exitJSFun(cx, fun, script, enter);
57757: #endif
50455:     return true;
50455: }
50455: 
57757: JS_DEFINE_CALLINFO_3(static, BOOL, functionProbe, CONTEXT, FUNCTION, INT32, 0, ACCSET_ALL)
50455: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_RETURN()
17409: {
33564:     /* A return from callDepth 0 terminates the current loop, except for recursion. */
26557:     if (callDepth == 0) {
26557:         AUDIT(returnLoopExits);
33542:         return endLoop();
26557:     }
26557: 
37694:     putActivationObjects();
30248: 
57757:     if (Probes::callTrackingActive(cx)) {
57719:         LIns* args[] = { w.immi(0), w.nameImmpNonGC(cx->fp()->fun()), cx_ins };
56750:         LIns* call_ins = w.call(&functionProbe_ci, args);
56750:         guard(false, w.eqi0(call_ins), MISMATCH_EXIT);
50455:     }
50455: 
26557:     /* If we inlined this function call, make the return value available to the caller code. */
48470:     Value& rval = stackval(-1);
51446:     JSStackFrame *fp = cx->fp();
53840:     if (fp->isConstructing() && rval.isPrimitive()) {
53840:         rval_ins = get(&fp->thisValue());
18661:     } else {
18661:         rval_ins = get(&rval);
18001:     }
57812:     debug_only_stmt(JSAutoByteString funBytes);
29883:     debug_only_printf(LC_TMTracer,
29883:                       "returning from %s\n",
57822:                       fp->fun()->atom ?
57822:                         js_AtomToPrintableString(cx, fp->fun()->atom, &funBytes) :
57822:                         "<anonymous>");
60559:     clearReturningFrameFromNativeTracker();
26557: 
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_GOTO()
17409: {
26557:     /*
31942:      * If we hit a break or a continue to an outer loop, end the loop and
31942:      * generate an always-taken loop exit guard.  For other downward gotos
31942:      * (like if/else) continue recording.
26557:      */
53840:     jssrcnote* sn = js_GetSrcNote(cx->fp()->script(), cx->regs->pc);
26557: 
59880:     if (sn) {
59880:         if (SN_TYPE(sn) == SRC_BREAK) {
26557:             AUDIT(breakLoopExits);
33542:             return endLoop();
33542:         }
59880: 
59880:         /*
59880:          * Tracing labeled break isn't impossible, but does require potentially
59880:          * fixing up the block chain. See bug 616119.
59880:          */
59880:         if (SN_TYPE(sn) == SRC_BREAK2LABEL || SN_TYPE(sn) == SRC_CONT2LABEL)
59880:             RETURN_STOP_A("labeled break");
59880:     }
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_IFEQ()
17409: {
42717:     trackCfgMerges(cx->regs->pc);
20416:     return ifop();
17409: }
17926: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_IFNE()
17409: {
20416:     return ifop();
17409: }
17926: 
32709: LIns*
51097: TraceRecorder::newArguments(LIns* callee_ins, bool strict)
32709: {
56750:     LIns* global_ins = w.immpObjGC(globalObj);
56750:     LIns* argc_ins = w.nameImmi(cx->fp()->numActualArgs());
49124: 
49124:     LIns* args[] = { callee_ins, argc_ins, global_ins, cx_ins };
56750:     LIns* argsobj_ins = w.call(&js_NewArgumentsOnTrace_ci, args);
56750:     guard(false, w.eqp0(argsobj_ins), OOM_EXIT);
51097: 
51097:     if (strict) {
56750:         LIns* argsData_ins = w.getObjPrivatizedSlot(argsobj_ins, JSObject::JSSLOT_ARGS_DATA);
53840:         ptrdiff_t slotsOffset = offsetof(ArgumentsData, slots);
59902:         cx->fp()->forEachCanonicalActualArg(BoxArg(this, ArgsSlotOffsetAddress(argsData_ins,
56750:                                                                                slotsOffset)));
52503:     }
52503: 
52503:     return argsobj_ins;
32709: }
32709: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
32709: TraceRecorder::record_JSOP_ARGUMENTS()
32709: {
51446:     JSStackFrame* const fp = cx->fp();
51446: 
49124:     /* In an eval, 'arguments' will be a BINDNAME, which we don't trace. */
53840:     JS_ASSERT(!fp->isEvalFrame());
53840: 
53840:     if (fp->hasOverriddenArgs())
33542:         RETURN_STOP_A("Can't trace |arguments| if |arguments| is assigned to");
32709: 
53840:     LIns* a_ins = getFrameObjPtr(fp->addressOfArgs());
32709:     LIns* args_ins;
53840:     LIns* callee_ins = get(&fp->calleeValue());
53840:     bool strict = fp->fun()->inStrictMode();
47626:     if (a_ins->isImmP()) {
32709:         // |arguments| is set to 0 by EnterFrame on this trace, so call to create it.
51097:         args_ins = newArguments(callee_ins, strict);
32709:     } else {
32709:         // Generate LIR to create arguments only if it has not already been created.
32709: 
56750:         LIns* mem_ins = w.allocp(sizeof(JSObject *));
56750: 
56750:         LIns* isZero_ins = w.eqp0(a_ins);
55749:         if (isZero_ins->isImmI(0)) {
56750:             w.stAlloc(a_ins, mem_ins);
55749:         } else if (isZero_ins->isImmI(1)) {
55749:             LIns* call_ins = newArguments(callee_ins, strict);
56750:             w.stAlloc(call_ins, mem_ins);
56750:         } else {
56750:             LIns* br1 = w.jtUnoptimizable(isZero_ins);
56750:             w.stAlloc(a_ins, mem_ins);
56750:             LIns* br2 = w.j(NULL);
56750:             w.label(br1);
32709: 
51097:             LIns* call_ins = newArguments(callee_ins, strict);
56750:             w.stAlloc(call_ins, mem_ins);
56750:             w.label(br2);
56750:         }
56750:         args_ins = w.ldpAlloc(mem_ins);
32709:     }
32709: 
32709:     stack(0, args_ins);
53840:     setFrameObjPtr(fp->addressOfArgs(), args_ins);
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_DUP()
17409: {
17448:     stack(0, get(&stackval(-1)));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_DUP2()
17409: {
17448:     stack(0, get(&stackval(-2)));
17448:     stack(1, get(&stackval(-1)));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
21685: TraceRecorder::record_JSOP_SWAP()
21685: {
48470:     Value& l = stackval(-2);
48470:     Value& r = stackval(-1);
21685:     LIns* l_ins = get(&l);
21685:     LIns* r_ins = get(&r);
21685:     set(&r, l_ins);
21685:     set(&l, r_ins);
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
23097: TraceRecorder::record_JSOP_PICK()
23097: {
48470:     Value* sp = cx->regs->sp;
42717:     jsint n = cx->regs->pc[1];
51446:     JS_ASSERT(sp - (n+1) >= cx->fp()->base());
24381:     LIns* top = get(sp - (n+1));
23097:     for (jsint i = 0; i < n; ++i)
24381:         set(sp - (n+1) + i, get(sp - n + i));
23097:     set(&sp[-1], top);
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_SETCONST()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_BITOR()
17409: {
42688:     return InjectStatus(binary(LIR_ori));
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_BITXOR()
17409: {
42688:     return InjectStatus(binary(LIR_xori));
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_BITAND()
17409: {
42688:     return InjectStatus(binary(LIR_andi));
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_EQ()
17409: {
23093:     return equality(false, true);
18687: }
18687: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_NE()
17409: {
23093:     return equality(true, true);
17409: }
17926: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_LT()
17409: {
42688:     return relational(LIR_ltd, true);
17409: }
17926: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_LE()
17409: {
42688:     return relational(LIR_led, true);
17409: }
17926: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_GT()
17409: {
42688:     return relational(LIR_gtd, true);
17409: }
17926: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_GE()
17409: {
42688:     return relational(LIR_ged, true);
17409: }
17926: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_LSH()
17409: {
42688:     return InjectStatus(binary(LIR_lshi));
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_RSH()
17409: {
42688:     return InjectStatus(binary(LIR_rshi));
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_URSH()
17409: {
42688:     return InjectStatus(binary(LIR_rshui));
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_ADD()
17409: {
48470:     Value& r = stackval(-1);
48470:     Value& l = stackval(-2);
48470: 
48470:     if (!l.isPrimitive()) {
40896:         CHECK_STATUS_A(guardNativeConversion(l));
48470:         if (!r.isPrimitive()) {
40896:             CHECK_STATUS_A(guardNativeConversion(r));
41290:             return InjectStatus(callImacro(add_imacros.obj_obj));
41290:         }
41290:         return InjectStatus(callImacro(add_imacros.obj_any));
21685:     }
48470:     if (!r.isPrimitive()) {
40896:         CHECK_STATUS_A(guardNativeConversion(r));
41290:         return InjectStatus(callImacro(add_imacros.any_obj));
28175:     }
21685: 
48470:     if (l.isString() || r.isString()) {
21685:         LIns* args[] = { stringify(r), stringify(l), cx_ins };
56750:         LIns* concat = w.call(&js_ConcatStrings_ci, args);
56750:         guard(false, w.eqp0(concat), OOM_EXIT);
17872:         set(&l, concat);
33542:         return ARECORD_CONTINUE;
33542:     }
33542: 
42688:     return InjectStatus(binary(LIR_addd));
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_SUB()
17409: {
42688:     return InjectStatus(binary(LIR_subd));
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_MUL()
17409: {
42688:     return InjectStatus(binary(LIR_muld));
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_DIV()
17409: {
42688:     return InjectStatus(binary(LIR_divd));
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_MOD()
17409: {
42688:     return InjectStatus(binary(LIR_modd));
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_NOT()
17409: {
48470:     Value& v = stackval(-1);
48470:     if (v.isBoolean() || v.isUndefined()) {
56750:         set(&v, w.eqi0(w.eqiN(get(&v), 1)));
33542:         return ARECORD_CONTINUE;
19070:     }
48470:     if (v.isNumber()) {
21776:         LIns* v_ins = get(&v);
56750:         set(&v, w.ori(w.eqd0(v_ins), w.eqi0(w.eqd(v_ins, v_ins))));
33542:         return ARECORD_CONTINUE;
19554:     }
48470:     if (v.isObjectOrNull()) {
56750:         set(&v, w.eqp0(get(&v)));
33542:         return ARECORD_CONTINUE;
18769:     }
48470:     JS_ASSERT(v.isString());
56750:     set(&v, w.eqp0(w.getStringLength(get(&v))));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_BITNOT()
17409: {
42688:     return InjectStatus(unary(LIR_noti));
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_NEG()
17409: {
48470:     Value& v = stackval(-1);
48470: 
48470:     if (!v.isPrimitive()) {
40896:         CHECK_STATUS_A(guardNativeConversion(v));
41290:         return InjectStatus(callImacro(unary_imacros.sign));
28175:     }
23106: 
48470:     if (v.isNumber()) {
18787:         LIns* a = get(&v);
18787: 
30860:         /*
30860:          * If we're a promoted integer, we have to watch out for 0s since -0 is
30860:          * a double. Only follow this path if we're not an integer that's 0 and
30860:          * we're not a double that's zero.
18787:          */
46181:         if (oracle &&
46181:             !oracle->isInstructionUndemotable(cx->regs->pc) &&
59993:             IsPromotedInt32(a) &&
48470:             (!v.isInt32() || v.toInt32() != 0) &&
48501:             (!v.isDouble() || v.toDouble() != 0) &&
48470:             -v.toNumber() == (int)-v.toNumber())
38603:         {
27540:             VMSideExit* exit = snapshot(OVERFLOW_EXIT);
59993:             a = guard_xov(LIR_subi, w.immi(0), w.demoteToInt32(a), exit);
42688:             if (!a->isImmI() && a->isop(LIR_subxovi)) {
56750:                 guard(false, w.eqiN(a, 0), exit); // make sure we don't lose a -0
56750:             }
56750:             a = w.i2d(a);
56750:         } else {
56750:             a = w.negd(a);
18787:         }
18787: 
18787:         set(&v, a);
33542:         return ARECORD_CONTINUE;
18787:     }
23106: 
48470:     if (v.isNull()) {
56750:         set(&v, w.immd(-0.0));
33542:         return ARECORD_CONTINUE;
23106:     }
23106: 
48470:     if (v.isUndefined()) {
56750:         set(&v, w.immd(js_NaN));
40307:         return ARECORD_CONTINUE;
40307:     }
40307: 
48470:     if (v.isString()) {
59890:         LIns* ok_ins = w.allocp(sizeof(JSBool));
59890:         LIns* args[] = { ok_ins, get(&v), cx_ins };
59890:         LIns* num_ins = w.call(&js_StringToNumber_ci, args);
59890:         guard(false,
59890:               w.name(w.eqi0(w.ldiAlloc(ok_ins)), "guard(oom)"),
59890:               OOM_EXIT);
59890:         set(&v, w.negd(num_ins));
40307:         return ARECORD_CONTINUE;
40307:     }
40307: 
48470:     JS_ASSERT(v.isBoolean());
56750:     set(&v, w.negd(w.i2d(get(&v))));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
23106: TraceRecorder::record_JSOP_POS()
23106: {
48470:     Value& v = stackval(-1);
48470: 
48470:     if (!v.isPrimitive()) {
40896:         CHECK_STATUS_A(guardNativeConversion(v));
41290:         return InjectStatus(callImacro(unary_imacros.sign));
28175:     }
23106: 
48470:     if (v.isNumber())
48470:         return ARECORD_CONTINUE;
48470: 
48470:     if (v.isNull()) {
56750:         set(&v, w.immd(0));
33542:         return ARECORD_CONTINUE;
23225:     }
48470:     if (v.isUndefined()) {
56750:         set(&v, w.immd(js_NaN));
40307:         return ARECORD_CONTINUE;
40307:     }
40307: 
48470:     if (v.isString()) {
59890:         LIns* ok_ins = w.allocp(sizeof(JSBool));
59890:         LIns* args[] = { ok_ins, get(&v), cx_ins };
59890:         LIns* num_ins = w.call(&js_StringToNumber_ci, args);
59890:         guard(false,
59890:               w.name(w.eqi0(w.ldiAlloc(ok_ins)), "guard(oom)"),
59890:               OOM_EXIT);
59890:         set(&v, num_ins);
40307:         return ARECORD_CONTINUE;
40307:     }
40307: 
48470:     JS_ASSERT(v.isBoolean());
56750:     set(&v, w.i2d(get(&v)));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
23106: TraceRecorder::record_JSOP_PRIMTOP()
23106: {
23106:     // Either this opcode does nothing or we couldn't have traced here, because
23106:     // we'd have thrown an exception -- so do nothing if we actually hit this.
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
24873: TraceRecorder::record_JSOP_OBJTOP()
24873: {
48470:     Value& v = stackval(-1);
33542:     RETURN_IF_XML_A(v);
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: RecordingStatus
32615: TraceRecorder::getClassPrototype(JSObject* ctor, LIns*& proto_ins)
22626: {
35117:     // ctor must be a function created via js_InitClass.
33542: #ifdef DEBUG
48470:     Class *clasp = FUN_CLASP(GET_FUNCTION_PRIVATE(cx, ctor));
35117:     JS_ASSERT(clasp);
35117: 
37741:     TraceMonitor &localtm = JS_TRACE_MONITOR(cx);
33542: #endif
33542: 
48470:     Value pval;
31501:     if (!ctor->getProperty(cx, ATOM_TO_JSID(cx->runtime->atomState.classPrototypeAtom), &pval))
33542:         RETURN_ERROR("error getting prototype from constructor");
33542: 
35117:     // ctor.prototype is a permanent data property, so this lookup cannot have
35117:     // deep-aborted.
33542:     JS_ASSERT(localtm.recorder);
33542: 
25887: #ifdef DEBUG
25887:     JSBool ok, found;
25887:     uintN attrs;
25887:     ok = JS_GetPropertyAttributes(cx, ctor, js_class_prototype_str, &attrs, &found);
25887:     JS_ASSERT(ok);
25887:     JS_ASSERT(found);
25887:     JS_ASSERT((~attrs & (JSPROP_READONLY | JSPROP_PERMANENT)) == 0);
25887: #endif
35117: 
35117:     // Since ctor was built by js_InitClass, we can assert (rather than check)
35117:     // that pval is usable.
48470:     JS_ASSERT(!pval.isPrimitive());
48470:     JSObject *proto = &pval.toObject();
55746:     JS_ASSERT_IF(clasp != &js_ArrayClass, proto->emptyShapes[0]->getClass() == clasp);
35117: 
56750:     proto_ins = w.immpObjGC(proto);
33542:     return RECORD_CONTINUE;
33542: }
33542: 
33542: RecordingStatus
32615: TraceRecorder::getClassPrototype(JSProtoKey key, LIns*& proto_ins)
32615: {
33542: #ifdef DEBUG
37741:     TraceMonitor &localtm = JS_TRACE_MONITOR(cx);
33542: #endif
33542: 
32615:     JSObject* proto;
38604:     if (!js_GetClassPrototype(cx, globalObj, key, &proto))
33542:         RETURN_ERROR("error in js_GetClassPrototype");
33542: 
35117:     // This should not have reentered.
33542:     JS_ASSERT(localtm.recorder);
33542: 
38508: #ifdef DEBUG
52503:     /* Double-check that a native proto has a matching emptyShape. */
35117:     if (key != JSProto_Array) {
40430:         JS_ASSERT(proto->isNative());
55746:         JS_ASSERT(proto->emptyShapes);
55746:         EmptyShape *empty = proto->emptyShapes[0];
52503:         JS_ASSERT(empty);
52503:         JS_ASSERT(JSCLASS_CACHED_PROTO_KEY(empty->getClass()) == key);
38508:     }
38508: #endif
35117: 
56750:     proto_ins = w.immpObjGC(proto);
33542:     return RECORD_CONTINUE;
27933: }
27933: 
32669: #define IGNORE_NATIVE_CALL_COMPLETE_CALLBACK ((JSSpecializedNative*)1)
28086: 
33542: RecordingStatus
48470: TraceRecorder::newString(JSObject* ctor, uint32 argc, Value* argv, Value* rval)
28086: {
28086:     JS_ASSERT(argc == 1);
28086: 
48470:     if (!argv[0].isPrimitive()) {
40896:         CHECK_STATUS(guardNativeConversion(argv[0]));
41290:         return callImacro(new_imacros.String);
28175:     }
28086: 
28086:     LIns* proto_ins;
32615:     CHECK_STATUS(getClassPrototype(ctor, proto_ins));
32615: 
32615:     LIns* args[] = { stringify(argv[0]), proto_ins, cx_ins };
56750:     LIns* obj_ins = w.call(&js_String_tn_ci, args);
56750:     guard(false, w.eqp0(obj_ins), OOM_EXIT);
28086: 
28086:     set(rval, obj_ins);
32669:     pendingSpecializedNative = IGNORE_NATIVE_CALL_COMPLETE_CALLBACK;
33542:     return RECORD_CONTINUE;
33542: }
33542: 
33542: RecordingStatus
48470: TraceRecorder::newArray(JSObject* ctor, uint32 argc, Value* argv, Value* rval)
22626: {
28086:     LIns *proto_ins;
32615:     CHECK_STATUS(getClassPrototype(ctor, proto_ins));
22626: 
28086:     LIns *arr_ins;
59234:     if (argc == 0) {
59234:         LIns *args[] = { proto_ins, cx_ins };
59234:         arr_ins = w.call(&js::NewDenseEmptyArray_ci, args);
56750:         guard(false, w.eqp0(arr_ins), OOM_EXIT);
59234: 
59234:     } else if (argc == 1 && argv[0].isNumber()) {
59234:         /* Abort on RangeError if the double doesn't fit in a uint. */
59913:         LIns *len_ins;
59913:         CHECK_STATUS(makeNumberUint32(get(argv), &len_ins));
59913:         LIns *args[] = { proto_ins, len_ins, cx_ins };
59234:         arr_ins = w.call(&js::NewDenseUnallocatedArray_ci, args);
59234:         guard(false, w.eqp0(arr_ins), OOM_EXIT);
59234: 
59234:     } else {
59234:         LIns *args[] = { proto_ins, w.nameImmi(argc), cx_ins };
59234:         arr_ins = w.call(&js::NewDenseAllocatedArray_ci, args);
56750:         guard(false, w.eqp0(arr_ins), OOM_EXIT);
22626: 
56180:         // arr->slots[i] = box_jsval(vp[i]);  for i in 0..argc
56180:         LIns *slots_ins = NULL;
33159:         for (uint32 i = 0; i < argc && !outOfMemory(); i++) {
56180:             stobj_set_dslot(arr_ins, i, slots_ins, argv[i], get(&argv[i]));
22626:         }
22626:     }
28086: 
22626:     set(rval, arr_ins);
32669:     pendingSpecializedNative = IGNORE_NATIVE_CALL_COMPLETE_CALLBACK;
33542:     return RECORD_CONTINUE;
27933: }
27933: 
30847: JS_REQUIRES_STACK void
30847: TraceRecorder::propagateFailureToBuiltinStatus(LIns* ok_ins, LIns*& status_ins)
30847: {
30847:     /*
30847:      * Check the boolean return value (ok_ins) of a native JSNative,
30847:      * JSFastNative, or JSPropertyOp hook for failure. On failure, set the
37741:      * BUILTIN_ERROR bit of cx->builtinStatus.
30847:      *
30847:      * If the return value (ok_ins) is true, status' == status. Otherwise
37741:      * status' = status | BUILTIN_ERROR. We calculate (rval&1)^1, which is 1
30847:      * if rval is JS_FALSE (error), and then shift that by 1, which is the log2
37741:      * of BUILTIN_ERROR.
30847:      */
30847:     JS_STATIC_ASSERT(((JS_TRUE & 1) ^ 1) << 1 == 0);
37741:     JS_STATIC_ASSERT(((JS_FALSE & 1) ^ 1) << 1 == BUILTIN_ERROR);
56750:     status_ins = w.ori(status_ins, w.lshiN(w.xoriN(w.andiN(ok_ins, 1), 1), 1));
56750:     w.stStateField(status_ins, builtinStatus);
30847: }
30847: 
30847: JS_REQUIRES_STACK void
52503: TraceRecorder::emitNativePropertyOp(const Shape* shape, LIns* obj_ins,
48470:                                     bool setflag, LIns* addr_boxed_val_ins)
48470: {
48470:     JS_ASSERT(addr_boxed_val_ins->isop(LIR_allocp));
52503:     JS_ASSERT(setflag ? !shape->hasSetterValue() : !shape->hasGetterValue());
52503:     JS_ASSERT(setflag ? !shape->hasDefaultSetter() : !shape->hasDefaultGetterOrIsMethod());
30847: 
31444:     enterDeepBailCall();
30847: 
56750:     w.stStateField(addr_boxed_val_ins, nativeVp);
57719:     w.stStateField(w.immi(1), nativeVpLen);
30847: 
34351:     CallInfo* ci = new (traceAlloc()) CallInfo();
52503:     ci->_address = uintptr_t(setflag ? shape->setterOp() : shape->getterOp());
47594:     ci->_typesig = CallInfo::typeSig4(ARGTYPE_I, ARGTYPE_P, ARGTYPE_P, ARGTYPE_P, ARGTYPE_P);
39910:     ci->_isPure = 0;
48613:     ci->_storeAccSet = ACCSET_STORE_ANY;
30847:     ci->_abi = ABI_CDECL;
30847: #ifdef DEBUG
30847:     ci->_name = "JSPropertyOp";
30847: #endif
56750:     LIns* args[] = { addr_boxed_val_ins, w.immpIdGC(SHAPE_USERID(shape)), obj_ins, cx_ins };
56750:     LIns* ok_ins = w.call(ci, args);
30847: 
32678:     // Cleanup. Immediately clear nativeVp before we might deep bail.
56750:     w.stStateField(w.immpNull(), nativeVp);
31444:     leaveDeepBailCall();
30847: 
30847:     // Guard that the call succeeded and builtinStatus is still 0.
30847:     // If the native op succeeds but we deep-bail here, the result value is
30847:     // lost!  Therefore this can only be used for setters of shared properties.
30847:     // In that case we ignore the result value anyway.
56750:     LIns* status_ins = w.ldiStateField(builtinStatus);
30847:     propagateFailureToBuiltinStatus(ok_ins, status_ins);
56750:     guard(true, w.eqi0(status_ins), STATUS_EXIT);
30847: }
30847: 
33542: JS_REQUIRES_STACK RecordingStatus
32678: TraceRecorder::emitNativeCall(JSSpecializedNative* sn, uintN argc, LIns* args[], bool rooted)
32669: {
32669:     if (JSTN_ERRTYPE(sn) == FAIL_STATUS) {
26552:         // This needs to capture the pre-call state of the stack. So do not set
32669:         // pendingSpecializedNative before taking this snapshot.
32669:         JS_ASSERT(!pendingSpecializedNative);
26552: 
60574:         // Take snapshot for DeepBail and store it in tm->bailExit.
53557:         enterDeepBailCall();
26552:     }
26552: 
56750:     LIns* res_ins = w.call(sn->builtin, args);
32678: 
32678:     // Immediately unroot the vp as soon we return since we might deep bail next.
32678:     if (rooted)
56750:         w.stStateField(w.immpNull(), nativeVp);
32678: 
26552:     rval_ins = res_ins;
32669:     switch (JSTN_ERRTYPE(sn)) {
27059:       case FAIL_NULL:
56750:         guard(false, w.eqp0(res_ins), OOM_EXIT);
27059:         break;
27059:       case FAIL_NEG:
56750:         res_ins = w.i2d(res_ins);
56750:         guard(false, w.ltdN(res_ins, 0), OOM_EXIT);
27059:         break;
48470:       case FAIL_NEITHER:
56750:           guard(false, w.eqiN(res_ins, JS_NEITHER), OOM_EXIT);
27059:         break;
27059:       default:;
27059:     }
26552: 
26552:     set(&stackval(0 - (2 + argc)), res_ins);
26552: 
26552:     /*
28086:      * The return value will be processed by NativeCallComplete since
26552:      * we have to know the actual return value type for calls that return
41857:      * jsval.
26552:      */
32669:     pendingSpecializedNative = sn;
26552: 
33542:     return RECORD_CONTINUE;
26552: }
26552: 
26552: /*
30860:  * Check whether we have a specialized implementation for this native
30860:  * invocation.
26552:  */
33542: JS_REQUIRES_STACK RecordingStatus
32669: TraceRecorder::callSpecializedNative(JSNativeTraceInfo *trcinfo, uintN argc,
32669:                                      bool constructing)
32669: {
51446:     JSStackFrame* const fp = cx->fp();
42717:     jsbytecode *pc = cx->regs->pc;
21511: 
48470:     Value& fval = stackval(0 - (2 + argc));
48470:     Value& tval = stackval(0 - (1 + argc));
26552: 
18641:     LIns* this_ins = get(&tval);
20431: 
26676:     LIns* args[nanojit::MAXARGS];
32669:     JSSpecializedNative *sn = trcinfo->specializations;
32669:     JS_ASSERT(sn);
20431:     do {
32669:         if (((sn->flags & JSTN_CONSTRUCTOR) != 0) != constructing)
17651:             continue;
17634: 
32669:         uintN knownargc = strlen(sn->argtypes);
17870:         if (argc != knownargc)
18115:             continue;
17870: 
32669:         intN prefixc = strlen(sn->prefix);
20431:         JS_ASSERT(prefixc <= 3);
17870:         LIns** argp = &args[argc + prefixc - 1];
17870:         char argtype;
17870: 
29880: #if defined DEBUG
18172:         memset(args, 0xCD, sizeof(args));
18172: #endif
18172: 
20431:         uintN i;
20431:         for (i = prefixc; i--; ) {
32669:             argtype = sn->prefix[i];
20431:             if (argtype == 'C') {
20431:                 *argp = cx_ins;
20431:             } else if (argtype == 'T') { /* this, as an object */
48470:                 if (tval.isPrimitive())
20431:                     goto next_specialization;
20431:                 *argp = this_ins;
20431:             } else if (argtype == 'S') { /* this, as a string */
48470:                 if (!tval.isString())
23228:                     goto next_specialization;
23226:                 *argp = this_ins;
32613:             } else if (argtype == 'f') {
56750:                 *argp = w.immpObjGC(&fval.toObject());
32615:             } else if (argtype == 'p') {
48470:                 CHECK_STATUS(getClassPrototype(&fval.toObject(), *argp));
32615:             } else if (argtype == 'R') {
56750:                 *argp = w.nameImmpNonGC(cx->runtime);
20431:             } else if (argtype == 'P') {
24600:                 // FIXME: Set pc to imacpc when recording JSOP_CALL inside the
24600:                 //        JSOP_GETELEM imacro (bug 476559).
33564:                 if ((*pc == JSOP_CALL) &&
53840:                     fp->hasImacropc() && *fp->imacropc() == JSOP_GETELEM)
56750:                     *argp = w.nameImmpNonGC(fp->imacropc());
24600:                 else
56750:                     *argp = w.nameImmpNonGC(pc);
20431:             } else if (argtype == 'D') { /* this, as a number */
48470:                 if (!tval.isNumber())
20431:                     goto next_specialization;
20431:                 *argp = this_ins;
55569:             } else if (argtype == 'M') {
59735:                 MathCache *mathCache = GetMathCache(cx);
55569:                 if (!mathCache)
55569:                     return RECORD_ERROR;
56750:                 *argp = w.nameImmpNonGC(mathCache);
20431:             } else {
20431:                 JS_NOT_REACHED("unknown prefix arg type");
20431:             }
20431:             argp--;
20431:         }
20431: 
20431:         for (i = knownargc; i--; ) {
48470:             Value& arg = stackval(0 - (i + 1));
22634:             *argp = get(&arg);
20431: 
32669:             argtype = sn->argtypes[i];
20431:             if (argtype == 'd' || argtype == 'i') {
48470:                 if (!arg.isNumber())
20431:                     goto next_specialization;
20431:                 if (argtype == 'i')
41265:                     *argp = d2i(*argp);
32615:             } else if (argtype == 'o') {
48470:                 if (arg.isPrimitive())
32615:                     goto next_specialization;
20431:             } else if (argtype == 's') {
48470:                 if (!arg.isString())
20431:                     goto next_specialization;
32615:             } else if (argtype == 'r') {
32615:                 if (!VALUE_IS_REGEXP(cx, arg))
32615:                     goto next_specialization;
32615:             } else if (argtype == 'f') {
48470:                 if (!IsFunctionObject(arg))
32615:                     goto next_specialization;
20431:             } else if (argtype == 'v') {
48470:                 *argp = box_value_for_native_call(arg, *argp);
20431:             } else {
20431:                 goto next_specialization;
20431:             }
20431:             argp--;
20431:         }
29880: #if defined DEBUG
18172:         JS_ASSERT(args[0] != (LIns *)0xcdcdcdcd);
18172: #endif
32678:         return emitNativeCall(sn, argc, args, false);
28086: 
28086: next_specialization:;
32669:     } while ((sn++)->flags & JSTN_MORE);
28086: 
33542:     return RECORD_STOP;
33542: }
33542: 
56184: static JSBool FASTCALL
56184: ceilReturningInt(jsdouble x, int32 *out)
56184: {
56184:     jsdouble r = js_math_ceil_impl(x);
56184:     return JSDOUBLE_IS_INT32(r, out);
56184: }
56184: 
56184: static JSBool FASTCALL
56184: floorReturningInt(jsdouble x, int32 *out)
56184: {
56184:     jsdouble r = js_math_floor_impl(x);
56184:     return JSDOUBLE_IS_INT32(r, out);
56184: }
56184: 
56184: static JSBool FASTCALL
56184: roundReturningInt(jsdouble x, int32 *out)
56184: {
56184:     jsdouble r = js_math_round_impl(x);
56184:     return JSDOUBLE_IS_INT32(r, out);
56184: }
56184: 
56215: /*
56215:  * These functions store into their second argument, so they need to
56215:  * be annotated accordingly. To be future-proof, we use ACCSET_STORE_ANY
56215:  * so that new callers don't have to remember to update the annotation.
56215:  */
56215: JS_DEFINE_CALLINFO_2(static, BOOL, ceilReturningInt, DOUBLE, INT32PTR, 0, ACCSET_STORE_ANY)
56215: JS_DEFINE_CALLINFO_2(static, BOOL, floorReturningInt, DOUBLE, INT32PTR, 0, ACCSET_STORE_ANY)
56215: JS_DEFINE_CALLINFO_2(static, BOOL, roundReturningInt, DOUBLE, INT32PTR, 0, ACCSET_STORE_ANY)
56184: 
56184: JS_REQUIRES_STACK RecordingStatus
56184: TraceRecorder::callFloatReturningInt(uintN argc, const nanojit::CallInfo *ci)
56184: {
56184:     Value& arg = stackval(-1);
56750:     LIns* resptr_ins = w.allocp(sizeof(int32));
56184:     LIns* args[] = { resptr_ins, get(&arg) };
56750:     LIns* fits_ins = w.call(ci, args);
56750: 
56750:     guard(false, w.eqi0(fits_ins), OVERFLOW_EXIT);
56750: 
56750:     LIns* res_ins = w.ldiAlloc(resptr_ins);
56750: 
56750:     set(&stackval(0 - (2 + argc)), w.i2d(res_ins));
56184: 
56184:     pendingSpecializedNative = IGNORE_NATIVE_CALL_COMPLETE_CALLBACK;
56184: 
56184:     return RECORD_CONTINUE;
56184: }
56184: 
33542: JS_REQUIRES_STACK RecordingStatus
28086: TraceRecorder::callNative(uintN argc, JSOp mode)
28086: {
28086:     LIns* args[5];
28086: 
57712:     JS_ASSERT(mode == JSOP_CALL || mode == JSOP_NEW || mode == JSOP_FUNAPPLY ||
57712:               mode == JSOP_FUNCALL);
28086: 
48470:     Value* vp = &stackval(0 - (2 + argc));
48470:     JSObject* funobj = &vp[0].toObject();
28086:     JSFunction* fun = GET_FUNCTION_PRIVATE(cx, funobj);
53557:     Native native = fun->u.n.native;
31900: 
31900:     switch (argc) {
31900:       case 1:
49126:         if (vp[2].isNumber() && mode == JSOP_CALL) {
49109:             if (native == js_math_ceil || native == js_math_floor || native == js_math_round) {
31900:                 LIns* a = get(&vp[2]);
56184:                 int32 result;
59993:                 if (IsPromotedInt32OrUint32(a)) {
31900:                     set(&vp[0], a);
32669:                     pendingSpecializedNative = IGNORE_NATIVE_CALL_COMPLETE_CALLBACK;
33542:                     return RECORD_CONTINUE;
31900:                 }
56184:                 if (native == js_math_floor) {
56184:                     if (floorReturningInt(vp[2].toNumber(), &result))
56184:                         return callFloatReturningInt(argc, &floorReturningInt_ci);
56184:                 } else if (native == js_math_ceil) {
56184:                     if (ceilReturningInt(vp[2].toNumber(), &result))
56184:                         return callFloatReturningInt(argc, &ceilReturningInt_ci);
56184:                 } else if (native == js_math_round) {
56184:                     if (roundReturningInt(vp[2].toNumber(), &result))
56184:                         return callFloatReturningInt(argc, &roundReturningInt_ci);
56184:                 }
56725:             } else if (native == js_math_abs) {
56725:                 LIns* a = get(&vp[2]);
59993:                 if (IsPromotedInt32(a) && vp[2].toNumber() != INT_MIN) {
59993:                     a = w.demoteToInt32(a);
56725:                     /* abs(INT_MIN) can't be done using integers;  exit if we see it. */
56750:                     LIns* intMin_ins = w.name(w.immi(0x80000000), "INT_MIN");
56750:                     LIns* isIntMin_ins = w.name(w.eqi(a, intMin_ins), "isIntMin");
56725:                     guard(false, isIntMin_ins, MISMATCH_EXIT);
56750:                     LIns* neg_ins = w.negi(a);
56750:                     LIns* isNeg_ins = w.name(w.ltiN(a, 0), "isNeg");
56750:                     LIns* abs_ins = w.name(w.cmovi(isNeg_ins, neg_ins, a), "abs");
56750:                     set(&vp[0], w.i2d(abs_ins));
56725:                     pendingSpecializedNative = IGNORE_NATIVE_CALL_COMPLETE_CALLBACK;
56725:                     return RECORD_CONTINUE;
56725:                 }
31900:             }
49109:             if (vp[1].isString()) {
49109:                 JSString *str = vp[1].toString();
53557:                 if (native == js_str_charAt) {
55556:                     jsdouble i = vp[2].toNumber();
59235:                     if (JSDOUBLE_IS_NaN(i))
59235:                       i = 0;
55556:                     if (i < 0 || i >= str->length())
55556:                         RETURN_STOP("charAt out of bounds");
49109:                     LIns* str_ins = get(&vp[1]);
49109:                     LIns* idx_ins = get(&vp[2]);
55556:                     LIns* char_ins;
55556:                     CHECK_STATUS(getCharAt(str, str_ins, idx_ins, mode, &char_ins));
55556:                     set(&vp[0], char_ins);
49109:                     pendingSpecializedNative = IGNORE_NATIVE_CALL_COMPLETE_CALLBACK;
49109:                     return RECORD_CONTINUE;
53557:                 } else if (native == js_str_charCodeAt) {
49109:                     jsdouble i = vp[2].toNumber();
59235:                     if (JSDOUBLE_IS_NaN(i))
59235:                       i = 0;
49109:                     if (i < 0 || i >= str->length())
49109:                         RETURN_STOP("charCodeAt out of bounds");
49109:                     LIns* str_ins = get(&vp[1]);
49109:                     LIns* idx_ins = get(&vp[2]);
55556:                     LIns* charCode_ins;
55556:                     CHECK_STATUS(getCharCodeAt(str, str_ins, idx_ins, &charCode_ins));
55556:                     set(&vp[0], charCode_ins);
49109:                     pendingSpecializedNative = IGNORE_NATIVE_CALL_COMPLETE_CALLBACK;
49109:                     return RECORD_CONTINUE;
49109:                 }
49109:             }
53558:         } else if (vp[2].isString() && mode == JSOP_CALL) {
53558:             if (native == js_regexp_exec) {
53558:                 jsbytecode *pc = cx->regs->pc;
53558:                 /*
53558:                  * If we see any of these sequences, the result is unused:
53558:                  * - call / pop
53558:                  * - call / trace / pop
53558:                  *
53558:                  * If we see any of these sequences, the result is only tested for nullness:
53558:                  * - call / ifeq
53558:                  * - call / trace / ifeq
53558:                  * - call / not / ifeq
53558:                  * - call / trace / not / ifeq
53558:                  *
53601:                  * In either case, we replace the call to RegExp.exec() on the
53601:                  * stack with a call to RegExp.test() because "r.exec(s) !=
53558:                  * null" is equivalent to "r.test(s)".  This avoids building
53618:                  * the result array, which can be expensive.  This requires
53618:                  * that RegExp.prototype.test() hasn't been changed;  we check this.
53558:                  */
53558:                 if (pc[0] == JSOP_CALL) {
53558:                     if ((pc[JSOP_CALL_LENGTH] == JSOP_POP) ||
53558:                         (pc[JSOP_CALL_LENGTH] == JSOP_TRACE &&
53558:                          pc[JSOP_CALL_LENGTH + JSOP_TRACE_LENGTH] == JSOP_POP) ||
53558:                         (pc[JSOP_CALL_LENGTH] == JSOP_IFEQ) ||
53558:                         (pc[JSOP_CALL_LENGTH] == JSOP_TRACE &&
53558:                          pc[JSOP_CALL_LENGTH + JSOP_TRACE_LENGTH] == JSOP_IFEQ) ||
53558:                         (pc[JSOP_CALL_LENGTH] == JSOP_NOT &&
53558:                          pc[JSOP_CALL_LENGTH + JSOP_NOT_LENGTH] == JSOP_IFEQ) ||
53558:                         (pc[JSOP_CALL_LENGTH] == JSOP_TRACE &&
53558:                          pc[JSOP_CALL_LENGTH + JSOP_TRACE_LENGTH] == JSOP_NOT &&
53558:                          pc[JSOP_CALL_LENGTH + JSOP_TRACE_LENGTH + JSOP_NOT_LENGTH] == JSOP_IFEQ))
53558:                     {
53601:                         JSObject* proto;
57686:                         jsid id = ATOM_TO_JSID(cx->runtime->atomState.testAtom);
53618:                         /* Get RegExp.prototype.test() and check it hasn't been changed. */
57686:                         if (js_GetClassPrototype(cx, NULL, JSProto_RegExp, &proto)) {
57686:                             if (JSObject *tmp = HasNativeMethod(proto, id, js_regexp_test)) {
57686:                                 vp[0] = ObjectValue(*tmp);
57686:                                 funobj = tmp;
57686:                                 fun = tmp->getFunctionPrivate();
57686:                                 native = js_regexp_test;
53618:                             }
53601:                         }
53558:                     }
53558:                 }
53558:             }
49109:         }
31900:         break;
32658: 
31900:       case 2:
49126:         if (vp[2].isNumber() && vp[3].isNumber() && mode == JSOP_CALL &&
32571:             (native == js_math_min || native == js_math_max)) {
31900:             LIns* a = get(&vp[2]);
31900:             LIns* b = get(&vp[3]);
59993:             if (IsPromotedInt32(a) && IsPromotedInt32(b)) {
59993:                 a = w.demoteToInt32(a);
59993:                 b = w.demoteToInt32(b);
56750:                 LIns* cmp = (native == js_math_min) ? w.lti(a, b) : w.gti(a, b);
56750:                 set(&vp[0], w.i2d(w.cmovi(cmp, a, b)));
32669:                 pendingSpecializedNative = IGNORE_NATIVE_CALL_COMPLETE_CALLBACK;
33542:                 return RECORD_CONTINUE;
31900:             }
59993:             if (IsPromotedUint32(a) && IsPromotedUint32(b)) {
59993:                 a = w.demoteToUint32(a);
59993:                 b = w.demoteToUint32(b);
59993:                 LIns* cmp = (native == js_math_min) ? w.ltui(a, b) : w.gtui(a, b);
59993:                 set(&vp[0], w.ui2d(w.cmovi(cmp, a, b)));
59993:                 pendingSpecializedNative = IGNORE_NATIVE_CALL_COMPLETE_CALLBACK;
59993:                 return RECORD_CONTINUE;
59993:             }
31900:         }
31900:         break;
31900:     }
28086: 
32669:     if (fun->flags & JSFUN_TRCINFO) {
32669:         JSNativeTraceInfo *trcinfo = FUN_TRCINFO(fun);
53557:         JS_ASSERT(trcinfo && fun->u.n.native == trcinfo->native);
32669: 
32669:         /* Try to call a type specialized version of the native. */
32669:         if (trcinfo->specializations) {
33542:             RecordingStatus status = callSpecializedNative(trcinfo, argc, mode == JSOP_NEW);
33542:             if (status != RECORD_STOP)
27933:                 return status;
26552:         }
32669:     }
26552: 
28326:     if (native == js_fun_apply || native == js_fun_call)
33542:         RETURN_STOP("trying to call native apply or call");
28326: 
28086:     // Allocate the vp vector and emit code to root it.
53557:     uintN vplen = 2 + argc;
56750:     LIns* invokevp_ins = w.allocp(vplen * sizeof(Value));
28086: 
28086:     // vp[0] is the callee.
56750:     box_value_into(vp[0], w.immpObjGC(funobj), AllocSlotsAddress(invokevp_ins));
28086: 
28086:     // Calculate |this|.
28086:     LIns* this_ins;
28086:     if (mode == JSOP_NEW) {
48470:         Class* clasp = fun->u.n.clasp;
28086:         JS_ASSERT(clasp != &js_SlowArrayClass);
28086:         if (!clasp)
28086:             clasp = &js_ObjectClass;
28086:         JS_ASSERT(((jsuword) clasp & 3) == 0);
28086: 
55503:         // Abort on |new Function|. js_CreateThis would allocate a regular-
28086:         // sized JSObject, not a Function-sized one. (The Function ctor would
28086:         // deep-bail anyway but let's not go there.)
28086:         if (clasp == &js_FunctionClass)
33542:             RETURN_STOP("new Function");
28086: 
48622:         if (!clasp->isNative())
33542:             RETURN_STOP("new with non-native ops");
28086: 
53557:         if (fun->isConstructor()) {
53557:             vp[1].setMagicWithObjectOrNullPayload(NULL);
56750:             newobj_ins = w.immpMagicNull();
50489: 
50489:             /* Treat this as a regular call, the constructor will behave correctly. */
50489:             mode = JSOP_CALL;
50489:         } else {
56750:             args[0] = w.immpObjGC(funobj);
57719:             args[1] = w.immpNonGC(clasp);
32615:             args[2] = cx_ins;
56750:             newobj_ins = w.call(&js_CreateThisFromTrace_ci, args);
56750:             guard(false, w.eqp0(newobj_ins), OOM_EXIT);
50496: 
50496:             /*
50496:              * emitNativeCall may take a snapshot below. To avoid having a type
50496:              * mismatch (e.g., where get(&vp[1]) is an object and vp[1] is
50496:              * null), we make sure vp[1] is some object. The actual object
50496:              * doesn't matter; JSOP_NEW and InvokeConstructor both overwrite
50496:              * vp[1] without observing its value.
50496:              *
50496:              * N.B. tracing specializes for functions, so pick a non-function.
50496:              */
50496:             vp[1].setObject(*globalObj);
50489:         }
48470:         this_ins = newobj_ins;
28086:     } else {
28086:         this_ins = get(&vp[1]);
48470:     }
48592:     set(&vp[1], this_ins);
56750:     box_value_into(vp[1], this_ins, AllocSlotsAddress(invokevp_ins, 1));
28086: 
28086:     // Populate argv.
28086:     for (uintN n = 2; n < 2 + argc; n++) {
56750:         box_value_into(vp[n], get(&vp[n]), AllocSlotsAddress(invokevp_ins, n));
28086:         // For a very long argument list we might run out of LIR space, so
28086:         // check inside the loop.
33159:         if (outOfMemory())
33542:             RETURN_STOP("out of memory in argument list");
28086:     }
28086: 
28086:     // Populate extra slots, including the return value slot for a slow native.
28086:     if (2 + argc < vplen) {
28086:         for (uintN n = 2 + argc; n < vplen; n++) {
56750:             box_undefined_into(AllocSlotsAddress(invokevp_ins, n));
33159:             if (outOfMemory())
33542:                 RETURN_STOP("out of memory in extra slots");
28086:         }
28086:     }
28086: 
28086:     // Set up arguments for the JSNative or JSFastNative.
28086:     if (mode == JSOP_NEW)
33542:         RETURN_STOP("untraceable fast native constructor");
28086:     native_rval_ins = invokevp_ins;
28086:     args[0] = invokevp_ins;
56750:     args[1] = w.immi(argc);
28086:     args[2] = cx_ins;
53557:     uint32 typesig = CallInfo::typeSig3(ARGTYPE_I, ARGTYPE_P, ARGTYPE_I, ARGTYPE_P);
28086: 
32669:     // Generate CallInfo and a JSSpecializedNative structure on the fly.
32669:     // Do not use JSTN_UNBOX_AFTER for mode JSOP_NEW because
32669:     // record_NativeCallComplete unboxes the result specially.
26552: 
34351:     CallInfo* ci = new (traceAlloc()) CallInfo();
26552:     ci->_address = uintptr_t(fun->u.n.native);
39910:     ci->_isPure = 0;
48613:     ci->_storeAccSet = ACCSET_STORE_ANY;
26552:     ci->_abi = ABI_CDECL;
40325:     ci->_typesig = typesig;
26552: #ifdef DEBUG
57812:     ci->_name = js_anonymous_str;
57812:     if (fun->atom) {
57812:         JSAutoByteString bytes(cx, ATOM_TO_STRING(fun->atom));
57812:         if (!!bytes) {
57812:             size_t n = strlen(bytes.ptr()) + 1;
57812:             char *buffer = new (traceAlloc()) char[n];
57812:             memcpy(buffer, bytes.ptr(), n);
57812:             ci->_name = buffer;
57812:         }
57812:     }
26552:  #endif
26552: 
32669:     // Generate a JSSpecializedNative structure on the fly.
32669:     generatedSpecializedNative.builtin = ci;
32669:     generatedSpecializedNative.flags = FAIL_STATUS | ((mode == JSOP_NEW)
28241:                                                         ? JSTN_CONSTRUCTOR
28086:                                                         : JSTN_UNBOX_AFTER);
32669:     generatedSpecializedNative.prefix = NULL;
32669:     generatedSpecializedNative.argtypes = NULL;
26552: 
32726:     // We only have to ensure that the values we wrote into the stack buffer
32726:     // are rooted if we actually make it to the call, so only set nativeVp and
32726:     // nativeVpLen immediately before emitting the call code. This way we avoid
32726:     // leaving trace with a bogus nativeVp because we fall off trace while unboxing
32726:     // values into the stack buffer.
56750:     w.stStateField(w.nameImmi(vplen), nativeVpLen);
56750:     w.stStateField(invokevp_ins, nativeVp);
32726: 
28086:     // argc is the original argc here. It is used to calculate where to place
28086:     // the return value.
32678:     return emitNativeCall(&generatedSpecializedNative, argc, args, true);
28086: }
28086: 
33542: JS_REQUIRES_STACK RecordingStatus
28086: TraceRecorder::functionCall(uintN argc, JSOp mode)
26552: {
48470:     Value& fval = stackval(0 - (2 + argc));
51446:     JS_ASSERT(&fval >= cx->fp()->base());
48470: 
48470:     if (!IsFunctionObject(fval))
33542:         RETURN_STOP("callee is not a function");
26552: 
48470:     Value& tval = stackval(0 - (1 + argc));
26552: 
26552:     /*
26552:      * If callee is not constant, it's a shapeless call and we have to guard
26552:      * explicitly that we will get this callee again at runtime.
26552:      */
41265:     if (!get(&fval)->isImmP())
27933:         CHECK_STATUS(guardCallee(fval));
26552: 
26552:     /*
26552:      * Require that the callee be a function object, to avoid guarding on its
26552:      * class here. We know if the callee and this were pushed by JSOP_CALLNAME
26552:      * or JSOP_CALLPROP that callee is a *particular* function, since these hit
26552:      * the property cache and guard on the object (this) in which the callee
26552:      * was found. So it's sufficient to test here that the particular function
26552:      * is interpreted, not guard on that condition.
26552:      *
26552:      * Bytecode sequences that push shapeless callees must guard on the callee
26552:      * class being Function and the function being interpreted.
26552:      */
48470:     JSFunction* fun = GET_FUNCTION_PRIVATE(cx, &fval.toObject());
26552: 
57757:     if (Probes::callTrackingActive(cx)) {
50455:         JSScript *script = FUN_SCRIPT(fun);
50455:         if (!script || !script->isEmpty()) {
57719:             LIns* args[] = { w.immi(1), w.nameImmpNonGC(fun), cx_ins };
56750:             LIns* call_ins = w.call(&functionProbe_ci, args);
56750:             guard(false, w.eqi0(call_ins), MISMATCH_EXIT);
50455:         }
50455:     }
50455: 
55503:     if (FUN_INTERPRETED(fun))
28086:         return interpretedFunctionCall(fval, fun, argc, mode == JSOP_NEW);
26552: 
53557:     Native native = fun->maybeNative();
48470:     Value* argv = &tval + 1;
26552:     if (native == js_Array)
48470:         return newArray(&fval.toObject(), argc, argv, &fval);
28240:     if (native == js_String && argc == 1) {
28086:         if (mode == JSOP_NEW)
48470:             return newString(&fval.toObject(), 1, argv, &fval);
48470:         if (!argv[0].isPrimitive()) {
40896:             CHECK_STATUS(guardNativeConversion(argv[0]));
41290:             return callImacro(call_imacros.String);
28175:         }
28086:         set(&fval, stringify(argv[0]));
32669:         pendingSpecializedNative = IGNORE_NATIVE_CALL_COMPLETE_CALLBACK;
33542:         return RECORD_CONTINUE;
28086:     }
28086: 
50455:     RecordingStatus rs = callNative(argc, mode);
57757:     if (Probes::callTrackingActive(cx)) {
57719:         LIns* args[] = { w.immi(0), w.nameImmpNonGC(fun), cx_ins };
56750:         LIns* call_ins = w.call(&functionProbe_ci, args);
56750:         guard(false, w.eqi0(call_ins), MISMATCH_EXIT);
50455:     }
50455:     return rs;
20431: }
20431: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
20431: TraceRecorder::record_JSOP_NEW()
20431: {
42717:     uintN argc = GET_ARGC(cx->regs->pc);
42717:     cx->assertValidStackDepth(argc + 2);
33542:     return InjectStatus(functionCall(argc, JSOP_NEW));
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18300: TraceRecorder::record_JSOP_DELNAME()
18300: {
33542:     return ARECORD_STOP;
33542: }
33542: 
59878: static JSBool JS_FASTCALL
54169: DeleteIntKey(JSContext* cx, JSObject* obj, int32 i, JSBool strict)
43224: {
47492:     LeaveTraceIfGlobalObject(cx, obj);
60160:     LeaveTraceIfArgumentsObject(cx, obj);
48470:     Value v = BooleanValue(false);
56554:     jsid id;
56554:     if (INT_FITS_IN_JSID(i)) {
56554:         id = INT_TO_JSID(i);
56554:     } else {
56554:         if (!js_ValueToStringId(cx, Int32Value(i), &id)) {
56554:             SetBuiltinError(cx);
56554:             return false;
56554:         }
56554:     }
56554: 
54169:     if (!obj->deleteProperty(cx, id, &v, strict))
43224:         SetBuiltinError(cx);
48470:     return v.toBoolean();
43224: }
54169: JS_DEFINE_CALLINFO_4(extern, BOOL_FAIL, DeleteIntKey, CONTEXT, OBJECT, INT32, BOOL,
54169:                      0, ACCSET_STORE_ANY)
43224: 
59878: static JSBool JS_FASTCALL
54169: DeleteStrKey(JSContext* cx, JSObject* obj, JSString* str, JSBool strict)
43224: {
47492:     LeaveTraceIfGlobalObject(cx, obj);
60160:     LeaveTraceIfArgumentsObject(cx, obj);
48470:     Value v = BooleanValue(false);
43224:     jsid id;
43224: 
43224:     /*
43224:      * NB: JSOP_DELPROP does not need js_ValueToStringId to atomize, but (see
43224:      * jsatominlines.h) that helper early-returns if the computed property name
43224:      * string is already atomized, and we are *not* on a perf-critical path!
43224:      */
54169:     if (!js_ValueToStringId(cx, StringValue(str), &id) || !obj->deleteProperty(cx, id, &v, strict))
43224:         SetBuiltinError(cx);
48470:     return v.toBoolean();
43224: }
54169: JS_DEFINE_CALLINFO_4(extern, BOOL_FAIL, DeleteStrKey, CONTEXT, OBJECT, STRING, BOOL,
54169:                      0, ACCSET_STORE_ANY)
43224: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18300: TraceRecorder::record_JSOP_DELPROP()
18300: {
48470:     Value& lval = stackval(-1);
48470:     if (lval.isPrimitive())
43262:         RETURN_STOP_A("JSOP_DELPROP on primitive base expression");
48470:     if (&lval.toObject() == globalObj)
47492:         RETURN_STOP_A("JSOP_DELPROP on global property");
43224: 
43224:     JSAtom* atom = atoms[GET_INDEX(cx->regs->pc)];
43224: 
43262:     enterDeepBailCall();
56750:     LIns* args[] = { strictModeCode_ins, w.immpAtomGC(atom), get(&lval), cx_ins };
56750:     LIns* rval_ins = w.call(&DeleteStrKey_ci, args);
56750: 
56750:     LIns* status_ins = w.ldiStateField(builtinStatus);
56750:     pendingGuardCondition = w.eqi0(status_ins);
43262:     leaveDeepBailCall();
43262: 
43224:     set(&lval, rval_ins);
43224:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18300: TraceRecorder::record_JSOP_DELELEM()
18300: {
48470:     Value& lval = stackval(-2);
48470:     if (lval.isPrimitive())
43240:         RETURN_STOP_A("JSOP_DELELEM on primitive base expression");
48470:     if (&lval.toObject() == globalObj)
47492:         RETURN_STOP_A("JSOP_DELELEM on global property");
60160:     if (lval.toObject().isArguments())
60160:         RETURN_STOP_A("JSOP_DELELEM on the |arguments| object");
43224: 
48470:     Value& idx = stackval(-1);
43224:     LIns* rval_ins;
43237: 
43262:     enterDeepBailCall();
48470:     if (hasInt32Repr(idx)) {
55556:         LIns* num_ins;
55556:         CHECK_STATUS_A(makeNumberInt32(get(&idx), &num_ins));
55556:         LIns* args[] = { strictModeCode_ins, num_ins, get(&lval), cx_ins };
56750:         rval_ins = w.call(&DeleteIntKey_ci, args);
48470:     } else if (idx.isString()) {
54169:         LIns* args[] = { strictModeCode_ins, get(&idx), get(&lval), cx_ins };
56750:         rval_ins = w.call(&DeleteStrKey_ci, args);
43237:     } else {
43240:         RETURN_STOP_A("JSOP_DELELEM on non-int, non-string index");
43237:     }
43237: 
56750:     LIns* status_ins = w.ldiStateField(builtinStatus);
56750:     pendingGuardCondition = w.eqi0(status_ins);
43262:     leaveDeepBailCall();
43262: 
43224:     set(&lval, rval_ins);
43224:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18300: TraceRecorder::record_JSOP_TYPEOF()
18300: {
48470:     Value& r = stackval(-1);
18300:     LIns* type;
48470:     if (r.isString()) {
56750:         type = w.immpAtomGC(cx->runtime->atomState.typeAtoms[JSTYPE_STRING]);
48470:     } else if (r.isNumber()) {
56750:         type = w.immpAtomGC(cx->runtime->atomState.typeAtoms[JSTYPE_NUMBER]);
48470:     } else if (r.isUndefined()) {
56750:         type = w.immpAtomGC(cx->runtime->atomState.typeAtoms[JSTYPE_VOID]);
48470:     } else if (r.isBoolean()) {
56750:         type = w.immpAtomGC(cx->runtime->atomState.typeAtoms[JSTYPE_BOOLEAN]);
48470:     } else if (r.isNull()) {
56750:         type = w.immpAtomGC(cx->runtime->atomState.typeAtoms[JSTYPE_OBJECT]);
48470:     } else {
48470:         if (r.toObject().isFunction()) {
56750:             type = w.immpAtomGC(cx->runtime->atomState.typeAtoms[JSTYPE_FUNCTION]);
18300:         } else {
18300:             LIns* args[] = { get(&r), cx_ins };
56750:             type = w.call(&js_TypeOfObject_ci, args);
18300:         }
18300:     }
18300:     set(&r, type);
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18300: TraceRecorder::record_JSOP_VOID()
18300: {
56750:     stack(-1, w.immiUndefined());
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18300: TraceRecorder::record_JSOP_INCNAME()
18300: {
18300:     return incName(1);
18300: }
18300: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18300: TraceRecorder::record_JSOP_INCPROP()
18300: {
18300:     return incProp(1);
18300: }
18300: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18300: TraceRecorder::record_JSOP_INCELEM()
18300: {
33542:     return InjectStatus(incElem(1));
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18300: TraceRecorder::record_JSOP_DECNAME()
18300: {
18300:     return incName(-1);
18300: }
18300: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18300: TraceRecorder::record_JSOP_DECPROP()
18300: {
18300:     return incProp(-1);
18300: }
18300: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18300: TraceRecorder::record_JSOP_DECELEM()
18300: {
33542:     return InjectStatus(incElem(-1));
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18300: TraceRecorder::incName(jsint incr, bool pre)
18300: {
48470:     Value* vp;
30647:     LIns* v_ins;
59903:     LIns* v_ins_after;
31075:     NameResult nr;
31480: 
33542:     CHECK_STATUS_A(name(vp, v_ins, nr));
48470:     Value v = nr.tracked ? *vp : nr.v;
59903:     Value v_after;
59903:     CHECK_STATUS_A(incHelper(v, v_ins, v_after, v_ins_after, incr));
59903:     LIns* v_ins_result = pre ? v_ins_after : v_ins;
31075:     if (nr.tracked) {
59903:         set(vp, v_ins_after);
59903:         stack(0, v_ins_result);
33542:         return ARECORD_CONTINUE;
27933:     }
27933: 
52503:     if (!nr.obj->isCall())
33542:         RETURN_STOP_A("incName on unsupported object class");
33542: 
59903:     CHECK_STATUS_A(setCallProp(nr.obj, nr.obj_ins, nr.shape, v_ins_after, v_after));
59903:     stack(0, v_ins_result);
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18300: TraceRecorder::record_JSOP_NAMEINC()
18300: {
18300:     return incName(1, false);
18300: }
18300: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18300: TraceRecorder::record_JSOP_PROPINC()
18300: {
18300:     return incProp(1, false);
18300: }
18300: 
18300: // XXX consolidate with record_JSOP_GETELEM code...
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18300: TraceRecorder::record_JSOP_ELEMINC()
18300: {
33542:     return InjectStatus(incElem(1, false));
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18300: TraceRecorder::record_JSOP_NAMEDEC()
18300: {
21805:     return incName(-1, false);
18300: }
18300: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18300: TraceRecorder::record_JSOP_PROPDEC()
18300: {
18300:     return incProp(-1, false);
18300: }
18300: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18300: TraceRecorder::record_JSOP_ELEMDEC()
18300: {
33542:     return InjectStatus(incElem(-1, false));
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18300: TraceRecorder::record_JSOP_GETPROP()
18300: {
18300:     return getProp(stackval(-1));
18300: }
18300: 
60780: /*
60780:  * If possible, lookup obj[id] without calling any resolve hooks or touching
60780:  * any non-native objects, store the results in *pobjp and *shapep (NULL if no
60780:  * such property exists), and return true.
60780:  *
60780:  * If a safe lookup is not possible, return false; *pobjp and *shapep are
60780:  * undefined.
60780:  */
60780: static bool
60780: SafeLookup(JSContext *cx, JSObject* obj, jsid id, JSObject** pobjp, const Shape** shapep)
60780: {
60780:     do {
60780:         // Avoid non-native lookupProperty hooks.
60780:         if (obj->getOps()->lookupProperty)
60780:             return false;
60780: 
60780:         if (const Shape *shape = obj->nativeLookup(id)) {
60780:             *pobjp = obj;
60780:             *shapep = shape;
60780:             return true;
60780:         }
60780: 
60780:         // Avoid resolve hooks.
60780:         if (obj->getClass()->resolve != JS_ResolveStub)
60780:             return false;
60780:     } while ((obj = obj->getProto()) != NULL);
60780:     *pobjp = NULL;
60780:     *shapep = NULL;
60780:     return true;
60780: }
60780: 
60780: /*
60780:  * Lookup the property for the SETPROP/SETNAME/SETMETHOD instruction at pc.
60780:  * Emit guards to ensure that the result at run time is the same.
60780:  */
60780: JS_REQUIRES_STACK RecordingStatus
60780: TraceRecorder::lookupForSetPropertyOp(JSObject* obj, LIns* obj_ins, jsid id,
60780:                                       bool* safep, JSObject** pobjp, const Shape** shapep)
60780: {
60780:     // We could consult the property cache here, but the contract for
60780:     // PropertyCache::testForSet is intricate enough that it's a lot less code
60780:     // to do a SafeLookup.
60780:     *safep = SafeLookup(cx, obj, id, pobjp, shapep);
60780:     if (!*safep)
60780:         return RECORD_CONTINUE;
60780: 
60780:     VMSideExit *exit = snapshot(BRANCH_EXIT);
60780:     if (*shapep) {
60780:         CHECK_STATUS(guardShape(obj_ins, obj, obj->shape(), "guard_kshape", exit));
60780:         if (obj != *pobjp && *pobjp != globalObj) {
60780:             CHECK_STATUS(guardShape(w.immpObjGC(*pobjp), *pobjp, (*pobjp)->shape(),
60780:                                     "guard_vshape", exit));
60780:         }
60780:     } else {
60780:         for (;;) {
60780:             if (obj != globalObj)
60780:                 CHECK_STATUS(guardShape(obj_ins, obj, obj->shape(), "guard_proto_chain", exit));
60780:             obj = obj->getProto();
60780:             if (!obj)
60780:                 break;
60780:             obj_ins = w.immpObjGC(obj);
60780:         }
60780:     }
60780:     return RECORD_CONTINUE;
60780: }
60780: 
60780: static JSBool FASTCALL
60780: MethodWriteBarrier(JSContext* cx, JSObject* obj, uint32 slot, const Value* v)
60780: {
60780:     bool ok = obj->methodWriteBarrier(cx, slot, *v);
60780:     JS_ASSERT(WasBuiltinSuccessful(cx));
60780:     return ok;
60780: }
60780: JS_DEFINE_CALLINFO_4(static, BOOL_FAIL, MethodWriteBarrier, CONTEXT, OBJECT, UINT32, CVALUEPTR,
60780:                      0, ACCSET_STORE_ANY)
41855: 
41855: /* Emit a specialized, inlined copy of js_NativeSet. */
33542: JS_REQUIRES_STACK RecordingStatus
52503: TraceRecorder::nativeSet(JSObject* obj, LIns* obj_ins, const Shape* shape,
48470:                          const Value &v, LIns* v_ins)
30847: {
52503:     uint32 slot = shape->slot;
60780:     JS_ASSERT((slot != SHAPE_INVALID_SLOT) == shape->hasSlot());
60780:     JS_ASSERT_IF(shape->hasSlot(), obj->nativeContains(*shape));
60780: 
60780:     /*
60780:      * We do not trace assignment to properties that have both a non-default
60780:      * setter and a slot, for several reasons.
30847:      *
30847:      * First, that would require sampling rt->propertyRemovals before and after
30847:      * (see js_NativeSet), and even more code to handle the case where the two
30847:      * samples differ. A mere guard is not enough, because you can't just bail
30847:      * off trace in the middle of a property assignment without storing the
30847:      * value and making the stack right.
30847:      *
30847:      * If obj is the global object, there are two additional problems. We would
30847:      * have to emit still more code to store the result in the object (not the
30847:      * native global frame) if the setter returned successfully after
30847:      * deep-bailing.  And we would have to cope if the run-time type of the
30847:      * setter's return value differed from the record-time type of v, in which
30847:      * case unboxing would fail and, having called a native setter, we could
30847:      * not just retry the instruction in the interpreter.
60780:      *
60780:      * If obj is branded, we would have a similar problem recovering from a
60780:      * failed call to MethodWriteBarrier.
60780:      */
60780:     if (!shape->hasDefaultSetter() && slot != SHAPE_INVALID_SLOT)
60780:         RETURN_STOP("can't trace set of property with setter and slot");
60780: 
60780:     // These two cases are strict-mode errors and can't be traced.
60780:     if (shape->hasGetterValue() && shape->hasDefaultSetter())
60780:         RETURN_STOP("can't set a property that has only a getter");
60780:     if (shape->isDataDescriptor() && !shape->writable())
60780:         RETURN_STOP("can't assign to readonly property");
30847: 
30847:     // Call the setter, if any.
60780:     if (!shape->hasDefaultSetter()) {
60780:         if (shape->hasSetterValue())
60780:             RETURN_STOP("can't trace JavaScript function setter yet");
52503:         emitNativePropertyOp(shape, obj_ins, true, box_value_into_alloc(v, v_ins));
60780:     }
60780: 
52503:     if (slot != SHAPE_INVALID_SLOT) {
60780:         if (obj->brandedOrHasMethodBarrier()) {
60780:             if (obj == globalObj) {
60780:                 // Because the trace is type-specialized to the global object's
60780:                 // slots, no run-time check is needed. Avoid recording a global
60780:                 // shape change, though.
60780:                 JS_ASSERT(obj->nativeContains(*shape));
60780:                 if (IsFunctionObject(obj->getSlot(slot)))
60780:                     RETURN_STOP("can't trace set of function-valued global property");
60780:             } else {
60780:                 // Setting a function-valued property might need to rebrand the
60780:                 // object. Call the method write barrier. Note that even if the
60780:                 // property is not function-valued now, it might be on trace.
60780:                 enterDeepBailCall();
60780:                 LIns* args[] = {box_value_into_alloc(v, v_ins), w.immi(slot), obj_ins, cx_ins};
60780:                 LIns* ok_ins = w.call(&MethodWriteBarrier_ci, args);
60780:                 guard(false, w.eqi0(ok_ins), OOM_EXIT);
60780:                 leaveDeepBailCall();
60780:             }
60780:         }
60780: 
60780:         // Store the value.
41855:         if (obj == globalObj) {
41855:             if (!lazilyImportGlobalSlot(slot))
41855:                 RETURN_STOP("lazy import of global slot failed");
41855:             set(&obj->getSlotRef(slot), v_ins);
41855:         } else {
56180:             LIns* slots_ins = NULL;
56180:             stobj_set_slot(obj, obj_ins, slot, slots_ins, v, v_ins);
41855:         }
41855:     }
41855: 
41855:     return RECORD_CONTINUE;
41855: }
41855: 
41855: JS_REQUIRES_STACK RecordingStatus
60780: TraceRecorder::addDataProperty(JSObject* obj)
60780: {
60780:     if (!obj->isExtensible())
60780:         RETURN_STOP("assignment adds property to non-extensible object");
60780: 
60780:     // If obj is the global, the global shape is about to change. Note also
60780:     // that since we do not record this case, SETNAME and SETPROP are identical
60780:     // as far as the tracer is concerned. (js_CheckUndeclaredVarAssignment
60780:     // distinguishes the two, in the interpreter.)
41855:     if (obj == globalObj)
60780:         RETURN_STOP("set new property of global object"); // global shape change
60780: 
60780:     // js_AddProperty does not call the addProperty hook.
60780:     Class* clasp = obj->getClass();
60780:     if (clasp->addProperty != Valueify(JS_PropertyStub))
60780:         RETURN_STOP("set new property of object with addProperty hook");
60780: 
60780:     // See comment in TR::nativeSet about why we do not support setting a
60780:     // property that has both a setter and a slot.
60780:     if (clasp->setProperty != Valueify(JS_PropertyStub))
60780:         RETURN_STOP("set new property with setter and slot");
60780: 
60780: #ifdef DEBUG
60780:     addPropShapeBefore = obj->lastProperty();
60780: #endif
60780:     return RECORD_CONTINUE;
60780: }
60780: 
60780: JS_REQUIRES_STACK AbortableRecordingStatus
60780: TraceRecorder::record_AddProperty(JSObject *obj)
60780: {
60780:     Value& objv = stackval(-2);
60780:     JS_ASSERT(&objv.toObject() == obj);
60780:     LIns* obj_ins = get(&objv);
60780:     Value& v = stackval(-1);
60780:     LIns* v_ins = get(&v);
60780:     const Shape* shape = obj->lastProperty();
60780: 
60780: #ifdef DEBUG
60780:     JS_ASSERT(addPropShapeBefore);
60780:     if (obj->inDictionaryMode())
60780:         JS_ASSERT(shape->previous()->matches(addPropShapeBefore));
60780:     else
60780:         JS_ASSERT(shape->previous() == addPropShapeBefore);
60780:     JS_ASSERT(shape->isDataDescriptor());
60780:     JS_ASSERT(shape->hasDefaultSetter());
60780:     addPropShapeBefore = NULL;
60780: #endif
60780: 
60780:     if (obj->inDictionaryMode())
60780:         RETURN_STOP_A("assignment adds property to dictionary"); // FIXME: bug 625900
60780: 
60780:     // On trace, call js_Add{,Atom}Property to do the dirty work.
56750:     LIns* args[] = { w.immpShapeGC(shape), obj_ins, cx_ins };
60780:     jsbytecode op = *cx->regs->pc;
60780:     bool isDefinitelyAtom = (op == JSOP_SETPROP);
42694:     const CallInfo *ci = isDefinitelyAtom ? &js_AddAtomProperty_ci : &js_AddProperty_ci;
56750:     LIns* ok_ins = w.call(ci, args);
56750:     guard(false, w.eqi0(ok_ins), OOM_EXIT);
60780: 
60780:     // Box the value and store it in the new slot.
60780:     CHECK_STATUS_A(InjectStatus(nativeSet(obj, obj_ins, shape, v, v_ins)));
60780: 
60780:     // Finish off a SET instruction by moving sp[-1] to sp[-2].
60780:     if (op == JSOP_SETPROP || op == JSOP_SETNAME || op == JSOP_SETMETHOD)
60780:         set(&objv, v_ins);
60780:     return ARECORD_CONTINUE;
30847: }
30847: 
33542: JS_REQUIRES_STACK RecordingStatus
48470: TraceRecorder::setUpwardTrackedVar(Value* stackVp, const Value &v, LIns* v_ins)
48470: {
48470:     JSValueType stackT = determineSlotType(stackVp);
48470:     JSValueType otherT = getCoercedType(v);
41076: 
41076:     bool promote = true;
41076: 
41076:     if (stackT != otherT) {
59993:         if (stackT == JSVAL_TYPE_DOUBLE && otherT == JSVAL_TYPE_INT32 && IsPromotedInt32(v_ins))
41076:             promote = false;
41076:         else
41076:             RETURN_STOP("can't trace this upvar mutation");
41076:     }
41076: 
41076:     set(stackVp, v_ins, promote);
41076: 
41076:     return RECORD_CONTINUE;
41076: }
41076: 
41076: JS_REQUIRES_STACK RecordingStatus
52503: TraceRecorder::setCallProp(JSObject *callobj, LIns *callobj_ins, const Shape *shape,
48470:                            LIns *v_ins, const Value &v)
31449: {
31449:     // Set variables in on-trace-stack call objects by updating the tracker.
31449:     JSStackFrame *fp = frameIfInRange(callobj);
31449:     if (fp) {
52503:         if (shape->setterOp() == SetCallArg) {
52503:             JS_ASSERT(shape->hasShortID());
52503:             uintN slot = uint16(shape->shortid);
53840:             Value *vp2 = &fp->formalArg(slot);
41076:             CHECK_STATUS(setUpwardTrackedVar(vp2, v, v_ins));
33542:             return RECORD_CONTINUE;
31449:         }
52503:         if (shape->setterOp() == SetCallVar) {
52503:             JS_ASSERT(shape->hasShortID());
52503:             uintN slot = uint16(shape->shortid);
48470:             Value *vp2 = &fp->slots()[slot];
41076:             CHECK_STATUS(setUpwardTrackedVar(vp2, v, v_ins));
33542:             return RECORD_CONTINUE;
33542:         }
33542:         RETURN_STOP("can't trace special CallClass setter");
31449:     }
31449: 
35480:     if (!callobj->getPrivate()) {
35480:         // Because the parent guard in guardCallee ensures this Call object
35480:         // will be the same object now and on trace, and because once a Call
35480:         // object loses its frame it never regains one, on trace we will also
35480:         // have a null private in the Call object. So all we need to do is
35480:         // write the value to the Call object's slot.
52503:         intN slot = uint16(shape->shortid);
52503:         if (shape->setterOp() == SetCallArg) {
52503:             JS_ASSERT(slot < ArgClosureTraits::slot_count(callobj));
52503:             slot += ArgClosureTraits::slot_offset(callobj);
52503:         } else if (shape->setterOp() == SetCallVar) {
52503:             JS_ASSERT(slot < VarClosureTraits::slot_count(callobj));
52503:             slot += VarClosureTraits::slot_offset(callobj);
35483:         } else {
35483:             RETURN_STOP("can't trace special CallClass setter");
35483:         }
35483: 
35483:         // Now assert that the shortid get we did above was ok. Have to do it
35483:         // after the RETURN_STOP above, since in that case we may in fact not
35483:         // have a valid shortid; but we don't use it in that case anyway.
52503:         JS_ASSERT(shape->hasShortID());
52503: 
56180:         LIns* slots_ins = NULL;
56180:         stobj_set_dslot(callobj_ins, slot, slots_ins, v, v_ins);
35480:         return RECORD_CONTINUE;
35480:     }
35480: 
35480:     // This is the hard case: we have a JSStackFrame private, but it's not in
35480:     // range.  During trace execution we may or may not have a JSStackFrame
35480:     // anymore.  Call the standard builtins, which handle that situation.
35480: 
31449:     // Set variables in off-trace-stack call objects by calling standard builtins.
31075:     const CallInfo* ci = NULL;
52503:     if (shape->setterOp() == SetCallArg)
31075:         ci = &js_SetCallArg_ci;
52503:     else if (shape->setterOp() == SetCallVar)
31075:         ci = &js_SetCallVar_ci;
31075:     else
33542:         RETURN_STOP("can't trace special CallClass setter");
31075: 
37009:     // Even though the frame is out of range, later we might be called as an
37009:     // inner trace such that the target variable is defined in the outer trace
53840:     // entry frame. For simplicity, we just fall off trace.
53840:     guard(false,
56750:           w.eqp(entryFrameIns(), w.ldpObjPrivate(callobj_ins)),
53840:           MISMATCH_EXIT);
53840: 
31075:     LIns* args[] = {
48470:         box_value_for_native_call(v, v_ins),
56750:         w.nameImmw(JSID_BITS(SHAPE_USERID(shape))),
31075:         callobj_ins,
31075:         cx_ins
31075:     };
56750:     LIns* call_ins = w.call(ci, args);
56750:     guard(false, w.name(w.eqi0(call_ins), "guard(set upvar)"), STATUS_EXIT);
37009: 
33542:     return RECORD_CONTINUE;
33542: }
33542: 
60780: /*
60780:  * Emit a specialized, inlined copy of js_SetPropertyHelper for the current
60780:  * instruction. On success, *deferredp is true if a call to record_AddProperty
60780:  * is expected.
60780:  */
60780: JS_REQUIRES_STACK RecordingStatus
60780: TraceRecorder::setProperty(JSObject* obj, LIns* obj_ins, const Value &v, LIns* v_ins,
60780:                            bool* deferredp)
60780: {
60780:     *deferredp = false;
60780: 
60780:     JSAtom *atom = atoms[GET_INDEX(cx->regs->pc)];
60780:     jsid id = ATOM_TO_JSID(atom);
60780: 
60780:     if (obj->getOps()->setProperty)
60780:         RETURN_STOP("non-native object");  // FIXME: bug 625900
60780: 
60780:     bool safe;
60780:     JSObject* pobj;
60780:     const Shape* shape;
60780:     CHECK_STATUS(lookupForSetPropertyOp(obj, obj_ins, id, &safe, &pobj, &shape));
60780:     if (!safe)
60780:         RETURN_STOP("setprop: lookup fail"); // FIXME: bug 625900
60780: 
60780:     // Handle Call objects specially. The Call objects we create on trace are
60780:     // not fully populated until we leave trace. Calling the setter on such an
60780:     // object wouldn't work.
60780:     if (obj->isCall())
60780:         return setCallProp(obj, obj_ins, shape, v_ins, v);
60780: 
60780:     // Handle setting a property that is not found on obj or anywhere on its
60780:     // the prototype chain.
60780:     if (!shape) {
60780:         *deferredp = true;
60780:         return addDataProperty(obj);
60780:     }
60780: 
60780:     // Check whether we can assign to/over the existing property.
60780:     if (shape->isAccessorDescriptor()) {
60780:         if (shape->hasDefaultSetter())
60780:             RETURN_STOP("setting accessor property with no setter");
60780:     } else if (!shape->writable()) {
60780:         RETURN_STOP("setting readonly data property");
60780:     }
60780: 
60780:     // Handle setting an existing own property.
60780:     if (pobj == obj) {
60780:         if (*cx->regs->pc == JSOP_SETMETHOD) {
60780:             if (shape->isMethod() && &shape->methodObject() == &v.toObject())
60780:                 return RECORD_CONTINUE;
60780:             RETURN_STOP("setmethod: property exists");
60780:         }
60780:         return nativeSet(obj, obj_ins, shape, v, v_ins);
60780:     }
60780: 
60780:     // If shape is an inherited non-SHARED property, we will add a new,
60780:     // shadowing data property.
60780:     if (shape->hasSlot()) {
60780:         // Avoid being tripped up by legacy special case for shortids, where
60780:         // the new shadowing data property inherits the setter.
60780:         if (shape->hasShortID() && !shape->hasDefaultSetter())
60780:             RETURN_STOP("shadowing assignment with shortid");
60780:         *deferredp = true;
60780:         return addDataProperty(obj);
60780:     }
60780: 
60780:     // Handle setting an inherited SHARED property.
60780:     // If it has the default setter, the assignment is a no-op.
60780:     if (shape->hasDefaultSetter() && !shape->hasGetterValue())
60780:         return RECORD_CONTINUE;
60780:     return nativeSet(obj, obj_ins, shape, v, v_ins);
60780: }
60780: 
60800: /* Record a JSOP_SET{PROP,NAME,METHOD} instruction. */
60780: JS_REQUIRES_STACK RecordingStatus
60780: TraceRecorder::recordSetPropertyOp()
60780: {
60780:     Value& l = stackval(-2);
60780:     if (!l.isObject())
60780:         RETURN_STOP("set property of primitive");
60780:     JSObject* obj = &l.toObject();
60780:     LIns* obj_ins = get(&l);
60780: 
48470:     Value& r = stackval(-1);
60780:     LIns* r_ins = get(&r);
60780: 
60780:     bool deferred;
60780:     CHECK_STATUS(setProperty(obj, obj_ins, r, r_ins, &deferred));
60780: 
60780:     // Finish off a SET instruction by moving sp[-1] to sp[-2]. But if
60780:     // record_AddProperty is going be called, we're not done with sp[-2] yet,
60780:     // so delay this move until the end of record_AddProperty.
60780:     if (!deferred)
60780:         set(&l, r_ins);
60780:     return RECORD_CONTINUE;
60780: }
60780: 
60780: JS_REQUIRES_STACK AbortableRecordingStatus
60780: TraceRecorder::record_JSOP_SETPROP()
60780: {
60780:     return InjectStatus(recordSetPropertyOp());
60780: }
60780: 
60780: JS_REQUIRES_STACK AbortableRecordingStatus
60780: TraceRecorder::record_JSOP_SETMETHOD()
60780: {
60780:     return InjectStatus(recordSetPropertyOp());
60780: }
60780: 
60780: JS_REQUIRES_STACK AbortableRecordingStatus
60780: TraceRecorder::record_JSOP_SETNAME()
60780: {
60780:     return InjectStatus(recordSetPropertyOp());
60780: }
60780: 
60780: JS_REQUIRES_STACK RecordingStatus
60780: TraceRecorder::recordInitPropertyOp(jsbytecode op)
60780: {
48470:     Value& l = stackval(-2);
60780:     JSObject* obj = &l.toObject();
60780:     LIns* obj_ins = get(&l);
60780:     JS_ASSERT(obj->getClass() == &js_ObjectClass);
60780: 
60780:     Value& v = stackval(-1);
60780:     LIns* v_ins = get(&v);
60780: 
60780:     JSAtom* atom = atoms[GET_INDEX(cx->regs->pc)];
61209:     jsid id = js_CheckForStringIndex(ATOM_TO_JSID(atom));
60780: 
60780:     // If obj already has this property (because JSOP_NEWOBJECT already set its
60780:     // shape or because the id appears more than once in the initializer), just
60780:     // set it. The existing property can't be an accessor property: we wouldn't
60780:     // get here, as JSOP_SETTER can't be recorded.
60780:     if (const Shape* shape = obj->nativeLookup(id)) {
60780:         // Don't assign a bare (non-cloned) function to an ordinary or method
60780:         // property. The opposite case, assigning some other value to a method,
60780:         // is OK. nativeSet emits code that trips the write barrier.
60780:         if (op == JSOP_INITMETHOD)
60780:             RETURN_STOP("initmethod: property exists");
60780:         JS_ASSERT(shape->isDataDescriptor());
60780:         JS_ASSERT(shape->hasSlot());
60780:         JS_ASSERT(shape->hasDefaultSetter());
60780:         return nativeSet(obj, obj_ins, shape, v, v_ins);
60780:     }
60780: 
60800:     // Duplicate the interpreter's special treatment of __proto__. Unlike the
60800:     // SET opcodes, JSOP_INIT{PROP,METHOD} do not write to the stack.
60800:     if (atom == cx->runtime->atomState.protoAtom) {
60800:         bool deferred;
60800:         return setProperty(obj, obj_ins, v, v_ins, &deferred);
60800:     }
60780: 
60780:     // Define a new property.
60780:     return addDataProperty(obj);
60780: }
60780: 
60780: JS_REQUIRES_STACK AbortableRecordingStatus
60780: TraceRecorder::record_JSOP_INITPROP()
60780: {
60780:     return InjectStatus(recordInitPropertyOp(JSOP_INITPROP));
60780: }
60780: 
60780: JS_REQUIRES_STACK AbortableRecordingStatus
60780: TraceRecorder::record_JSOP_INITMETHOD()
60780: {
60780:     return InjectStatus(recordInitPropertyOp(JSOP_INITMETHOD));
19093: }
19093: 
32761: JS_REQUIRES_STACK VMSideExit*
31444: TraceRecorder::enterDeepBailCall()
31444: {
60574:     // Take snapshot for DeepBail and store it in tm->bailExit.
31444:     VMSideExit* exit = snapshot(DEEP_BAIL_EXIT);
60574:     w.stTraceMonitorField(w.nameImmpNonGC(exit), bailExit);
31444: 
31444:     // Tell nanojit not to discard or defer stack writes before this call.
56750:     w.xbarrier(createGuardRecord(exit));
33560: 
33560:     // Forget about guarded shapes, since deep bailers can reshape the world.
33560:     forgetGuardedShapes();
32761:     return exit;
31444: }
31444: 
31444: JS_REQUIRES_STACK void
31444: TraceRecorder::leaveDeepBailCall()
31444: {
60574:     // Keep tm->bailExit null when it's invalid.
60574:     w.stTraceMonitorField(w.immpNull(), bailExit);
31444: }
31444: 
31444: JS_REQUIRES_STACK void
48470: TraceRecorder::finishGetProp(LIns* obj_ins, LIns* vp_ins, LIns* ok_ins, Value* outp)
31444: {
31444:     // Store the boxed result (and this-object, if JOF_CALLOP) before the
31444:     // guard. The deep-bail case requires this. If the property get fails,
31444:     // these slots will be ignored anyway.
48470:     // N.B. monitorRecording expects get(outp)->isLoad()
56180:     JS_ASSERT(vp_ins->isop(LIR_allocp));
56750:     LIns* result_ins = w.lddAlloc(vp_ins);
39913:     set(outp, result_ins);
42717:     if (js_CodeSpec[*cx->regs->pc].format & JOF_CALLOP)
39913:         set(outp + 1, obj_ins);
31444: 
31444:     // We need to guard on ok_ins, but this requires a snapshot of the state
31444:     // after this op. monitorRecording will do it for us.
31444:     pendingGuardCondition = ok_ins;
31444: 
31444:     // Note there is a boxed result sitting on the stack. The caller must leave
31444:     // it there for the time being, since the return type is not yet
31444:     // known. monitorRecording will emit the code to unbox it.
31444:     pendingUnboxSlot = outp;
31444: }
31444: 
31829: static inline bool
31829: RootedStringToId(JSContext* cx, JSString** namep, jsid* idp)
31829: {
31829:     JSString* name = *namep;
31829:     if (name->isAtomized()) {
48470:         *idp = INTERNED_STRING_TO_JSID(name);
31829:         return true;
31829:     }
31829: 
31829:     JSAtom* atom = js_AtomizeString(cx, name, 0);
31829:     if (!atom)
31829:         return false;
31829:     *namep = ATOM_TO_STRING(atom); /* write back to GC root */
31829:     *idp = ATOM_TO_JSID(atom);
31829:     return true;
31829: }
31829: 
54172: static const size_t PIC_TABLE_ENTRY_COUNT = 32;
54172: 
54172: struct PICTableEntry
54172: {
54172:     jsid    id;
54172:     uint32  shape;
54172:     uint32  slot;
54172: };
54172: 
54172: struct PICTable
54172: {
54172:     PICTable() : entryCount(0) {}
54172: 
54172:     PICTableEntry   entries[PIC_TABLE_ENTRY_COUNT];
54172:     uint32          entryCount;
54172: 
54172:     bool scan(uint32 shape, jsid id, uint32 *slotOut) {
54172:         for (size_t i = 0; i < entryCount; ++i) {
54172:             PICTableEntry &entry = entries[i];
54172:             if (entry.shape == shape && entry.id == id) {
54172:                 *slotOut = entry.slot;
54172:                 return true;
54172:             }
54172:         }
54172:         return false;
54172:     }
54172: 
54172:     void update(uint32 shape, jsid id, uint32 slot) {
54172:         if (entryCount >= PIC_TABLE_ENTRY_COUNT)
54172:             return;
54172:         PICTableEntry &newEntry = entries[entryCount++];
54172:         newEntry.shape = shape;
54172:         newEntry.id = id;
54172:         newEntry.slot = slot;
54172:     }
54172: };
54172: 
31524: static JSBool FASTCALL
54172: GetPropertyByName(JSContext* cx, JSObject* obj, JSString** namep, Value* vp, PICTable *picTable)
31524: {
37741:     LeaveTraceIfGlobalObject(cx, obj);
31524: 
31524:     jsid id;
54172:     if (!RootedStringToId(cx, namep, &id)) {
37741:         SetBuiltinError(cx);
32658:         return false;
31444:     }
54172:     
54172:     /* Delegate to the op, if present. */
54172:     PropertyIdOp op = obj->getOps()->getProperty;
54172:     if (op) {
56817:         bool result = op(cx, obj, obj, id, vp);
54172:         if (!result)
54172:             SetBuiltinError(cx);
60574:         return WasBuiltinSuccessful(cx);
54172:     }
54172: 
54172:     /* Try to hit in the cache. */
54172:     uint32 slot;
54172:     if (picTable->scan(obj->shape(), id, &slot)) {
54172:         *vp = obj->getSlot(slot);
60574:         return WasBuiltinSuccessful(cx);
31829:     }
54172: 
54172:     const Shape *shape;
54172:     JSObject *holder;
56817:     if (!js_GetPropertyHelperWithShape(cx, obj, obj, id, JSGET_METHOD_BARRIER, vp, &shape,
56817:                                        &holder)) {
54172:         SetBuiltinError(cx);
54172:         return false;
54172:     }
54172: 
54172:     /* Only update the table when the object is the holder of the property. */
54172:     if (obj == holder && shape->hasSlot()) {
54172:         /*
54172:          * Note: we insert the non-normalized id into the table so you don't need to
54172:          * normalize it before hitting in the table (faster lookup).
54172:          */
54172:         picTable->update(obj->shape(), id, shape->slot);
54172:     }
54172:     
60574:     return WasBuiltinSuccessful(cx);
54172: }
54172: JS_DEFINE_CALLINFO_5(static, BOOL_FAIL, GetPropertyByName, CONTEXT, OBJECT, STRINGPTR, VALUEPTR,
54172:                      PICTABLE,
48613:                      0, ACCSET_STORE_ANY)
31444: 
31829: // Convert the value in a slot to a string and store the resulting string back
31829: // in the slot (typically in order to root it).
33542: JS_REQUIRES_STACK RecordingStatus
48470: TraceRecorder::primitiveToStringInPlace(Value* vp)
48470: {
48470:     Value v = *vp;
48470:     JS_ASSERT(v.isPrimitive());
48470: 
48470:     if (!v.isString()) {
31829:         // v is not a string. Turn it into one. js_ValueToString is safe
31829:         // because v is not an object.
31829:         JSString *str = js_ValueToString(cx, v);
41777:         JS_ASSERT(TRACE_RECORDER(cx) == this);
31444:         if (!str)
33542:             RETURN_ERROR("failed to stringify element id");
48470:         v.setString(str);
31829:         set(vp, stringify(*vp));
31829: 
31829:         // Write the string back to the stack to save the interpreter some work
31829:         // and to ensure snapshots get the correct type for this slot.
31829:         *vp = v;
31829:     }
33542:     return RECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK RecordingStatus
48470: TraceRecorder::getPropertyByName(LIns* obj_ins, Value* idvalp, Value* outp)
31829: {
31829:     CHECK_STATUS(primitiveToStringInPlace(idvalp));
31444:     enterDeepBailCall();
31444: 
31444:     // Call GetPropertyByName. The vp parameter points to stack because this is
31444:     // what the interpreter currently does. obj and id are rooted on the
31444:     // interpreter stack, but the slot at vp is not a root.
56750:     LIns* vp_ins = w.name(w.allocp(sizeof(Value)), "vp");
56750:     LIns* idvalp_ins = w.name(addr(idvalp), "idvalp");
54172:     PICTable *picTable = new (traceAlloc()) PICTable();
56750:     LIns* pic_ins = w.nameImmpNonGC(picTable);
54172:     LIns* args[] = {pic_ins, vp_ins, idvalp_ins, obj_ins, cx_ins};
56750:     LIns* ok_ins = w.call(&GetPropertyByName_ci, args);
31444: 
31444:     // GetPropertyByName can assign to *idvalp, so the tracker has an incorrect
31444:     // entry for that address. Correct it. (If the value in the address is
31444:     // never used again, the usual case, Nanojit will kill this load.)
56750:     // The Address could be made more precise with some effort (idvalp_ins may
56750:     // be a stack location), but it's not worth it because this case is rare.
56750:     tracker.set(idvalp, w.ldp(AnyAddress(idvalp_ins)));
31444: 
31444:     finishGetProp(obj_ins, vp_ins, ok_ins, outp);
31444:     leaveDeepBailCall();
33542:     return RECORD_CONTINUE;
31444: }
31444: 
31444: static JSBool FASTCALL
48470: GetPropertyByIndex(JSContext* cx, JSObject* obj, int32 index, Value* vp)
31444: {
37741:     LeaveTraceIfGlobalObject(cx, obj);
31444: 
40403:     AutoIdRooter idr(cx);
31501:     if (!js_Int32ToId(cx, index, idr.addr()) || !obj->getProperty(cx, idr.id(), vp)) {
37741:         SetBuiltinError(cx);
24489:         return JS_FALSE;
31444:     }
60574:     return WasBuiltinSuccessful(cx);
31444: }
48470: JS_DEFINE_CALLINFO_4(static, BOOL_FAIL, GetPropertyByIndex, CONTEXT, OBJECT, INT32, VALUEPTR, 0,
48613:                      ACCSET_STORE_ANY)
31444: 
33542: JS_REQUIRES_STACK RecordingStatus
48470: TraceRecorder::getPropertyByIndex(LIns* obj_ins, LIns* index_ins, Value* outp)
31444: {
55556:     CHECK_STATUS(makeNumberInt32(index_ins, &index_ins));
31444: 
31444:     // See note in getPropertyByName about vp.
31444:     enterDeepBailCall();
56750:     LIns* vp_ins = w.name(w.allocp(sizeof(Value)), "vp");
31444:     LIns* args[] = {vp_ins, index_ins, obj_ins, cx_ins};
56750:     LIns* ok_ins = w.call(&GetPropertyByIndex_ci, args);
31444:     finishGetProp(obj_ins, vp_ins, ok_ins, outp);
31444:     leaveDeepBailCall();
33542:     return RECORD_CONTINUE;
31444: }
24489: 
32557: static JSBool FASTCALL
48470: GetPropertyById(JSContext* cx, JSObject* obj, jsid id, Value* vp)
32557: {
37741:     LeaveTraceIfGlobalObject(cx, obj);
32557:     if (!obj->getProperty(cx, id, vp)) {
37741:         SetBuiltinError(cx);
32557:         return JS_FALSE;
32557:     }
60574:     return WasBuiltinSuccessful(cx);
32557: }
48470: JS_DEFINE_CALLINFO_4(static, BOOL_FAIL, GetPropertyById, CONTEXT, OBJECT, JSID, VALUEPTR,
48613:                      0, ACCSET_STORE_ANY)
32557: 
33542: JS_REQUIRES_STACK RecordingStatus
48470: TraceRecorder::getPropertyById(LIns* obj_ins, Value* outp)
32557: {
32557:     // Find the atom.
32557:     JSAtom* atom;
42717:     jsbytecode* pc = cx->regs->pc;
32557:     const JSCodeSpec& cs = js_CodeSpec[*pc];
32557:     if (*pc == JSOP_LENGTH) {
32557:         atom = cx->runtime->atomState.lengthAtom;
32557:     } else if (JOF_TYPE(cs.format) == JOF_ATOM) {
32557:         atom = atoms[GET_INDEX(pc)];
32557:     } else {
32557:         JS_ASSERT(JOF_TYPE(cs.format) == JOF_SLOTATOM);
32557:         atom = atoms[GET_INDEX(pc + SLOTNO_LEN)];
32557:     }
32557: 
48470:     JS_STATIC_ASSERT(sizeof(jsid) == sizeof(void *));
48470:     jsid id = ATOM_TO_JSID(atom);
48470: 
32557:     // Call GetPropertyById. See note in getPropertyByName about vp.
32557:     enterDeepBailCall();
56750:     LIns* vp_ins = w.name(w.allocp(sizeof(Value)), "vp");
56750:     LIns* args[] = {vp_ins, w.nameImmw(JSID_BITS(id)), obj_ins, cx_ins};
56750:     LIns* ok_ins = w.call(&GetPropertyById_ci, args);
32557:     finishGetProp(obj_ins, vp_ins, ok_ins, outp);
32557:     leaveDeepBailCall();
33542:     return RECORD_CONTINUE;
32557: }
32557: 
32558: /* Manually inlined, specialized copy of js_NativeGet. */
32558: static JSBool FASTCALL
52503: GetPropertyWithNativeGetter(JSContext* cx, JSObject* obj, Shape* shape, Value* vp)
32558: {
37741:     LeaveTraceIfGlobalObject(cx, obj);
32558: 
32558: #ifdef DEBUG
32558:     JSProperty* prop;
32558:     JSObject* pobj;
52503:     JS_ASSERT(obj->lookupProperty(cx, shape->id, &pobj, &prop));
52503:     JS_ASSERT(prop == (JSProperty*) shape);
32558: #endif
32558: 
52503:     // Shape::get contains a special case for With objects. We can elide it
52503:     // here because With objects are, we claim, never on the operand stack
52503:     // while recording.
38497:     JS_ASSERT(obj->getClass() != &js_WithClass);
32558: 
48470:     vp->setUndefined();
52503:     if (!shape->getterOp()(cx, obj, SHAPE_USERID(shape), vp)) {
37741:         SetBuiltinError(cx);
32558:         return JS_FALSE;
32558:     }
60574:     return WasBuiltinSuccessful(cx);
32558: }
32558: JS_DEFINE_CALLINFO_4(static, BOOL_FAIL, GetPropertyWithNativeGetter,
52503:                      CONTEXT, OBJECT, SHAPE, VALUEPTR, 0, ACCSET_STORE_ANY)
32558: 
33542: JS_REQUIRES_STACK RecordingStatus
52503: TraceRecorder::getPropertyWithNativeGetter(LIns* obj_ins, const Shape* shape, Value* outp)
52503: {
52503:     JS_ASSERT(!shape->hasGetterValue());
52503:     JS_ASSERT(shape->slot == SHAPE_INVALID_SLOT);
52503:     JS_ASSERT(!shape->hasDefaultGetterOrIsMethod());
32558: 
32558:     // Call GetPropertyWithNativeGetter. See note in getPropertyByName about vp.
32558:     // FIXME - We should call the getter directly. Using a builtin function for
32558:     // now because it buys some extra asserts. See bug 508310.
32558:     enterDeepBailCall();
56750:     LIns* vp_ins = w.name(w.allocp(sizeof(Value)), "vp");
56750:     LIns* args[] = {vp_ins, w.nameImmpNonGC(shape), obj_ins, cx_ins};
56750:     LIns* ok_ins = w.call(&GetPropertyWithNativeGetter_ci, args);
32558:     finishGetProp(obj_ins, vp_ins, ok_ins, outp);
32558:     leaveDeepBailCall();
33542:     return RECORD_CONTINUE;
33542: }
33542: 
41290: JS_REQUIRES_STACK RecordingStatus
52503: TraceRecorder::getPropertyWithScriptGetter(JSObject *obj, LIns* obj_ins, const Shape* shape)
41290: {
41290:     if (!canCallImacro())
41290:         RETURN_STOP("cannot trace script getter, already in imacro");
41290: 
41290:     // Rearrange the stack in preparation for the imacro, taking care to adjust
41290:     // the interpreter state and the tracker in the same way. This adjustment
41290:     // is noted in imacros.jsasm with .fixup tags.
52503:     Value getter = shape->getterValue();
48470:     Value*& sp = cx->regs->sp;
42717:     switch (*cx->regs->pc) {
41290:       case JSOP_GETPROP:
41290:         sp++;
41290:         sp[-1] = sp[-2];
41290:         set(&sp[-1], get(&sp[-2]));
41290:         sp[-2] = getter;
56750:         set(&sp[-2], w.immpObjGC(&getter.toObject()));
41290:         return callImacroInfallibly(getprop_imacros.scriptgetter);
41290: 
41290:       case JSOP_CALLPROP:
41290:         sp += 2;
41290:         sp[-2] = getter;
56750:         set(&sp[-2], w.immpObjGC(&getter.toObject()));
41290:         sp[-1] = sp[-3];
41290:         set(&sp[-1], get(&sp[-3]));
41290:         return callImacroInfallibly(callprop_imacros.scriptgetter);
41290: 
41290:       case JSOP_GETTHISPROP:
41290:       case JSOP_GETARGPROP:
41290:       case JSOP_GETLOCALPROP:
41290:         sp += 2;
41290:         sp[-2] = getter;
56750:         set(&sp[-2], w.immpObjGC(&getter.toObject()));
48470:         sp[-1] = ObjectValue(*obj);
41290:         set(&sp[-1], obj_ins);
41290:         return callImacroInfallibly(getthisprop_imacros.scriptgetter);
41290: 
41290:       default:
41290:         RETURN_STOP("cannot trace script getter for this opcode");
41290:     }
41290: }
41290: 
55556: JS_REQUIRES_STACK RecordingStatus
55556: TraceRecorder::getCharCodeAt(JSString *str, LIns* str_ins, LIns* idx_ins, LIns** out)
55556: {
55556:     CHECK_STATUS(makeNumberInt32(idx_ins, &idx_ins));
56750:     idx_ins = w.ui2p(idx_ins);
57719:     LIns *lengthAndFlags_ins = w.ldpStringLengthAndFlags(str_ins);
56750:     if (MaybeBranch mbr = w.jt(w.eqp0(w.andp(lengthAndFlags_ins, w.nameImmw(JSString::ROPE_BIT)))))
56750:     {
59890:         LIns *args[] = { str_ins, cx_ins };
59890:         LIns *ok_ins = w.call(&js_Flatten_ci, args);
59890:         guard(false, w.eqi0(ok_ins), OOM_EXIT);
56750:         w.label(mbr);
55749:     }
49109: 
49109:     guard(true,
59888:           w.ltup(idx_ins, w.rshupN(lengthAndFlags_ins, JSString::LENGTH_SHIFT)),
49109:           snapshot(MISMATCH_EXIT));
56750:     *out = w.i2d(w.getStringChar(str_ins, idx_ins));
55556:     return RECORD_CONTINUE;
49109: }
49109: 
49109: JS_STATIC_ASSERT(sizeof(JSString) == 16 || sizeof(JSString) == 32);
49109: 
55505: 
49109: JS_REQUIRES_STACK LIns*
55505: TraceRecorder::getUnitString(LIns* str_ins, LIns* idx_ins)
55505: {
56750:     LIns *ch_ins = w.getStringChar(str_ins, idx_ins);
56750:     guard(true, w.ltuiN(ch_ins, UNIT_STRING_LIMIT), MISMATCH_EXIT);
56750:     return w.addp(w.nameImmpNonGC(JSString::unitStringTable),
56750:                   w.lshpN(w.ui2p(ch_ins), (sizeof(JSString) == 16) ? 4 : 5));
55505: }
55505: 
55556: JS_REQUIRES_STACK RecordingStatus
55556: TraceRecorder::getCharAt(JSString *str, LIns* str_ins, LIns* idx_ins, JSOp mode, LIns** out)
55556: {
55556:     CHECK_STATUS(makeNumberInt32(idx_ins, &idx_ins));
56750:     idx_ins = w.ui2p(idx_ins);
56750:     LIns *lengthAndFlags_ins = w.ldpStringLengthAndFlags(str_ins);
56750:     if (MaybeBranch mbr = w.jt(w.eqp0(w.andp(lengthAndFlags_ins,
56750:                                              w.nameImmw(JSString::ROPE_BIT)))))
56750:     {
59890:         LIns *args[] = { str_ins, cx_ins };
59890:         LIns *ok_ins = w.call(&js_Flatten_ci, args);
59890:         guard(false, w.eqi0(ok_ins), OOM_EXIT);
56750:         w.label(mbr);
56750:     }
56750: 
59888:     LIns* inRange = w.ltup(idx_ins, w.rshupN(lengthAndFlags_ins, JSString::LENGTH_SHIFT));
55505: 
54427:     if (mode == JSOP_GETELEM) {
55505:         guard(true, inRange, MISMATCH_EXIT);
55505: 
55556:         *out = getUnitString(str_ins, idx_ins);
55505:     } else {
56750:         LIns *phi_ins = w.allocp(sizeof(JSString *));
56750:         w.stAlloc(w.nameImmpNonGC(cx->runtime->emptyString), phi_ins);
56750: 
56750:         if (MaybeBranch mbr = w.jf(inRange)) {
55505:             LIns *unitstr_ins = getUnitString(str_ins, idx_ins);
56750:             w.stAlloc(unitstr_ins, phi_ins);
56750:             w.label(mbr);
56750:         }
56750:         *out = w.ldpAlloc(phi_ins);
55556:     }
55556:     return RECORD_CONTINUE;
49109: }
49109: 
49111: // Typed array tracing depends on EXPANDED_LOADSTORE and F2I
49111: #if NJ_EXPANDED_LOADSTORE_SUPPORTED && NJ_F2I_SUPPORTED
49111: static bool OkToTraceTypedArrays = true;
49111: #else
49111: static bool OkToTraceTypedArrays = false;
49111: #endif
49111: 
55525: JS_REQUIRES_STACK void
55525: TraceRecorder::guardNotHole(LIns *argsobj_ins, LIns *idx_ins)
55525: {
55746:     // vp = &argsobj->slots[JSSLOT_ARGS_DATA].slots[idx]
56750:     LIns* argsData_ins = w.getObjPrivatizedSlot(argsobj_ins, JSObject::JSSLOT_ARGS_DATA);
56750:     LIns* slotOffset_ins = w.addp(w.nameImmw(offsetof(ArgumentsData, slots)),
56750:                                   w.ui2p(w.muliN(idx_ins, sizeof(Value))));
56750:     LIns* vp_ins = w.addp(argsData_ins, slotOffset_ins);
55525: 
55525:     guard(false,
59902:           w.name(is_boxed_magic(ArgsSlotOffsetAddress(vp_ins), JS_ARGS_HOLE),
59902:                  "guard(not deleted arg)"),
55525:           MISMATCH_EXIT);
55525: }
55525: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18300: TraceRecorder::record_JSOP_GETELEM()
18300: {
42717:     bool call = *cx->regs->pc == JSOP_CALLELEM;
26551: 
48470:     Value& idx = stackval(-1);
48470:     Value& lval = stackval(-2);
19983: 
19983:     LIns* obj_ins = get(&lval);
19979:     LIns* idx_ins = get(&idx);
19979: 
26274:     // Special case for array-like access of strings.
48470:     if (lval.isString() && hasInt32Repr(idx)) {
26551:         if (call)
33542:             RETURN_STOP_A("JSOP_CALLELEM on a string");
26274:         int i = asInt32(idx);
48470:         if (size_t(i) >= lval.toString()->length())
33542:             RETURN_STOP_A("Invalid string index in JSOP_GETELEM");
55556:         LIns* char_ins;
55556:         CHECK_STATUS_A(getCharAt(lval.toString(), obj_ins, idx_ins, JSOP_GETELEM, &char_ins));
55556:         set(&lval, char_ins);
33542:         return ARECORD_CONTINUE;
19983:     }
19983: 
48470:     if (lval.isPrimitive())
33542:         RETURN_STOP_A("JSOP_GETLEM on a primitive");
33542:     RETURN_IF_XML_A(lval);
19979: 
48470:     JSObject* obj = &lval.toObject();
31444:     if (obj == globalObj)
33542:         RETURN_STOP_A("JSOP_GETELEM on global");
19979:     LIns* v_ins;
19979: 
26274:     /* Property access using a string name or something we have to stringify. */
48470:     if (!idx.isInt32()) {
48470:         if (!idx.isPrimitive())
33542:             RETURN_STOP_A("object used as index");
33542: 
33542:         return InjectStatus(getPropertyByName(obj_ins, &idx, &lval));
19979:     }
19979: 
40405:     if (obj->isArguments()) {
55525:         // Don't even try to record if out of range or reading a deleted arg
55525:         int32 int_idx = idx.toInt32();
55525:         if (int_idx < 0 || int_idx >= (int32)obj->getArgsInitialLength())
55525:             RETURN_STOP_A("cannot trace arguments with out of range index");
55525:         if (obj->getArgsElement(int_idx).isMagic(JS_ARGS_HOLE))
55525:             RETURN_STOP_A("reading deleted args element");
55525: 
55525:         // Only trace reading arguments out of active, tracked frame
30248:         unsigned depth;
31460:         JSStackFrame *afp = guardArguments(obj, obj_ins, &depth);
30248:         if (afp) {
53840:             Value* vp = &afp->canonicalActualArg(int_idx);
41265:             if (idx_ins->isImmD()) {
55525:                 JS_ASSERT(int_idx == (int32)idx_ins->immD());
56750:                 guardNotHole(obj_ins, w.nameImmi(int_idx));
30248:                 v_ins = get(vp);
30248:             } else {
30248:                 // If the index is not a constant expression, we generate LIR to load the value from
30248:                 // the native stack area. The guard on js_ArgumentClass above ensures the up-to-date
30248:                 // value has been written back to the native stack area.
55556:                 CHECK_STATUS_A(makeNumberInt32(idx_ins, &idx_ins));
53840: 
53840:                 /*
53840:                  * For small nactual,
53840:                  * 0 <= int_idx < nactual iff unsigned(int_idx) < unsigned(nactual).
53840:                  */
37693:                 guard(true,
56750:                       w.name(w.ltui(idx_ins, w.nameImmui(afp->numActualArgs())),
37693:                              "guard(upvar index in range)"),
53840:                       MISMATCH_EXIT);
37693: 
55525:                 guardNotHole(obj_ins, idx_ins);
55525: 
48470:                 JSValueType type = getCoercedType(*vp);
30248: 
30248:                 // Guard that the argument has the same type on trace as during recording.
30248:                 LIns* typemap_ins;
37694:                 if (depth == 0) {
30248:                     // In this case, we are in the same frame where the arguments object was created.
30248:                     // The entry type map is not necessarily up-to-date, so we capture a new type map
30248:                     // for this point in the code.
30860:                     unsigned stackSlots = NativeStackSlots(cx, 0 /* callDepth */);
48470:                     JSValueType* typemap = new (traceAlloc()) JSValueType[stackSlots];
30248:                     DetermineTypesVisitor detVisitor(*this, typemap);
30248:                     VisitStackSlots(detVisitor, cx, 0);
56750:                     typemap_ins = w.nameImmpNonGC(typemap + 2 /* callee, this */);
30248:                 } else {
30248:                     // In this case, we are in a deeper frame from where the arguments object was
30248:                     // created. The type map at the point of the call out from the creation frame
30248:                     // is accurate.
30248:                     // Note: this relies on the assumption that we abort on setting an element of
30248:                     // an arguments object in any deeper frame.
56750:                     LIns* fip_ins = w.ldpRstack(lirbuf->rp, (callDepth-depth)*sizeof(FrameInfo*));
56750:                     typemap_ins = w.addp(fip_ins, w.nameImmw(sizeof(FrameInfo) + 2/*callee,this*/ * sizeof(JSValueType)));
56750:                 }
56750: 
56750:                 LIns* type_ins = w.lduc2uiConstTypeMapEntry(typemap_ins, idx_ins);
30248:                 guard(true,
56750:                       w.name(w.eqi(type_ins, w.immi(type)), "guard(type-stable upvar)"),
30248:                       BRANCH_EXIT);
30248: 
30248:                 // Read the value out of the native stack area.
53840:                 size_t stackOffset = nativespOffset(&afp->canonicalActualArg(0));
56750:                 LIns* args_addr_ins = w.addp(lirbuf->sp, w.nameImmw(stackOffset));
56750:                 LIns* argi_addr_ins = w.addp(args_addr_ins,
56750:                                              w.ui2p(w.muli(idx_ins, w.nameImmi(sizeof(double)))));
56750: 
56750:                 // The Address could be more precise, but ValidateWriter
40353:                 // doesn't recognise the complex expression involving 'sp' as
56750:                 // an stack access, and it's not worth the effort to be
48613:                 // more precise because this case is rare.
56750:                 v_ins = stackLoad(AnyAddress(argi_addr_ins), type);
30248:             }
30248:             JS_ASSERT(v_ins);
30248:             set(&lval, v_ins);
36559:             if (call)
36559:                 set(&idx, obj_ins);
33542:             return ARECORD_CONTINUE;
33542:         }
33542:         RETURN_STOP_A("can't reach arguments object's frame");
38497:     }
38497: 
38497:     if (obj->isDenseArray()) {
28411:         // Fast path for dense arrays accessed with a integer index.
48470:         Value* vp;
18300:         LIns* addr_ins;
31444: 
53615:         VMSideExit* branchExit = snapshot(BRANCH_EXIT);
53615:         guardDenseArray(obj_ins, branchExit);
53615:         CHECK_STATUS_A(denseArrayElement(lval, idx, vp, v_ins, addr_ins, branchExit));
19983:         set(&lval, v_ins);
26551:         if (call)
26551:             set(&idx, obj_ins);
33542:         return ARECORD_CONTINUE;
38497:     }
38497: 
38497:     if (OkToTraceTypedArrays && js_IsTypedArray(obj)) {
37754:         // Fast path for typed arrays accessed with a integer index.
48470:         Value* vp;
48613:         guardClass(obj_ins, obj->getClass(), snapshot(BRANCH_EXIT), LOAD_CONST);
56750:         CHECK_STATUS_A(typedArrayElement(lval, idx, vp, v_ins));
37754:         set(&lval, v_ins);
37754:         if (call)
37754:             set(&idx, obj_ins);
37754:         return ARECORD_CONTINUE;
33542:     }
33542: 
33542:     return InjectStatus(getPropertyByIndex(obj_ins, idx_ins, &lval));
31444: }
31444: 
24489: /* Functions used by JSOP_SETELEM */
24489: 
31829: static JSBool FASTCALL
54169: SetPropertyByName(JSContext* cx, JSObject* obj, JSString** namep, Value* vp, JSBool strict)
31829: {
37741:     LeaveTraceIfGlobalObject(cx, obj);
31829: 
31524:     jsid id;
54169:     if (!RootedStringToId(cx, namep, &id) || !obj->setProperty(cx, id, vp, strict)) {
37741:         SetBuiltinError(cx);
54169:         return false;
31829:     }
60574:     return WasBuiltinSuccessful(cx);
31829: }
54169: JS_DEFINE_CALLINFO_5(static, BOOL_FAIL, SetPropertyByName, 
54169:                      CONTEXT, OBJECT, STRINGPTR, VALUEPTR, BOOL,
48613:                      0, ACCSET_STORE_ANY)
31829: 
31829: static JSBool FASTCALL
48470: InitPropertyByName(JSContext* cx, JSObject* obj, JSString** namep, ValueArgType arg)
31829: {
37741:     LeaveTraceIfGlobalObject(cx, obj);
31829: 
31829:     jsid id;
31829:     if (!RootedStringToId(cx, namep, &id) ||
48470:         !obj->defineProperty(cx, id, ValueArgToConstRef(arg), NULL, NULL, JSPROP_ENUMERATE)) {
37741:         SetBuiltinError(cx);
31524:         return JS_FALSE;
31829:     }
60574:     return WasBuiltinSuccessful(cx);
31829: }
48470: JS_DEFINE_CALLINFO_4(static, BOOL_FAIL, InitPropertyByName, CONTEXT, OBJECT, STRINGPTR, VALUE,
48613:                      0, ACCSET_STORE_ANY)
31829: 
33542: JS_REQUIRES_STACK RecordingStatus
48470: TraceRecorder::initOrSetPropertyByName(LIns* obj_ins, Value* idvalp, Value* rvalp, bool init)
31829: {
31829:     CHECK_STATUS(primitiveToStringInPlace(idvalp));
31829: 
48470:     if (init) {
48470:         LIns* v_ins = box_value_for_native_call(*rvalp, get(rvalp));
31829:         enterDeepBailCall();
56750:         LIns* idvalp_ins = w.name(addr(idvalp), "idvalp");
48470:         LIns* args[] = {v_ins, idvalp_ins, obj_ins, cx_ins};
56750:         pendingGuardCondition = w.call(&InitPropertyByName_ci, args);
31829:     } else {
31829:         // See note in getPropertyByName about vp.
48470:         LIns* vp_ins = box_value_into_alloc(*rvalp, get(rvalp));
48470:         enterDeepBailCall();
56750:         LIns* idvalp_ins = w.name(addr(idvalp), "idvalp");
54169:         LIns* args[] = { strictModeCode_ins, vp_ins, idvalp_ins, obj_ins, cx_ins };
56750:         pendingGuardCondition = w.call(&SetPropertyByName_ci, args);
48470:     }
31829: 
31829:     leaveDeepBailCall();
33542:     return RECORD_CONTINUE;
31524: }
31524: 
31517: static JSBool FASTCALL
54169: SetPropertyByIndex(JSContext* cx, JSObject* obj, int32 index, Value* vp, JSBool strict)
31829: {
37741:     LeaveTraceIfGlobalObject(cx, obj);
31829: 
40403:     AutoIdRooter idr(cx);
54169:     if (!js_Int32ToId(cx, index, idr.addr()) || !obj->setProperty(cx, idr.id(), vp, strict)) {
37741:         SetBuiltinError(cx);
54169:         return false;
31829:     }
60574:     return WasBuiltinSuccessful(cx);
31829: }
54169: JS_DEFINE_CALLINFO_5(static, BOOL_FAIL, SetPropertyByIndex, CONTEXT, OBJECT, INT32, VALUEPTR, BOOL,
48613:                      0, ACCSET_STORE_ANY)
31829: 
31829: static JSBool FASTCALL
48470: InitPropertyByIndex(JSContext* cx, JSObject* obj, int32 index, ValueArgType arg)
31829: {
37741:     LeaveTraceIfGlobalObject(cx, obj);
31829: 
40403:     AutoIdRooter idr(cx);
31829:     if (!js_Int32ToId(cx, index, idr.addr()) ||
48470:         !obj->defineProperty(cx, idr.id(), ValueArgToConstRef(arg), NULL, NULL, JSPROP_ENUMERATE)) {
37741:         SetBuiltinError(cx);
31524:         return JS_FALSE;
31829:     }
60574:     return WasBuiltinSuccessful(cx);
31829: }
48613: JS_DEFINE_CALLINFO_4(static, BOOL_FAIL, InitPropertyByIndex, CONTEXT, OBJECT, INT32, VALUE,
48613:                      0, ACCSET_STORE_ANY)
31829: 
33542: JS_REQUIRES_STACK RecordingStatus
48470: TraceRecorder::initOrSetPropertyByIndex(LIns* obj_ins, LIns* index_ins, Value* rvalp, bool init)
31829: {
55556:     CHECK_STATUS(makeNumberInt32(index_ins, &index_ins));
31829: 
48470:     if (init) {
48470:         LIns* rval_ins = box_value_for_native_call(*rvalp, get(rvalp));
31829:         enterDeepBailCall();
31829:         LIns* args[] = {rval_ins, index_ins, obj_ins, cx_ins};
56750:         pendingGuardCondition = w.call(&InitPropertyByIndex_ci, args);
31829:     } else {
31829:         // See note in getPropertyByName about vp.
48470:         LIns* vp_ins = box_value_into_alloc(*rvalp, get(rvalp));
48470:         enterDeepBailCall();
54169:         LIns* args[] = {strictModeCode_ins, vp_ins, index_ins, obj_ins, cx_ins};
56750:         pendingGuardCondition = w.call(&SetPropertyByIndex_ci, args);
48470:     }
31829: 
31829:     leaveDeepBailCall();
33542:     return RECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
35466: TraceRecorder::setElem(int lval_spindex, int idx_spindex, int v_spindex)
35466: {
48470:     Value& v = stackval(v_spindex);
48470:     Value& idx = stackval(idx_spindex);
48470:     Value& lval = stackval(lval_spindex);
48470: 
48470:     if (lval.isPrimitive())
33542:         RETURN_STOP_A("left JSOP_SETELEM operand is not an object");
33542:     RETURN_IF_XML_A(lval);
19979: 
48470:     JSObject* obj = &lval.toObject();
19983:     LIns* obj_ins = get(&lval);
19979:     LIns* idx_ins = get(&idx);
18300:     LIns* v_ins = get(&v);
19979: 
52503:     if (obj->isArguments())
35054:         RETURN_STOP_A("can't trace setting elements of the |arguments| object");
35054: 
41785:     if (obj == globalObj)
41785:         RETURN_STOP_A("can't trace setting elements on the global object");
41785: 
48470:     if (!idx.isInt32()) {
48470:         if (!idx.isPrimitive())
33542:             RETURN_STOP_A("non-primitive index");
33542:         CHECK_STATUS_A(initOrSetPropertyByName(obj_ins, &idx, &v,
42717:                                              *cx->regs->pc == JSOP_INITELEM));
37754:     } else if (OkToTraceTypedArrays && js_IsTypedArray(obj)) {
37754:         // Fast path: assigning to element of typed array.
53614:         VMSideExit* branchExit = snapshot(BRANCH_EXIT);
37754: 
37754:         // Ensure array is a typed array and is the same type as what was written
53614:         guardClass(obj_ins, obj->getClass(), branchExit, LOAD_CONST);
37754: 
37754:         js::TypedArray* tarray = js::TypedArray::fromJSObject(obj);
37754: 
56750:         LIns* priv_ins = w.ldpObjPrivate(obj_ins);
37754: 
40294:         // The index was on the stack and is therefore a LIR float; force it to
37754:         // be an integer.                              
55556:         CHECK_STATUS_A(makeNumberInt32(idx_ins, &idx_ins));
37754: 
37754:         // Ensure idx >= 0 && idx < length (by using uint32)
59251:         CHECK_STATUS_A(guard(true,
59251:                              w.name(w.ltui(idx_ins, w.ldiConstTypedArrayLength(priv_ins)),
59251:                                     "inRange"),
59251:                              OVERFLOW_EXIT, /* abortIfAlwaysExits = */true));
37754: 
37754:         // We're now ready to store
56750:         LIns* data_ins = w.ldpConstTypedArrayData(priv_ins);
56750:         LIns* pidx_ins = w.ui2p(idx_ins);
41810:         LIns* typed_v_ins = v_ins;
41810: 
40294:         // If it's not a number, convert objects to NaN,
40294:         // null to 0, and call StringToNumber or BooleanOrUndefinedToNumber
40294:         // for those.
48470:         if (!v.isNumber()) {
48470:             if (v.isNull()) {
56750:                 typed_v_ins = w.immd(0);
48470:             } else if (v.isUndefined()) {
56750:                 typed_v_ins = w.immd(js_NaN);
48470:             } else if (v.isString()) {
59890:                 LIns* ok_ins = w.allocp(sizeof(JSBool));
59890:                 LIns* args[] = { ok_ins, typed_v_ins, cx_ins };
56750:                 typed_v_ins = w.call(&js_StringToNumber_ci, args);
59890:                 guard(false,
59890:                       w.name(w.eqi0(w.ldiAlloc(ok_ins)), "guard(oom)"),
59890:                       OOM_EXIT);
48470:             } else if (v.isBoolean()) {
48470:                 JS_ASSERT(v.isBoolean());
56750:                 typed_v_ins = w.i2d(typed_v_ins);
56750:             } else {
56750:                 typed_v_ins = w.immd(js_NaN);
40294:             }
40294:         }
40294: 
40294:         switch (tarray->type) {
40294:           case js::TypedArray::TYPE_INT8:
40294:           case js::TypedArray::TYPE_INT16:
40294:           case js::TypedArray::TYPE_INT32:
41810:             typed_v_ins = d2i(typed_v_ins);
40294:             break;
40294:           case js::TypedArray::TYPE_UINT8:
40294:           case js::TypedArray::TYPE_UINT16:
40294:           case js::TypedArray::TYPE_UINT32:
56750:             typed_v_ins = d2u(typed_v_ins);
40294:             break;
40294:           case js::TypedArray::TYPE_UINT8_CLAMPED:
59993:             if (IsPromotedInt32(typed_v_ins)) {
59993:                 typed_v_ins = w.demoteToInt32(typed_v_ins);
56750:                 typed_v_ins = w.cmovi(w.ltiN(typed_v_ins, 0),
56750:                                       w.immi(0),
56750:                                       w.cmovi(w.gtiN(typed_v_ins, 0xff),
56750:                                                      w.immi(0xff),
56750:                                                      typed_v_ins));
56750:             } else {
56750:                 typed_v_ins = w.call(&js_TypedArray_uint8_clamp_double_ci, &typed_v_ins);
39943:             }
40294:             break;
40294:           case js::TypedArray::TYPE_FLOAT32:
40294:           case js::TypedArray::TYPE_FLOAT64:
40294:             // Do nothing, this is already a float
40294:             break;
40294:           default:
40294:             JS_NOT_REACHED("Unknown typed array type in tracer");       
38515:         }
38515: 
37754:         switch (tarray->type) {
37754:           case js::TypedArray::TYPE_INT8:
40294:           case js::TypedArray::TYPE_UINT8_CLAMPED:
37754:           case js::TypedArray::TYPE_UINT8:
56750:             w.sti2cTypedArrayElement(typed_v_ins, data_ins, pidx_ins);
37754:             break;
37754:           case js::TypedArray::TYPE_INT16:
37754:           case js::TypedArray::TYPE_UINT16:
56750:             w.sti2sTypedArrayElement(typed_v_ins, data_ins, pidx_ins);
37754:             break;
37754:           case js::TypedArray::TYPE_INT32:
37754:           case js::TypedArray::TYPE_UINT32:
56750:             w.stiTypedArrayElement(typed_v_ins, data_ins, pidx_ins);
37754:             break;
37754:           case js::TypedArray::TYPE_FLOAT32:
56750:             w.std2fTypedArrayElement(typed_v_ins, data_ins, pidx_ins);
37754:             break;
38512:           case js::TypedArray::TYPE_FLOAT64:
56750:             w.stdTypedArrayElement(typed_v_ins, data_ins, pidx_ins);
38927:             break;
37754:           default:
37754:             JS_NOT_REACHED("Unknown typed array type in tracer");       
37754:         }
48470:     } else if (idx.toInt32() < 0 || !obj->isDenseArray()) {
33542:         CHECK_STATUS_A(initOrSetPropertyByIndex(obj_ins, idx_ins, &v,
42717:                                                 *cx->regs->pc == JSOP_INITELEM));
31829:     } else {
31829:         // Fast path: assigning to element of dense array.
53614:         VMSideExit* branchExit = snapshot(BRANCH_EXIT);
53614:         VMSideExit* mismatchExit = snapshot(MISMATCH_EXIT);
26274: 
26274:         // Make sure the array is actually dense.
42749:         if (!obj->isDenseArray()) 
42749:             return ARECORD_STOP;
53614:         guardDenseArray(obj_ins, branchExit);
26274: 
31829:         // The index was on the stack and is therefore a LIR float. Force it to
31829:         // be an integer.
55556:         CHECK_STATUS_A(makeNumberInt32(idx_ins, &idx_ins));
54426: 
53614:         if (!js_EnsureDenseArrayCapacity(cx, obj, idx.toInt32()))
53614:             RETURN_STOP_A("couldn't ensure dense array capacity for setelem");
53614: 
53614:         // Grow the array if the index exceeds the capacity.  This happens
53614:         // rarely, eg. less than 1% of the time in SunSpider.
56750:         LIns* capacity_ins = w.ldiDenseArrayCapacity(obj_ins);
56750:         /*
56750:          * It's important that CSE works across this control-flow diamond
56750:          * because it really helps series of interleaved GETELEM and SETELEM
56750:          * operations.  Likewise with the diamond below.
56750:          */
56750:         w.pauseAddingCSEValues();
57719:         if (MaybeBranch mbr = w.jt(w.name(w.ltui(idx_ins, capacity_ins), "inRange"))) {
53614:             LIns* args[] = { idx_ins, obj_ins, cx_ins };
56750:             LIns* res_ins = w.call(&js_EnsureDenseArrayCapacity_ci, args);
56750:             guard(false, w.eqi0(res_ins), mismatchExit);
56750:             w.label(mbr);
56750:         }
56750:         w.resumeAddingCSEValues();
53614: 
53614:         // Get the address of the element.
57719:         LIns *elemp_ins = w.name(w.getDslotAddress(obj_ins, idx_ins), "elemp");
53614: 
53614:         // If we are overwriting a hole:
53614:         // - Guard that we don't have any indexed properties along the prototype chain.
53614:         // - Check if the length has changed;  if so, update it to index+1.
53614:         // This happens moderately often, eg. close to 10% of the time in
53614:         // SunSpider, and for some benchmarks it's close to 100%.
57719:         Address dslotAddr = DSlotsAddress(elemp_ins);
58285:         LIns* isHole_ins = w.name(is_boxed_magic(dslotAddr, JS_ARRAY_HOLE),
57719:                                   "isHole");
56750:         w.pauseAddingCSEValues();
57708:         if (MaybeBranch mbr1 = w.jf(isHole_ins)) {
57756:             /*
57756:              * It's important that this use branchExit, not mismatchExit, since
57756:              * changes to shapes should just mean we compile a new branch, not
57756:              * throw the whole trace away.
57756:              */
57756:             CHECK_STATUS_A(guardPrototypeHasNoIndexedProperties(obj, obj_ins, branchExit));
57708:             LIns* length_ins = w.lduiObjPrivate(obj_ins);
57708:             if (MaybeBranch mbr2 = w.jt(w.ltui(idx_ins, length_ins))) {
57708:                 LIns* newLength_ins = w.name(w.addiN(idx_ins, 1), "newLength");
57708:                 w.stuiObjPrivate(obj_ins, newLength_ins);
57708:                 w.label(mbr2);
57708:             }
57708:             w.label(mbr1);
56750:         }
56750:         w.resumeAddingCSEValues();
53614: 
53614:         // Right, actually set the element.
56750:         box_value_into(v, v_ins, dslotAddr);
31829:     }
18300: 
42717:     jsbytecode* pc = cx->regs->pc;
18300:     if (*pc == JSOP_SETELEM && pc[JSOP_SETELEM_LENGTH] != JSOP_POP)
19983:         set(&lval, v_ins);
19993: 
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
35466: TraceRecorder::record_JSOP_SETELEM()
35466: {
35466:     return setElem(-3, -2, -1);
35466: }
35466: 
35466: JS_REQUIRES_STACK AbortableRecordingStatus
18300: TraceRecorder::record_JSOP_CALLNAME()
18300: {
53840:     JSObject* obj = &cx->fp()->scopeChain();
18300:     if (obj != globalObj) {
48470:         Value* vp;
30647:         LIns* ins;
31075:         NameResult nr;
33542:         CHECK_STATUS_A(scopeChainProp(obj, vp, ins, nr));
30647:         stack(0, ins);
56750:         stack(1, w.immiUndefined());
56750:         return ARECORD_CONTINUE;
56750:     }
56750: 
56750:     LIns* obj_ins = w.immpObjGC(globalObj);
18300:     JSObject* obj2;
40374:     PCVal pcval;
25633: 
33542:     CHECK_STATUS_A(test_property_cache(obj, obj_ins, obj2, pcval));
18300: 
48470:     if (pcval.isNull() || !pcval.isFunObj())
33542:         RETURN_STOP_A("callee is not an object");
25633: 
56750:     stack(0, w.immpObjGC(&pcval.toFunObj()));
56750:     stack(1, w.immiUndefined());
33542:     return ARECORD_CONTINUE;
27933: }
27933: 
30860: JS_DEFINE_CALLINFO_5(extern, UINT32, GetUpvarArgOnTrace, CONTEXT, UINT32, INT32, UINT32,
48613:                      DOUBLEPTR, 0, ACCSET_STORE_ANY)
30860: JS_DEFINE_CALLINFO_5(extern, UINT32, GetUpvarVarOnTrace, CONTEXT, UINT32, INT32, UINT32,
48613:                      DOUBLEPTR, 0, ACCSET_STORE_ANY)
30860: JS_DEFINE_CALLINFO_5(extern, UINT32, GetUpvarStackOnTrace, CONTEXT, UINT32, INT32, UINT32,
48613:                      DOUBLEPTR, 0, ACCSET_STORE_ANY)
28268: 
28923: /*
30860:  * Record LIR to get the given upvar. Return the LIR instruction for the upvar
30860:  * value. NULL is returned only on a can't-happen condition with an invalid
30860:  * typemap. The value of the upvar is returned as v.
28923:  */
28923: JS_REQUIRES_STACK LIns*
48470: TraceRecorder::upvar(JSScript* script, JSUpvarArray* uva, uintN index, Value& v)
28923: {
28275:     /*
28969:      * Try to find the upvar in the current trace's tracker. For &vr to be
48582:      * the address of the jsval found in js::GetUpvar, we must initialize
28969:      * vr directly with the result, so it is a reference to the same location.
28969:      * It does not work to assign the result to v, because v is an already
28969:      * existing reference that points to something else.
28969:      */
47573:     UpvarCookie cookie = uva->vector[index];
48582:     const Value& vr = GetUpvar(cx, script->staticLevel, cookie);
28969:     v = vr;
32742: 
36662:     if (LIns* ins = attemptImport(&vr))
36662:         return ins;
28275: 
28275:     /*
30860:      * The upvar is not in the current trace, so get the upvar value exactly as
30860:      * the interpreter does and unbox.
28275:      */
47573:     uint32 level = script->staticLevel - cookie.level();
47573:     uint32 cookieSlot = cookie.slot();
48582:     JSStackFrame* fp = cx->findFrameAtLevel(level);
29021:     const CallInfo* ci;
29021:     int32 slot;
53840:     if (!fp->isFunctionFrame() || fp->isEvalFrame()) {
30860:         ci = &GetUpvarStackOnTrace_ci;
29022:         slot = cookieSlot;
51120:     } else if (cookieSlot < fp->numFormalArgs()) {
30860:         ci = &GetUpvarArgOnTrace_ci;
29021:         slot = cookieSlot;
47573:     } else if (cookieSlot == UpvarCookie::CALLEE_SLOT) {
30860:         ci = &GetUpvarArgOnTrace_ci;
29021:         slot = -2;
29021:     } else {
30860:         ci = &GetUpvarVarOnTrace_ci;
51120:         slot = cookieSlot - fp->numFormalArgs();
29021:     }
29021: 
56750:     LIns* outp = w.allocp(sizeof(double));
28268:     LIns* args[] = {
28268:         outp,
56750:         w.nameImmi(callDepth),
56750:         w.nameImmi(slot),
56750:         w.nameImmi(level),
28268:         cx_ins
28268:     };
56750:     LIns* call_ins = w.call(ci, args);
48470:     JSValueType type = getCoercedType(v);
28268:     guard(true,
56750:           w.name(w.eqi(call_ins, w.immi(type)), "guard(type-stable upvar)"),
28268:           BRANCH_EXIT);
56750:     return stackLoad(AllocSlotsAddress(outp), type);
30248: }
30248: 
30248: /*
30860:  * Generate LIR to load a value from the native stack. This method ensures that
30860:  * the correct LIR load operator is used.
30248:  */
40871: LIns*
56750: TraceRecorder::stackLoad(Address addr, uint8 type)
56750: {
28268:     switch (type) {
48470:       case JSVAL_TYPE_DOUBLE:
56750:         return w.ldd(addr);
48470:       case JSVAL_TYPE_NONFUNOBJ:
48470:       case JSVAL_TYPE_STRING:
48470:       case JSVAL_TYPE_FUNOBJ:
48470:       case JSVAL_TYPE_NULL:
56750:         return w.ldp(addr);
48470:       case JSVAL_TYPE_INT32:
56750:         return w.i2d(w.ldi(addr));
48470:       case JSVAL_TYPE_BOOLEAN:
48470:       case JSVAL_TYPE_UNDEFINED:
48470:       case JSVAL_TYPE_MAGIC:
56750:         return w.ldi(addr);
48470:       case JSVAL_TYPE_BOXED:
28268:       default:
29896:         JS_NOT_REACHED("found jsval type in an upvar type map entry");
28923:         return NULL;
28268:     }
28923: }
28923: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
52503: TraceRecorder::record_JSOP_GETFCSLOT()
27012: {
53840:     JSObject& callee = cx->fp()->callee();
53840:     LIns* callee_ins = get(&cx->fp()->calleeValue());
27012: 
56750:     LIns* upvars_ins = w.getObjPrivatizedSlot(callee_ins, JSObject::JSSLOT_FLAT_CLOSURE_UPVARS);
52503: 
42717:     unsigned index = GET_UINT16(cx->regs->pc);
56750:     LIns *v_ins = unbox_value(callee.getFlatClosureUpvar(index),
56750:                               FCSlotsAddress(upvars_ins, index),
56750:                               snapshot(BRANCH_EXIT));
52503:     stack(0, v_ins);
52503:     return ARECORD_CONTINUE;
52503: }
52503: 
52503: JS_REQUIRES_STACK AbortableRecordingStatus
52503: TraceRecorder::record_JSOP_CALLFCSLOT()
52503: {
52503:     CHECK_STATUS_A(record_JSOP_GETFCSLOT());
56750:     stack(1, w.immiUndefined());
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK RecordingStatus
48470: TraceRecorder::guardCallee(Value& callee)
48470: {
53840:     JSObject& callee_obj = callee.toObject();
53840:     JS_ASSERT(callee_obj.isFunction());
53840:     JSFunction* callee_fun = (JSFunction*) callee_obj.getPrivate();
38549: 
38549:     /*
38549:      * First, guard on the callee's function (JSFunction*) identity. This is
38549:      * necessary since tracing always inlines function calls. But note that
38549:      * TR::functionCall avoids calling TR::guardCallee for constant methods
38549:      * (those hit in the property cache from JSOP_CALLPROP).
38549:      */
27540:     VMSideExit* branchExit = snapshot(BRANCH_EXIT);
22623:     LIns* callee_ins = get(&callee);
36361:     tree->gcthings.addUnique(callee);
38549: 
27540:     guard(true,
56750:           w.eqp(w.ldpObjPrivate(callee_ins), w.nameImmpNonGC(callee_fun)),
27540:           branchExit);
35479: 
35479:     /*
38549:      * Second, consider guarding on the parent scope of the callee.
38549:      *
38549:      * As long as we guard on parent scope, we are guaranteed when recording
38549:      * variable accesses for a Call object having no private data that we can
38549:      * emit code that avoids checking for an active JSStackFrame for the Call
38549:      * object (which would hold fresh variable values -- the Call object's
52503:      * slots would be stale until the stack frame is popped). This is because
38549:      * Call objects can't pick up a new stack frame in their private slot once
38549:      * they have none. TR::callProp and TR::setCallProp depend on this fact and
38549:      * document where; if this guard is removed make sure to fix those methods.
38549:      * Search for the "parent guard" comments in them.
38549:      *
38549:      * In general, a loop in an escaping function scoped by Call objects could
38549:      * be traced before the function has returned, and the trace then triggered
38549:      * after, or vice versa. The function must escape, i.e., be a "funarg", or
38549:      * else there's no need to guard callee parent at all. So once we know (by
38549:      * static analysis) that a function may escape, we cannot avoid guarding on
38549:      * either the private data of the Call object or the Call object itself, if
38549:      * we wish to optimize for the particular deactivated stack frame (null
38549:      * private data) case as noted above.
38549:      */
59968:     if (callee_fun->isInterpreted() &&
59968:         (!FUN_NULL_CLOSURE(callee_fun) || callee_fun->script()->bindings.hasUpvars())) {
53840:         JSObject* parent = callee_obj.getParent();
38549: 
38549:         if (parent != globalObj) {
52503:             if (!parent->isCall())
38549:                 RETURN_STOP("closure scoped by neither the global object nor a Call object");
38549: 
22623:             guard(true,
56750:                   w.eqp(w.ldpObjParent(callee_ins), w.immpObjGC(parent)),
27540:                   branchExit);
38549:         }
38549:     }
33542:     return RECORD_CONTINUE;
27933: }
27933: 
31460: /*
31460:  * Prepare the given |arguments| object to be accessed on trace. If the return
31460:  * value is non-NULL, then the given |arguments| object refers to a frame on
31460:  * the current trace and is guaranteed to refer to the same frame on trace for
31460:  * all later executions.
31460:  */
31460: JS_REQUIRES_STACK JSStackFrame *
31460: TraceRecorder::guardArguments(JSObject *obj, LIns* obj_ins, unsigned *depthp)
31460: {
40405:     JS_ASSERT(obj->isArguments());
31460: 
31460:     JSStackFrame *afp = frameIfInRange(obj, depthp);
31460:     if (!afp)
31460:         return NULL;
31460: 
31460:     VMSideExit *exit = snapshot(MISMATCH_EXIT);
51095:     guardClass(obj_ins, obj->getClass(), exit, LOAD_CONST);
31460: 
53840:     LIns* args_ins = getFrameObjPtr(afp->addressOfArgs());
56750:     LIns* cmp = w.eqp(args_ins, obj_ins);
56750:     guard(true, cmp, exit);
31460:     return afp;
31460: }
31460: 
33542: JS_REQUIRES_STACK RecordingStatus
48470: TraceRecorder::interpretedFunctionCall(Value& fval, JSFunction* fun, uintN argc, bool constructing)
18300: {
34290:     /*
34290:      * The function's identity (JSFunction and therefore JSScript) is guarded,
59220:      * so we can optimize away the function call if the corresponding script is
59220:      * empty. No need to worry about crossing globals or relocating argv, even,
59220:      * in this case!
59220:      */
59220:     if (fun->script()->isEmpty()) {
55503:         LIns* rval_ins;
55503:         if (constructing) {
56750:             LIns* args[] = { get(&fval), w.nameImmpNonGC(&js_ObjectClass), cx_ins };
56750:             LIns* tv_ins = w.call(&js_CreateThisFromTrace_ci, args);
56750:             guard(false, w.eqp0(tv_ins), OOM_EXIT);
55503:             rval_ins = tv_ins;
55503:         } else {
56750:             rval_ins = w.immiUndefined();
55503:         }
34290:         stack(-2 - argc, rval_ins);
34290:         return RECORD_CONTINUE;
34290:     }
34290: 
48470:     if (fval.toObject().getGlobal() != globalObj)
33542:         RETURN_STOP("JSOP_CALL or JSOP_NEW crosses global scopes");
19149: 
51446:     JSStackFrame* const fp = cx->fp();
18300: 
19085:     // Generate a type map for the outgoing frame and stash it in the LIR
30860:     unsigned stackSlots = NativeStackSlots(cx, 0 /* callDepth */);
33161:     FrameInfo* fi = (FrameInfo*)
48470:         tempAlloc().alloc(sizeof(FrameInfo) + stackSlots * sizeof(JSValueType));
48470:     JSValueType* typemap = (JSValueType*)(fi + 1);
29880: 
29880:     DetermineTypesVisitor detVisitor(*this, typemap);
29882:     VisitStackSlots(detVisitor, cx, 0);
19085: 
31924:     JS_ASSERT(argc < FrameInfo::CONSTRUCTING_FLAG);
31924: 
36361:     tree->gcthings.addUnique(fval);
42717:     fi->pc = cx->regs->pc;
53840:     fi->imacpc = fp->maybeImacropc();
42717:     fi->spdist = cx->regs->sp - fp->slots();
39928:     fi->set_argc(uint16(argc), constructing);
35083:     fi->callerHeight = stackSlots - (2 + argc);
53840:     fi->callerArgc = fp->isGlobalFrame() || fp->isEvalFrame() ? 0 : fp->numActualArgs();
18300: 
36361:     if (callDepth >= tree->maxCallDepth)
36361:         tree->maxCallDepth = callDepth + 1;
18300: 
33563:     fi = traceMonitor->frameCache->memoize(fi);
33563:     if (!fi)
33563:         RETURN_STOP("out of memory");
56750:     w.stRstack(w.nameImmpNonGC(fi), lirbuf->rp, callDepth * sizeof(FrameInfo*));
18300: 
33564: #if defined JS_JIT_SPEW
33567:     debug_only_printf(LC_TMTracer, "iFC frameinfo=%p, stack=%d, map=", (void*)fi,
33564:                       fi->callerHeight);
33564:     for (unsigned i = 0; i < fi->callerHeight; i++)
48470:         debug_only_printf(LC_TMTracer, "%c", TypeToChar(fi->get_typemap()[i]));
33564:     debug_only_print0(LC_TMTracer, "\n");
33564: #endif
33564: 
48470:     updateAtoms(fun->u.i.script);
33542:     return RECORD_CONTINUE;
33542: }
33542: 
57712: /*
57712:  * We implement JSOP_FUNAPPLY/JSOP_FUNCALL using imacros
57712:  */
57712: static inline JSOp
57712: GetCallMode(JSStackFrame *fp)
57712: {
57712:     if (fp->hasImacropc()) {
57712:         JSOp op = (JSOp) *fp->imacropc();
57712:         if (op == JSOP_FUNAPPLY || op == JSOP_FUNCALL)
57712:             return op;
57712:     }
57712:     return JSOP_CALL;
57712: }
57712: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18300: TraceRecorder::record_JSOP_CALL()
18300: {
42717:     uintN argc = GET_ARGC(cx->regs->pc);
42717:     cx->assertValidStackDepth(argc + 2);
57712:     return InjectStatus(functionCall(argc, GetCallMode(cx->fp())));
57712: }
57712: 
57712: static jsbytecode* funapply_imacro_table[] = {
57712:     funapply_imacros.apply0,
57712:     funapply_imacros.apply1,
57712:     funapply_imacros.apply2,
57712:     funapply_imacros.apply3,
57712:     funapply_imacros.apply4,
57712:     funapply_imacros.apply5,
57712:     funapply_imacros.apply6,
57712:     funapply_imacros.apply7,
57712:     funapply_imacros.apply8
23097: };
23097: 
57712: static jsbytecode* funcall_imacro_table[] = {
57712:     funcall_imacros.call0,
57712:     funcall_imacros.call1,
57712:     funcall_imacros.call2,
57712:     funcall_imacros.call3,
57712:     funcall_imacros.call4,
57712:     funcall_imacros.call5,
57712:     funcall_imacros.call6,
57712:     funcall_imacros.call7,
57712:     funcall_imacros.call8
23097: };
23097: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
57712: TraceRecorder::record_JSOP_FUNCALL()
57712: {
57712:     return record_JSOP_FUNAPPLY();
57712: }
57712: 
57712: JS_REQUIRES_STACK AbortableRecordingStatus
57712: TraceRecorder::record_JSOP_FUNAPPLY()
21452: {
42717:     jsbytecode *pc = cx->regs->pc;
22634:     uintN argc = GET_ARGC(pc);
42717:     cx->assertValidStackDepth(argc + 2);
42717: 
48470:     Value* vp = cx->regs->sp - (argc + 2);
22634:     jsuint length = 0;
22634:     JSObject* aobj = NULL;
22634:     LIns* aobj_ins = NULL;
23097: 
53840:     JS_ASSERT(!cx->fp()->hasImacropc());
22634: 
48470:     if (!IsFunctionObject(vp[0]))
22634:         return record_JSOP_CALL();
33542:     RETURN_IF_XML_A(vp[0]);
22634: 
48470:     JSObject* obj = &vp[0].toObject();
22634:     JSFunction* fun = GET_FUNCTION_PRIVATE(cx, obj);
22634:     if (FUN_INTERPRETED(fun))
22634:         return record_JSOP_CALL();
22634: 
53557:     bool apply = fun->u.n.native == js_fun_apply;
53557:     if (!apply && fun->u.n.native != js_fun_call)
22634:         return record_JSOP_CALL();
22634: 
22634:     /*
23097:      * We don't trace apply and call with a primitive 'this', which is the
55523:      * first positional parameter, unless 'this' is null.  That's ok.
55523:      */
55523:     if (argc > 0 && !vp[2].isObjectOrNull())
23097:         return record_JSOP_CALL();
23097: 
23097:     /*
23097:      * Guard on the identity of this, which is the function we are applying.
23097:      */
48470:     if (!IsFunctionObject(vp[1]))
33542:         RETURN_STOP_A("callee is not a function");
33542:     CHECK_STATUS_A(guardCallee(vp[1]));
23097: 
22634:     if (apply && argc >= 2) {
23097:         if (argc != 2)
33542:             RETURN_STOP_A("apply with excess arguments");
48470:         if (vp[3].isPrimitive())
33542:             RETURN_STOP_A("arguments parameter of apply is primitive");
48470:         aobj = &vp[3].toObject();
22634:         aobj_ins = get(&vp[3]);
22634: 
22634:         /*
30860:          * We trace dense arrays and arguments objects. The code we generate
30860:          * for apply uses imacros to handle a specific number of arguments.
30248:          */
39928:         if (aobj->isDenseArray()) {
42749:             guardDenseArray(aobj_ins, MISMATCH_EXIT);
40796:             length = aobj->getArrayLength();
23097:             guard(true,
56750:                   w.eqiN(w.lduiObjPrivate(aobj_ins), length),
22634:                   BRANCH_EXIT);
40405:         } else if (aobj->isArguments()) {
31460:             unsigned depth;
31460:             JSStackFrame *afp = guardArguments(aobj, aobj_ins, &depth);
30248:             if (!afp)
33542:                 RETURN_STOP_A("can't reach arguments object's frame");
55510:             if (aobj->isArgsLengthOverridden())
55510:                 RETURN_STOP_A("can't trace arguments with overridden length");
55510:             guardArgsLengthNotAssigned(aobj_ins);
51120:             length = afp->numActualArgs();
30248:         } else {
33542:             RETURN_STOP_A("arguments parameter of apply is not a dense array or argments object");
30248:         }
30248: 
57712:         if (length >= JS_ARRAY_LENGTH(funapply_imacro_table))
33542:             RETURN_STOP_A("too many arguments to apply");
33542: 
57712:         return InjectStatus(callImacro(funapply_imacro_table[length]));
57712:     }
57712: 
57712:     if (argc >= JS_ARRAY_LENGTH(funcall_imacro_table))
33542:         RETURN_STOP_A("too many arguments to call");
33542: 
57712:     return InjectStatus(callImacro(funcall_imacro_table[argc]));
21452: }
21452: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
28086: TraceRecorder::record_NativeCallComplete()
28086: {
32669:     if (pendingSpecializedNative == IGNORE_NATIVE_CALL_COMPLETE_CALLBACK)
33542:         return ARECORD_CONTINUE;
28086: 
50489: #ifdef DEBUG
50489:     JS_ASSERT(pendingSpecializedNative);
42717:     jsbytecode* pc = cx->regs->pc;
57712:     JS_ASSERT(*pc == JSOP_CALL || *pc == JSOP_FUNCALL || *pc == JSOP_FUNAPPLY ||
57712:               *pc == JSOP_NEW || *pc == JSOP_SETPROP);
50489: #endif
26552: 
48470:     Value& v = stackval(-1);
26552:     LIns* v_ins = get(&v);
26552: 
30847:     /*
30847:      * At this point the generated code has already called the native function
30847:      * and we can no longer fail back to the original pc location (JSOP_CALL)
30847:      * because that would cause the interpreter to re-execute the native
30847:      * function, which might have side effects.
30847:      *
30847:      * Instead, the snapshot() call below sees that we are currently parked on
30847:      * a traceable native's JSOP_CALL instruction, and it will advance the pc
30847:      * to restore by the length of the current opcode.  If the native's return
30847:      * type is jsval, snapshot() will also indicate in the type map that the
30847:      * element on top of the stack is a boxed value which doesn't need to be
48470:      * boxed if the type guard generated by unbox_value() fails.
30847:      */
24612: 
32669:     if (JSTN_ERRTYPE(pendingSpecializedNative) == FAIL_STATUS) {
60574:         leaveDeepBailCall();
56750: 
56750:         LIns* status = w.ldiStateField(builtinStatus);
32669:         if (pendingSpecializedNative == &generatedSpecializedNative) {
26552:             LIns* ok_ins = v_ins;
26552: 
26552:             /*
26552:              * If we run a generic traceable native, the return value is in the argument
28086:              * vector for native function calls. The actual return value of the native is a JSBool
28086:              * indicating the error status.
28086:              */
48470: 
56750:             Address nativeRvalAddr = AllocSlotsAddress(native_rval_ins);
50489:             if (pendingSpecializedNative->flags & JSTN_CONSTRUCTOR) {
48470:                 LIns *cond_ins;
48470:                 LIns *x;
48470: 
48470:                 // v_ins    := the object payload from native_rval_ins
48470:                 // cond_ins := true if native_rval_ins contains a JSObject*
56750:                 unbox_any_object(nativeRvalAddr, &v_ins, &cond_ins);
48470:                 // x        := v_ins if native_rval_ins contains a JSObject*, NULL otherwise
57719:                 x = w.cmovp(cond_ins, v_ins, w.immw(0));
48470:                 // v_ins    := newobj_ins if native_rval_ins doesn't contain a JSObject*,
48470:                 //             the object payload from native_rval_ins otherwise
56750:                 v_ins = w.cmovp(w.eqp0(x), newobj_ins, x);
56750:             } else {
56750:                 v_ins = w.ldd(nativeRvalAddr);
28086:             }
26552:             set(&v, v_ins);
26552: 
30847:             propagateFailureToBuiltinStatus(ok_ins, status);
30847:         }
56750:         guard(true, w.eqi0(status), STATUS_EXIT);
24612:     }
24612: 
32669:     if (pendingSpecializedNative->flags & JSTN_UNBOX_AFTER) {
26972:         /*
26972:          * If we side exit on the unboxing code due to a type change, make sure that the boxed
26972:          * value is actually currently associated with that location, and that we are talking
26972:          * about the top of the stack here, which is where we expected boxed values.
26972:          */
42717:         JS_ASSERT(&v == &cx->regs->sp[-1] && get(&v) == v_ins);
56750:         set(&v, unbox_value(v, AllocSlotsAddress(native_rval_ins), snapshot(BRANCH_EXIT)));
48470:     } else if (pendingSpecializedNative->flags &
48470:                (JSTN_RETURN_NULLABLE_STR | JSTN_RETURN_NULLABLE_OBJ)) {
48470:         guard(v.isNull(),
56750:               w.name(w.eqp0(v_ins), "guard(nullness)"),
48470:               BRANCH_EXIT);
32669:     } else if (JSTN_ERRTYPE(pendingSpecializedNative) == FAIL_NEG) {
41265:         /* Already added i2d in functionCall. */
48470:         JS_ASSERT(v.isNumber());
24612:     } else {
20966:         /* Convert the result to double if the builtin returns int32. */
48470:         if (v.isNumber() &&
40325:             pendingSpecializedNative->builtin->returnType() == ARGTYPE_I) {
56750:             set(&v, w.i2d(v_ins));
20966:         }
20405:     }
20405: 
32669:     // We'll null pendingSpecializedNative in monitorRecording, on the next op
32669:     // cycle.  There must be a next op since the stack is non-empty.
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
48470: TraceRecorder::name(Value*& vp, LIns*& ins, NameResult& nr)
18096: {
53840:     JSObject* obj = &cx->fp()->scopeChain();
52825:     JSOp op = JSOp(*cx->regs->pc);
52825:     if (js_CodeSpec[op].format & JOF_GNAME)
52825:         obj = obj->getGlobal();
18096:     if (obj != globalObj)
31075:         return scopeChainProp(obj, vp, ins, nr);
18115: 
18115:     /* Can't use prop here, because we don't want unboxing from global slots. */
56750:     LIns* obj_ins = w.immpObjGC(globalObj);
18096:     uint32 slot;
27897: 
27897:     JSObject* obj2;
40374:     PCVal pcval;
27897: 
27897:     /*
27897:      * Property cache ensures that we are dealing with an existing property,
27897:      * and guards the shape for us.
27897:      */
33542:     CHECK_STATUS_A(test_property_cache(obj, obj_ins, obj2, pcval));
27897: 
30860:     /* Abort if property doesn't exist (interpreter will report an error.) */
40374:     if (pcval.isNull())
33542:         RETURN_STOP_A("named property not found");
18096: 
30860:     /* Insist on obj being the directly addressed object. */
27897:     if (obj2 != obj)
33542:         RETURN_STOP_A("name() hit prototype chain");
27897: 
27897:     /* Don't trace getter or setter calls, our caller wants a direct slot. */
52503:     if (pcval.isShape()) {
52503:         const Shape* shape = pcval.toShape();
52503:         if (!isValidSlot(obj, shape))
33542:             RETURN_STOP_A("name() not accessing a valid slot");
52503:         slot = shape->slot;
27897:     } else {
40374:         if (!pcval.isSlot())
33542:             RETURN_STOP_A("PCE is not a slot");
40374:         slot = pcval.toSlot();
27897:     }
27897: 
18096:     if (!lazilyImportGlobalSlot(slot))
33542:         RETURN_STOP_A("lazy import of global slot failed");
18096: 
40410:     vp = &obj->getSlotRef(slot);
30647:     ins = get(vp);
31075:     nr.tracked = true;
33542:     return ARECORD_CONTINUE;
27933: }
27933: 
32658: static JSObject* FASTCALL
52503: MethodReadBarrier(JSContext* cx, JSObject* obj, Shape* shape, JSObject* funobj)
32658: {
48470:     Value v = ObjectValue(*funobj);
48470:     AutoValueRooter tvr(cx, v);
32658: 
52503:     if (!obj->methodReadBarrier(cx, *shape, tvr.addr()))
32658:         return NULL;
48470:     return &tvr.value().toObject();
32658: }
52503: JS_DEFINE_CALLINFO_4(static, OBJECT_FAIL, MethodReadBarrier, CONTEXT, OBJECT, SHAPE, OBJECT,
48613:                      0, ACCSET_STORE_ANY)
32658: 
32557: /*
32557:  * Get a property. The current opcode has JOF_ATOM.
32557:  *
32557:  * There are two modes. The caller must pass nonnull pointers for either outp
32557:  * or both slotp and v_insp. In the latter case, we require a plain old
32557:  * property with a slot; if the property turns out to be anything else, abort
32557:  * tracing (rather than emit a call to a native getter or GetAnyProperty).
32557:  */
33542: JS_REQUIRES_STACK AbortableRecordingStatus
48470: TraceRecorder::prop(JSObject* obj, LIns* obj_ins, uint32 *slotp, LIns** v_insp, Value *outp)
32557: {
40468:     /*
40468:      * Insist that obj have js_SetProperty as its set object-op. This suffices
40468:      * to prevent a rogue obj from being used on-trace (loaded via obj_ins),
40468:      * because we will guard on shape (or else global object identity) and any
40468:      * object not having the same op must have a different class, and therefore
40468:      * must differ in its shape (or not be the global object).
40468:      */
48622:     if (!obj->isDenseArray() && obj->getOps()->getProperty)
52503:         RETURN_STOP_A("non-dense-array, non-native js::ObjectOps::getProperty");
40468: 
32557:     JS_ASSERT((slotp && v_insp && !outp) || (!slotp && !v_insp && outp));
32557: 
17758:     /*
18115:      * Property cache ensures that we are dealing with an existing property,
18115:      * and guards the shape for us.
18115:      */
18115:     JSObject* obj2;
40374:     PCVal pcval;
33542:     CHECK_STATUS_A(test_property_cache(obj, obj_ins, obj2, pcval));
17665: 
42587:     /* Check for nonexistent property reference, which results in undefined. */
40374:     if (pcval.isNull()) {
32557:         if (slotp)
33542:             RETURN_STOP_A("property not found");
32557: 
25633:         /*
29872:          * We could specialize to guard on just JSClass.getProperty, but a mere
29872:          * class guard is simpler and slightly faster.
29872:          */
48470:         if (obj->getClass()->getProperty != Valueify(JS_PropertyStub)) {
33542:             RETURN_STOP_A("can't trace through access to undefined property if "
29872:                           "JSClass.getProperty hook isn't stubbed");
29872:         }
48613:         guardClass(obj_ins, obj->getClass(), snapshot(MISMATCH_EXIT), LOAD_NORMAL);
29872: 
29872:         /*
25633:          * This trace will be valid as long as neither the object nor any object
29872:          * on its prototype chain changes shape.
29874:          *
29874:          * FIXME: This loop can become a single shape guard once bug 497789 has
29874:          * been fixed.
25633:          */
27540:         VMSideExit* exit = snapshot(BRANCH_EXIT);
29513:         do {
40430:             if (obj->isNative()) {
41777:                 CHECK_STATUS_A(guardShape(obj_ins, obj, obj->shape(), "guard(shape)", exit));
42749:             } else if (obj->isDenseArray()) {
42749:                 guardDenseArray(obj_ins, exit);
42749:             } else {
33542:                 RETURN_STOP_A("non-native object involved in undefined property access");
32777:             }
29513:         } while (guardHasPrototype(obj, obj_ins, &obj, &obj_ins, exit));
25633: 
56750:         set(outp, w.immiUndefined());
33542:         return ARECORD_CONTINUE;
17998:     }
17998: 
41290:     return InjectStatus(propTail(obj, obj_ins, obj2, pcval, slotp, v_insp, outp));
41290: }
41290: 
41290: JS_REQUIRES_STACK RecordingStatus
40374: TraceRecorder::propTail(JSObject* obj, LIns* obj_ins, JSObject* obj2, PCVal pcval,
48470:                         uint32 *slotp, LIns** v_insp, Value *outp)
37685: {
42717:     const JSCodeSpec& cs = js_CodeSpec[*cx->regs->pc];
30847:     uint32 setflags = (cs.format & (JOF_INCDEC | JOF_FOR));
30847:     JS_ASSERT(!(cs.format & JOF_SET));
18143: 
52503:     const Shape* shape;
32557:     uint32 slot;
32658:     bool isMethod;
32658: 
52503:     if (pcval.isShape()) {
52503:         shape = pcval.toShape();
52503:         JS_ASSERT(obj2->nativeContains(*shape));
52503: 
52503:         if (setflags && !shape->hasDefaultSetter())
41290:             RETURN_STOP("non-stub setter");
52503:         if (setflags && !shape->writable())
41290:             RETURN_STOP("writing to a readonly property");
52503:         if (!shape->hasDefaultGetterOrIsMethod()) {
32557:             if (slotp)
41290:                 RETURN_STOP("can't trace non-stub getter for this opcode");
52503:             if (shape->hasGetterValue())
52503:                 return getPropertyWithScriptGetter(obj, obj_ins, shape);
52503:             if (shape->slot == SHAPE_INVALID_SLOT)
52503:                 return getPropertyWithNativeGetter(obj_ins, shape, outp);
41290:             return getPropertyById(obj_ins, outp);
18115:         }
52503:         if (!obj2->containsSlot(shape->slot))
41290:             RETURN_STOP("no valid slot");
52503:         slot = shape->slot;
52503:         isMethod = shape->isMethod();
52503:         JS_ASSERT_IF(isMethod, obj2->hasMethodBarrier());
18115:     } else {
40374:         if (!pcval.isSlot())
41290:             RETURN_STOP("PCE is not a slot");
40374:         slot = pcval.toSlot();
52503:         shape = NULL;
32658:         isMethod = false;
32658:     }
32658: 
32658:     /* We have a slot. Check whether it is direct or in a prototype. */
25092:     if (obj2 != obj) {
25092:         if (setflags)
41290:             RETURN_STOP("JOF_INCDEC|JOF_FOR opcode hit prototype chain");
25092: 
25092:         /*
37685:          * We're getting a prototype property. Two cases:
37685:          *
37685:          * 1. If obj2 is obj's immediate prototype we must walk up from obj,
37685:          * since direct and immediate-prototype cache hits key on obj's shape,
37685:          * not its identity.
37685:          *
37685:          * 2. Otherwise obj2 is higher up the prototype chain and we've keyed
37685:          * on obj's identity, and since setting __proto__ reshapes all objects
37685:          * along the old prototype chain, then provided we shape-guard obj2,
37685:          * we can "teleport" directly to obj2 by embedding it as a constant
37685:          * (this constant object instruction will be CSE'ed with the constant
37685:          * emitted by test_property_cache, whose shape is guarded).
37685:          */
56750:         obj_ins = (obj2 == obj->getProto()) ? w.ldpObjProto(obj_ins) : w.immpObjGC(obj2);
37685:         obj = obj2;
25092:     }
25092: 
48588:     LIns* v_ins;
48588:     if (obj2 == globalObj) {
48588:         if (isMethod)
48588:             RETURN_STOP("get global method");
48588:         if (!lazilyImportGlobalSlot(slot))
48588:             RETURN_STOP("lazy import of global slot failed");
48588:         v_ins = get(&globalObj->getSlotRef(slot));
48588:     } else {
48588:         v_ins = unbox_slot(obj, obj_ins, slot, snapshot(BRANCH_EXIT));
48588:     }
26972: 
32658:     /*
32658:      * Joined function object stored as a method must be cloned when extracted
32658:      * as a property value other than a callee. Note that shapes cover method
32658:      * value as well as other property attributes and order, so this condition
32658:      * is trace-invariant.
32658:      *
32658:      * We do not impose the method read barrier if in an imacro, assuming any
32658:      * property gets it does (e.g., for 'toString' from JSOP_NEW) will not be
32658:      * leaked to the calling script.
32658:      */
53840:     if (isMethod && !cx->fp()->hasImacropc()) {
33560:         enterDeepBailCall();
56750:         LIns* args[] = { v_ins, w.immpShapeGC(shape), obj_ins, cx_ins };
56750:         v_ins = w.call(&MethodReadBarrier_ci, args);
33560:         leaveDeepBailCall();
32658:     }
32658: 
32557:     if (slotp) {
32557:         *slotp = slot;
32557:         *v_insp = v_ins;
32557:     }
32557:     if (outp)
39913:         set(outp, v_ins);
41290:     return RECORD_CONTINUE;
33542: }
33542: 
59956: /*
59956:  * When we end up with a hole, read it as undefined, and make sure to set
59956:  * addr_ins to null.
59956:  */
33542: JS_REQUIRES_STACK RecordingStatus
48470: TraceRecorder::denseArrayElement(Value& oval, Value& ival, Value*& vp, LIns*& v_ins,
53615:                                  LIns*& addr_ins, VMSideExit* branchExit)
28411: {
48470:     JS_ASSERT(oval.isObject() && ival.isInt32());
48470: 
48470:     JSObject* obj = &oval.toObject();
19983:     LIns* obj_ins = get(&oval);
48470:     jsint idx = ival.toInt32();
55556:     LIns* idx_ins;
55556:     CHECK_STATUS(makeNumberInt32(get(&ival), &idx_ins));
17758: 
48537:     /*
48537:      * Arrays have both a length and a capacity, but we only need to check
48537:      * |index < capacity|;  in the case where |length < index < capacity|
48537:      * the entries [length..capacity-1] will have already been marked as
48537:      * holes by resizeDenseArrayElements() so we can read them and get
48537:      * the correct value.
48537:      */
56750:     LIns* capacity_ins = w.ldiDenseArrayCapacity(obj_ins);
41782:     jsuint capacity = obj->getDenseArrayCapacity();
48537:     bool within = (jsuint(idx) < capacity);
27891:     if (!within) {
48537:         /* If not idx < capacity, stay on trace (and read value as undefined). */
56750:         guard(true, w.geui(idx_ins, capacity_ins), branchExit);
48537: 
53614:         CHECK_STATUS(guardPrototypeHasNoIndexedProperties(obj, obj_ins, snapshot(MISMATCH_EXIT)));
25883: 
56750:         v_ins = w.immiUndefined();
20972:         addr_ins = NULL;
33542:         return RECORD_CONTINUE;
20972:     }
20404: 
48537:     /* Guard that index is within capacity. */
57719:     guard(true, w.name(w.ltui(idx_ins, capacity_ins), "inRange"), branchExit);
27891: 
27891:     /* Load the value and guard on its type to unbox it. */
55746:     vp = &obj->slots[jsuint(idx)];
48470: 	JS_ASSERT(sizeof(Value) == 8); // The |3| in the following statement requires this.
57719:     addr_ins = w.name(w.getDslotAddress(obj_ins, idx_ins), "elemp");
56750:     v_ins = unbox_value(*vp, DSlotsAddress(addr_ins), branchExit);
41113: 
41113:     /* Don't let the hole value escape. Turn it into an undefined. */
48470:     if (vp->isMagic()) {
53614:         CHECK_STATUS(guardPrototypeHasNoIndexedProperties(obj, obj_ins, snapshot(MISMATCH_EXIT)));
56750:         v_ins = w.immiUndefined();
59956:         addr_ins = NULL;
19053:     }
33542:     return RECORD_CONTINUE;
33542: }
33542: 
48822: /* See comments in TypedArrayTemplate<double>::copyIndexToValue. */
48822: LIns *
48822: TraceRecorder::canonicalizeNaNs(LIns *dval_ins)
48822: {
48822:     /* NaNs are the only floating point values that do not == themselves. */
56750:     LIns *isnonnan_ins = w.eqd(dval_ins, dval_ins);
56750:     return w.cmovd(isnonnan_ins, dval_ins, w.immd(js_NaN));
56750: }
56750: 
56750: JS_REQUIRES_STACK AbortableRecordingStatus
56750: TraceRecorder::typedArrayElement(Value& oval, Value& ival, Value*& vp, LIns*& v_ins)
37754: {
48470:     JS_ASSERT(oval.isObject() && ival.isInt32());
48470: 
48470:     JSObject* obj = &oval.toObject();
37754:     LIns* obj_ins = get(&oval);
48470:     jsint idx = ival.toInt32();
55556:     LIns* idx_ins;
55556:     CHECK_STATUS_A(makeNumberInt32(get(&ival), &idx_ins));
56750:     LIns* pidx_ins = w.ui2p(idx_ins);
37754: 
37754:     js::TypedArray* tarray = js::TypedArray::fromJSObject(obj);
37754:     JS_ASSERT(tarray);
37754: 
37754:     /* priv_ins will load the TypedArray* */
56750:     LIns* priv_ins = w.ldpObjPrivate(obj_ins);
37754: 
40294:     /* for out-of-range, do the same thing that the interpreter does, which is return undefined */
40294:     if ((jsuint) idx >= tarray->length) {
60545:         CHECK_STATUS_A(guard(false,
56750:                              w.ltui(idx_ins, w.ldiConstTypedArrayLength(priv_ins)),
60545:                              BRANCH_EXIT,
60545:                              /* abortIfAlwaysExits = */true));
56750:         v_ins = w.immiUndefined();
40294:         return ARECORD_CONTINUE;
40294:     }
37754: 
37754:     /*
37754:      * Ensure idx < length
37754:      *
48470:      * NOTE! mLength is uint32, but it's guaranteed to fit in a Value
37754:      * int, so we can treat it as either signed or unsigned.
37754:      * If the index happens to be negative, when it's treated as
37754:      * unsigned it'll be a very large int, and thus won't be less than
37754:      * length.
37754:      */
37754:     guard(true,
59251:           w.name(w.ltui(idx_ins, w.ldiConstTypedArrayLength(priv_ins)), "inRange"),
40294:           BRANCH_EXIT);
37754: 
37754:     /* We are now ready to load.  Do a different type of load
37754:      * depending on what type of thing we're loading. */
56750:     LIns* data_ins = w.ldpConstTypedArrayData(priv_ins);
37754: 
37754:     switch (tarray->type) {
37754:       case js::TypedArray::TYPE_INT8:
56750:         v_ins = w.i2d(w.ldc2iTypedArrayElement(data_ins, pidx_ins));
37754:         break;
37754:       case js::TypedArray::TYPE_UINT8:
38515:       case js::TypedArray::TYPE_UINT8_CLAMPED:
56750:         v_ins = w.ui2d(w.lduc2uiTypedArrayElement(data_ins, pidx_ins));
37754:         break;
37754:       case js::TypedArray::TYPE_INT16:
56750:         v_ins = w.i2d(w.lds2iTypedArrayElement(data_ins, pidx_ins));
37754:         break;
37754:       case js::TypedArray::TYPE_UINT16:
56750:         v_ins = w.ui2d(w.ldus2uiTypedArrayElement(data_ins, pidx_ins));
37754:         break;
37754:       case js::TypedArray::TYPE_INT32:
56750:         v_ins = w.i2d(w.ldiTypedArrayElement(data_ins, pidx_ins));
37754:         break;
37754:       case js::TypedArray::TYPE_UINT32:
56750:         v_ins = w.ui2d(w.ldiTypedArrayElement(data_ins, pidx_ins));
37754:         break;
37754:       case js::TypedArray::TYPE_FLOAT32:
56750:         v_ins = canonicalizeNaNs(w.ldf2dTypedArrayElement(data_ins, pidx_ins));
37754:         break;
38512:       case js::TypedArray::TYPE_FLOAT64:
56750:         v_ins = canonicalizeNaNs(w.lddTypedArrayElement(data_ins, pidx_ins));
38512:         break;
37754:       default:
37754:         JS_NOT_REACHED("Unknown typed array type in tracer");
37754:     }
37754: 
37754:     return ARECORD_CONTINUE;
37754: }
37754: 
37754: JS_REQUIRES_STACK AbortableRecordingStatus
17758: TraceRecorder::getProp(JSObject* obj, LIns* obj_ins)
17758: {
42717:     JSOp op = JSOp(*cx->regs->pc);
37685:     const JSCodeSpec& cs = js_CodeSpec[op];
37685: 
17758:     JS_ASSERT(cs.ndefs == 1);
32557:     return prop(obj, obj_ins, NULL, NULL, &stackval(-cs.nuses));
27933: }
27933: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
48470: TraceRecorder::getProp(Value& v)
48470: {
48470:     if (v.isPrimitive())
33542:         RETURN_STOP_A("primitive lhs");
17665: 
48470:     return getProp(&v.toObject(), get(&v));
17665: }
17665: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_NAME()
17409: {
48470:     Value* vp;
30647:     LIns* v_ins;
31075:     NameResult nr;
33542:     CHECK_STATUS_A(name(vp, v_ins, nr));
30647:     stack(0, v_ins);
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_DOUBLE()
17409: {
48470:     double d = consts[GET_INDEX(cx->regs->pc)].toDouble();
56750:     stack(0, w.immd(d));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_STRING()
17409: {
42717:     JSAtom* atom = atoms[GET_INDEX(cx->regs->pc)];
56750:     stack(0, w.immpAtomGC(atom));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_ZERO()
17409: {
56750:     stack(0, w.immd(0));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_ONE()
17409: {
56750:     stack(0, w.immd(1));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_NULL()
17409: {
56750:     stack(0, w.immpNull());
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_THIS()
17409: {
17688:     LIns* this_ins;
33542:     CHECK_STATUS_A(getThis(this_ins));
17688:     stack(0, this_ins);
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_FALSE()
17409: {
56750:     stack(0, w.immi(0));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_TRUE()
17409: {
56750:     stack(0, w.immi(1));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_OR()
17409: {
20416:     return ifop();
17409: }
17926: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_AND()
17409: {
20416:     return ifop();
17409: }
17926: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_TABLESWITCH()
17409: {
25099: #ifdef NANOJIT_IA32
26557:     /* Handle tableswitches specially -- prepare a jump table if needed. */
31060:     return tableswitch();
25099: #else
33542:     return InjectStatus(switchop());
33542: #endif
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_LOOKUPSWITCH()
17409: {
33542:     return InjectStatus(switchop());
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_STRICTEQ()
17409: {
59890:     CHECK_STATUS_A(strictEquality(true, false));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_STRICTNE()
17409: {
59890:     CHECK_STATUS_A(strictEquality(false, false));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_OBJECT()
17409: {
51446:     JSStackFrame* const fp = cx->fp();
53840:     JSScript* script = fp->script();
42717:     unsigned index = atoms - script->atomMap.vector + GET_INDEX(cx->regs->pc);
18027: 
18027:     JSObject* obj;
32723:     obj = script->getObject(index);
56750:     stack(0, w.immpObjGC(obj));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_POP()
17409: {
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_TRAP()
17899: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_GETARG()
17409: {
42717:     stack(0, arg(GET_ARGNO(cx->regs->pc)));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_SETARG()
17409: {
42717:     arg(GET_ARGNO(cx->regs->pc), stack(-1));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18005: TraceRecorder::record_JSOP_GETLOCAL()
17409: {
42717:     stack(0, var(GET_SLOTNO(cx->regs->pc)));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18005: TraceRecorder::record_JSOP_SETLOCAL()
17409: {
42717:     var(GET_SLOTNO(cx->regs->pc), stack(-1));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_UINT16()
17409: {
56750:     stack(0, w.immd(GET_UINT16(cx->regs->pc)));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_NEWINIT()
17409: {
58056:     initDepth++;
58056:     hadNewInit = true;
58056: 
58056:     JSProtoKey key = JSProtoKey(cx->regs->pc[1]);
55746: 
27012:     LIns* proto_ins;
33542:     CHECK_STATUS_A(getClassPrototype(key, proto_ins));
32615: 
48604:     LIns *v_ins;
48604:     if (key == JSProto_Array) {
59234:         LIns *args[] = { proto_ins, cx_ins };
59234:         v_ins = w.call(&NewDenseEmptyArray_ci, args);
58056:     } else {
58056:         LIns *args[] = { w.immpNull(), proto_ins, cx_ins };
56750:         v_ins = w.call(&js_InitializerObject_ci, args);
56750:     }
56750:     guard(false, w.eqp0(v_ins), OOM_EXIT);
32615:     stack(0, v_ins);
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
58056: TraceRecorder::record_JSOP_NEWARRAY()
58056: {
58056:     initDepth++;
58056: 
58056:     LIns* proto_ins;
58056:     CHECK_STATUS_A(getClassPrototype(JSProto_Array, proto_ins));
58056: 
58056:     unsigned count = GET_UINT24(cx->regs->pc);
59234:     LIns *args[] = { proto_ins, w.immi(count), cx_ins };
59234:     LIns *v_ins = w.call(&NewDenseAllocatedArray_ci, args);
58056: 
58056:     guard(false, w.eqp0(v_ins), OOM_EXIT);
58056:     stack(0, v_ins);
58056:     return ARECORD_CONTINUE;
58056: }
58056: 
58056: JS_REQUIRES_STACK AbortableRecordingStatus
58056: TraceRecorder::record_JSOP_NEWOBJECT()
58056: {
58056:     initDepth++;
58056: 
58056:     LIns* proto_ins;
58056:     CHECK_STATUS_A(getClassPrototype(JSProto_Object, proto_ins));
58056: 
58056:     JSObject* baseobj = cx->fp()->script()->getObject(getFullIndex(0));
58056: 
58056:     LIns *args[] = { w.immpObjGC(baseobj), proto_ins, cx_ins };
58056:     LIns *v_ins = w.call(&js_InitializerObject_ci, args);
58056: 
58056:     guard(false, w.eqp0(v_ins), OOM_EXIT);
58056:     stack(0, v_ins);
58056:     return ARECORD_CONTINUE;
58056: }
58056: 
58056: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_ENDINIT()
17409: {
58056:     initDepth--;
58056:     if (initDepth == 0)
58056:         hadNewInit = false;
58056: 
24198: #ifdef DEBUG
48470:     Value& v = stackval(-1);
48470:     JS_ASSERT(!v.isPrimitive());
24198: #endif
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_INITELEM()
17409: {
58056:     Value& v = stackval(-1);
58056:     Value& idx = stackval(-2);
58056:     Value& lval = stackval(-3);
58056: 
58056:     // The object is either a dense Array or an Object.  Only handle the dense case here.
58056:     // Also skip array initializers which might be unoptimized NEWINIT initializers.
58056:     if (!lval.toObject().isDenseArray() || hadNewInit)
35466:         return setElem(-3, -2, -1);
58056: 
58056:     // The index is always the same constant integer.
58056:     JS_ASSERT(idx.isInt32());
58056: 
58056:     // Nothing to do for holes, the array's length has already been set.
58056:     if (v.isMagic(JS_ARRAY_HOLE))
58056:         return ARECORD_CONTINUE;
58056: 
58056:     LIns* obj_ins = get(&lval);
58056:     LIns* v_ins = get(&v);
58056: 
58056:     // Set the element.
58056:     LIns *slots_ins = w.ldpObjSlots(obj_ins);
58056:     box_value_into(v, v_ins, DSlotsAddress(slots_ins, idx.toInt32()));
58056: 
58056:     return ARECORD_CONTINUE;
17409: }
17899: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_DEFSHARP()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_USESHARP()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_INCARG()
17409: {
42717:     return InjectStatus(inc(argval(GET_ARGNO(cx->regs->pc)), 1));
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18005: TraceRecorder::record_JSOP_INCLOCAL()
17409: {
42717:     return InjectStatus(inc(varval(GET_SLOTNO(cx->regs->pc)), 1));
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_DECARG()
17409: {
42717:     return InjectStatus(inc(argval(GET_ARGNO(cx->regs->pc)), -1));
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18005: TraceRecorder::record_JSOP_DECLOCAL()
17409: {
42717:     return InjectStatus(inc(varval(GET_SLOTNO(cx->regs->pc)), -1));
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_ARGINC()
17409: {
42717:     return InjectStatus(inc(argval(GET_ARGNO(cx->regs->pc)), 1, false));
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18005: TraceRecorder::record_JSOP_LOCALINC()
17409: {
42717:     return InjectStatus(inc(varval(GET_SLOTNO(cx->regs->pc)), 1, false));
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_ARGDEC()
17409: {
42717:     return InjectStatus(inc(argval(GET_ARGNO(cx->regs->pc)), -1, false));
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18005: TraceRecorder::record_JSOP_LOCALDEC()
17409: {
42717:     return InjectStatus(inc(varval(GET_SLOTNO(cx->regs->pc)), -1, false));
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
21685: TraceRecorder::record_JSOP_IMACOP()
21685: {
53840:     JS_ASSERT(cx->fp()->hasImacropc());
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
42641: static JSBool FASTCALL
60155: ObjectToIterator(JSContext* cx, JSObject *obj, int32 flags, Value* vp)
60155: {
60155:     vp->setObject(*obj);
60155:     bool ok = js_ValueToIterator(cx, flags, vp);
42641:     if (!ok) {
42641:         SetBuiltinError(cx);
42641:         return false;
42641:     }
60574:     return WasBuiltinSuccessful(cx);
42641: }
60155: JS_DEFINE_CALLINFO_4(static, BOOL_FAIL, ObjectToIterator, CONTEXT, OBJECT, INT32, VALUEPTR,
48613:                      0, ACCSET_STORE_ANY)
42641: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_ITER()
17409: {
48470:     Value& v = stackval(-1);
48470:     if (v.isPrimitive())
33542:         RETURN_STOP_A("for-in on a primitive value");
42641: 
33542:     RETURN_IF_XML_A(v);
24384: 
42641:     LIns *obj_ins = get(&v);
42717:     jsuint flags = cx->regs->pc[1];
41987: 
42641:     enterDeepBailCall();
42641: 
60155:     LIns* vp_ins = w.allocp(sizeof(Value));
60155:     LIns* args[] = { vp_ins, w.immi(flags), obj_ins, cx_ins };
56750:     LIns* ok_ins = w.call(&ObjectToIterator_ci, args);
42641: 
42641:     // We need to guard on ok_ins, but this requires a snapshot of the state
42641:     // after this op. monitorRecording will do it for us.
42641:     pendingGuardCondition = ok_ins;
42641: 
60155:     // ObjectToIterator can deep-bail without throwing, leaving a value of
60155:     // unknown type in *vp (it can be either a function or a non-function
60155:     // object). Use the same mechanism as finishGetProp to arrange for
60155:     // LeaveTree to deal with this value.
60155:     pendingUnboxSlot = cx->regs->sp - 1;
60155:     set(pendingUnboxSlot, w.name(w.lddAlloc(vp_ins), "iterval"));
60155: 
42641:     leaveDeepBailCall();
42641: 
42641:     return ARECORD_CONTINUE;
42641: }
42641: 
42641: static JSBool FASTCALL
48470: IteratorMore(JSContext *cx, JSObject *iterobj, Value *vp)
42641: {
53545:     if (!js_IteratorMore(cx, iterobj, vp)) {
59879:         SetBuiltinError(cx);
42641:         return false;
59879:     }
60574:     return WasBuiltinSuccessful(cx);
42641: }
48613: JS_DEFINE_CALLINFO_3(extern, BOOL_FAIL, IteratorMore, CONTEXT, OBJECT, VALUEPTR,
48613:                      0, ACCSET_STORE_ANY)
42641: 
42641: JS_REQUIRES_STACK AbortableRecordingStatus
42641: TraceRecorder::record_JSOP_MOREITER()
42641: {
48470:     Value& iterobj_val = stackval(-1);
48470:     if (iterobj_val.isPrimitive())
33542:         RETURN_STOP_A("for-in on a primitive value");
42641: 
33542:     RETURN_IF_XML_A(iterobj_val);
42641: 
48470:     JSObject* iterobj = &iterobj_val.toObject();
24310:     LIns* iterobj_ins = get(&iterobj_val);
42641:     LIns* cond_ins;
42641: 
61055:     /*
61055:      * JSOP_FOR* already guards on this, but in certain rare cases we might
61055:      * record misformed loop traces. Note that it's not necessary to guard on
61055:      * ni->flags (nor do we in unboxNextValue), because the different
61055:      * iteration type will guarantee a different entry typemap.
61055:      */
48622:     if (iterobj->hasClass(&js_IteratorClass)) {
48622:         guardClass(iterobj_ins, &js_IteratorClass, snapshot(BRANCH_EXIT), LOAD_NORMAL);
48470: 
61055:         NativeIterator *ni = (NativeIterator *) iterobj->getPrivate();
61055:         if (ni->isKeyIter()) {
56750:             LIns *ni_ins = w.ldpObjPrivate(iterobj_ins);
56750:             LIns *cursor_ins = w.ldpIterCursor(ni_ins);
56750:             LIns *end_ins = w.ldpIterEnd(ni_ins);
42641: 
56750:             cond_ins = w.ltp(cursor_ins, end_ins);
61055:             stack(0, cond_ins);
61055:             return ARECORD_CONTINUE;
61055:         }
42641:     } else {
48622:         guardNotClass(iterobj_ins, &js_IteratorClass, snapshot(BRANCH_EXIT), LOAD_NORMAL);
61055:     }
42749: 
42641:     enterDeepBailCall();
42641: 
56750:     LIns* vp_ins = w.allocp(sizeof(Value));
42641:     LIns* args[] = { vp_ins, iterobj_ins, cx_ins };
59988:     pendingGuardCondition = w.call(&IteratorMore_ci, args);
59988: 
42641:     leaveDeepBailCall();
42641: 
59988:     cond_ins = is_boxed_true(AllocSlotsAddress(vp_ins));
42641:     stack(0, cond_ins);
42641: 
42641:     return ARECORD_CONTINUE;
42641: }
42641: 
42641: static JSBool FASTCALL
42641: CloseIterator(JSContext *cx, JSObject *iterobj)
42641: {
48470:     if (!js_CloseIterator(cx, iterobj)) {
42641:         SetBuiltinError(cx);
42641:         return false;
42641:     }
60574:     return WasBuiltinSuccessful(cx);
42641: }
48613: JS_DEFINE_CALLINFO_2(extern, BOOL_FAIL, CloseIterator, CONTEXT, OBJECT, 0, ACCSET_STORE_ANY)
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18085: TraceRecorder::record_JSOP_ENDITER()
18085: {
48470:     JS_ASSERT(!stackval(-1).isPrimitive());
42642: 
42641:     enterDeepBailCall();
42641: 
42641:     LIns* args[] = { stack(-1), cx_ins };
56750:     LIns* ok_ins = w.call(&CloseIterator_ci, args);
42641: 
42641:     // We need to guard on ok_ins, but this requires a snapshot of the state
42641:     // after this op. monitorRecording will do it for us.
42641:     pendingGuardCondition = ok_ins;
42641: 
42641:     leaveDeepBailCall();
42641: 
42641:     return ARECORD_CONTINUE;
42641: }
42641: 
48470: #if JS_BITS_PER_WORD == 32
48470: JS_REQUIRES_STACK void
56750: TraceRecorder::storeMagic(JSWhyMagic why, Address addr)
56750: {
56750:     w.stiValuePayload(w.immpMagicWhy(why), addr);
56750:     w.stiValueTag(w.immpMagicWhy(JSVAL_TAG_MAGIC), addr);
48470: }
48470: #elif JS_BITS_PER_WORD == 64
48470: JS_REQUIRES_STACK void
56750: TraceRecorder::storeMagic(JSWhyMagic why, Address addr)
56750: {
56750:     LIns *magic = w.nameImmq(BUILD_JSVAL(JSVAL_TAG_MAGIC, why));
56750:     w.stq(magic, addr);
48470: }
48470: #endif
48470: 
42641: JS_REQUIRES_STACK AbortableRecordingStatus
42641: TraceRecorder::unboxNextValue(LIns* &v_ins)
42641: {
48470:     Value &iterobj_val = stackval(-1);
48470:     JSObject *iterobj = &iterobj_val.toObject();
42641:     LIns* iterobj_ins = get(&iterobj_val);
42641: 
48622:     if (iterobj->hasClass(&js_IteratorClass)) {
48622:         guardClass(iterobj_ins, &js_IteratorClass, snapshot(BRANCH_EXIT), LOAD_NORMAL);
42641:         NativeIterator *ni = (NativeIterator *) iterobj->getPrivate();
48470: 
56750:         LIns *ni_ins = w.ldpObjPrivate(iterobj_ins);
56750:         LIns *cursor_ins = w.ldpIterCursor(ni_ins);
42641: 
42641:         /* Emit code to stringify the id if necessary. */
56750:         Address cursorAddr = IterPropsAddress(cursor_ins);
61055:         if (ni->isKeyIter()) {
48470:             /* Read the next id from the iterator. */
61055:             jsid id = *ni->current();
56750:             LIns *id_ins = w.name(w.ldp(cursorAddr), "id");
48470: 
42641:             /*
42641:              * Most iterations over object properties never have to actually deal with
42641:              * any numeric properties, so we guard here instead of branching.
42641:              */
56750:             guard(JSID_IS_STRING(id), is_string_id(id_ins), BRANCH_EXIT);
48470: 
48470:             if (JSID_IS_STRING(id)) {
48470:                 v_ins = unbox_string_id(id_ins);
58068:             } else if (JSID_IS_INT(id)) {
42641:                 /* id is an integer, convert to a string. */
48470:                 LIns *id_to_int_ins = unbox_int_id(id_ins);
48470:                 LIns* args[] = { id_to_int_ins, cx_ins };
56750:                 v_ins = w.call(&js_IntToString_ci, args);
56750:                 guard(false, w.eqp0(v_ins), OOM_EXIT);
58068:             } else {
58068: #if JS_HAS_XML_SUPPORT
58068:                 JS_ASSERT(JSID_IS_OBJECT(id));
58068:                 JS_ASSERT(JSID_TO_OBJECT(id)->isXMLId());
58068:                 RETURN_STOP_A("iterated over a property with an XML id");
58068: #else
58068:                 JS_NEVER_REACHED("unboxNextValue");
58068: #endif
48470:             }
48470: 
48470:             /* Increment the cursor by one jsid and store it back. */
56750:             cursor_ins = w.addp(cursor_ins, w.nameImmw(sizeof(jsid)));
56750:             w.stpIterCursor(cursor_ins, ni_ins);
61055:             return ARECORD_CONTINUE;
61055:         }
48622:     } else {
48622:         guardNotClass(iterobj_ins, &js_IteratorClass, snapshot(BRANCH_EXIT), LOAD_NORMAL);
61055:     }
61055: 
42749: 
56750:     Address iterValueAddr = CxAddress(iterValue);
56750:     v_ins = unbox_value(cx->iterValue, iterValueAddr, snapshot(BRANCH_EXIT));
56750:     storeMagic(JS_NO_ITER_VALUE, iterValueAddr);
42641: 
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18085: TraceRecorder::record_JSOP_FORNAME()
18085: {
48470:     Value* vp;
30647:     LIns* x_ins;
31075:     NameResult nr;
33542:     CHECK_STATUS_A(name(vp, x_ins, nr));
31075:     if (!nr.tracked)
33542:         RETURN_STOP_A("forname on non-tracked value not supported");
42641:     LIns* v_ins;
42641:     CHECK_STATUS_A(unboxNextValue(v_ins));
42641:     set(vp, v_ins);
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
60526: TraceRecorder::record_JSOP_FORGNAME()
60526: {
60526:     return record_JSOP_FORNAME();
60526: }
60526: 
60526: JS_REQUIRES_STACK AbortableRecordingStatus
18136: TraceRecorder::record_JSOP_FORPROP()
18136: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18136: TraceRecorder::record_JSOP_FORELEM()
18136: {
42641:     LIns* v_ins;
42641:     CHECK_STATUS_A(unboxNextValue(v_ins));
42641:     stack(0, v_ins);
42641:     return ARECORD_CONTINUE;
18085: }
18085: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18085: TraceRecorder::record_JSOP_FORARG()
18085: {
42641:     LIns* v_ins;
42641:     CHECK_STATUS_A(unboxNextValue(v_ins));
42717:     arg(GET_ARGNO(cx->regs->pc), v_ins);
42641:     return ARECORD_CONTINUE;
18085: }
18085: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18085: TraceRecorder::record_JSOP_FORLOCAL()
18085: {
42641:     LIns* v_ins;
42641:     CHECK_STATUS_A(unboxNextValue(v_ins));
42717:     var(GET_SLOTNO(cx->regs->pc), v_ins);
42641:     return ARECORD_CONTINUE;
17899: }
17899: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_POPN()
17409: {
33542:     return ARECORD_CONTINUE;
27933: }
27933: 
61226: static inline bool
61226: IsFindableCallObj(JSObject *obj)
61226: {
61226:     return obj->isCall() &&
61226:            (obj->callIsForEval() || obj->getCallObjCalleeFunction()->isHeavyweight());
61226: }
61226: 
32589: /*
36362:  * Generate LIR to reach |obj2| from |obj| by traversing the scope chain. The
36362:  * generated code also ensures that any call objects found have not changed shape.
32589:  *
32589:  *      obj               starting object
32589:  *      obj_ins           LIR instruction representing obj
36362:  *      targetObj         end object for traversal
36362:  *      targetIns [out]   LIR instruction representing obj2
32589:  */
33542: JS_REQUIRES_STACK RecordingStatus
36362: TraceRecorder::traverseScopeChain(JSObject *obj, LIns *obj_ins, JSObject *targetObj,
36362:                                   LIns *&targetIns)
32589: {
32761:     VMSideExit* exit = NULL;
36362: 
36362:     /*
36362:      * Scope chains are often left "incomplete", and reified lazily when
36362:      * necessary, since doing so is expensive. When creating null and flat
36362:      * closures on trace (the only kinds supported), the global object is
36362:      * hardcoded as the parent, since reifying the scope chain on trace
36362:      * would be extremely difficult. This is because block objects need frame
36362:      * pointers, which do not exist on trace, and thus would require magic
36362:      * similar to arguments objects or reification of stack frames. Luckily,
36362:      * for null and flat closures, these blocks are unnecessary.
36362:      *
36362:      * The problem, as exposed by bug 523793, is that this means creating a
36362:      * fixed traversal on trace can be inconsistent with the shorter scope
36362:      * chain used when executing a trace. To address this, perform an initial
36362:      * sweep of the scope chain to make sure that if there is a heavyweight
36362:      * function with a call object, and there is also a block object, the
36362:      * trace is safely aborted.
36362:      *
36362:      * If there is no call object, we must have arrived at the global object,
36362:      * and can bypass the scope chain traversal completely.
36362:      */
36362:     bool foundCallObj = false;
36362:     bool foundBlockObj = false;
36362:     JSObject* searchObj = obj;
36362: 
36362:     for (;;) {
36362:         if (searchObj != globalObj) {
61226:             if (searchObj->isBlock())
36362:                 foundBlockObj = true;
61226:             else if (IsFindableCallObj(searchObj))
36362:                 foundCallObj = true;
36362:         }
36362: 
36362:         if (searchObj == targetObj)
36362:             break;
36362: 
39930:         searchObj = searchObj->getParent();
36362:         if (!searchObj)
36362:             RETURN_STOP("cannot traverse this scope chain on trace");
36362:     }
36362: 
36362:     if (!foundCallObj) {
36362:         JS_ASSERT(targetObj == globalObj);
56750:         targetIns = w.nameImmpNonGC(globalObj);
36362:         return RECORD_CONTINUE;
36362:     }
36362: 
36362:     if (foundBlockObj)
36362:         RETURN_STOP("cannot traverse this scope chain on trace");
36362: 
36362:     /* There was a call object, or should be a call object now. */
32589:     for (;;) {
32589:         if (obj != globalObj) {
32589:             if (!js_IsCacheableNonGlobalScope(obj))
33542:                 RETURN_STOP("scope chain lookup crosses non-cacheable object");
32589: 
32589:             // We must guard on the shape of all call objects for heavyweight functions
32589:             // that we traverse on the scope chain: if the shape changes, a variable with
32589:             // the same name may have been inserted in the scope chain.
61226:             if (IsFindableCallObj(obj)) {
32761:                 if (!exit)
32761:                     exit = snapshot(BRANCH_EXIT);
32589:                 guard(true,
56750:                       w.name(w.eqiN(w.ldiObjShape(obj_ins), obj->shape()), "guard_shape"),
32761:                       exit);
32589:             }
32589:         }
32589: 
53650:         JS_ASSERT(!obj->isBlock());
36362: 
36362:         if (obj == targetObj)
32589:             break;
32589: 
39930:         obj = obj->getParent();
56750:         obj_ins = w.ldpObjParent(obj_ins);
32589:     }
32589: 
36362:     targetIns = obj_ins;
33542:     return RECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_BINDNAME()
17409: {
51446:     JSStackFrame* const fp = cx->fp();
27881:     JSObject *obj;
27638: 
53840:     if (!fp->isFunctionFrame()) {
53840:         obj = &fp->scopeChain();
27638: 
47487: #ifdef DEBUG
47487:         JSStackFrame *fp2 = fp;
47487: #endif
47487: 
32589:         // In global code, fp->scopeChain can only contain blocks whose values
32589:         // are still on the stack.  We never use BINDNAME to refer to these.
53650:         while (obj->isBlock()) {
27638:             // The block's values are still on the stack.
47487: #ifdef DEBUG
51056:             // NB: fp2 can't be a generator frame, because !fp->hasFunction.
47487:             while (obj->getPrivate() != fp2) {
53840:                 JS_ASSERT(fp2->isEvalOrDebuggerFrame());
53840:                 fp2 = fp2->prev();
47487:                 if (!fp2)
47487:                     JS_NOT_REACHED("bad stack frame");
47487:             }
47487: #endif
39930:             obj = obj->getParent();
27638:             // Blocks always have parents.
27881:             JS_ASSERT(obj);
27881:         }
32589: 
41111:         // If anything other than Block, Call, DeclEnv, and the global object
41111:         // is on the scope chain, we shouldn't be recording. Of those, only
41111:         // Block and global can be present in global code.
41111:         JS_ASSERT(obj == globalObj);
31061: 
31061:         /*
30860:          * The trace is specialized to this global object. Furthermore, we know it
30860:          * is the sole 'global' object on the scope chain: we set globalObj to the
30860:          * scope chain element with no parent, and we reached it starting from the
30860:          * function closure or the current scopeChain, so there is nothing inner to
30860:          * it. Therefore this must be the right base object.
30860:          */
56750:         stack(0, w.immpObjGC(obj));
33542:         return ARECORD_CONTINUE;
27933:     }
27933: 
32589:     // We can't trace BINDNAME in functions that contain direct calls to eval,
32589:     // as they might add bindings which previously-traced references would have
32589:     // to see.
53840:     if (JSFUN_HEAVYWEIGHT_TEST(fp->fun()->flags))
33542:         RETURN_STOP_A("BINDNAME in heavyweight function.");
32589: 
32589:     // We don't have the scope chain on trace, so instead we get a start object
32589:     // that is on the scope chain and doesn't skip the target object (the one
32589:     // that contains the property).
53840:     Value *callee = &cx->fp()->calleeValue();
48470:     obj = callee->toObject().getParent();
32589:     if (obj == globalObj) {
56750:         stack(0, w.immpObjGC(obj));
56750:         return ARECORD_CONTINUE;
56750:     }
56750:     LIns *obj_ins = w.ldpObjParent(get(callee));
32589: 
32589:     // Find the target object.
42717:     JSAtom *atom = atoms[GET_INDEX(cx->regs->pc)];
32589:     jsid id = ATOM_TO_JSID(atom);
43207:     JSContext *localCx = cx;
53840:     JSObject *obj2 = js_FindIdentifierBase(cx, &fp->scopeChain(), id);
42829:     if (!obj2)
42829:         RETURN_ERROR_A("error in js_FindIdentifierBase");
43207:     if (!TRACE_RECORDER(localCx))
43207:         return ARECORD_ABORTED;
52503:     if (obj2 != globalObj && !obj2->isCall())
33542:         RETURN_STOP_A("BINDNAME on non-global, non-call object");
32589: 
32589:     // Generate LIR to get to the target object from the start object.
32589:     LIns *obj2_ins;
33542:     CHECK_STATUS_A(traverseScopeChain(obj, obj_ins, obj2, obj2_ins));
32589: 
32589:     // If |obj2| is the global object, we can refer to it directly instead of walking up
32589:     // the scope chain. There may still be guards on intervening call objects.
56750:     stack(0, obj2 == globalObj ? w.immpObjGC(obj2) : obj2_ins);
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_THROW()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_IN()
17409: {
48470:     Value& rval = stackval(-1);
48470:     Value& lval = stackval(-2);
48470: 
48470:     if (rval.isPrimitive())
33542:         RETURN_STOP_A("JSOP_IN on non-object right operand");
48470:     JSObject* obj = &rval.toObject();
21784:     LIns* obj_ins = get(&rval);
18495: 
18495:     jsid id;
21784:     LIns* x;
48470:     if (lval.isInt32()) {
48470:         if (!js_Int32ToId(cx, lval.toInt32(), &id))
48470:             RETURN_ERROR_A("OOM converting left operand of JSOP_IN to string");
58285: 
58285:         if (obj->isDenseArray()) {
58285:             // Fast path for dense arrays
58285:             VMSideExit* branchExit = snapshot(BRANCH_EXIT);
58285:             guardDenseArray(obj_ins, branchExit);
58285: 
58285:             // If our proto has indexed props, all bets are off on our
58285:             // "false" values and out-of-bounds access.  Just guard on
58285:             // that.
58285:             CHECK_STATUS_A(guardPrototypeHasNoIndexedProperties(obj, obj_ins,
58285:                                                                 snapshot(MISMATCH_EXIT)));
58285: 
58285:             LIns* idx_ins;
58285:             CHECK_STATUS_A(makeNumberInt32(get(&lval), &idx_ins));
58285:             idx_ins = w.name(idx_ins, "index");
58285:             LIns* capacity_ins = w.ldiDenseArrayCapacity(obj_ins);
58285:             LIns* inRange = w.ltui(idx_ins, capacity_ins);
58285: 
58285:             if (jsuint(lval.toInt32()) < obj->getDenseArrayCapacity()) {
58285:                 guard(true, inRange, branchExit);
58285: 
58285:                 LIns *elem_ins = w.getDslotAddress(obj_ins, idx_ins);
58285:                 // Need to make sure we don't have a hole
58285:                 LIns *is_hole_ins =
58285:                     is_boxed_magic(DSlotsAddress(elem_ins), JS_ARRAY_HOLE);
58285: 
58285:                 // Set x to true (index in our array) if is_hole_ins == 0
58285:                 x = w.eqi0(is_hole_ins);
58285:             } else {
58285:                 guard(false, inRange, branchExit);
58285:                 x = w.nameImmi(0);
58285:             }
58285:         } else {
55556:             LIns* num_ins;
55556:             CHECK_STATUS_A(makeNumberInt32(get(&lval), &num_ins));
55556:             LIns* args[] = { num_ins, obj_ins, cx_ins };
56750:             x = w.call(&js_HasNamedPropertyInt32_ci, args);
58285:         }
48470:     } else if (lval.isString()) {
21784:         if (!js_ValueToStringId(cx, lval, &id))
33542:             RETURN_ERROR_A("left operand of JSOP_IN didn't convert to a string-id");
21784:         LIns* args[] = { get(&lval), obj_ins, cx_ins };
56750:         x = w.call(&js_HasNamedProperty_ci, args);
18495:     } else {
33542:         RETURN_STOP_A("string or integer expected");
21784:     }
21784: 
56750:     guard(false, w.eqiN(x, JS_NEITHER), OOM_EXIT);
56750:     x = w.eqiN(x, 1);
18495: 
37741:     TraceMonitor &localtm = *traceMonitor;
33171: 
18495:     JSObject* obj2;
18495:     JSProperty* prop;
39928:     JSBool ok = obj->lookupProperty(cx, id, &obj2, &prop);
33171: 
43222:     if (!ok)
43222:         RETURN_ERROR_A("obj->lookupProperty failed in JSOP_IN");
43222: 
33171:     /* lookupProperty can reenter the interpreter and kill |this|. */
56567:     if (!localtm.recorder)
42830:         return ARECORD_ABORTED;
33171: 
21784:     bool cond = prop != NULL;
18495: 
30860:     /*
30860:      * The interpreter fuses comparisons and the following branch, so we have
30860:      * to do that here as well.
30860:      */
57805:     jsbytecode *pc = cx->regs->pc;
57805:     fuseIf(pc + 1, cond, x);
57805: 
57805:     /* If the branch was to a loop header, we may need to close it. */
57805:     if (pc[1] == JSOP_IFNE || pc[1] == JSOP_IFEQ)
57805:         CHECK_STATUS_A(checkTraceEnd(pc + 1));
18495: 
30860:     /*
30860:      * We update the stack after the guard. This is safe since the guard bails
30860:      * out at the comparison and the interpreter will therefore re-execute the
30860:      * comparison. This way the value of the condition doesn't have to be
30860:      * calculated and saved on the stack in most cases.
30860:      */
18495:     set(&lval, x);
33542:     return ARECORD_CONTINUE;
27933: }
27933: 
31830: static JSBool FASTCALL
48547: HasInstanceOnTrace(JSContext* cx, JSObject* ctor, ValueArgType arg)
48470: {
48470:     const Value &argref = ValueArgToConstRef(arg);
31524:     JSBool result = JS_FALSE;
48547:     if (!HasInstance(cx, ctor, &argref, &result))
37741:         SetBuiltinError(cx);
29363:     return result;
29363: }
48613: JS_DEFINE_CALLINFO_3(static, BOOL_FAIL, HasInstanceOnTrace, CONTEXT, OBJECT, VALUE,
48613:                      0, ACCSET_STORE_ANY)
29363: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_INSTANCEOF()
17409: {
31830:     // If the rhs isn't an object, we are headed for a TypeError.
48470:     Value& ctor = stackval(-1);
48470:     if (ctor.isPrimitive())
33542:         RETURN_STOP_A("non-object on rhs of instanceof");
31830: 
48470:     Value& val = stackval(-2);
48470:     LIns* val_ins = box_value_for_native_call(val, get(&val));
31830: 
31830:     enterDeepBailCall();
31830:     LIns* args[] = {val_ins, get(&ctor), cx_ins};
56750:     stack(-2, w.call(&HasInstanceOnTrace_ci, args));
56750:     LIns* status_ins = w.ldiStateField(builtinStatus);
56750:     pendingGuardCondition = w.eqi0(status_ins);
31830:     leaveDeepBailCall();
31830: 
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_DEBUGGER()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_GOSUB()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_RETSUB()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_EXCEPTION()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_LINENO()
17409: {
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
54855: TraceRecorder::record_JSOP_BLOCKCHAIN()
54855: {
54855:     return ARECORD_CONTINUE;
54855: }
54855: 
54855: JS_REQUIRES_STACK AbortableRecordingStatus
54855: TraceRecorder::record_JSOP_NULLBLOCKCHAIN()
54855: {
54855:     return ARECORD_CONTINUE;
54855: }
54855: 
54855: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_CONDSWITCH()
17409: {
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_CASE()
17409: {
59890:     CHECK_STATUS_A(strictEquality(true, true));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_DEFAULT()
17409: {
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_EVAL()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_ENUMELEM()
17409: {
35466:     /*
48470:      * To quote from jsinterp.cpp's JSOP_ENUMELEM case:
35466:      * Funky: the value to set is under the [obj, id] pair.
35466:      */
35466:     return setElem(-2, -1, -3);
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_GETTER()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_SETTER()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_DEFFUN()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
27012: TraceRecorder::record_JSOP_DEFFUN_FC()
27012: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_DEFCONST()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_DEFVAR()
17409: {
33542:     return ARECORD_STOP;
17409: }
17926: 
27012: jsatomid
27012: TraceRecorder::getFullIndex(ptrdiff_t pcoff)
27012: {
42717:     jsatomid index = GET_INDEX(cx->regs->pc + pcoff);
53840:     index += atoms - cx->fp()->script()->atomMap.vector;
27012:     return index;
27012: }
27012: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
27012: TraceRecorder::record_JSOP_LAMBDA()
17409: {
17763:     JSFunction* fun;
53840:     fun = cx->fp()->script()->getFunction(getFullIndex());
27012: 
33139:     /*
33139:      * Emit code to clone a null closure parented by this recorder's global
33139:      * object, in order to preserve function object evaluation rules observable
33139:      * via identity and mutation. But don't clone if our result is consumed by
33139:      * JSOP_SETMETHOD or JSOP_INITMETHOD, since we optimize away the clone for
33139:      * these combinations and clone only if the "method value" escapes.
35018:      *
48470:      * See jsinterp.cpp, the JSOP_LAMBDA null closure case. The JSOP_SETMETHOD and
35018:      * JSOP_INITMETHOD logic governing the early ARECORD_CONTINUE returns below
35018:      * must agree with the corresponding break-from-do-while(0) logic there.
33139:      */
37694:     if (FUN_NULL_CLOSURE(fun)) {
39930:         if (FUN_OBJECT(fun)->getParent() != globalObj)
37694:             RETURN_STOP_A("Null closure function object parent must be global object");
48599: 
57755:         jsbytecode *pc2 = AdvanceOverBlockchainOp(cx->regs->pc + JSOP_LAMBDA_LENGTH);
48599:         JSOp op2 = JSOp(*pc2);
48599: 
48599:         if (op2 == JSOP_INITMETHOD) {
56750:             stack(0, w.immpObjGC(FUN_OBJECT(fun)));
48599:             return ARECORD_CONTINUE;
48599:         }
33139: 
35018:         if (op2 == JSOP_SETMETHOD) {
48470:             Value lval = stackval(-1);
48470: 
48599:             if (!lval.isPrimitive() && lval.toObject().canHaveMethodBarrier()) {
56750:                 stack(0, w.immpObjGC(FUN_OBJECT(fun)));
35018:                 return ARECORD_CONTINUE;
35018:             }
48599:         } else if (fun->joinable()) {
48599:             if (op2 == JSOP_CALL) {
48599:                 /*
48599:                  * Array.prototype.sort and String.prototype.replace are
48599:                  * optimized as if they are special form. We know that they
48599:                  * won't leak the joined function object in obj, therefore
48599:                  * we don't need to clone that compiler- created function
48599:                  * object for identity/mutation reasons.
48599:                  */
48599:                 int iargc = GET_ARGC(pc2);
48599: 
48599:                 /*
48599:                  * Note that we have not yet pushed obj as the final argument,
48599:                  * so regs.sp[1 - (iargc + 2)], and not regs.sp[-(iargc + 2)],
48599:                  * is the callee for this JSOP_CALL.
48599:                  */
48678:                 const Value &cref = cx->regs->sp[1 - (iargc + 2)];
48678:                 JSObject *callee;
48678: 
48678:                 if (IsFunctionObject(cref, &callee)) {
53557:                     JSFunction *calleeFun = callee->getFunctionPrivate();
53557:                     Native native = calleeFun->maybeNative();
53557: 
53557:                     if ((iargc == 1 && native == array_sort) ||
53557:                         (iargc == 2 && native == str_replace)) {
56750:                         stack(0, w.immpObjGC(FUN_OBJECT(fun)));
33542:                         return ARECORD_CONTINUE;
33139:                     }
48678:                 }
48599:             } else if (op2 == JSOP_NULL) {
48599:                 pc2 += JSOP_NULL_LENGTH;
48599:                 op2 = JSOp(*pc2);
48599: 
48599:                 if (op2 == JSOP_CALL && GET_ARGC(pc2) == 0) {
56750:                     stack(0, w.immpObjGC(FUN_OBJECT(fun)));
48599:                     return ARECORD_CONTINUE;
48599:                 }
48599:             }
48599:         }
33139: 
27012:         LIns *proto_ins;
33542:         CHECK_STATUS_A(getClassPrototype(JSProto_Function, proto_ins));
27012: 
56750:         LIns* args[] = { w.immpObjGC(globalObj), proto_ins, w.immpFunGC(fun), cx_ins };
56750:         LIns* x = w.call(&js_NewNullClosure_ci, args);
27012:         stack(0, x);
33542:         return ARECORD_CONTINUE;
33542:     }
37694: 
56729:     if (GetBlockChainFast(cx, cx->fp(), JSOP_LAMBDA, JSOP_LAMBDA_LENGTH))
53567:         RETURN_STOP_A("Unable to trace creating lambda in let");
53567: 
38508:     LIns *proto_ins;
38508:     CHECK_STATUS_A(getClassPrototype(JSProto_Function, proto_ins));
37694:     LIns* scopeChain_ins = scopeChain();
37694:     JS_ASSERT(scopeChain_ins);
56750:     LIns* args[] = { proto_ins, scopeChain_ins, w.nameImmpNonGC(fun), cx_ins };
56750:     LIns* call_ins = w.call(&js_CloneFunctionObject_ci, args);
37694:     guard(false,
56750:           w.name(w.eqp0(call_ins), "guard(js_CloneFunctionObject)"),
37694:           OOM_EXIT);
37694:     stack(0, call_ins);
37694: 
37694:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
27012: TraceRecorder::record_JSOP_LAMBDA_FC()
27012: {
28923:     JSFunction* fun;
53840:     fun = cx->fp()->script()->getFunction(getFullIndex());
28923: 
39930:     if (FUN_OBJECT(fun)->getParent() != globalObj)
35065:         return ARECORD_STOP;
33607: 
56729:     if (GetBlockChainFast(cx, cx->fp(), JSOP_LAMBDA_FC, JSOP_LAMBDA_FC_LENGTH))
53567:         RETURN_STOP_A("Unable to trace creating lambda in let");
53567: 
56750:     LIns* args[] = { scopeChain(), w.immpFunGC(fun), cx_ins };
56750:     LIns* closure_ins = w.call(&js_AllocFlatClosure_ci, args);
28923:     guard(false,
56750:           w.name(w.eqp(closure_ins, w.immpNull()), "guard(js_AllocFlatClosure)"),
28923:           OOM_EXIT);
28923: 
59968:     JSScript *script = fun->script();
59968:     if (script->bindings.hasUpvars()) {
59968:         JSUpvarArray *uva = script->upvars();
56750:         LIns* upvars_ins = w.getObjPrivatizedSlot(closure_ins,
52503:                                                   JSObject::JSSLOT_FLAT_CLOSURE_UPVARS);
52503: 
28923:         for (uint32 i = 0, n = uva->length; i < n; i++) {
48470:             Value v;
59968:             LIns* v_ins = upvar(script, uva, i, v);
52503:             if (!v_ins)
52503:                 return ARECORD_STOP;
52503: 
56750:             box_value_into(v, v_ins, FCSlotsAddress(upvars_ins, i));
52503:         }
52503:     }
52503: 
52503:     stack(0, closure_ins);
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
27012: TraceRecorder::record_JSOP_CALLEE()
27012: {
53840:     stack(0, get(&cx->fp()->calleeValue()));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18020: TraceRecorder::record_JSOP_SETLOCALPOP()
18020: {
42717:     var(GET_SLOTNO(cx->regs->pc), stack(-1));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
23075: TraceRecorder::record_JSOP_IFPRIMTOP()
23075: {
23075:     // Traces are type-specialized, including null vs. object, so we need do
48470:     // nothing here. The upstream unbox_value called after valueOf or toString
23075:     // from an imacro (e.g.) will fork the trace for us, allowing us to just
23075:     // follow along mindlessly :-).
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18020: TraceRecorder::record_JSOP_SETCALL()
18020: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18020: TraceRecorder::record_JSOP_TRY()
18020: {
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18020: TraceRecorder::record_JSOP_FINALLY()
18020: {
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18020: TraceRecorder::record_JSOP_NOP()
18020: {
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18020: TraceRecorder::record_JSOP_ARGSUB()
18020: {
51446:     JSStackFrame* const fp = cx->fp();
60562: 
60562:     /*
60562:      * The arguments object or its absence in the frame is part of the typemap,
60562:      * so a record-time check suffices here. We don't bother tracing ARGSUB in
60562:      * the case of an arguments object exising, because ARGSUB and to a lesser
60562:      * extent ARGCNT are emitted to avoid arguments object creation.
60562:      */
60562:     if (!fp->hasArgsObj() && !fp->fun()->isHeavyweight()) {
42717:         uintN slot = GET_ARGNO(cx->regs->pc);
51120:         if (slot >= fp->numActualArgs())
41134:             RETURN_STOP_A("can't trace out-of-range arguments");
60562: 
53840:         stack(0, get(&cx->fp()->canonicalActualArg(slot)));
33542:         return ARECORD_CONTINUE;
33542:     }
33542:     RETURN_STOP_A("can't trace JSOP_ARGSUB hard case");
33542: }
33542: 
37214: JS_REQUIRES_STACK LIns*
37214: TraceRecorder::guardArgsLengthNotAssigned(LIns* argsobj_ins)
37214: {
51092:     // The following implements JSObject::isArgsLengthOverridden on trace.
51092:     // ARGS_LENGTH_OVERRIDDEN_BIT is set if length was overridden.
56750:     LIns *len_ins = w.getArgsLength(argsobj_ins);
56750:     LIns *ovr_ins = w.andi(len_ins, w.nameImmi(JSObject::ARGS_LENGTH_OVERRIDDEN_BIT));
56750:     guard(true, w.eqi0(ovr_ins), MISMATCH_EXIT);
37214:     return len_ins;
37214: }
37214: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18020: TraceRecorder::record_JSOP_ARGCNT()
18020: {
51446:     JSStackFrame * const fp = cx->fp();
51446: 
53840:     if (fp->fun()->flags & JSFUN_HEAVYWEIGHT)
33542:         RETURN_STOP_A("can't trace heavyweight JSOP_ARGCNT");
33111: 
33111:     // argc is fixed on trace, so ideally we would simply generate LIR for
33111:     // constant argc. But the user can mutate arguments.length in the
33111:     // interpreter, so we have to check for that in the trace entry frame.
33113:     // We also have to check that arguments.length has not been mutated
33113:     // at record time, because if so we will generate incorrect constant
33113:     // LIR, which will assert in alu().
53840:     if (fp->hasArgsObj() && fp->argsObj().isArgsLengthOverridden())
33542:         RETURN_STOP_A("can't trace JSOP_ARGCNT if arguments.length has been modified");
53840:     LIns *a_ins = getFrameObjPtr(fp->addressOfArgs());
33111:     if (callDepth == 0) {
56750:         if (MaybeBranch mbr = w.jt(w.eqp0(a_ins))) {
37214:             guardArgsLengthNotAssigned(a_ins);
56750:             w.label(mbr);
56750:         }
56750:     }
56750:     stack(0, w.immd(fp->numActualArgs()));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
19970: TraceRecorder::record_DefLocalFunSetSlot(uint32 slot, JSObject* obj)
19970: {
27012:     JSFunction* fun = GET_FUNCTION_PRIVATE(cx, obj);
27012: 
39930:     if (FUN_NULL_CLOSURE(fun) && FUN_OBJECT(fun)->getParent() == globalObj) {
27012:         LIns *proto_ins;
33542:         CHECK_STATUS_A(getClassPrototype(JSProto_Function, proto_ins));
27012: 
56750:         LIns* args[] = { w.immpObjGC(globalObj), proto_ins, w.immpFunGC(fun), cx_ins };
56750:         LIns* x = w.call(&js_NewNullClosure_ci, args);
27012:         var(slot, x);
33542:         return ARECORD_CONTINUE;
33542:     }
33542: 
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18020: TraceRecorder::record_JSOP_DEFLOCALFUN()
18020: {
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
27012: TraceRecorder::record_JSOP_DEFLOCALFUN_FC()
27012: {
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_GOTOX()
17409: {
26557:     return record_JSOP_GOTO();
17409: }
17926: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_IFEQX()
17409: {
17611:     return record_JSOP_IFEQ();
17611: }
17926: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_IFNEX()
17409: {
17611:     return record_JSOP_IFNE();
17611: }
17926: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_ORX()
17409: {
17611:     return record_JSOP_OR();
17611: }
17926: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_ANDX()
17409: {
17611:     return record_JSOP_AND();
17611: }
17926: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_GOSUBX()
17409: {
17611:     return record_JSOP_GOSUB();
17611: }
17926: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_CASEX()
17409: {
59890:     CHECK_STATUS_A(strictEquality(true, true));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_DEFAULTX()
17409: {
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_TABLESWITCHX()
17409: {
26557:     return record_JSOP_TABLESWITCH();
17611: }
17926: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_LOOKUPSWITCHX()
17409: {
33542:     return InjectStatus(switchop());
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_BACKPATCH()
17409: {
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_BACKPATCH_POP()
17409: {
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_THROWING()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_SETRVAL()
17409: {
17844:     // If we implement this, we need to update JSOP_STOP.
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_RETRVAL()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_REGEXP()
17409: {
51446:     JSStackFrame* const fp = cx->fp();
53840:     JSScript* script = fp->script();
42717:     unsigned index = atoms - script->atomMap.vector + GET_INDEX(cx->regs->pc);
38500: 
38508:     LIns* proto_ins;
38508:     CHECK_STATUS_A(getClassPrototype(JSProto_RegExp, proto_ins));
38508: 
38500:     LIns* args[] = {
38508:         proto_ins,
56750:         w.immpObjGC(script->getRegExp(index)),
38500:         cx_ins
38500:     };
56750:     LIns* regex_ins = w.call(&js_CloneRegExpObject_ci, args);
56750:     guard(false, w.eqp0(regex_ins), OOM_EXIT);
38500: 
38500:     stack(0, regex_ins);
38500:     return ARECORD_CONTINUE;
17409: }
17926: 
17926: // begin JS_HAS_XML_SUPPORT
17926: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_DEFXMLNS()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_ANYNAME()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_QNAMEPART()
17409: {
24625:     return record_JSOP_STRING();
17409: }
17926: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_QNAMECONST()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_QNAME()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_TOATTRNAME()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_TOATTRVAL()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_ADDATTRNAME()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_ADDATTRVAL()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_BINDXMLNAME()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_SETXMLNAME()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_XMLNAME()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_DESCENDANTS()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_FILTER()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_ENDFILTER()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_TOXML()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_TOXMLLIST()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_XMLTAGEXPR()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_XMLELTEXPR()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_XMLCDATA()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_XMLCOMMENT()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_XMLPI()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_GETFUNNS()
17926: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_STARTXML()
17926: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_STARTXMLEXPR()
17926: {
33542:     return ARECORD_STOP;
17926: }
17926: 
17926: // end JS_HAS_XML_SUPPORT
17926: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_CALLPROP()
17409: {
48470:     Value& l = stackval(-1);
17870:     JSObject* obj;
17870:     LIns* obj_ins;
20427:     LIns* this_ins;
48470:     if (!l.isPrimitive()) {
48470:         obj = &l.toObject();
17870:         obj_ins = get(&l);
20427:         this_ins = obj_ins; // |this| for subsequent call
17870:     } else {
38604:         JSProtoKey protoKey;
29883:         debug_only_stmt(const char* protoname = NULL;)
48470:         if (l.isString()) {
38604:             protoKey = JSProto_String;
29883:             debug_only_stmt(protoname = "String.prototype";)
48470:         } else if (l.isNumber()) {
38604:             protoKey = JSProto_Number;
29883:             debug_only_stmt(protoname = "Number.prototype";)
48470:         } else if (l.isBoolean()) {
38604:             protoKey = JSProto_Boolean;
29883:             debug_only_stmt(protoname = "Boolean.prototype";)
17870:         } else {
48470:             JS_ASSERT(l.isNull() || l.isUndefined());
33542:             RETURN_STOP_A("callprop on null or void");
17870:         }
17870: 
38604:         if (!js_GetClassPrototype(cx, NULL, protoKey, &obj))
33542:             RETURN_ERROR_A("GetClassPrototype failed!");
17870: 
56750:         obj_ins = w.immpObjGC(obj);
56750:         debug_only_stmt(obj_ins = w.name(obj_ins, protoname);)
20427:         this_ins = get(&l); // use primitive as |this|
17870:     }
17870: 
17632:     JSObject* obj2;
40374:     PCVal pcval;
33542:     CHECK_STATUS_A(test_property_cache(obj, obj_ins, obj2, pcval));
17998: 
40374:     if (pcval.isNull())
37685:         RETURN_STOP_A("callprop of missing method");
37685: 
48470:     if (pcval.isFunObj()) {
48470:         if (l.isPrimitive()) {
48470:             JSFunction* fun = GET_FUNCTION_PRIVATE(cx, &pcval.toFunObj());
59941:             if (fun->isInterpreted() && !fun->inStrictMode())
33542:                 RETURN_STOP_A("callee does not accept primitive |this|");
19054:         }
56750:         set(&l, w.immpObjGC(&pcval.toFunObj()));
48470:     } else {
48470:         if (l.isPrimitive())
37685:             RETURN_STOP_A("callprop of primitive method");
52503:         JS_ASSERT_IF(pcval.isShape(), !pcval.toShape()->isMethod());
41290:         CHECK_STATUS_A(propTail(obj, obj_ins, obj2, pcval, NULL, NULL, &l));
37685:     }
20427:     stack(0, this_ins);
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_DELDESC()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_UINT24()
17409: {
56750:     stack(0, w.immd(GET_UINT24(cx->regs->pc)));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_INDEXBASE()
17611: {
42717:     atoms += GET_INDEXBASE(cx->regs->pc);
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_RESETBASE()
17611: {
48470:     updateAtoms();
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_RESETBASE0()
17611: {
48470:     updateAtoms();
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_CALLELEM()
17409: {
26551:     return record_JSOP_GETELEM();
17409: }
17611: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_STOP()
17611: {
59907:     JSStackFrame *fp = cx->fp();
59907: 
56566:     /* A return from callDepth 0 terminates the current loop, except for recursion. */
59907:     if (callDepth == 0 && !fp->hasImacropc()) {
56566:         AUDIT(returnLoopExits);
56566:         return endLoop();
56566:     }
56566: 
53840:     if (fp->hasImacropc()) {
30860:         /*
30860:          * End of imacro, so return true to the interpreter immediately. The
30860:          * interpreter's JSOP_STOP case will return from the imacro, back to
30860:          * the pc after the calling op, still in the same JSStackFrame.
30860:          */
53840:         updateAtoms(fp->script());
33542:         return ARECORD_CONTINUE;
21685:     }
21685: 
37694:     putActivationObjects();
31847: 
57757:     if (Probes::callTrackingActive(cx)) {
57719:         LIns* args[] = { w.immi(0), w.nameImmpNonGC(cx->fp()->fun()), cx_ins };
56750:         LIns* call_ins = w.call(&functionProbe_ci, args);
56750:         guard(false, w.eqi0(call_ins), MISMATCH_EXIT);
50455:     }
50455: 
18001:     /*
18001:      * We know falling off the end of a constructor returns the new object that
18001:      * was passed in via fp->argv[-1], while falling off the end of a function
18001:      * returns undefined.
18001:      *
18001:      * NB: we do not support script rval (eval, API users who want the result
18001:      * of the last expression-statement, debugger API calls).
18001:      */
53840:     if (fp->isConstructing()) {
53840:         rval_ins = get(&fp->thisValue());
18001:     } else {
56750:         rval_ins = w.immiUndefined();
18001:     }
60559:     clearReturningFrameFromNativeTracker();
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_GETXPROP()
17409: {
48470:     Value& l = stackval(-1);
48470:     if (l.isPrimitive())
33542:         RETURN_STOP_A("primitive-this for GETXPROP?");
17686: 
48470:     Value* vp;
30647:     LIns* v_ins;
31075:     NameResult nr;
33542:     CHECK_STATUS_A(name(vp, v_ins, nr));
30647:     stack(-1, v_ins);
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_CALLXMLNAME()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_TYPEOFEXPR()
17409: {
18019:     return record_JSOP_TYPEOF();
17409: }
17611: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_ENTERBLOCK()
17409: {
22907:     JSObject* obj;
53840:     obj = cx->fp()->script()->getObject(getFullIndex(0));
22907: 
56750:     LIns* void_ins = w.immiUndefined();
22907:     for (int i = 0, n = OBJ_BLOCK_COUNT(cx, obj); i < n; i++)
22907:         stack(i, void_ins);
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_LEAVEBLOCK()
17409: {
38560:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18005: TraceRecorder::record_JSOP_GENERATOR()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18005: TraceRecorder::record_JSOP_YIELD()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_ARRAYPUSH()
17409: {
42717:     uint32_t slot = GET_UINT16(cx->regs->pc);
53840:     JS_ASSERT(cx->fp()->numFixed() <= slot);
51446:     JS_ASSERT(cx->fp()->slots() + slot < cx->regs->sp - 1);
51446:     Value &arrayval = cx->fp()->slots()[slot];
48470:     JS_ASSERT(arrayval.isObject());
48470:     JS_ASSERT(arrayval.toObject().isDenseArray());
24861:     LIns *array_ins = get(&arrayval);
48470:     Value &elt = stackval(-1);
48470:     LIns *elt_ins = box_value_for_native_call(elt, get(&elt));
24861: 
57710:     enterDeepBailCall();
57710: 
24861:     LIns *args[] = { elt_ins, array_ins, cx_ins };
57710:     pendingGuardCondition = w.call(&js_ArrayCompPush_tn_ci, args);
57710: 
57710:     leaveDeepBailCall();
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_ENUMCONSTELEM()
17409: {
33542:     return ARECORD_STOP;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_LEAVEBLOCKEXPR()
17409: {
22907:     LIns* v_ins = stack(-1);
42717:     int n = -1 - GET_UINT16(cx->regs->pc);
22907:     stack(n, v_ins);
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_GETTHISPROP()
17409: {
17688:     LIns* this_ins;
17758: 
33542:     CHECK_STATUS_A(getThis(this_ins));
30860: 
30860:     /*
55713:      * It's safe to just use cx->fp->thisValue() here because getThis() returns
53455:      * ARECORD_STOP or ARECORD_ERROR if thisv is not available.
51097:      */
55713:     const Value &thisv = cx->fp()->thisValue();
55713:     if (!thisv.isObject())
55713:         RETURN_STOP_A("primitive this for GETTHISPROP");
55713: 
55713:     CHECK_STATUS_A(getProp(&thisv.toObject(), this_ins));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_GETARGPROP()
17409: {
42717:     return getProp(argval(GET_ARGNO(cx->regs->pc)));
17409: }
17611: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18005: TraceRecorder::record_JSOP_GETLOCALPROP()
17409: {
42717:     return getProp(varval(GET_SLOTNO(cx->regs->pc)));
17409: }
17611: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_INDEXBASE1()
17611: {
17611:     atoms += 1 << 16;
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_INDEXBASE2()
17611: {
17611:     atoms += 2 << 16;
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_INDEXBASE3()
17611: {
17611:     atoms += 3 << 16;
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
18005: TraceRecorder::record_JSOP_CALLLOCAL()
17409: {
42717:     uintN slot = GET_SLOTNO(cx->regs->pc);
18003:     stack(0, var(slot));
56750:     stack(1, w.immiUndefined());
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_CALLARG()
17409: {
42717:     uintN slot = GET_ARGNO(cx->regs->pc);
18003:     stack(0, arg(slot));
56750:     stack(1, w.immiUndefined());
33542:     return ARECORD_CONTINUE;
17409: }
17611: 
42641: JS_REQUIRES_STACK AbortableRecordingStatus
52825: TraceRecorder::record_JSOP_BINDGNAME()
52825: {
56750:     stack(0, w.immpObjGC(globalObj));
52825:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_INT8()
17409: {
56750:     stack(0, w.immd(GET_INT8(cx->regs->pc)));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_INT32()
17409: {
56750:     stack(0, w.immd(GET_INT32(cx->regs->pc)));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_LENGTH()
17409: {
48470:     Value& l = stackval(-1);
48470:     if (l.isPrimitive()) {
48470:         if (!l.isString())
33542:             RETURN_STOP_A("non-string primitive JSOP_LENGTH unsupported");
56750:         set(&l, w.i2d(w.p2i(w.getStringLength(get(&l)))));
33542:         return ARECORD_CONTINUE;
17869:     }
17869: 
48470:     JSObject* obj = &l.toObject();
26285:     LIns* obj_ins = get(&l);
30248: 
40405:     if (obj->isArguments()) {
31460:         unsigned depth;
31460:         JSStackFrame *afp = guardArguments(obj, obj_ins, &depth);
31460:         if (!afp)
33542:             RETURN_STOP_A("can't reach arguments object's frame");
31460: 
37214:         // We must both check at record time and guard at run time that
37214:         // arguments.length has not been reassigned, redefined or deleted.
40864:         if (obj->isArgsLengthOverridden())
37214:             RETURN_STOP_A("can't trace JSOP_ARGCNT if arguments.length has been modified");
37214:         LIns* slot_ins = guardArgsLengthNotAssigned(obj_ins);
37214: 
51092:         // slot_ins is the value from the slot; right-shift to get the length
51092:         // (see JSObject::getArgsInitialLength in jsfun.cpp).
56750:         LIns* v_ins = w.i2d(w.rshiN(slot_ins, JSObject::ARGS_PACKED_BITS_COUNT));
30248:         set(&l, v_ins);
33542:         return ARECORD_CONTINUE;
30248:     }
30248: 
26285:     LIns* v_ins;
39928:     if (obj->isArray()) {
39928:         if (obj->isDenseArray()) {
42749:             guardDenseArray(obj_ins, BRANCH_EXIT);
42749:         } else {
42749:             JS_ASSERT(obj->isSlowArray());
48613:             guardClass(obj_ins, &js_SlowArrayClass, snapshot(BRANCH_EXIT), LOAD_NORMAL);
41989:         }
58072:         v_ins = w.lduiObjPrivate(obj_ins);
58072:         if (obj->getArrayLength() <= JSVAL_INT_MAX) {
58072:             guard(true, w.leui(v_ins, w.immi(JSVAL_INT_MAX)), BRANCH_EXIT);
58072:             v_ins = w.i2d(v_ins);
58072:         } else {
58072:             v_ins = w.ui2d(v_ins);
58072:         }
37765:     } else if (OkToTraceTypedArrays && js_IsTypedArray(obj)) {
37765:         // Ensure array is a typed array and is the same type as what was written
48613:         guardClass(obj_ins, obj->getClass(), snapshot(BRANCH_EXIT), LOAD_NORMAL);
56750:         v_ins = w.i2d(w.ldiConstTypedArrayLength(w.ldpObjPrivate(obj_ins)));
26285:     } else {
40430:         if (!obj->isNative())
33542:             RETURN_STOP_A("can't trace length property access on non-array, non-native object");
26285:         return getProp(obj, obj_ins);
26285:     }
17715:     set(&l, v_ins);
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
17926: TraceRecorder::record_JSOP_HOLE()
17409: {
56750:     stack(0, w.immpMagicWhy(JS_ARRAY_HOLE));
33542:     return ARECORD_CONTINUE;
33542: }
33542: 
33542: AbortableRecordingStatus
32776: TraceRecorder::record_JSOP_TRACE()
25627: {
33542:     return ARECORD_CONTINUE;
25627: }
25627: 
56217: AbortableRecordingStatus
56217: TraceRecorder::record_JSOP_NOTRACE()
56217: {
56217:     return ARECORD_CONTINUE;
56217: }
56217: 
37685: JSBool FASTCALL
37685: js_Unbrand(JSContext *cx, JSObject *obj)
37685: {
37685:     return obj->unbrand(cx);
37685: }
37685: 
48613: JS_DEFINE_CALLINFO_2(extern, BOOL, js_Unbrand, CONTEXT, OBJECT, 0, ACCSET_STORE_ANY)
37685: 
37685: JS_REQUIRES_STACK AbortableRecordingStatus
37685: TraceRecorder::record_JSOP_UNBRAND()
37685: {
37685:     LIns* args_ins[] = { stack(-1), cx_ins };
56750:     LIns* call_ins = w.call(&js_Unbrand_ci, args_ins);
56750:     guard(false, w.eqi0(call_ins), OOM_EXIT);
37685:     return ARECORD_CONTINUE;
37685: }
37685: 
33542: JS_REQUIRES_STACK AbortableRecordingStatus
38549: TraceRecorder::record_JSOP_UNBRANDTHIS()
38549: {
60796:     /* In case of primitive this, do nothing. */
60796:     JSStackFrame *fp = cx->fp();
60796:     if (fp->fun()->inStrictMode() && !fp->thisValue().isObject())
60796:         return ARECORD_CONTINUE;
60796: 
38549:     LIns* this_ins;
38549:     RecordingStatus status = getThis(this_ins);
38549:     if (status != RECORD_CONTINUE)
38549:         return InjectStatus(status);
38549: 
38549:     LIns* args_ins[] = { this_ins, cx_ins };
56750:     LIns* call_ins = w.call(&js_Unbrand_ci, args_ins);
56750:     guard(false, w.eqi0(call_ins), OOM_EXIT);
38549:     return ARECORD_CONTINUE;
38549: }
38549: 
38549: JS_REQUIRES_STACK AbortableRecordingStatus
32729: TraceRecorder::record_JSOP_SHARPINIT()
32729: {
33542:     return ARECORD_STOP;
32729: }
32729: 
52555: JS_REQUIRES_STACK AbortableRecordingStatus
52555: TraceRecorder::record_JSOP_GETGLOBAL()
52555: {
53840:     uint32 slot = cx->fp()->script()->getGlobalSlot(GET_SLOTNO(cx->regs->pc));
52555:     if (!lazilyImportGlobalSlot(slot))
52555:          RETURN_STOP_A("lazy import of global slot failed");
52555: 
52555:     stack(0, get(&globalObj->getSlotRef(slot)));
52555:     return ARECORD_CONTINUE;
52555: }
52555: 
52555: JS_REQUIRES_STACK AbortableRecordingStatus
60526: TraceRecorder::record_JSOP_CALLGLOBAL()
52555: {
53840:     uint32 slot = cx->fp()->script()->getGlobalSlot(GET_SLOTNO(cx->regs->pc));
52555:     if (!lazilyImportGlobalSlot(slot))
52555:          RETURN_STOP_A("lazy import of global slot failed");
52555: 
52813:     Value &v = globalObj->getSlotRef(slot);
52555:     stack(0, get(&v));
56750:     stack(1, w.immiUndefined());
52555:     return ARECORD_CONTINUE;
52555: }
52555: 
52555: JS_REQUIRES_STACK AbortableRecordingStatus
52825: TraceRecorder::record_JSOP_GETGNAME()
52825: {
52825:     return record_JSOP_NAME();
52825: }
52825: 
52825: JS_REQUIRES_STACK AbortableRecordingStatus
52825: TraceRecorder::record_JSOP_SETGNAME()
52825: {
52825:     return record_JSOP_SETNAME();
52825: }
52825: 
52825: JS_REQUIRES_STACK AbortableRecordingStatus
52825: TraceRecorder::record_JSOP_GNAMEDEC()
52825: {
52825:     return record_JSOP_NAMEDEC();
52825: }
52825: 
52825: JS_REQUIRES_STACK AbortableRecordingStatus
52825: TraceRecorder::record_JSOP_GNAMEINC()
52825: {
52825:     return record_JSOP_NAMEINC();
52825: }
52825: 
52825: JS_REQUIRES_STACK AbortableRecordingStatus
52825: TraceRecorder::record_JSOP_DECGNAME()
52825: {
52825:     return record_JSOP_DECNAME();
52825: }
52825: 
52825: JS_REQUIRES_STACK AbortableRecordingStatus
52825: TraceRecorder::record_JSOP_INCGNAME()
52825: {
52825:     return record_JSOP_INCNAME();
52825: }
52825: 
52825: JS_REQUIRES_STACK AbortableRecordingStatus
52825: TraceRecorder::record_JSOP_CALLGNAME()
52825: {
52825:     return record_JSOP_CALLNAME();
52825: }
52825: 
28952: #define DBG_STUB(OP)                                                          \
33542:     JS_REQUIRES_STACK AbortableRecordingStatus                                \
28952:     TraceRecorder::record_##OP()                                              \
28952:     {                                                                         \
33542:         RETURN_STOP_A("can't trace " #OP);                                    \
28952:     }
28952: 
28952: DBG_STUB(JSOP_GETUPVAR_DBG)
28952: DBG_STUB(JSOP_CALLUPVAR_DBG)
28952: DBG_STUB(JSOP_DEFFUN_DBGFC)
28952: DBG_STUB(JSOP_DEFLOCALFUN_DBGFC)
28952: DBG_STUB(JSOP_LAMBDA_DBGFC)
28952: 
21459: #ifdef JS_JIT_SPEW
30860: /*
30860:  * Print information about entry typemaps and unstable exits for all peers
30860:  * at a PC.
30860:  */
21433: void
37741: DumpPeerStability(TraceMonitor* tm, const void* ip, JSObject* globalObj, uint32 globalShape,
28244:                   uint32 argc)
21433: {
35044:     TreeFragment* f;
21456:     bool looped = false;
21685:     unsigned length = 0;
21433: 
33748:     for (f = LookupLoop(tm, ip, globalObj, globalShape, argc); f != NULL; f = f->peer) {
36361:         if (!f->code())
21433:             continue;
31469:         debug_only_printf(LC_TMRecorder, "Stability of fragment %p:\nENTRY STACK=", (void*)f);
21456:         if (looped)
36361:             JS_ASSERT(f->nStackTypes == length);
36361:         for (unsigned i = 0; i < f->nStackTypes; i++)
48470:             debug_only_printf(LC_TMRecorder, "%c", TypeToChar(f->stackTypeMap()[i]));
31469:         debug_only_print0(LC_TMRecorder, " GLOBALS=");
36361:         for (unsigned i = 0; i < f->nGlobalTypes(); i++)
48470:             debug_only_printf(LC_TMRecorder, "%c", TypeToChar(f->globalTypeMap()[i]));
31469:         debug_only_print0(LC_TMRecorder, "\n");
36361:         UnstableExit* uexit = f->unstableExits;
21433:         while (uexit != NULL) {
31469:             debug_only_print0(LC_TMRecorder, "EXIT  ");
48470:             JSValueType* m = uexit->exit->fullTypeMap();
31469:             debug_only_print0(LC_TMRecorder, "STACK=");
21433:             for (unsigned i = 0; i < uexit->exit->numStackSlots; i++)
48470:                 debug_only_printf(LC_TMRecorder, "%c", TypeToChar(m[i]));
31469:             debug_only_print0(LC_TMRecorder, " GLOBALS=");
31469:             for (unsigned i = 0; i < uexit->exit->numGlobalSlots; i++) {
31469:                 debug_only_printf(LC_TMRecorder, "%c",
48470:                                   TypeToChar(m[uexit->exit->numStackSlots + i]));
31469:             }
31469:             debug_only_print0(LC_TMRecorder, "\n");
21433:             uexit = uexit->next;
21433:         }
36361:         length = f->nStackTypes;
21456:         looped = true;
21433:     }
21433: }
21433: #endif
21433: 
29368: #ifdef MOZ_TRACEVIS
29368: 
29368: FILE* traceVisLogFile = NULL;
31063: JSHashTable *traceVisScriptTable = NULL;
29368: 
29368: JS_FRIEND_API(bool)
38585: StartTraceVis(const char* filename = "tracevis.dat")
29368: {
29368:     if (traceVisLogFile) {
29368:         // If we're currently recording, first we must stop.
38585:         StopTraceVis();
29368:     }
29368: 
29368:     traceVisLogFile = fopen(filename, "wb");
29368:     if (!traceVisLogFile)
29368:         return false;
29368: 
29368:     return true;
29368: }
29368: 
29368: JS_FRIEND_API(JSBool)
54568: StartTraceVisNative(JSContext *cx, uintN argc, jsval *vp)
29368: {
29368:     JSBool ok;
29368: 
54568:     if (argc > 0 && JSVAL_IS_STRING(JS_ARGV(cx, vp)[0])) {
54568:         JSString *str = JSVAL_TO_STRING(JS_ARGV(cx, vp)[0]);
29368:         char *filename = js_DeflateString(cx, str->chars(), str->length());
29368:         if (!filename)
29368:             goto error;
38585:         ok = StartTraceVis(filename);
30851:         cx->free(filename);
29368:     } else {
38585:         ok = StartTraceVis();
29368:     }
29368: 
29368:     if (ok) {
29368:         fprintf(stderr, "started TraceVis recording\n");
54568:         JS_SET_RVAL(cx, vp, JSVAL_VOID);
54568:         return true;
29368:     }
29368: 
29368:   error:
29368:     JS_ReportError(cx, "failed to start TraceVis recording");
54568:     return false;
29368: }
29368: 
29368: JS_FRIEND_API(bool)
38585: StopTraceVis()
29368: {
29368:     if (!traceVisLogFile)
29368:         return false;
29368: 
29368:     fclose(traceVisLogFile); // not worth checking the result
29368:     traceVisLogFile = NULL;
29368: 
29368:     return true;
29368: }
29368: 
29368: JS_FRIEND_API(JSBool)
54568: StopTraceVisNative(JSContext *cx, uintN argc, jsval *vp)
38585: {
38585:     JSBool ok = StopTraceVis();
29368: 
54568:     if (ok) {
29368:         fprintf(stderr, "stopped TraceVis recording\n");
54568:         JS_SET_RVAL(cx, vp, JSVAL_VOID);
54568:     } else {
29368:         JS_ReportError(cx, "TraceVis isn't running");
54568:     }
29368: 
29368:     return ok;
29368: }
29368: 
29368: #endif /* MOZ_TRACEVIS */
33564: 
33564: JS_REQUIRES_STACK void
48470: TraceRecorder::captureStackTypes(unsigned callDepth, JSValueType* typeMap)
33564: {
46181:     CaptureTypesVisitor capVisitor(cx, typeMap, !!oracle);
33564:     VisitStackSlots(capVisitor, cx, callDepth);
33564: }
33564: 
33564: JS_REQUIRES_STACK void
48470: TraceRecorder::determineGlobalTypes(JSValueType* typeMap)
33564: {
33564:     DetermineTypesVisitor detVisitor(*this, typeMap);
36361:     VisitGlobalSlots(detVisitor, cx, *tree->globalSlots);
33564: }
33564: 
53133: #ifdef JS_METHODJIT
53133: 
53133: class AutoRetBlacklist
53133: {
53133:     jsbytecode* pc;
56551:     bool* blacklist;
53133: 
53133:   public:
56551:     AutoRetBlacklist(jsbytecode* pc, bool* blacklist)
53133:       : pc(pc), blacklist(blacklist)
53133:     { }
53133: 
53133:     ~AutoRetBlacklist()
53133:     {
56551:         *blacklist = IsBlacklisted(pc);
53133:     }
53133: };
53133: 
53133: JS_REQUIRES_STACK TracePointAction
57802: RecordTracePoint(JSContext* cx, uintN& inlineCallCount, bool* blacklist, bool execAllowed)
53133: {
53482:     JSStackFrame* fp = cx->fp();
53133:     TraceMonitor* tm = &JS_TRACE_MONITOR(cx);
53133:     jsbytecode* pc = cx->regs->pc;
53133: 
53133:     JS_ASSERT(!TRACE_RECORDER(cx));
60157:     JS_ASSERT(!tm->profile);
53133: 
53840:     JSObject* globalObj = cx->fp()->scopeChain().getGlobal();
53133:     uint32 globalShape = -1;
53133:     SlotList* globalSlots = NULL;
53133: 
53133:     AutoRetBlacklist autoRetBlacklist(pc, blacklist);
53133: 
53133:     if (!CheckGlobalObjectShape(cx, tm, globalObj, &globalShape, &globalSlots)) {
53133:         Backoff(cx, pc);
53133:         return TPA_Nothing;
53133:     }
53133: 
53840:     uint32 argc = entryFrameArgc(cx);
53133:     TreeFragment* tree = LookupOrAddLoop(tm, pc, globalObj, globalShape, argc);
53133: 
53133:     debug_only_printf(LC_TMTracer,
53133:                       "Looking for compat peer %d@%d, from %p (ip: %p)\n",
53482:                       js_FramePCToLineNumber(cx, cx->fp()),
53482:                       FramePCOffset(cx, cx->fp()), (void*)tree, tree->ip);
53133: 
53133:     if (tree->code() || tree->peer) {
53133:         uintN count;
53133:         TreeFragment* match = FindVMCompatiblePeer(cx, globalObj, tree, count);
53133:         if (match) {
53133:             VMSideExit* lr = NULL;
53133:             VMSideExit* innermostNestedGuard = NULL;
53133: 
56551:             if (!execAllowed) {
56551:                 /* We've already compiled a trace for it, but we don't want to use that trace. */
56551:                 Blacklist((jsbytecode*)tree->root->ip);
56551:                 return TPA_Nothing;
56551:             }
56551: 
53133:             /* Best case - just go and execute. */
53133:             if (!ExecuteTree(cx, match, inlineCallCount, &innermostNestedGuard, &lr))
53133:                 return TPA_Error;
53133: 
53133:             if (!lr)
53133:                 return TPA_Nothing;
53133: 
53133:             switch (lr->exitType) {
53133:               case UNSTABLE_LOOP_EXIT:
58070:                 if (!AttemptToStabilizeTree(cx, globalObj, lr, NULL, NULL, 0))
53133:                     return TPA_RanStuff;
53133:                 break;
53133: 
56185:               case MUL_ZERO_EXIT:
53133:               case OVERFLOW_EXIT:
56185:                 if (lr->exitType == MUL_ZERO_EXIT)
56185:                     tm->oracle->markInstructionSlowZeroTest(cx->regs->pc);
56185:                 else
53133:                     tm->oracle->markInstructionUndemotable(cx->regs->pc);
53133:                 /* FALL THROUGH */
53133:               case BRANCH_EXIT:
53133:               case CASE_EXIT:
56217:                 if (!AttemptToExtendTree(cx, lr, NULL, NULL, NULL))
53133:                     return TPA_RanStuff;
53133:                 break;
53133: 
53133:               case LOOP_EXIT:
53133:                 if (!innermostNestedGuard)
53133:                     return TPA_RanStuff;
56217:                 if (!AttemptToExtendTree(cx, innermostNestedGuard, lr, NULL, NULL))
53133:                     return TPA_RanStuff;
53133:                 break;
53133: 
53133:               default:
53133:                 return TPA_RanStuff;
53133:             }
53133: 
53133:             JS_ASSERT(TRACE_RECORDER(cx));
53133: 
53133:             goto interpret;
53133:         }
53133: 
53133:         if (count >= MAXPEERS) {
53133:             debug_only_print0(LC_TMTracer, "Blacklisted: too many peer trees.\n");
53133:             Blacklist((jsbytecode*)tree->root->ip);
53133:             return TPA_Nothing;
53133:         }
53133:     }
53133: 
53133:     if (++tree->hits() < HOTLOOP)
53133:         return TPA_Nothing;
53133:     if (!ScopeChainCheck(cx, tree))
53133:         return TPA_Nothing;
58070:     if (!RecordTree(cx, tree->first, NULL, NULL, 0, globalSlots))
53133:         return TPA_Nothing;
53133: 
53133:   interpret:
53133:     JS_ASSERT(TRACE_RECORDER(cx));
53133: 
53133:     /* Locked and loaded with a recorder. Ask the interperter to go run some code. */
53626:     if (!Interpret(cx, fp, inlineCallCount, JSINTERP_RECORD))
53133:         return TPA_Error;
53133: 
60211:     JS_ASSERT(!cx->isExceptionPending());
56551:     
53133:     return TPA_RanStuff;
53133: }
53133: 
60158: LoopProfile::LoopProfile(JSStackFrame *entryfp, jsbytecode *top, jsbytecode *bottom)
60158:     : entryScript(entryfp->script()),
60158:       entryfp(entryfp),
56551:       top(top),
56551:       bottom(bottom),
60534:       hits(0),
60542:       undecided(false),
60542:       unprofitable(false)
60157: {
60157:     reset();
60157: }
60157: 
60157: void
60157: LoopProfile::reset()
60157: {
60157:     profiled = false;
60157:     traceOK = false;
60157:     numAllOps = 0;
60157:     numSelfOps = 0;
60157:     numSelfOpsMult = 0;
60157:     branchMultiplier = 1;
60157:     shortLoop = false;
60157:     maybeShortLoop = false;
60157:     numInnerLoops = 0;
60157:     loopStackDepth = 0;
60157:     sp = 0;
60157: 
60157:     PodArrayZero(allOps);
60157:     PodArrayZero(selfOps);
56551: }
56551: 
56551: MonitorResult
56551: LoopProfile::profileLoopEdge(JSContext* cx, uintN& inlineCallCount)
56551: {
56551:     if (cx->regs->pc == top) {
56551:         debug_only_print0(LC_TMProfiler, "Profiling complete (edge)\n");
56551:         decide(cx);
56551:     } else {
56551:         /* Record an inner loop invocation. */
60158:         JSStackFrame *fp = cx->fp();
56551:         jsbytecode *pc = cx->regs->pc;
56551:         bool found = false;
56551: 
56551:         /* We started with the most deeply nested one first, since it gets hit most often.*/
56551:         for (int i = int(numInnerLoops)-1; i >= 0; i--) {
60158:             if (innerLoops[i].entryfp == fp && innerLoops[i].top == pc) {
56551:                 innerLoops[i].iters++;
56551:                 found = true;
60158:                 break;
56551:             }
56551:         }
56551: 
56551:         if (!found && numInnerLoops < PROFILE_MAX_INNER_LOOPS)
60158:             innerLoops[numInnerLoops++] = InnerLoop(fp, pc, NULL);
56551:     }
56551: 
56551:     return MONITOR_NOT_RECORDING;
56551: }
56551: 
56551: 
56551: static const uintN PROFILE_HOTLOOP = 61;
56551: static const uintN MAX_PROFILE_OPS = 4096;
56551: 
56551: static jsbytecode *
56551: GetLoopBottom(JSContext *cx)
56551: {
57807:     return GetLoopBottom(cx, cx->regs->pc);
56551: }
56551: 
56551: static LoopProfile *
56551: LookupOrAddProfile(JSContext *cx, TraceMonitor *tm, void** traceData, uintN *traceEpoch)
56551: {
56551:     LoopProfile *prof;
56551: 
56551:     /*
56551:      * We try to keep a pointer to the loop profile inside the TRACE IC.
56551:      * We also keep a pointer inside a hashtable for when we need to
56551:      * look up nested loops (or when ICs are disabled).
56551:      *
56551:      * Memory for the profile is allocated in the dataAlloc for the
56551:      * trace monitor. Since this thing can get flushed periodically,
56551:      * we use epochs to decide if the profile in the MIC is valid, as
56551:      * follows. Every time the trace monitor is flushed,
56551:      * |tm->flushEpoch| is incremented. When storing the profile in
56551:      * the IC, we store the current |tm->flushEpoch| along with it.
56551:      * Before pulling a profile out of the IC, we check that its
56551:      * stored epoch is still up-to-date with |tm->flushEpoch|.
56551:      * This ensures that no flush has happened in between.
56551:      */
56551: 
57880: #if JS_MONOIC
56551:     if (*traceData && *traceEpoch == tm->flushEpoch) {
56551:         prof = (LoopProfile *)*traceData;
56551:     } else {
56551:         jsbytecode* pc = cx->regs->pc;
56551:         jsbytecode* bottom = GetLoopBottom(cx);
56551:         if (!bottom)
56551:             return NULL;
60158:         prof = new (*tm->dataAlloc) LoopProfile(cx->fp(), pc, bottom);
56551:         *traceData = prof;
56551:         *traceEpoch = tm->flushEpoch;
56551:         tm->loopProfiles->put(pc, prof);
56551:     }
57880: #else
56551:     LoopProfileMap &table = *tm->loopProfiles;
56551:     jsbytecode* pc = cx->regs->pc;
56551:     if (LoopProfileMap::AddPtr p = table.lookupForAdd(pc)) {
56551:         prof = p->value;
56551:     } else {
56551:         jsbytecode* bottom = GetLoopBottom(cx);
56551:         if (!bottom)
56551:             return NULL;
60158:         prof = new (*tm->dataAlloc) LoopProfile(cx->fp(), pc, bottom);
56551:         table.add(p, pc, prof);
56551:     }
57880: #endif
56551: 
56551:     return prof;
56551: }
56551: 
60534: static LoopProfile *
60534: LookupLoopProfile(JSContext *cx, jsbytecode *pc)
60534: {
60534:     TraceMonitor* tm = &JS_TRACE_MONITOR(cx);
60534:     LoopProfileMap &table = *tm->loopProfiles;
60534:     if (LoopProfileMap::Ptr p = table.lookup(pc)) {
60534:         JS_ASSERT(p->value->top == pc);
60534:         return p->value;
60534:     } else
60534:         return NULL;
60534: }
60534: 
56551: JS_REQUIRES_STACK TracePointAction
56551: MonitorTracePoint(JSContext *cx, uintN& inlineCallCount, bool* blacklist,
60534:                   void** traceData, uintN *traceEpoch, uint32 *loopCounter, uint32 hits)
56551: {
56551:     if (!cx->profilingEnabled)
57802:         return RecordTracePoint(cx, inlineCallCount, blacklist, true);
56551: 
56551:     *blacklist = false;
56551: 
56551:     TraceMonitor *tm = &JS_TRACE_MONITOR(cx);
56551:     /*
56551:      * We may have re-entered Interpret while profiling. We don't profile
56551:      * the nested invocation.
56551:      */
56551:     if (tm->profile)
56551:         return TPA_Nothing;
56551: 
60534:     jsbytecode* pc = cx->regs->pc;
56551:     LoopProfile *prof = LookupOrAddProfile(cx, tm, traceData, traceEpoch);
56551:     if (!prof) {
56551:         *blacklist = true;
56551:         return TPA_Nothing;
56551:     }
56551: 
60534:     prof->hits += hits;
60534:     if (prof->hits < PROFILE_HOTLOOP)
56551:         return TPA_Nothing;
56551: 
56551:     AutoRetBlacklist autoRetBlacklist(cx->regs->pc, blacklist);
56551: 
56551:     if (prof->profiled) {
56551:         if (prof->traceOK) {
57802:             return RecordTracePoint(cx, inlineCallCount, blacklist, prof->execOK);
56551:         } else {
56551:             return TPA_Nothing;
56551:         }
56551:     }
56551: 
57880:     debug_only_printf(LC_TMProfiler, "Profiling at line %d\n",
56551:                       js_FramePCToLineNumber(cx, cx->fp()));
56551: 
56551:     tm->profile = prof;
56551: 
56551:     if (!Interpret(cx, cx->fp(), inlineCallCount, JSINTERP_PROFILE))
56551:         return TPA_Error;
56551: 
60211:     JS_ASSERT(!cx->isExceptionPending());
56551: 
60534:     /* Look it up again since a reset may have happened during Interpret. */
60534:     prof = LookupLoopProfile(cx, pc);
60534:     if (prof && prof->undecided) {
60534:         *loopCounter = 3000;
60534:         prof->reset();
60534:     }
60534: 
56551:     return TPA_RanStuff;
56551: }
56551: 
56551: /*
57880:  * Returns true if pc is within the given loop.
57880:  * If we're in a different script, then we must have come from
57880:  * a call instruction within the loop (since we check if we're within
57880:  * the loop before each instruction) so we're still in the loop.
56551:  */
56551: template<class T>
56551: static inline bool
60158: PCWithinLoop(JSStackFrame *fp, jsbytecode *pc, T& loop)
60158: {
60158:     return fp > loop.entryfp || (fp == loop.entryfp && pc >= loop.top && pc <= loop.bottom);
56551: }
56551: 
56551: LoopProfile::ProfileAction
56551: LoopProfile::profileOperation(JSContext* cx, JSOp op)
56551: {
56551:     TraceMonitor* tm = &JS_TRACE_MONITOR(cx);
56551: 
56551:     if (profiled) {
56551:         tm->profile = NULL;
56551:         return ProfComplete;
56551:     }
56551: 
56551:     jsbytecode *pc = cx->regs->pc;
60158:     JSStackFrame *fp = cx->fp();
60158:     JSScript *script = fp->script();
60158: 
60158:     if (!PCWithinLoop(fp, pc, *this)) {
60157:         debug_only_printf(LC_TMProfiler, "Profiling complete (loop exit) at line %u\n",
56551:                           js_FramePCToLineNumber(cx, cx->fp()));
56551:         tm->profile->decide(cx);
56551:         tm->profile = NULL;
56551:         return ProfComplete;
56551:     }
56551: 
60158:     while (loopStackDepth > 0 && !PCWithinLoop(fp, pc, loopStack[loopStackDepth-1])) {
57802:         debug_only_print0(LC_TMProfiler, "Profiler: Exiting inner loop\n");
56551:         loopStackDepth--;
56551:     }
56551: 
56551:     if (op == JSOP_TRACE || op == JSOP_NOTRACE) {
56551:         if (pc != top && (loopStackDepth == 0 || pc != loopStack[loopStackDepth-1].top)) {
56551:             if (loopStackDepth == PROFILE_MAX_INNER_LOOPS) {
56551:                 debug_only_print0(LC_TMProfiler, "Profiling complete (maxnest)\n");
56551:                 tm->profile->decide(cx);
56551:                 tm->profile = NULL;
56551:                 return ProfComplete;
56551:             }
56551: 
60157:             debug_only_printf(LC_TMProfiler, "Profiler: Entering inner loop at line %d\n",
60157:                               js_FramePCToLineNumber(cx, cx->fp()));
60158:             loopStack[loopStackDepth++] = InnerLoop(fp, pc, GetLoopBottom(cx));
56551:         }
56551:     }
56551: 
56551:     numAllOps++;
56551:     if (loopStackDepth == 0) {
56551:         numSelfOps++;
56551:         numSelfOpsMult += branchMultiplier;
56551:     }
56551: 
56551:     if (op == JSOP_ADD || op == JSOP_SUB || op == JSOP_MUL || op == JSOP_DIV) {
56551:         Value& v1 = cx->regs->sp[-1];
56551:         Value& v2 = cx->regs->sp[-2];
56551: 
56551:         /* If either operand is a double, treat it as a floating-point op. */
56551:         if (v1.isDouble() || v2.isDouble())
56551:             increment(OP_FLOAT);
56551:         else if (v1.isInt32() || v2.isInt32())
56551:             increment(OP_INT);
56551:     }
56551: 
56551:     if (op == JSOP_EQ || op == JSOP_NE)
56551:         increment(OP_EQ);
56551: 
56551:     if (op == JSOP_BITOR || op == JSOP_BITXOR || op == JSOP_BITAND
56551:         || op == JSOP_LSH || op == JSOP_RSH || op == JSOP_URSH || op == JSOP_BITNOT)
56551:     {
56551:         increment(OP_BIT);
56551:     }
56551: 
56551:     if (op == JSOP_EVAL)
56551:         increment(OP_EVAL);
56551: 
56551:     if (op == JSOP_NEW)
56551:         increment(OP_NEW);
56551: 
56562:     if (op == JSOP_GETELEM || op == JSOP_SETELEM) {
56562:         Value& lval = cx->regs->sp[op == JSOP_GETELEM ? -2 : -3];
56562:         if (lval.isObject() && js_IsTypedArray(&lval.toObject()))
56562:             increment(OP_TYPED_ARRAY);
60543:         else if (lval.isObject() && lval.toObject().isDenseArray() && op == JSOP_GETELEM)
60543:             increment(OP_ARRAY_READ);
56562:     }
56562: 
56551:     if (op == JSOP_CALL) {
56551:         increment(OP_CALL);
56551: 
56551:         uintN argc = GET_ARGC(cx->regs->pc);
56551:         Value &v = cx->regs->sp[-((int)argc + 2)];
56551:         JSObject *callee;
56551:         if (IsFunctionObject(v, &callee)) {
56551:             JSFunction *fun = callee->getFunctionPrivate();
56551:             if (fun->isInterpreted()) {
56551:                 if (cx->fp()->isFunctionFrame() && fun == cx->fp()->fun())
56551:                     increment(OP_RECURSIVE);
56551:             } else {
56551:                 js::Native native = fun->u.n.native;
56551:                 if (js_IsMathFunction(JS_JSVALIFY_NATIVE(native)))
56551:                     increment(OP_FLOAT);
56551:             }
56551:         }
56551:     }
56551: 
56551:     if (op == JSOP_CALLPROP && loopStackDepth == 0)
60158:         branchMultiplier *= mjit::GetCallTargetCount(script, pc);
56551: 
57800:     if (op == JSOP_TABLESWITCH) {
57800:         jsint low = GET_JUMP_OFFSET(pc + JUMP_OFFSET_LEN);
57800:         jsint high = GET_JUMP_OFFSET(pc + JUMP_OFFSET_LEN*2);
57800:         branchMultiplier *= high - low + 1;
57800:     }
57800: 
57800:     if (op == JSOP_LOOKUPSWITCH)
57800:         branchMultiplier *= GET_UINT16(pc + JUMP_OFFSET_LEN);
57800:     
56551:     if (numAllOps >= MAX_PROFILE_OPS) {
56551:         debug_only_print0(LC_TMProfiler, "Profiling complete (maxops)\n");
56551:         tm->profile->decide(cx);
56551:         tm->profile = NULL;
56551:         return ProfComplete;
56551:     }
56551: 
56551:     /* These are the places where the interpreter skips over branches. */
56551:     jsbytecode *testPC = cx->regs->pc;
56551:     if (op == JSOP_EQ || op == JSOP_NE || op == JSOP_LT || op == JSOP_GT
56551:         || op == JSOP_LE || op == JSOP_GE || op == JSOP_IN || op == JSOP_MOREITER)
56551:     {
56551:         const JSCodeSpec *cs = &js_CodeSpec[op];
56551:         ptrdiff_t oplen = cs->length;
56551:         JS_ASSERT(oplen != -1);
56551: 
56551:         if (cx->regs->pc - script->code + oplen < ptrdiff_t(script->length))
56603:             if (cx->regs->pc[oplen] == JSOP_IFEQ || cx->regs->pc[oplen] == JSOP_IFNE)
56551:                 testPC = cx->regs->pc + oplen;
56551:     }
56551: 
56551:     /* Check if we're exiting the loop being profiled. */
60158:     JSOp testOp = js_GetOpcode(cx, script, testPC);
56551:     if (testOp == JSOP_IFEQ || testOp == JSOP_IFNE || testOp == JSOP_GOTO
56551:         || testOp == JSOP_AND || testOp == JSOP_OR)
56551:     {
56551:         ptrdiff_t len = GET_JUMP_OFFSET(testPC);
56603:         if (testPC + len == top && (op == JSOP_LT || op == JSOP_LE)) {
56603:             StackValue v = stackAt(-1);
56603:             if (v.hasValue && v.value < 8)
56551:                 shortLoop = true;
56551:         }
56603: 
56551:         if (testPC + len == top && (op == JSOP_LT || op == JSOP_LE)
56551:             && cx->regs->sp[-2].isInt32() && cx->regs->sp[-2].toInt32() < 16)
56551:         {
56551:             maybeShortLoop = true;
56551:         }
56603: 
56551:         if (testOp != JSOP_GOTO && len > 0) {
56603:             bool isConst;
56603:             if (testOp == JSOP_IFEQ || testOp == JSOP_IFNE)
56603:                 isConst = stackAt(-1).isConst && stackAt(-2).isConst;
56603:             else
56603:                 isConst = stackAt(-1).isConst;
56603: 
56551:             increment(OP_FWDJUMP);
56603:             if (loopStackDepth == 0 && !isConst)
56551:                 branchMultiplier *= 2;
56551:         }
56551:     }
56551: 
56603:     if (op == JSOP_INT8) {
56603:         stackPush(StackValue(true, GET_INT8(cx->regs->pc)));
56603:     } else if (op == JSOP_STRING) {
56603:         stackPush(StackValue(true));
56603:     } else if (op == JSOP_TYPEOF || op == JSOP_TYPEOFEXPR) {
56603:         stackPush(StackValue(true));
56603:     } else if (op == JSOP_EQ || op == JSOP_NE) {
56603:         StackValue v1 = stackAt(-1);
56603:         StackValue v2 = stackAt(-2);
56603:         stackPush(StackValue(v1.isConst && v2.isConst));
56603:     } else if (op == JSOP_AND) {
56603:         bool b = !!js_ValueToBoolean(cx->regs->sp[-1]);
56603:         StackValue v = stackAt(-1);
56603:         if (b)
56603:             stackPop();
56603:     } else {
56603:         stackClear();
56603:     }
56603:     
56551:     return ProfContinue;
56551: }
56551: 
57802: /*
57802:  * Returns true if the loop would probably take a long time to
60157:  * compile.
57802:  */
56551: bool
57802: LoopProfile::isCompilationExpensive(JSContext *cx, uintN depth)
57802: {
57802:     if (depth == 0)
57802:         return true;
57802: 
60534:     if (!profiled)
60534:         return false;
60534: 
56551:     /* Too many ops to compile? */
56551:     if (numSelfOps == MAX_PROFILE_OPS)
57802:         return true;
56551: 
56551:     /* Is the code too branchy? */
60544:     if (numSelfOpsMult > numSelfOps*100000)
57802:         return true;
56551: 
56551:     /* Ensure that inner loops aren't too expensive. */
56551:     for (uintN i=0; i<numInnerLoops; i++) {
56551:         LoopProfile *prof = LookupLoopProfile(cx, innerLoops[i].top);
60534:         if (!prof || prof->isCompilationExpensive(cx, depth-1))
57802:             return true;
57802:     }
57802: 
57802:     return false;
56551: }
56551: 
56551: /*
56551:  * This function recognizes loops that are short and that contain
56551:  * jumps. The tracer does badly with these loops because it
56551:  * needs to do a lot of side exits, which are somewhat
56551:  * expensive.
56551:  */
56551: bool
60542: LoopProfile::isCompilationUnprofitable(JSContext *cx, uintN goodOps)
60542: {
60534:     if (!profiled)
60534:         return false;
60534: 
60542:     if (goodOps <= 22 && allOps[OP_FWDJUMP])
57802:         return true;
56551:     
56551:     /* Ensure that inner loops aren't fleeting. */
56551:     for (uintN i=0; i<numInnerLoops; i++) {
56551:         LoopProfile *prof = LookupLoopProfile(cx, innerLoops[i].top);
60542:         if (!prof || prof->unprofitable)
57802:             return true;
57802:     }
57802: 
57802:     return false;
56551: }
56551: 
56551: /* After profiling is done, this method decides whether to trace the loop. */
56551: void
56551: LoopProfile::decide(JSContext *cx)
56551: {
60534:     bool wasUndecided = undecided;
60534:     bool wasTraceOK = traceOK;
60534:     
56551:     profiled = true;
60534:     traceOK = false;
60534:     undecided = false;
56551: 
56551: #ifdef DEBUG
60158:     uintN line = js_PCToLineNumber(cx, entryScript, top);
60158: 
60158:     debug_only_printf(LC_TMProfiler, "LOOP %s:%d\n", entryScript->filename, line);
56551: 
56551:     for (uintN i=0; i<numInnerLoops; i++) {
56551:         InnerLoop &loop = innerLoops[i];
56551:         if (LoopProfile *prof = LookupLoopProfile(cx, loop.top)) {
60158:             uintN line = js_PCToLineNumber(cx, prof->entryScript, prof->top);
56551:             debug_only_printf(LC_TMProfiler, "NESTED %s:%d (%d iters)\n",
60158:                               prof->entryScript->filename, line, loop.iters);
56551:         }
56551:     }
56551:     debug_only_printf(LC_TMProfiler, "FEATURE float %d\n", allOps[OP_FLOAT]);
56551:     debug_only_printf(LC_TMProfiler, "FEATURE int %d\n", allOps[OP_INT]);
56551:     debug_only_printf(LC_TMProfiler, "FEATURE bit %d\n", allOps[OP_BIT]);
56551:     debug_only_printf(LC_TMProfiler, "FEATURE equality %d\n", allOps[OP_EQ]);
56551:     debug_only_printf(LC_TMProfiler, "FEATURE eval %d\n", allOps[OP_EVAL]);
56551:     debug_only_printf(LC_TMProfiler, "FEATURE new %d\n", allOps[OP_NEW]);
56551:     debug_only_printf(LC_TMProfiler, "FEATURE call %d\n", allOps[OP_CALL]);
60543:     debug_only_printf(LC_TMProfiler, "FEATURE arrayread %d\n", allOps[OP_ARRAY_READ]);
56562:     debug_only_printf(LC_TMProfiler, "FEATURE typedarray %d\n", allOps[OP_TYPED_ARRAY]);
56551:     debug_only_printf(LC_TMProfiler, "FEATURE fwdjump %d\n", allOps[OP_FWDJUMP]);
56551:     debug_only_printf(LC_TMProfiler, "FEATURE recursive %d\n", allOps[OP_RECURSIVE]);
56551:     debug_only_printf(LC_TMProfiler, "FEATURE shortLoop %d\n", shortLoop);
56551:     debug_only_printf(LC_TMProfiler, "FEATURE maybeShortLoop %d\n", maybeShortLoop);
56551:     debug_only_printf(LC_TMProfiler, "FEATURE numAllOps %d\n", numAllOps);
56551:     debug_only_printf(LC_TMProfiler, "FEATURE selfOps %d\n", numSelfOps);
56551:     debug_only_printf(LC_TMProfiler, "FEATURE selfOpsMult %g\n", numSelfOpsMult);
56551: #endif
56551: 
56551:     if (count(OP_RECURSIVE)) {
60157:         debug_only_print0(LC_TMProfiler, "NOTRACE: recursive\n");
56551:     } else if (count(OP_EVAL)) {
60157:         debug_only_print0(LC_TMProfiler, "NOTRACE: eval\n");
59947:     } else if (numInnerLoops > 7) {
60157:         debug_only_print0(LC_TMProfiler, "NOTRACE: >3 inner loops\n");
56551:     } else if (shortLoop) {
60157:         debug_only_print0(LC_TMProfiler, "NOTRACE: short\n");
60157:     } else if (isCompilationExpensive(cx, 4)) {
60157:         debug_only_print0(LC_TMProfiler, "NOTRACE: expensive\n");
56603:     } else if (maybeShortLoop && numInnerLoops < 2) {
60534:         if (wasUndecided) {
60157:             debug_only_print0(LC_TMProfiler, "NOTRACE: maybe short\n");
56551:         } else {
60534:             debug_only_print0(LC_TMProfiler, "UNDECIDED: maybe short\n");
60534:             undecided = true; /* Profile the loop again to see if it's still short. */
60534:         }
60534:     } else {
56551:         uintN goodOps = 0;
56551: 
56551:         /* The tracer handles these ops well because of type specialization. */
60543:         goodOps += count(OP_FLOAT)*10 + count(OP_BIT)*11 + count(OP_INT)*5 + count(OP_EQ)*15;
56551: 
56551:         /* The tracer handles these ops well because of inlining. */
56551:         goodOps += (count(OP_CALL) + count(OP_NEW))*20;
56551: 
56562:         /* The tracer specialized typed array access. */
56562:         goodOps += count(OP_TYPED_ARRAY)*10;
56562: 
60543:         /* The methodjit is faster at array writes, but the tracer is faster for reads. */
60543:         goodOps += count(OP_ARRAY_READ)*15;
60543: 
56562:         debug_only_printf(LC_TMProfiler, "FEATURE goodOps %u\n", goodOps);
56562: 
60542:         unprofitable = isCompilationUnprofitable(cx, goodOps);
60542:         if (unprofitable)
60157:             debug_only_print0(LC_TMProfiler, "NOTRACE: unprofitable\n");
60157:         else if (goodOps >= numAllOps)
56551:             traceOK = true;
56551:     }
56551: 
60158:     debug_only_printf(LC_TMProfiler, "TRACE %s:%d = %d\n", entryScript->filename, line, traceOK);
56551: 
56551:     if (traceOK) {
56551:         /* Unblacklist the inner loops. */
56551:         for (uintN i=0; i<numInnerLoops; i++) {
56551:             InnerLoop &loop = innerLoops[i];
56551:             LoopProfile *prof = LookupLoopProfile(cx, loop.top);
56551:             if (prof) {
56551:                 /*
56551:                  * Note that execOK for the inner loop is left unchanged. So even
56551:                  * if we trace the inner loop, we will never call that trace
56551:                  * on its own. We'll only call it from this trace.
56551:                  */
56551:                 prof->traceOK = true;
59972:                 if (IsBlacklisted(loop.top)) {
56551:                     debug_only_printf(LC_TMProfiler, "Unblacklisting at %d\n",
60158:                                       js_PCToLineNumber(cx, prof->entryScript, loop.top));
60158:                     Unblacklist(prof->entryScript, loop.top);
57802:                 }
57802:             }
57802:         }
59972:     }
57802: 
60534:     execOK = traceOK;
60534:     traceOK = wasTraceOK || traceOK;
60534: 
60534:     if (!traceOK && !undecided) {
56551:         debug_only_printf(LC_TMProfiler, "Blacklisting at %d\n", line);
56551:         Blacklist(top);
56551:     }
56551: 
56603:     debug_only_print0(LC_TMProfiler, "\n");
56551: }
56551: 
56551: JS_REQUIRES_STACK MonitorResult
56551: MonitorLoopEdge(JSContext* cx, uintN& inlineCallCount)
56551: {
56551:     TraceMonitor *tm = &JS_TRACE_MONITOR(cx);
56551:     if (tm->profile)
56551:         return tm->profile->profileLoopEdge(cx, inlineCallCount);
57880:     else
57880:         return RecordLoopEdge(cx, inlineCallCount);
56551: }
56551: 
56551: void
56551: AbortProfiling(JSContext *cx)
56551: {
56551:     debug_only_print0(LC_TMProfiler, "Profiling complete (aborted)\n");
56551:     TraceMonitor *tm = &JS_TRACE_MONITOR(cx);
60544:     tm->profile->profiled = true;
60544:     tm->profile->traceOK = false;
60544:     tm->profile->execOK = false;
56551:     tm->profile = NULL;
56551: }
56551: 
56551: #else /* JS_METHODJIT */
56551: 
56551: JS_REQUIRES_STACK MonitorResult
56551: MonitorLoopEdge(JSContext* cx, uintN& inlineCallCount)
56551: {
57880:     return RecordLoopEdge(cx, inlineCallCount);
56551: }
56551: 
56551: #endif /* JS_METHODJIT */
33564: 
60157: uint32
60157: GetHotloop(JSContext *cx)
60157: {
60157: #ifdef JS_METHODJIT
60157:     if (cx->profilingEnabled)
60157:         return PROFILE_HOTLOOP;
60157:     else
60157: #endif
60157:         return 1;
60157: }
60157: 
37741: } /* namespace js */
56180: 
