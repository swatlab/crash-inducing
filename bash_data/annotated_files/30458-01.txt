22705: /* -*- Mode: C++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*-
18056:  * vim: set ts=4 sw=4 et tw=99:
17181:  *
17181:  * ***** BEGIN LICENSE BLOCK *****
17181:  * Version: MPL 1.1/GPL 2.0/LGPL 2.1
17181:  *
17181:  * The contents of this file are subject to the Mozilla Public License Version
17181:  * 1.1 (the "License"); you may not use this file except in compliance with
17181:  * the License. You may obtain a copy of the License at
17181:  * http://www.mozilla.org/MPL/
17181:  *
17181:  * Software distributed under the License is distributed on an "AS IS" basis,
17181:  * WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
17181:  * for the specific language governing rights and limitations under the
17181:  * License.
17181:  *
17181:  * The Original Code is Mozilla SpiderMonkey JavaScript 1.9 code, released
17181:  * May 28, 2008.
17181:  *
17181:  * The Initial Developer of the Original Code is
17339:  *   Brendan Eich <brendan@mozilla.org>
17181:  *
17181:  * Contributor(s):
17339:  *   Andreas Gal <gal@mozilla.com>
17671:  *   Mike Shaver <shaver@mozilla.org>
17671:  *   David Anderson <danderson@mozilla.com>
17181:  *
17181:  * Alternatively, the contents of this file may be used under the terms of
17181:  * either of the GNU General Public License Version 2 or later (the "GPL"),
17181:  * or the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
17181:  * in which case the provisions of the GPL or the LGPL are applicable instead
17181:  * of those above. If you wish to allow use of your version of this file only
17181:  * under the terms of either the GPL or the LGPL, and not to allow others to
17181:  * use your version of this file under the terms of the MPL, indicate your
17181:  * decision by deleting the provisions above and replace them with the notice
17181:  * and other provisions required by the GPL or the LGPL. If you do not delete
17181:  * the provisions above, a recipient may use your version of this file under
17181:  * the terms of any one of the MPL, the GPL or the LGPL.
17181:  *
17181:  * ***** END LICENSE BLOCK ***** */
17181: 
26316: #include "jsstdint.h"
17923: #include "jsbit.h"              // low-level (NSPR-based) headers next
17923: #include "jsprf.h"
17666: #include <math.h>               // standard headers next
26053: 
26053: #if defined(_MSC_VER) || defined(__MINGW32__)
26053: #include <malloc.h>
17666: #ifdef _MSC_VER
17666: #define alloca _alloca
17666: #endif
26053: #endif
19058: #ifdef SOLARIS
19058: #include <alloca.h>
19058: #endif
25627: #include <limits.h>
17181: 
21062: #include "nanojit/nanojit.h"
25718: #include "jsapi.h"              // higher-level library and API headers
25718: #include "jsarray.h"
17421: #include "jsbool.h"
17630: #include "jscntxt.h"
17949: #include "jsdbgapi.h"
17949: #include "jsemit.h"
17630: #include "jsfun.h"
17630: #include "jsinterp.h"
17899: #include "jsiter.h"
17666: #include "jsobj.h"
17863: #include "jsopcode.h"
18115: #include "jsregexp.h"
17949: #include "jsscope.h"
17630: #include "jsscript.h"
19990: #include "jsdate.h"
20969: #include "jsstaticcheck.h"
17407: #include "jstracer.h"
28175: #include "jsxml.h"
17293: 
30283: #include "jsatominlines.h"
30283: 
17666: #include "jsautooplen.h"        // generated headers last
23075: #include "imacros.c.out"
17597: 
28175: #if JS_HAS_XML_SUPPORT
28175: #define ABORT_IF_XML(v)                                                       \
28175:     JS_BEGIN_MACRO                                                            \
28175:     if (!JSVAL_IS_PRIMITIVE(v) && OBJECT_IS_XML(BOGUS_CX, JSVAL_TO_OBJECT(v)))\
28175:         ABORT_TRACE("xml detected");                                          \
28175:     JS_END_MACRO
28175: #else
28175: #define ABORT_IF_XML(cx, v) ((void) 0)
28175: #endif
28175: 
19995: /* Never use JSVAL_IS_BOOLEAN because it restricts the value (true, false) and
19995:    the type. What you want to use is JSVAL_TAG(x) == JSVAL_BOOLEAN and then
19995:    handle the undefined case properly (bug 457363). */
19995: #undef JSVAL_IS_BOOLEAN
19995: #define JSVAL_IS_BOOLEAN(x) JS_STATIC_ASSERT(0)
19995: 
29896: JS_STATIC_ASSERT(sizeof(JSTraceType) == 1);
27541: 
20399: /* Map to translate a type tag into a printable representation. */
27541: static const char typeChar[] = "OIDXSNBF";
23075: static const char tagChar[]  = "OIDISIBI";
20399: 
25627: /* Blacklist parameters. */
25627: 
19591: /* Number of iterations of a loop where we start tracing.  That is, we don't
19592:    start tracing until the beginning of the HOTLOOP-th iteration. */
17821: #define HOTLOOP 2
17821: 
25627: /* Attempt recording this many times before blacklisting permanently. */
25937: #define BL_ATTEMPTS 2
25627: 
28409: /* Skip this many hits before attempting recording again, after an aborted attempt. */
25627: #define BL_BACKOFF 32
25627: 
17821: /* Number of times we wait to exit on a side exit before we try to extend the tree. */
18290: #define HOTEXIT 1
17821: 
25627: /* Number of times we try to extend the tree along a side exit. */
25627: #define MAXEXIT 3
25627: 
25627: /* Maximum number of peer trees allowed. */
25627: #define MAXPEERS 9
25627: 
18051: /* Max call depths for inlining. */
19070: #define MAX_CALLDEPTH 10
17852: 
18118: /* Max native stack size. */
27062: #define MAX_NATIVE_STACK_SLOTS 1024
18118: 
18133: /* Max call stack size. */
27062: #define MAX_CALL_STACK_ENTRIES 64
18133: 
27889: /* Max global object size. */
27889: #define MAX_GLOBAL_SLOTS 4096
27889: 
23447: /* Max memory needed to rebuild the interpreter stack when falling off trace. */
23447: #define MAX_INTERP_STACK_BYTES                                                \
23447:     (MAX_NATIVE_STACK_SLOTS * sizeof(jsval) +                                 \
28086:      MAX_CALL_STACK_ENTRIES * sizeof(JSInlineFrame) +                         \
28086:      sizeof(JSInlineFrame)) /* possibly slow native frame at top of stack */
23447: 
18781: /* Max number of branches per tree. */
26534: #define MAX_BRANCHES 32
18781: 
27933: #define CHECK_STATUS(expr)                                                    \
27933:     JS_BEGIN_MACRO                                                            \
27933:         JSRecordingStatus _status = (expr);                                   \
27933:         if (_status != JSRS_CONTINUE)                                        \
27933:           return _status;                                                     \
27933:     JS_END_MACRO
27933: 
21459: #ifdef JS_JIT_SPEW
27933: #define ABORT_TRACE_RV(msg, value)                                    \
27933:     JS_BEGIN_MACRO                                                            \
29883:         debug_only_printf(LC_TMAbort, "abort: %d: %s\n", __LINE__, (msg));    \
27933:         return (value);                                                       \
27933:     JS_END_MACRO
17630: #else
27933: #define ABORT_TRACE_RV(msg, value)   return (value)
17630: #endif
17630: 
27933: #define ABORT_TRACE(msg)         ABORT_TRACE_RV(msg, JSRS_STOP)
27933: #define ABORT_TRACE_ERROR(msg)   ABORT_TRACE_RV(msg, JSRS_ERROR)
27933: 
21459: #ifdef JS_JIT_SPEW
19592: struct __jitstats {
19592: #define JITSTAT(x) uint64 x;
19592: #include "jitstats.tbl"
19592: #undef JITSTAT
19623: } jitstats = { 0LL, };
19623: 
19623: JS_STATIC_ASSERT(sizeof(jitstats) % sizeof(uint64) == 0);
19592: 
19592: enum jitstat_ids {
19592: #define JITSTAT(x) STAT ## x ## ID,
19592: #include "jitstats.tbl"
19592: #undef JITSTAT
19598:     STAT_IDS_TOTAL
19592: };
19592: 
19592: static JSPropertySpec jitstats_props[] = {
19592: #define JITSTAT(x) { #x, STAT ## x ## ID, JSPROP_ENUMERATE | JSPROP_READONLY | JSPROP_PERMANENT },
19592: #include "jitstats.tbl"
19592: #undef JITSTAT
19592:     { 0 }
19592: };
19592: 
19592: static JSBool
19592: jitstats_getProperty(JSContext *cx, JSObject *obj, jsid id, jsval *vp)
19592: {
19592:     int index = -1;
19592: 
19592:     if (JSVAL_IS_STRING(id)) {
19592:         JSString* str = JSVAL_TO_STRING(id);
19592:         if (strcmp(JS_GetStringBytes(str), "HOTLOOP") == 0) {
19592:             *vp = INT_TO_JSVAL(HOTLOOP);
19592:             return JS_TRUE;
19592:         }
19592:     }
19592: 
19592:     if (JSVAL_IS_INT(id))
19592:         index = JSVAL_TO_INT(id);
19592: 
19592:     uint64 result = 0;
19592:     switch (index) {
19623: #define JITSTAT(x) case STAT ## x ## ID: result = jitstats.x; break;
19592: #include "jitstats.tbl"
19592: #undef JITSTAT
19592:       default:
19592:         *vp = JSVAL_VOID;
19592:         return JS_TRUE;
19592:     }
19592: 
19592:     if (result < JSVAL_INT_MAX) {
19592:         *vp = INT_TO_JSVAL(result);
19592:         return JS_TRUE;
19592:     }
19592:     char retstr[64];
19606:     JS_snprintf(retstr, sizeof retstr, "%llu", result);
19592:     *vp = STRING_TO_JSVAL(JS_NewStringCopyZ(cx, retstr));
19592:     return JS_TRUE;
19592: }
19592: 
19592: JSClass jitstats_class = {
19592:     "jitstats",
19592:     JSCLASS_HAS_PRIVATE,
19592:     JS_PropertyStub,       JS_PropertyStub,
19592:     jitstats_getProperty,  JS_PropertyStub,
19592:     JS_EnumerateStub,      JS_ResolveStub,
19592:     JS_ConvertStub,        JS_FinalizeStub,
19592:     JSCLASS_NO_OPTIONAL_MEMBERS
19592: };
19592: 
19592: void
19592: js_InitJITStatsClass(JSContext *cx, JSObject *glob)
19592: {
19592:     JS_InitClass(cx, glob, NULL, &jitstats_class, NULL, 0, jitstats_props, NULL, NULL, NULL);
19592: }
19592: 
19623: #define AUDIT(x) (jitstats.x++)
17726: #else
17726: #define AUDIT(x) ((void)0)
21459: #endif /* JS_JIT_SPEW */
17726: 
17870: #define INS_CONST(c)        addName(lir->insImm(c), #c)
26397: #define INS_CONSTPTR(p)     addName(lir->insImmPtr(p), #p)
25218: #define INS_CONSTFUNPTR(p)  addName(lir->insImmPtr(JS_FUNC_TO_DATA_PTR(void*, p)), #p)
28086: #define INS_CONSTWORD(v)    addName(lir->insImmPtr((void *) v), #v)
30248: #define INS_VOID()          INS_CONST(JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID))
17870: 
17409: using namespace avmplus;
17293: using namespace nanojit;
17293: 
17409: static GC gc = GC();
18056: static avmplus::AvmCore s_core = avmplus::AvmCore();
18056: static avmplus::AvmCore* core = &s_core;
17185: 
21459: #ifdef JS_JIT_SPEW
21433: void
28244: js_DumpPeerStability(JSTraceMonitor* tm, const void* ip, JSObject* globalObj, uint32 globalShape, uint32 argc);
21433: #endif
21433: 
29883: /* We really need a better way to configure the JIT. Shaver, where is
29883:    my fancy JIT object? */
29883: /* NB: this is raced on, if jstracer.cpp should ever be running MT.
29883:    I think it's harmless tho. */
26545: static bool did_we_check_processor_features = false;
17997: 
29883: /* ------ Debug logging control ------ */
29883: 
29883: /* All the logging control stuff lives in here.  It is shared between
29883:    all threads, but I think that's OK. */
29883: LogControl js_LogController;
29883: 
21459: #ifdef JS_JIT_SPEW
29883: 
29883: /* NB: this is raced on too, if jstracer.cpp should ever be running MT.
29883:    Also harmless. */
29883: static bool did_we_set_up_debug_logging = false;
29883: 
29883: static void
29883: js_InitJITLogController ( void )
29883: {
29883:     char *tm, *tmf;
29883:     uint32_t bits;
29883: 
29883:     js_LogController.lcbits = 0;
29883: 
29883:     tm = getenv("TRACEMONKEY");
29883:     if (tm) goto help;
29883: 
29883:     tmf = getenv("TMFLAGS");
29883:     if (!tmf) return;
29883: 
29883:     /* This is really a cheap hack as far as flag decoding goes. */
29883:     if (strstr(tmf, "help")) goto help;
29883: 
29883:     bits = 0;
29883:     /* flags for jstracer.cpp */
29883:     if (strstr(tmf, "minimal"))     bits |= LC_TMMinimal;
29883:     if (strstr(tmf, "tracer"))      bits |= LC_TMTracer;
29883:     if (strstr(tmf, "recorder"))    bits |= LC_TMRecorder;
29883:     if (strstr(tmf, "patcher"))     bits |= LC_TMPatcher;
29883:     if (strstr(tmf, "abort"))       bits |= LC_TMAbort;
29883:     if (strstr(tmf, "stats"))       bits |= LC_TMStats;
29883:     if (strstr(tmf, "regexp"))      bits |= LC_TMRegexp;
29883:     /* flags for nanojit */
29883:     if (strstr(tmf, "liveness"))    bits |= LC_Liveness;
29883:     if (strstr(tmf, "readlir"))     bits |= LC_ReadLIR;
29883:     if (strstr(tmf, "aftersf_sp"))  bits |= LC_AfterSF_SP;
29883:     if (strstr(tmf, "aftersf_rp"))  bits |= LC_AfterSF_RP;
29883:     if (strstr(tmf, "regalloc"))    bits |= LC_RegAlloc;
29883:     if (strstr(tmf, "assembly"))    bits |= LC_Assembly;
29883:     if (strstr(tmf, "nocodeaddrs")) bits |= LC_NoCodeAddrs;
29883: 
29892:     if (strstr(tmf, "full")) {
29892:         bits |= LC_TMMinimal | LC_TMTracer | LC_TMRecorder | LC_TMPatcher | LC_TMAbort |
29892:                 LC_TMAbort   | LC_TMStats  | LC_TMRegexp   | LC_Liveness  | LC_ReadLIR |
30277:                 LC_AfterSF_SP | LC_AfterSF_RP | LC_RegAlloc | LC_Assembly;
29892:     }
29892: 
29883:     js_LogController.lcbits = bits;
29883:     return;
29883: 
29883:    help:
29883:     fflush(NULL);
29883:     printf("\n");
29883:     printf("Debug output control help summary for TraceMonkey:\n");
29883:     printf("\n");
29883:     printf("TRACEMONKEY= is no longer used; use TMFLAGS= "
29883:            "instead.\n");
29883:     printf("\n");
29883:     printf("usage: TMFLAGS=option,option,option,... where options can be:\n");
29883:     printf("   help         show this message\n");
29883:     printf("   ------ options for jstracer & jsregexp ------\n");
29883:     printf("   minimal      ultra-minimalist output; try this first\n");
29892:     printf("   full         everything (old verbosity)\n");
29883:     printf("   tracer       tracer lifetime (FIXME:better description)\n");
29883:     printf("   recorder     trace recording stuff (FIXME:better description)\n");
29883:     printf("   patcher      patching stuff (FIXME:better description)\n");
29883:     printf("   abort        show trace recording aborts\n");
29883:     printf("   stats        show trace recording stats\n");
29883:     printf("   regexp       show compilation & entry for regexps\n");
29883:     printf("   ------ options for Nanojit ------\n");
29883:     printf("   liveness     show LIR liveness at start of rdr pipeline\n");
29883:     printf("   readlir      show LIR as it enters the reader pipeline\n");
29883:     printf("   aftersf_sp   show LIR after StackFilter(sp)\n");
29883:     printf("   aftersf_rp   show LIR after StackFilter(rp)\n");
29883:     printf("   regalloc     show regalloc details\n");
29883:     printf("   assembly     show final aggregated assembly code\n");
29883:     printf("   nocodeaddrs  don't show code addresses in assembly listings\n");
29883:     printf("\n");
29883:     printf("Exiting now.  Bye.\n");
29883:     printf("\n");
29883:     exit(0);
29883:     /*NOTREACHED*/
29883: }
18260: #endif
18260: 
29894: #if defined DEBUG
29893: static const char*
29894: getExitName(ExitType type)
29893: {
29893:     static const char* exitNames[] =
29893:     {
29893:     #define MAKE_EXIT_STRING(x) #x,
29893:     JS_TM_EXITCODES(MAKE_EXIT_STRING)
29893:     #undef MAKE_EXIT_STRING
29895:     NULL
29893:     };
29893: 
29900:     JS_ASSERT(type < TOTAL_EXIT_TYPES);
29893: 
29893:     return exitNames[type];
29893: }
29894: #endif
29893: 
17981: /* The entire VM shares one oracle. Collisions and concurrent updates are tolerated and worst
17981:    case cause performance regressions. */
19535: static Oracle oracle;
17981: 
17596: Tracker::Tracker()
17213: {
17293:     pagelist = 0;
17293: }
17239: 
17596: Tracker::~Tracker()
17293: {
17293:     clear();
17293: }
17293: 
17596: jsuword
17596: Tracker::getPageBase(const void* v) const
17293: {
17424:     return jsuword(v) & ~jsuword(NJ_PAGE_SIZE-1);
17293: }
17293: 
17596: struct Tracker::Page*
17596: Tracker::findPage(const void* v) const
17293: {
17424:     jsuword base = getPageBase(v);
17596:     struct Tracker::Page* p = pagelist;
17293:     while (p) {
17356:         if (p->base == base) {
17293:             return p;
17356:         }
17293:         p = p->next;
17293:     }
17293:     return 0;
17293: }
17293: 
17596: struct Tracker::Page*
17596: Tracker::addPage(const void* v) {
17424:     jsuword base = getPageBase(v);
17314:     struct Tracker::Page* p = (struct Tracker::Page*)
17596:         GC::Alloc(sizeof(*p) - sizeof(p->map) + (NJ_PAGE_SIZE >> 2) * sizeof(LIns*));
17293:     p->base = base;
17293:     p->next = pagelist;
17293:     pagelist = p;
17293:     return p;
17213: }
17213: 
17596: void
17596: Tracker::clear()
17213: {
17293:     while (pagelist) {
17293:         Page* p = pagelist;
17293:         pagelist = pagelist->next;
17293:         GC::Free(p);
17259:     }
17293: }
17293: 
17773: bool
17773: Tracker::has(const void *v) const
17773: {
17811:     return get(v) != NULL;
17811: }
17811: 
18230: #if defined NANOJIT_64BIT
18230: #define PAGEMASK 0x7ff
18230: #else
18230: #define PAGEMASK 0xfff
18230: #endif
18230: 
17811: LIns*
17811: Tracker::get(const void* v) const
17811: {
17773:     struct Tracker::Page* p = findPage(v);
17773:     if (!p)
17811:         return NULL;
18230:     return p->map[(jsuword(v) & PAGEMASK) >> 2];
17219: }
17247: 
17596: void
17596: Tracker::set(const void* v, LIns* i)
17247: {
17596:     struct Tracker::Page* p = findPage(v);
17293:     if (!p)
17293:         p = addPage(v);
18230:     p->map[(jsuword(v) & PAGEMASK) >> 2] = i;
17293: }
17293: 
28268: static inline jsuint argSlots(JSStackFrame* fp)
28244: {
28244:     return JS_MAX(fp->argc, fp->fun->nargs);
28244: }
28244: 
17464: static inline bool isNumber(jsval v)
17464: {
17464:     return JSVAL_IS_INT(v) || JSVAL_IS_DOUBLE(v);
17464: }
17464: 
17464: static inline jsdouble asNumber(jsval v)
17464: {
17464:     JS_ASSERT(isNumber(v));
17464:     if (JSVAL_IS_DOUBLE(v))
17464:         return *JSVAL_TO_DOUBLE(v);
17464:     return (jsdouble)JSVAL_TO_INT(v);
17464: }
17464: 
17479: static inline bool isInt32(jsval v)
17479: {
17479:     if (!isNumber(v))
17479:         return false;
17479:     jsdouble d = asNumber(v);
17759:     jsint i;
17759:     return JSDOUBLE_IS_INT(d, i);
17479: }
17479: 
26274: static inline jsint asInt32(jsval v)
26274: {
26274:     JS_ASSERT(isNumber(v));
26274:     if (JSVAL_IS_INT(v))
26274:         return JSVAL_TO_INT(v);
26274: #ifdef DEBUG
26274:     jsint i;
26274:     JS_ASSERT(JSDOUBLE_IS_INT(*JSVAL_TO_DOUBLE(v), i));
26274: #endif
26274:     return jsint(*JSVAL_TO_DOUBLE(v));
25099: }
25099: 
29896: /* Return TT_DOUBLE for all numbers (int and double) and the tag otherwise. */
29896: static inline JSTraceType getPromotedType(jsval v)
19576: {
27541:     if (JSVAL_IS_INT(v))
29896:         return TT_DOUBLE;
27541:     if (JSVAL_IS_OBJECT(v)) {
27541:         if (JSVAL_IS_NULL(v))
29896:             return TT_NULL;
27541:         if (HAS_FUNCTION_CLASS(JSVAL_TO_OBJECT(v)))
29896:             return TT_FUNCTION;
29896:         return TT_OBJECT;
29896:     }
29896:     uint8_t tag = JSVAL_TAG(v);
29896:     JS_ASSERT(tag == JSVAL_DOUBLE || tag == JSVAL_STRING || tag == JSVAL_BOOLEAN);
29896:     JS_STATIC_ASSERT(TT_DOUBLE == JSVAL_DOUBLE);
29896:     JS_STATIC_ASSERT(TT_STRING == JSVAL_STRING);
29896:     JS_STATIC_ASSERT(TT_PSEUDOBOOLEAN == JSVAL_BOOLEAN);
29896:     return JSTraceType(tag);
29896: }
29896: 
29896: /* Return TT_INT32 for all whole numbers that fit into signed 32-bit and the tag otherwise. */
29896: static inline JSTraceType getCoercedType(jsval v)
17891: {
27541:     if (isInt32(v))
29896:         return TT_INT32;
27541:     if (JSVAL_IS_OBJECT(v)) {
27541:         if (JSVAL_IS_NULL(v))
29896:             return TT_NULL;
27541:         if (HAS_FUNCTION_CLASS(JSVAL_TO_OBJECT(v)))
29896:             return TT_FUNCTION;
29896:         return TT_OBJECT;
29896:     }
29896:     uint8_t tag = JSVAL_TAG(v);
29896:     JS_ASSERT(tag == JSVAL_DOUBLE || tag == JSVAL_STRING || tag == JSVAL_BOOLEAN);
29896:     JS_STATIC_ASSERT(TT_DOUBLE == JSVAL_DOUBLE);
29896:     JS_STATIC_ASSERT(TT_STRING == JSVAL_STRING);
29896:     JS_STATIC_ASSERT(TT_PSEUDOBOOLEAN == JSVAL_BOOLEAN);
29896:     return JSTraceType(tag);
17891: }
17891: 
22613: /*
22613:  * Constant seed and accumulate step borrowed from the DJB hash.
22613:  */
22613: 
22613: #define ORACLE_MASK (ORACLE_SIZE - 1)
24307: #define FRAGMENT_TABLE_MASK (FRAGMENT_TABLE_SIZE - 1)
24307: #define HASH_SEED 5381
22613: 
22613: static inline void
24307: hash_accum(uintptr_t& h, uintptr_t i, uintptr_t mask)
24307: {
24307:     h = ((h << 5) + h + (mask & i)) & mask;
22613: }
22613: 
23456: JS_REQUIRES_STACK static inline int
22613: stackSlotHash(JSContext* cx, unsigned slot)
22613: {
24307:     uintptr_t h = HASH_SEED;
24307:     hash_accum(h, uintptr_t(cx->fp->script), ORACLE_MASK);
24307:     hash_accum(h, uintptr_t(cx->fp->regs->pc), ORACLE_MASK);
24307:     hash_accum(h, uintptr_t(slot), ORACLE_MASK);
22614:     return int(h);
22614: }
22614: 
23456: JS_REQUIRES_STACK static inline int
22613: globalSlotHash(JSContext* cx, unsigned slot)
22613: {
24307:     uintptr_t h = HASH_SEED;
22613:     JSStackFrame* fp = cx->fp;
22613: 
22613:     while (fp->down)
22613:         fp = fp->down;
22613: 
24307:     hash_accum(h, uintptr_t(fp->script), ORACLE_MASK);
24307:     hash_accum(h, uintptr_t(OBJ_SHAPE(JS_GetGlobalForObject(cx, fp->scopeChain))),
24307:                ORACLE_MASK);
24307:     hash_accum(h, uintptr_t(slot), ORACLE_MASK);
22614:     return int(h);
22613: }
22613: 
29354: static inline int
29354: pcHash(jsbytecode* pc)
29354: {
29354:     return int(uintptr_t(pc) & ORACLE_MASK);
29354: }
29354: 
24290: Oracle::Oracle()
24290: {
28171:     /* Grow the oracle bitsets to their (fixed) size here, once. */
28171:     _stackDontDemote.set(&gc, ORACLE_SIZE-1);
28171:     _globalDontDemote.set(&gc, ORACLE_SIZE-1);
24290:     clear();
24290: }
24290: 
17981: /* Tell the oracle that a certain global variable should not be demoted. */
23456: JS_REQUIRES_STACK void
22613: Oracle::markGlobalSlotUndemotable(JSContext* cx, unsigned slot)
22613: {
22613:     _globalDontDemote.set(&gc, globalSlotHash(cx, slot));
17981: }
17981: 
17981: /* Consult with the oracle whether we shouldn't demote a certain global variable. */
23456: JS_REQUIRES_STACK bool
22613: Oracle::isGlobalSlotUndemotable(JSContext* cx, unsigned slot) const
22613: {
22613:     return _globalDontDemote.get(globalSlotHash(cx, slot));
17981: }
17981: 
29354: /* Tell the oracle that a certain slot at a certain stack slot should not be demoted. */
23456: JS_REQUIRES_STACK void
22613: Oracle::markStackSlotUndemotable(JSContext* cx, unsigned slot)
22613: {
22613:     _stackDontDemote.set(&gc, stackSlotHash(cx, slot));
17981: }
17981: 
17981: /* Consult with the oracle whether we shouldn't demote a certain slot. */
23456: JS_REQUIRES_STACK bool
22613: Oracle::isStackSlotUndemotable(JSContext* cx, unsigned slot) const
22613: {
22613:     return _stackDontDemote.get(stackSlotHash(cx, slot));
17981: }
17981: 
29354: /* Tell the oracle that a certain slot at a certain bytecode location should not be demoted. */
29354: void
29354: Oracle::markInstructionUndemotable(jsbytecode* pc)
29354: {
29354:     _pcDontDemote.set(&gc, pcHash(pc));
29354: }
29354: 
29354: /* Consult with the oracle whether we shouldn't demote a certain bytecode location. */
29354: bool
29354: Oracle::isInstructionUndemotable(jsbytecode* pc) const
29354: {
29354:     return _pcDontDemote.get(pcHash(pc));
29354: }
29354: 
24290: void
24290: Oracle::clearDemotability()
18273: {
22613:     _stackDontDemote.reset();
22613:     _globalDontDemote.reset();
29354:     _pcDontDemote.reset();
22613: }
22613: 
28105: 
28105: struct PCHashEntry : public JSDHashEntryStub {
28105:     size_t          count;
28105: };
28105: 
28105: #define PC_HASH_COUNT 1024
28105: 
25627: static void
28105: js_Blacklist(jsbytecode* pc)
28105: {
29875:     AUDIT(blacklisted);
25627:     JS_ASSERT(*pc == JSOP_LOOP || *pc == JSOP_NOP);
25627:     *pc = JSOP_NOP;
25627: }
25627: 
25627: static void
28105: js_Backoff(JSContext *cx, jsbytecode* pc, Fragment* tree=NULL)
28105: {
28105:     JSDHashTable *table = &JS_TRACE_MONITOR(cx).recordAttempts;
28105: 
28105:     if (table->ops) {
28105:         PCHashEntry *entry = (PCHashEntry *)
28105:             JS_DHashTableOperate(table, pc, JS_DHASH_ADD);
28105: 
28105:         if (entry) {
28105:             if (!entry->key) {
28105:                 entry->key = pc;
28105:                 JS_ASSERT(entry->count == 0);
28105:             }
28105:             JS_ASSERT(JS_DHASH_ENTRY_IS_LIVE(&(entry->hdr)));
28105:             if (entry->count++ > (BL_ATTEMPTS * MAXPEERS)) {
28105:                 entry->count = 0;
28105:                 js_Blacklist(pc);
25627:                 return;
25627:             }
28105:         }
28105:     }
28105: 
28105:     if (tree) {
25627:         tree->hits() -= BL_BACKOFF;
28105: 
28105:         /*
28105:          * In case there is no entry or no table (due to OOM) or some
28105:          * serious imbalance in the recording-attempt distribution on a
28105:          * multitree, give each tree another chance to blacklist here as
28105:          * well.
28105:          */
28105:         if (++tree->recordAttempts > BL_ATTEMPTS)
28105:             js_Blacklist(pc);
28105:     }
28105: }
28105: 
28105: static void
28105: js_resetRecordingAttempts(JSContext *cx, jsbytecode* pc)
28105: {
28105:     JSDHashTable *table = &JS_TRACE_MONITOR(cx).recordAttempts;
28105:     if (table->ops) {
28105:         PCHashEntry *entry = (PCHashEntry *)
28105:             JS_DHashTableOperate(table, pc, JS_DHASH_LOOKUP);
28105: 
28105:         if (JS_DHASH_ENTRY_IS_FREE(&(entry->hdr)))
28105:             return;
28105:         JS_ASSERT(JS_DHASH_ENTRY_IS_LIVE(&(entry->hdr)));
28105:         entry->count = 0;
28105:     }
25627: }
25627: 
24307: static inline size_t
28244: fragmentHash(const void *ip, JSObject* globalObj, uint32 globalShape, uint32 argc)
24307: {
24307:     uintptr_t h = HASH_SEED;
24307:     hash_accum(h, uintptr_t(ip), FRAGMENT_TABLE_MASK);
26819:     hash_accum(h, uintptr_t(globalObj), FRAGMENT_TABLE_MASK);
24307:     hash_accum(h, uintptr_t(globalShape), FRAGMENT_TABLE_MASK);
28244:     hash_accum(h, uintptr_t(argc), FRAGMENT_TABLE_MASK);
24307:     return size_t(h);
24307: }
24307: 
28244: /*
28244:  * argc is cx->fp->argc at the trace loop header, i.e., the number of arguments
28244:  * pushed for the innermost JS frame. This is required as part of the fragment
28244:  * key because the fragment will write those arguments back to the interpreter
28244:  * stack when it exits, using its typemap, which implicitly incorporates a given
28244:  * value of argc. Without this feature, a fragment could be called as an inner
28244:  * tree with two different values of argc, and entry type checking or exit
28244:  * frame synthesis could crash.
28244:  */
24307: struct VMFragment : public Fragment
24307: {
28244:     VMFragment(const void* _ip, JSObject* _globalObj, uint32 _globalShape, uint32 _argc) :
24307:         Fragment(_ip),
24307:         next(NULL),
26819:         globalObj(_globalObj),
28244:         globalShape(_globalShape),
28244:         argc(_argc)
24307:     {}
24307:     VMFragment* next;
26819:     JSObject* globalObj;
24307:     uint32 globalShape;
28244:     uint32 argc;
24307: };
24307: 
24307: static VMFragment*
28244: getVMFragment(JSTraceMonitor* tm, const void *ip, JSObject* globalObj, uint32 globalShape,
28244:               uint32 argc)
28244: {
28244:     size_t h = fragmentHash(ip, globalObj, globalShape, argc);
24307:     VMFragment* vf = tm->vmfragments[h];
24307:     while (vf &&
26819:            ! (vf->globalObj == globalObj &&
26819:               vf->globalShape == globalShape &&
28244:               vf->ip == ip &&
28244:               vf->argc == argc)) {
24307:         vf = vf->next;
24307:     }
24307:     return vf;
24307: }
24307: 
26818: static VMFragment*
28244: getLoop(JSTraceMonitor* tm, const void *ip, JSObject* globalObj, uint32 globalShape,
28244:         uint32 argc)
28244: {
28244:     return getVMFragment(tm, ip, globalObj, globalShape, argc);
24307: }
24307: 
24307: static Fragment*
28244: getAnchor(JSTraceMonitor* tm, const void *ip, JSObject* globalObj, uint32 globalShape,
28244:           uint32 argc)
28244: {
28244:     VMFragment *f = new (&gc) VMFragment(ip, globalObj, globalShape, argc);
24307:     JS_ASSERT(f);
24307: 
28244:     Fragment *p = getVMFragment(tm, ip, globalObj, globalShape, argc);
24307: 
24307:     if (p) {
24307:         f->first = p;
24307:         /* append at the end of the peer list */
24307:         Fragment* next;
24307:         while ((next = p->peer) != NULL)
24307:             p = next;
24307:         p->peer = f;
24307:     } else {
24307:         /* this is the first fragment */
24307:         f->first = f;
28244:         size_t h = fragmentHash(ip, globalObj, globalShape, argc);
24307:         f->next = tm->vmfragments[h];
24307:         tm->vmfragments[h] = f;
24307:     }
24307:     f->anchor = f;
24307:     f->root = f;
24307:     f->kind = LoopTrace;
24307:     return f;
24307: }
24307: 
28913: #ifdef DEBUG
28913: static void
28913: ensureTreeIsUnique(JSTraceMonitor* tm, VMFragment* f, TreeInfo* ti)
28913: {
28913:     JS_ASSERT(f->root == f);
28913:     /*
28913:      * Check for duplicate entry type maps.  This is always wrong and hints at
28913:      * trace explosion since we are trying to stabilize something without
28913:      * properly connecting peer edges.
28913:      */
28913:     TreeInfo* ti_other;
28913:     for (Fragment* peer = getLoop(tm, f->ip, f->globalObj, f->globalShape, f->argc);
28913:          peer != NULL;
28913:          peer = peer->peer) {
28913:         if (!peer->code() || peer == f)
28913:             continue;
28913:         ti_other = (TreeInfo*)peer->vmprivate;
28913:         JS_ASSERT(ti_other);
28913:         JS_ASSERT(!ti->typeMap.matches(ti_other->typeMap));
28913:     }
28913: }
28913: #endif
28913: 
25937: static void
28244: js_AttemptCompilation(JSContext *cx, JSTraceMonitor* tm, JSObject* globalObj, jsbytecode* pc,
28244:                       uint32 argc)
25937: {
26726:     /*
26726:      * If we already permanently blacklisted the location, undo that.
26726:      */
26726:     JS_ASSERT(*(jsbytecode*)pc == JSOP_NOP || *(jsbytecode*)pc == JSOP_LOOP);
26726:     *(jsbytecode*)pc = JSOP_LOOP;
28105:     js_resetRecordingAttempts(cx, pc);
26726: 
26726:     /*
26726:      * Breath new live into all peer fragments at the designated loop header.
26726:      */
28244:     Fragment* f = (VMFragment*)getLoop(tm, pc, globalObj, OBJ_SHAPE(globalObj),
28244:                                        argc);
26726:     if (!f) {
26726:         /*
26726:          * If the global object's shape changed, we can't easily find the
26726:          * corresponding loop header via a hash table lookup. In this
26726:          * we simply bail here and hope that the fragment has another
26726:          * outstanding compilation attempt. This case is extremely rare.
26726:          */
26726:         return;
26726:     }
25937:     JS_ASSERT(f->root == f);
25937:     f = f->first;
25937:     while (f) {
25937:         JS_ASSERT(f->root == f);
25937:         --f->recordAttempts;
25937:         f->hits() = HOTLOOP;
25937:         f = f->peer;
25937:     }
25937: }
18273: 
28088: // Forward declarations.
20923: JS_DEFINE_CALLINFO_1(static, DOUBLE, i2f,  INT32, 1, 1)
20923: JS_DEFINE_CALLINFO_1(static, DOUBLE, u2f, UINT32, 1, 1)
20424: 
18773: static bool isi2f(LInsp i)
18773: {
18773:     if (i->isop(LIR_i2f))
18773:         return true;
18773: 
26545:     if (nanojit::AvmCore::config.soft_float &&
26545:         i->isop(LIR_qjoin) &&
18773:         i->oprnd1()->isop(LIR_call) &&
18773:         i->oprnd2()->isop(LIR_callh))
18773:     {
20915:         if (i->oprnd1()->callInfo() == &i2f_ci)
18773:             return true;
18773:     }
18773: 
18773:     return false;
18773: }
18773: 
18773: static bool isu2f(LInsp i)
18773: {
18773:     if (i->isop(LIR_u2f))
18773:         return true;
18773: 
26545:     if (nanojit::AvmCore::config.soft_float &&
26545:         i->isop(LIR_qjoin) &&
18773:         i->oprnd1()->isop(LIR_call) &&
18773:         i->oprnd2()->isop(LIR_callh))
18773:     {
20915:         if (i->oprnd1()->callInfo() == &u2f_ci)
18773:             return true;
18773:     }
18773: 
18773:     return false;
18773: }
18773: 
18773: static LInsp iu2fArg(LInsp i)
18773: {
26545:     if (nanojit::AvmCore::config.soft_float &&
26545:         i->isop(LIR_qjoin))
26545:     {
18773:         return i->oprnd1()->arg(0);
26545:     }
18773: 
18773:     return i->oprnd1();
18773: }
18773: 
18773: 
17451: static LIns* demote(LirWriter *out, LInsp i)
17451: {
17451:     if (i->isCall())
17451:         return callArgN(i, 0);
18773:     if (isi2f(i) || isu2f(i))
18773:         return iu2fArg(i);
17997:     if (i->isconst())
17997:         return i;
17451:     AvmAssert(i->isconstq());
28182:     double cf = i->imm64f();
17451:     int32_t ci = cf > 0x7fffffff ? uint32_t(cf) : int32_t(cf);
17451:     return out->insImm(ci);
17451: }
17451: 
17451: static bool isPromoteInt(LIns* i)
17451: {
26393:     if (isi2f(i) || i->isconst())
26393:         return true;
26393:     if (!i->isconstq())
26393:         return false;
28182:     jsdouble d = i->imm64f();
26393:     return d == jsdouble(jsint(d)) && !JSDOUBLE_IS_NEGZERO(d);
17451: }
17451: 
17451: static bool isPromoteUint(LIns* i)
17451: {
26393:     if (isu2f(i) || i->isconst())
26393:         return true;
26393:     if (!i->isconstq())
26393:         return false;
28182:     jsdouble d = i->imm64f();
26393:     return d == jsdouble(jsuint(d)) && !JSDOUBLE_IS_NEGZERO(d);
17451: }
17451: 
17451: static bool isPromote(LIns* i)
17451: {
17758:     return isPromoteInt(i) || isPromoteUint(i);
17451: }
17451: 
17800: static bool isconst(LIns* i, int32_t c)
17800: {
28182:     return i->isconst() && i->imm32() == c;
17800: }
17800: 
29354: /*
29354:  * Determine whether this operand is guaranteed to not overflow the specified
29354:  * integer operation.
29354:  */
29354: static bool overflowSafe(LOpcode op, LIns* i)
17796: {
17796:     LIns* c;
29354:     switch (op) {
29354:       case LIR_add:
29354:       case LIR_sub:
17796:           return (i->isop(LIR_and) && ((c = i->oprnd2())->isconst()) &&
28182:                   ((c->imm32() & 0xc0000000) == 0)) ||
17796:                  (i->isop(LIR_rsh) && ((c = i->oprnd2())->isconst()) &&
28182:                   ((c->imm32() > 0)));
29354:     default:
29354:         JS_ASSERT(op == LIR_mul);
29354:     }
29354:     return (i->isop(LIR_and) && ((c = i->oprnd2())->isconst()) &&
29354:             ((c->imm32() & 0xffff0000) == 0)) ||
29354:            (i->isop(LIR_ush) && ((c = i->oprnd2())->isconst()) &&
29354:             ((c->imm32() >= 16)));
17796: }
17796: 
26545: /* soft float support */
20424: 
28088: static jsdouble FASTCALL
28088: fneg(jsdouble x)
28088: {
28088:     return -x;
28088: }
20923: JS_DEFINE_CALLINFO_1(static, DOUBLE, fneg, DOUBLE, 1, 1)
28088: 
28088: static jsdouble FASTCALL
28088: i2f(int32 i)
28088: {
28088:     return i;
28088: }
28088: 
28088: static jsdouble FASTCALL
28088: u2f(jsuint u)
28088: {
28088:     return u;
28088: }
28088: 
28088: static int32 FASTCALL
28088: fcmpeq(jsdouble x, jsdouble y)
28088: {
28088:     return x==y;
28088: }
20923: JS_DEFINE_CALLINFO_2(static, INT32, fcmpeq, DOUBLE, DOUBLE, 1, 1)
28088: 
28088: static int32 FASTCALL
28088: fcmplt(jsdouble x, jsdouble y)
28088: {
28088:     return x < y;
28088: }
20923: JS_DEFINE_CALLINFO_2(static, INT32, fcmplt, DOUBLE, DOUBLE, 1, 1)
28088: 
28088: static int32 FASTCALL
28088: fcmple(jsdouble x, jsdouble y)
28088: {
28088:     return x <= y;
28088: }
20923: JS_DEFINE_CALLINFO_2(static, INT32, fcmple, DOUBLE, DOUBLE, 1, 1)
28088: 
28088: static int32 FASTCALL
28088: fcmpgt(jsdouble x, jsdouble y)
28088: {
28088:     return x > y;
28088: }
20923: JS_DEFINE_CALLINFO_2(static, INT32, fcmpgt, DOUBLE, DOUBLE, 1, 1)
28088: 
28088: static int32 FASTCALL
28088: fcmpge(jsdouble x, jsdouble y)
28088: {
28088:     return x >= y;
28088: }
20923: JS_DEFINE_CALLINFO_2(static, INT32, fcmpge, DOUBLE, DOUBLE, 1, 1)
28088: 
28088: static jsdouble FASTCALL
28088: fmul(jsdouble x, jsdouble y)
28088: {
28088:     return x * y;
28088: }
20923: JS_DEFINE_CALLINFO_2(static, DOUBLE, fmul, DOUBLE, DOUBLE, 1, 1)
28088: 
28088: static jsdouble FASTCALL
28088: fadd(jsdouble x, jsdouble y)
28088: {
28088:     return x + y;
28088: }
20923: JS_DEFINE_CALLINFO_2(static, DOUBLE, fadd, DOUBLE, DOUBLE, 1, 1)
28088: 
28088: static jsdouble FASTCALL
28088: fdiv(jsdouble x, jsdouble y)
28088: {
28088:     return x / y;
28088: }
20923: JS_DEFINE_CALLINFO_2(static, DOUBLE, fdiv, DOUBLE, DOUBLE, 1, 1)
28088: 
28088: static jsdouble FASTCALL
28088: fsub(jsdouble x, jsdouble y)
28088: {
28088:     return x - y;
28088: }
20923: JS_DEFINE_CALLINFO_2(static, DOUBLE, fsub, DOUBLE, DOUBLE, 1, 1)
20923: 
18773: class SoftFloatFilter: public LirWriter
18773: {
18773: public:
18773:     SoftFloatFilter(LirWriter* out):
18773:         LirWriter(out)
18773:     {
18773:     }
18773: 
20408:     LInsp quadCall(const CallInfo *ci, LInsp args[]) {
18773:         LInsp qlo, qhi;
18773: 
20408:         qlo = out->insCall(ci, args);
18773:         qhi = out->ins1(LIR_callh, qlo);
18773:         return out->qjoin(qlo, qhi);
18773:     }
18773: 
18773:     LInsp ins1(LOpcode v, LInsp s0)
18773:     {
18773:         if (v == LIR_fneg)
20915:             return quadCall(&fneg_ci, &s0);
18773: 
18773:         if (v == LIR_i2f)
20915:             return quadCall(&i2f_ci, &s0);
18773: 
18773:         if (v == LIR_u2f)
20915:             return quadCall(&u2f_ci, &s0);
18773: 
18773:         return out->ins1(v, s0);
18773:     }
18773: 
18773:     LInsp ins2(LOpcode v, LInsp s0, LInsp s1)
18773:     {
18773:         LInsp args[2];
18773:         LInsp bv;
18773: 
18773:         // change the numeric value and order of these LIR opcodes and die
18773:         if (LIR_fadd <= v && v <= LIR_fdiv) {
20915:             static const CallInfo *fmap[] = { &fadd_ci, &fsub_ci, &fmul_ci, &fdiv_ci };
18773: 
18773:             args[0] = s1;
18773:             args[1] = s0;
18773: 
18773:             return quadCall(fmap[v - LIR_fadd], args);
18773:         }
18773: 
18773:         if (LIR_feq <= v && v <= LIR_fge) {
20915:             static const CallInfo *fmap[] = { &fcmpeq_ci, &fcmplt_ci, &fcmpgt_ci, &fcmple_ci, &fcmpge_ci };
18773: 
18773:             args[0] = s1;
18773:             args[1] = s0;
18773: 
18773:             bv = out->insCall(fmap[v - LIR_feq], args);
18773:             return out->ins2(LIR_eq, bv, out->insImm(1));
18773:         }
18773: 
18773:         return out->ins2(v, s0, s1);
18773:     }
18773: 
20408:     LInsp insCall(const CallInfo *ci, LInsp args[])
18773:     {
18773:         // if the return type is ARGSIZE_F, we have
18773:         // to do a quadCall ( qjoin(call,callh) )
20408:         if ((ci->_argtypes & 3) == ARGSIZE_F)
20408:             return quadCall(ci, args);
20408: 
20408:         return out->insCall(ci, args);
18773:     }
18773: };
18773: 
17451: class FuncFilter: public LirWriter
17451: {
17451: public:
21799:     FuncFilter(LirWriter* out):
21799:         LirWriter(out)
17451:     {
17451:     }
17451: 
25213:     LInsp ins2(LOpcode v, LInsp s0, LInsp s1)
17451:     {
17451:         if (s0 == s1 && v == LIR_feq) {
17451:             if (isPromote(s0)) {
17451:                 // double(int) and double(uint) cannot be nan
17451:                 return insImm(1);
17451:             }
17451:             if (s0->isop(LIR_fmul) || s0->isop(LIR_fsub) || s0->isop(LIR_fadd)) {
17451:                 LInsp lhs = s0->oprnd1();
17451:                 LInsp rhs = s0->oprnd2();
17451:                 if (isPromote(lhs) && isPromote(rhs)) {
17451:                     // add/sub/mul promoted ints can't be nan
17451:                     return insImm(1);
17451:                 }
17451:             }
17538:         } else if (LIR_feq <= v && v <= LIR_fge) {
17451:             if (isPromoteInt(s0) && isPromoteInt(s1)) {
17451:                 // demote fcmp to cmp
17451:                 v = LOpcode(v + (LIR_eq - LIR_feq));
17797:                 return out->ins2(v, demote(out, s0), demote(out, s1));
17451:             } else if (isPromoteUint(s0) && isPromoteUint(s1)) {
17451:                 // uint compare
17451:                 v = LOpcode(v + (LIR_eq - LIR_feq));
17451:                 if (v != LIR_eq)
17451:                     v = LOpcode(v + (LIR_ult - LIR_lt)); // cmp -> ucmp
17797:                 return out->ins2(v, demote(out, s0), demote(out, s1));
17451:             }
17800:         } else if (v == LIR_or &&
17800:                    s0->isop(LIR_lsh) && isconst(s0->oprnd2(), 16) &&
17800:                    s1->isop(LIR_and) && isconst(s1->oprnd2(), 0xffff)) {
17800:             LIns* msw = s0->oprnd1();
17800:             LIns* lsw = s1->oprnd1();
17800:             LIns* x;
17800:             LIns* y;
17800:             if (lsw->isop(LIR_add) &&
17800:                 lsw->oprnd1()->isop(LIR_and) &&
17800:                 lsw->oprnd2()->isop(LIR_and) &&
17800:                 isconst(lsw->oprnd1()->oprnd2(), 0xffff) &&
17800:                 isconst(lsw->oprnd2()->oprnd2(), 0xffff) &&
17800:                 msw->isop(LIR_add) &&
17800:                 msw->oprnd1()->isop(LIR_add) &&
17800:                 msw->oprnd2()->isop(LIR_rsh) &&
17800:                 msw->oprnd1()->oprnd1()->isop(LIR_rsh) &&
17800:                 msw->oprnd1()->oprnd2()->isop(LIR_rsh) &&
17800:                 isconst(msw->oprnd2()->oprnd2(), 16) &&
17800:                 isconst(msw->oprnd1()->oprnd1()->oprnd2(), 16) &&
17800:                 isconst(msw->oprnd1()->oprnd2()->oprnd2(), 16) &&
17800:                 (x = lsw->oprnd1()->oprnd1()) == msw->oprnd1()->oprnd1()->oprnd1() &&
17800:                 (y = lsw->oprnd2()->oprnd1()) == msw->oprnd1()->oprnd2()->oprnd1() &&
17800:                 lsw == msw->oprnd2()->oprnd1()) {
17800:                 return out->ins2(LIR_add, x, y);
17800:             }
17451:         }
18776: 
17797:         return out->ins2(v, s0, s1);
17451:     }
17451: 
20408:     LInsp insCall(const CallInfo *ci, LInsp args[])
17451:     {
28086:         if (ci == &js_DoubleToUint32_ci) {
17478:             LInsp s0 = args[0];
17783:             if (s0->isconstq())
28182:                 return out->insImm(js_DoubleToECMAUint32(s0->imm64f()));
18773:             if (isi2f(s0) || isu2f(s0))
18773:                 return iu2fArg(s0);
20915:         } else if (ci == &js_DoubleToInt32_ci) {
28086:             LInsp s0 = args[0];
17587:             if (s0->isconstq())
28182:                 return out->insImm(js_DoubleToECMAInt32(s0->imm64f()));
21792:             if (s0->isop(LIR_fadd) || s0->isop(LIR_fsub)) {
17451:                 LInsp lhs = s0->oprnd1();
17451:                 LInsp rhs = s0->oprnd2();
17451:                 if (isPromote(lhs) && isPromote(rhs)) {
17451:                     LOpcode op = LOpcode(s0->opcode() & ~LIR64);
17451:                     return out->ins2(op, demote(out, lhs), demote(out, rhs));
17451:                 }
17451:             }
18773:             if (isi2f(s0) || isu2f(s0))
18773:                 return iu2fArg(s0);
20408:             // XXX ARM -- check for qjoin(call(UnboxDouble),call(UnboxDouble))
25707:             if (s0->isCall()) {
25707:                 const CallInfo* ci2 = s0->callInfo();
25707:                 if (ci2 == &js_UnboxDouble_ci) {
17847:                     LIns* args2[] = { callArgN(s0, 0) };
20915:                     return out->insCall(&js_UnboxInt32_ci, args2);
25707:                 } else if (ci2 == &js_StringToNumber_ci) {
25707:                     // callArgN's ordering is that as seen by the builtin, not as stored in
25707:                     // args here. True story!
17912:                     LIns* args2[] = { callArgN(s0, 1), callArgN(s0, 0) };
20915:                     return out->insCall(&js_StringToInt32_ci, args2);
25707:                 } else if (ci2 == &js_String_p_charCodeAt0_ci) {
25707:                     // Use a fast path builtin for a charCodeAt that converts to an int right away.
25707:                     LIns* args2[] = { callArgN(s0, 0) };
25707:                     return out->insCall(&js_String_p_charCodeAt0_int_ci, args2);
25707:                 } else if (ci2 == &js_String_p_charCodeAt_ci) {
25707:                     LIns* idx = callArgN(s0, 1);
25707:                     // If the index is not already an integer, force it to be an integer.
25707:                     idx = isPromote(idx)
25707:                         ? demote(out, idx)
25707:                         : out->insCall(&js_DoubleToInt32_ci, &idx);
25707:                     LIns* args2[] = { idx, callArgN(s0, 0) };
25707:                     return out->insCall(&js_String_p_charCodeAt_int_ci, args2);
25707:                 }
20915:             }
20915:         } else if (ci == &js_BoxDouble_ci) {
28086:             LInsp s0 = args[0];
17478:             JS_ASSERT(s0->isQuad());
25707:             if (isi2f(s0)) {
26546:                 LIns* args2[] = { iu2fArg(s0), args[1] };
20915:                 return out->insCall(&js_BoxInt32_ci, args2);
20915:             }
20915:             if (s0->isCall() && s0->callInfo() == &js_UnboxDouble_ci)
18289:                 return callArgN(s0, 0);
20408:         }
20408:         return out->insCall(ci, args);
17451:     }
17451: };
17451: 
30248: /*
30248:  * Visit the values in the given JSStackFrame that the tracer cares about. This visitor
30248:  * function is (implicitly) the primary definition of the native stack area layout. There
30248:  * are a few other independent pieces of code that must be maintained to assume the same
30248:  * layout. They are marked like this:
30248:  *
30248:  *   Duplicate native stack layout computation: see VisitFrameSlots header comment.
30248:  */
29880: template <typename Visitor>
29880: JS_REQUIRES_STACK static bool
29882: VisitFrameSlots(Visitor &visitor, unsigned depth, JSStackFrame *fp,
29882:                 JSStackFrame *up)
29882: {
29882:     if (depth > 0 && !VisitFrameSlots(visitor, depth-1, fp->down, fp))
29880:         return false;
29880: 
29880:     if (fp->callee) {
29880:         if (depth == 0) {
29880:             visitor.setStackSlotKind("args");
29880:             if (!visitor.visitStackSlots(&fp->argv[-2], argSlots(fp) + 2, fp))
29880:                 return false;
29880:         }
30248:         visitor.setStackSlotKind("arguments");
30248:         if (!visitor.visitStackSlots(&fp->argsobj, 1, fp))
30248:             return false;
29880:         visitor.setStackSlotKind("var");
29880:         if (!visitor.visitStackSlots(fp->slots, fp->script->nfixed, fp))
29880:             return false;
29880:     }
29880:     visitor.setStackSlotKind("stack");
29880:     JS_ASSERT(fp->regs->sp >= StackBase(fp));
29880:     if (!visitor.visitStackSlots(StackBase(fp),
29880:                                  size_t(fp->regs->sp - StackBase(fp)),
29882:                                  fp)) {
29880:         return false;
29882:     }
29880:     if (up) {
29880:         int missing = up->fun->nargs - up->argc;
29880:         if (missing > 0) {
29880:             visitor.setStackSlotKind("missing");
29880:             if (!visitor.visitStackSlots(fp->regs->sp, size_t(missing), fp))
29880:                 return false;
29880:         }
29880:     }
29880:     return true;
29880: }
29880: 
29880: template <typename Visitor>
29880: JS_REQUIRES_STACK static JS_ALWAYS_INLINE bool
29882: VisitStackSlots(Visitor &visitor, JSContext *cx, unsigned callDepth)
29882: {
29882:     return VisitFrameSlots(visitor, callDepth, cx->fp, NULL);
29880: }
29880: 
29880: template <typename Visitor>
29880: JS_REQUIRES_STACK static JS_ALWAYS_INLINE void
29882: VisitGlobalSlots(Visitor &visitor, JSContext *cx, JSObject *globalObj,
29882:                  unsigned ngslots, uint16 *gslots)
29882: {
29880:     for (unsigned n = 0; n < ngslots; ++n) {
29880:         unsigned slot = gslots[n];
29880:         visitor.visitGlobalSlot(&STOBJ_GET_SLOT(globalObj, slot), n, slot);
29880:     }
29880: }
29880: 
29880: class AdjustCallerTypeVisitor;
29880: 
29880: template <typename Visitor>
29880: JS_REQUIRES_STACK static JS_ALWAYS_INLINE void
29882: VisitGlobalSlots(Visitor &visitor, JSContext *cx, SlotList &gslots)
29882: {
29882:     VisitGlobalSlots(visitor, cx, JS_GetGlobalForObject(cx, cx->fp->scopeChain),
29880:                      gslots.length(), gslots.data());
29880: }
29880: 
29880: 
29880: template <typename Visitor>
29880: JS_REQUIRES_STACK static JS_ALWAYS_INLINE void
29882: VisitSlots(Visitor& visitor, JSContext* cx, JSObject* globalObj,
29882:            unsigned callDepth, unsigned ngslots, uint16* gslots)
29882: {
29882:     if (VisitStackSlots(visitor, cx, callDepth))
29882:         VisitGlobalSlots(visitor, cx, globalObj, ngslots, gslots);
29880: }
29880: 
29880: template <typename Visitor>
29880: JS_REQUIRES_STACK static JS_ALWAYS_INLINE void
29882: VisitSlots(Visitor& visitor, JSContext* cx, unsigned callDepth,
29882:            unsigned ngslots, uint16* gslots)
29882: {
29882:     VisitSlots(visitor, cx, JS_GetGlobalForObject(cx, cx->fp->scopeChain),
29880:                callDepth, ngslots, gslots);
29880: }
29880: 
29880: template <typename Visitor>
29880: JS_REQUIRES_STACK static JS_ALWAYS_INLINE void
29882: VisitSlots(Visitor &visitor, JSContext *cx, JSObject *globalObj,
29882:            unsigned callDepth, const SlotList& slots)
29882: {
29882:     VisitSlots(visitor, cx, globalObj, callDepth, slots.length(),
29880:                slots.data());
29880: }
29880: 
29880: template <typename Visitor>
29880: JS_REQUIRES_STACK static JS_ALWAYS_INLINE void
29882: VisitSlots(Visitor &visitor, JSContext *cx, unsigned callDepth,
29882:            const SlotList& slots)
29882: {
29882:     VisitSlots(visitor, cx, JS_GetGlobalForObject(cx, cx->fp->scopeChain),
29880:                callDepth, slots.length(), slots.data());
29880: }
29880: 
29880: 
29880: class SlotVisitorBase {
29882: #ifdef JS_JIT_SPEW
29880: protected:
29880:     char const *mStackSlotKind;
29880: public:
29882:     SlotVisitorBase() : mStackSlotKind(NULL) {}
29882:     JS_ALWAYS_INLINE const char *stackSlotKind() { return mStackSlotKind; }
29880:     JS_ALWAYS_INLINE void setStackSlotKind(char const *k) {
29880:         mStackSlotKind = k;
29880:     }
29882: #else
29882: public:
29882:     JS_ALWAYS_INLINE const char *stackSlotKind() { return NULL; }
29882:     JS_ALWAYS_INLINE void setStackSlotKind(char const *k) {}
29880: #endif
29880: };
29880: 
29882: struct CountSlotsVisitor : public SlotVisitorBase
29882: {
29880:     unsigned mCount;
29880:     bool mDone;
29880:     jsval* mStop;
29880: public:
29880:     JS_ALWAYS_INLINE CountSlotsVisitor(jsval* stop = NULL) :
29880:         mCount(0),
29880:         mDone(false),
29880:         mStop(stop)
29880:     {}
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
29880:     visitStackSlots(jsval *vp, size_t count, JSStackFrame* fp) {
29880:         if (mDone)
29880:             return false;
29880:         if (mStop && size_t(mStop - vp) < count) {
29880:             mCount += size_t(mStop - vp);
29880:             mDone = true;
29880:             return false;
29880:         }
29880:         mCount += count;
29880:         return true;
29880:     }
29880: 
29880:     JS_ALWAYS_INLINE unsigned count() {
29880:         return mCount;
29880:     }
29880: 
29880:     JS_ALWAYS_INLINE bool stopped() {
29880:         return mDone;
29880:     }
29880: };
17820: 
18144: /* Calculate the total number of native frame slots we need from this frame
18144:    all the way back to the entry frame, including the current stack usage. */
22652: JS_REQUIRES_STACK unsigned
18425: js_NativeStackSlots(JSContext *cx, unsigned callDepth)
18193: {
18193:     JSStackFrame* fp = cx->fp;
18144:     unsigned slots = 0;
29880:     unsigned depth = callDepth;
18144:     for (;;) {
30248:         /* Duplicate native stack layout computation: see VisitFrameSlots header comment. */
18144:         unsigned operands = fp->regs->sp - StackBase(fp);
18144:         slots += operands;
18144:         if (fp->callee)
30248:             slots += fp->script->nfixed + 1 /*argsobj*/;
29880:         if (depth-- == 0) {
18617:             if (fp->callee)
28244:                 slots += 2/*callee,this*/ + argSlots(fp);
29880: #ifdef DEBUG
29880:             CountSlotsVisitor visitor;
29882:             VisitStackSlots(visitor, cx, callDepth);
29880:             JS_ASSERT(visitor.count() == slots && !visitor.stopped());
18144: #endif
18144:             return slots;
18144:         }
18144:         JSStackFrame* fp2 = fp;
18144:         fp = fp->down;
18144:         int missing = fp2->fun->nargs - fp2->argc;
18144:         if (missing > 0)
18144:             slots += missing;
18144:     }
18425:     JS_NOT_REACHED("js_NativeStackSlots");
18144: }
18144: 
29882: class CaptureTypesVisitor : public SlotVisitorBase
29882: {
29880:     JSContext* mCx;
29896:     JSTraceType* mTypeMap;
29896:     JSTraceType* mPtr;
29880: 
29880: public:
29896:     JS_ALWAYS_INLINE CaptureTypesVisitor(JSContext* cx, JSTraceType* typeMap) :
29880:         mCx(cx),
29880:         mTypeMap(typeMap),
29880:         mPtr(typeMap)
29880:     {}
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE void
29880:     visitGlobalSlot(jsval *vp, unsigned n, unsigned slot) {
29896:             JSTraceType type = getCoercedType(*vp);
29896:             if (type == TT_INT32 &&
29880:                 oracle.isGlobalSlotUndemotable(mCx, slot))
29896:                 type = TT_DOUBLE;
29896:             JS_ASSERT(type != TT_JSVAL);
29883:             debug_only_printf(LC_TMTracer,
29883:                               "capture type global%d: %d=%c\n",
29883:                               n, type, typeChar[type]);
29880:             *mPtr++ = type;
29880:     }
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
29880:     visitStackSlots(jsval *vp, int count, JSStackFrame* fp) {
29880:         for (int i = 0; i < count; ++i) {
29896:             JSTraceType type = getCoercedType(vp[i]);
29896:             if (type == TT_INT32 &&
29880:                 oracle.isStackSlotUndemotable(mCx, length()))
29896:                 type = TT_DOUBLE;
29896:             JS_ASSERT(type != TT_JSVAL);
29883:             debug_only_printf(LC_TMTracer,
29883:                               "capture type %s%d: %d=%c\n",
29883:                               stackSlotKind(), i, type, typeChar[type]);
29880:             *mPtr++ = type;
29880:         }
29880:         return true;
29880:     }
29880: 
29880:     JS_ALWAYS_INLINE uintptr_t length() {
29880:         return mPtr - mTypeMap;
29880:     }
29880: };
29880: 
24246: /*
24246:  * Capture the type map for the selected slots of the global object and currently pending
24246:  * stack frames.
24246:  */
22652: JS_REQUIRES_STACK void
29880: TypeMap::captureTypes(JSContext* cx, JSObject* globalObj, SlotList& slots, unsigned callDepth)
29880: {
29880:     setLength(js_NativeStackSlots(cx, callDepth) + slots.length());
29880:     CaptureTypesVisitor visitor(cx, data());
29882:     VisitSlots(visitor, cx, globalObj, callDepth, slots);
29880:     JS_ASSERT(visitor.length() == length());
24246: }
24246: 
22652: JS_REQUIRES_STACK void
29880: TypeMap::captureMissingGlobalTypes(JSContext* cx, JSObject* globalObj, SlotList& slots, unsigned stackSlots)
24246: {
24246:     unsigned oldSlots = length() - stackSlots;
24246:     int diff = slots.length() - oldSlots;
24246:     JS_ASSERT(diff >= 0);
24246:     setLength(length() + diff);
29880:     CaptureTypesVisitor visitor(cx, data() + stackSlots + oldSlots);
29882:     VisitGlobalSlots(visitor, cx, globalObj, diff, slots.data() + oldSlots);
17986: }
17986: 
17986: /* Compare this type map to another one and see whether they match. */
17986: bool
18239: TypeMap::matches(TypeMap& other) const
18239: {
18239:     if (length() != other.length())
18239:         return false;
17987:     return !memcmp(data(), other.data(), length());
17985: }
17985: 
28956: /* Use the provided storage area to create a new type map that contains the partial type map
28956:    with the rest of it filled up from the complete type map. */
28956: static void
29896: mergeTypeMaps(JSTraceType** partial, unsigned* plength, JSTraceType* complete, unsigned clength, JSTraceType* mem)
28956: {
28956:     unsigned l = *plength;
28956:     JS_ASSERT(l < clength);
29896:     memcpy(mem, *partial, l * sizeof(JSTraceType));
29896:     memcpy(mem + l, complete + l, (clength - l) * sizeof(JSTraceType));
28956:     *partial = mem;
28956:     *plength = clength;
28956: }
28956: 
25491: /* Specializes a tree to any missing globals, including any dependent trees. */
25509: static JS_REQUIRES_STACK void
29880: specializeTreesToMissingGlobals(JSContext* cx, JSObject* globalObj, TreeInfo* root)
25491: {
25491:     TreeInfo* ti = root;
25491: 
29880:     ti->typeMap.captureMissingGlobalTypes(cx, globalObj, *ti->globalSlots, ti->nStackTypes);
25491:     JS_ASSERT(ti->globalSlots->length() == ti->typeMap.length() - ti->nStackTypes);
25491: 
25491:     for (unsigned i = 0; i < root->dependentTrees.length(); i++) {
29900:         ti = (TreeInfo*)root->dependentTrees[i]->vmprivate;
25491:         /* ti can be NULL if we hit the recording tree in emitTreeCall; this is harmless. */
25491:         if (ti && ti->nGlobalTypes() < ti->globalSlots->length())
29880:             specializeTreesToMissingGlobals(cx, globalObj, ti);
25491:     }
25491:     for (unsigned i = 0; i < root->linkedTrees.length(); i++) {
29900:         ti = (TreeInfo*)root->linkedTrees[i]->vmprivate;
25491:         if (ti && ti->nGlobalTypes() < ti->globalSlots->length())
29880:             specializeTreesToMissingGlobals(cx, globalObj, ti);
25491:     }
25491: }
25491: 
18650: static void
18650: js_TrashTree(JSContext* cx, Fragment* f);
18650: 
22652: JS_REQUIRES_STACK
21521: TraceRecorder::TraceRecorder(JSContext* cx, VMSideExit* _anchor, Fragment* _fragment,
29896:         TreeInfo* ti, unsigned stackSlots, unsigned ngslots, JSTraceType* typeMap,
28244:         VMSideExit* innermostNestedGuard, jsbytecode* outer, uint32 outerArgc)
18211: {
26972:     JS_ASSERT(!_fragment->vmprivate && ti && cx->fp->regs->pc == (jsbytecode*)_fragment->ip);
18211: 
26557:     /* Reset the fragment state we care about in case we got a recycled fragment. */
26557:     _fragment->lastIns = NULL;
26557: 
17351:     this->cx = cx;
18239:     this->traceMonitor = &JS_TRACE_MONITOR(cx);
17657:     this->globalObj = JS_GetGlobalForObject(cx, cx->fp->scopeChain);
26238:     this->lexicalBlock = cx->fp->blockChain;
17738:     this->anchor = _anchor;
17397:     this->fragment = _fragment;
17701:     this->lirbuf = _fragment->lirbuf;
18211:     this->treeInfo = ti;
21521:     this->callDepth = _anchor ? _anchor->calldepth : 0;
24293:     this->atoms = FrameAtomBase(cx, cx->fp);
19068:     this->deepAborted = false;
22609:     this->trashSelf = false;
19653:     this->global_dslots = this->globalObj->dslots;
26557:     this->loop = true; /* default assumption is we are compiling a loop */
21723:     this->wasRootFragment = _fragment == _fragment->root;
25627:     this->outer = outer;
28244:     this->outerArgc = outerArgc;
26972:     this->pendingTraceableNative = NULL;
28086:     this->newobj_ins = NULL;
26552:     this->generatedTraceableNative = new JSTraceableNative();
26552:     JS_ASSERT(generatedTraceableNative);
18260: 
29883: #ifdef JS_JIT_SPEW
29883:     debug_only_print0(LC_TMMinimal, "\n");
29883:     debug_only_printf(LC_TMMinimal, "Recording starting from %s:%u@%u\n",
29883:                       ti->treeFileName, ti->treeLineNumber, ti->treePCOffset);
29883: 
29883:     debug_only_printf(LC_TMTracer, "globalObj=%p, shape=%d\n",
29883:                       (void*)this->globalObj, OBJ_SHAPE(this->globalObj));
29883: #endif
17414: 
17370:     lir = lir_buf_writer = new (&gc) LirBufWriter(lirbuf);
29883:     debug_only_stmt(
29883:         if (js_LogController.lcbits & LC_TMRecorder) {
29883:            lir = verbose_filter
29883:                = new (&gc) VerboseWriter(&gc, lir, lirbuf->names, &js_LogController);
29883:         }
29883:     )
26545:     if (nanojit::AvmCore::config.soft_float)
18773:         lir = float_filter = new (&gc) SoftFloatFilter(lir);
26545:     else
26545:         float_filter = 0;
17370:     lir = cse_filter = new (&gc) CseFilter(lir, &gc);
17370:     lir = expr_filter = new (&gc) ExprFilter(lir);
21799:     lir = func_filter = new (&gc) FuncFilter(lir);
20893:     lir->ins0(LIR_start);
17663: 
20946:     if (!nanojit::AvmCore::config.tree_opt || fragment->root == fragment)
20893:         lirbuf->state = addName(lir->insParam(0, 0), "state");
20893: 
18230:     lirbuf->sp = addName(lir->insLoad(LIR_ldp, lirbuf->state, (int)offsetof(InterpState, sp)), "sp");
18230:     lirbuf->rp = addName(lir->insLoad(LIR_ldp, lirbuf->state, offsetof(InterpState, rp)), "rp");
18230:     cx_ins = addName(lir->insLoad(LIR_ldp, lirbuf->state, offsetof(InterpState, cx)), "cx");
18230:     eos_ins = addName(lir->insLoad(LIR_ldp, lirbuf->state, offsetof(InterpState, eos)), "eos");
18230:     eor_ins = addName(lir->insLoad(LIR_ldp, lirbuf->state, offsetof(InterpState, eor)), "eor");
23918: 
24246:     /* If we came from exit, we might not have enough global types. */
25491:     if (ti->globalSlots->length() > ti->nGlobalTypes())
29880:         specializeTreesToMissingGlobals(cx, globalObj, ti);
17334: 
17997:     /* read into registers all values on the stack and all globals we know so far */
24246:     import(treeInfo, lirbuf->sp, stackSlots, ngslots, callDepth, typeMap);
18284: 
22615:     if (fragment == fragment->root) {
25087:         /*
25087:          * We poll the operation callback request flag. It is updated asynchronously whenever
25087:          * the callback is to be invoked.
25087:          */
30238:         LIns* x = lir->insLoad(LIR_ld, cx_ins, offsetof(JSContext, operationCallbackFlag));
25087:         guard(true, lir->ins_eq0(x), snapshot(TIMEOUT_EXIT));
23455:     }
22615: 
18284:     /* If we are attached to a tree call guard, make sure the guard the inner tree exited from
18284:        is what we expect it to be. */
20931:     if (_anchor && _anchor->exitType == NESTED_EXIT) {
18284:         LIns* nested_ins = addName(lir->insLoad(LIR_ldp, lirbuf->state,
19590:                                                 offsetof(InterpState, lastTreeExitGuard)),
19590:                                                 "lastTreeExitGuard");
18712:         guard(true, lir->ins2(LIR_eq, nested_ins, INS_CONSTPTR(innermostNestedGuard)), NESTED_EXIT);
18284:     }
17334: }
17334: 
25102: TreeInfo::~TreeInfo()
25102: {
25102:     UnstableExit* temp;
25102: 
25102:     while (unstableExits) {
25102:         temp = unstableExits->next;
25102:         delete unstableExits;
25102:         unstableExits = temp;
25102:     }
25102: }
21433: 
17334: TraceRecorder::~TraceRecorder()
17334: {
21723:     JS_ASSERT(nextRecorderToAbort == NULL);
21723:     JS_ASSERT(treeInfo && (fragment || wasDeepAborted()));
21767: #ifdef DEBUG
21767:     TraceRecorder* tr = JS_TRACE_MONITOR(cx).abortStack;
21767:     while (tr != NULL)
21767:     {
21767:         JS_ASSERT(this != tr);
21767:         tr = tr->nextRecorderToAbort;
21767:     }
21767: #endif
21723:     if (fragment) {
21723:         if (wasRootFragment && !fragment->root->code()) {
18211:             JS_ASSERT(!fragment->root->vmprivate);
25102:             delete treeInfo;
18211:         }
22609: 
22609:         if (trashSelf)
22612:             js_TrashTree(cx, fragment->root);
22609: 
22609:         for (unsigned int i = 0; i < whichTreesToTrash.length(); i++)
29900:             js_TrashTree(cx, whichTreesToTrash[i]);
25102:     } else if (wasRootFragment) {
25102:         delete treeInfo;
21723:     }
17334: #ifdef DEBUG
29883:     debug_only_stmt( delete verbose_filter; )
17334: #endif
17370:     delete cse_filter;
17370:     delete expr_filter;
17453:     delete func_filter;
18773:     delete float_filter;
17370:     delete lir_buf_writer;
26552:     delete generatedTraceableNative;
17319: }
17319: 
21723: void TraceRecorder::removeFragmentoReferences()
21723: {
21723:     fragment = NULL;
21723: }
21723: 
26958: void TraceRecorder::deepAbort()
26958: {
29883:     debug_only_print0(LC_TMTracer|LC_TMAbort, "deep abort");
26958:     deepAborted = true;
26958: }
26958: 
17722: /* Add debug information to a LIR instruction as we emit it. */
17722: inline LIns*
17722: TraceRecorder::addName(LIns* ins, const char* name)
17722: {
24305: #ifdef JS_JIT_SPEW
29883:     /* We'll only ask for verbose Nanojit when .lcbits > 0, so
29883:        there's no point in adding names otherwise. */
29883:     if (js_LogController.lcbits > 0)
17722:         lirbuf->names->addName(ins, name);
17722: #endif
17722:     return ins;
17722: }
17722: 
17346: /* Determine the current call depth (starting with the entry frame.) */
17346: unsigned
17442: TraceRecorder::getCallDepth() const
17346: {
17789:     return callDepth;
17346: }
17346: 
17815: /* Determine the offset in the native global frame for a jsval we track */
17811: ptrdiff_t
17815: TraceRecorder::nativeGlobalOffset(jsval* p) const
17815: {
17900:     JS_ASSERT(isGlobal(p));
17900:     if (size_t(p - globalObj->fslots) < JS_INITIAL_NSLOTS)
26282:         return sizeof(InterpState) + size_t(p - globalObj->fslots) * sizeof(double);
26282:     return sizeof(InterpState) + ((p - globalObj->dslots) + JS_INITIAL_NSLOTS) * sizeof(double);
17815: }
17815: 
17894: /* Determine whether a value is a global stack slot */
17893: bool
17893: TraceRecorder::isGlobal(jsval* p) const
17893: {
17893:     return ((size_t(p - globalObj->fslots) < JS_INITIAL_NSLOTS) ||
27062:             (size_t(p - globalObj->dslots) < (STOBJ_NSLOTS(globalObj) - JS_INITIAL_NSLOTS)));
17893: }
17893: 
30248: /* 
30248:  * Return the offset in the native stack for the given jsval. More formally,
30248:  * |p| must be the address of a jsval that is represented in the native stack
30248:  * area. The return value is the offset, from InterpState::stackBase, in bytes,
30248:  * where the native representation of |*p| is stored. To get the offset relative
30248:  * to InterpState::sp, subtract TreeInfo::nativeStackBase.
30248:  */
22652: JS_REQUIRES_STACK ptrdiff_t
17815: TraceRecorder::nativeStackOffset(jsval* p) const
17346: {
29880:     CountSlotsVisitor visitor(p);
29882:     VisitStackSlots(visitor, cx, callDepth);
29880:     size_t offset = visitor.count() * sizeof(double);
17923:     /*
17923:      * If it's not in a pending frame, it must be on the stack of the current frame above
17923:      * sp but below fp->slots + script->nslots.
17923:      */
29880:     if (!visitor.stopped()) {
17923:         JS_ASSERT(size_t(p - cx->fp->slots) < cx->fp->script->nslots);
29880:         offset += size_t(p - cx->fp->regs->sp) * sizeof(double);
29880:     }
29880:     return offset;
17346: }
17346: 
17397: /* Track the maximum number of native frame slots we need during
17397:    execution. */
17397: void
17815: TraceRecorder::trackNativeStackUse(unsigned slots)
17815: {
17815:     if (slots > treeInfo->maxNativeStackSlots)
17815:         treeInfo->maxNativeStackSlots = slots;
17397: }
17397: 
21433: /* Unbox a jsval into a slot. Slots are wide enough to hold double values directly (instead of
21433:    storing a pointer to them). We now assert instead of type checking, the caller must ensure the
21433:    types are compatible. */
21433: static void
29896: ValueToNative(JSContext* cx, jsval v, JSTraceType type, double* slot)
29896: {
29896:     uint8_t tag = JSVAL_TAG(v);
18296:     switch (type) {
29896:       case TT_OBJECT:
27542:         JS_ASSERT(tag == JSVAL_OBJECT);
27542:         JS_ASSERT(!JSVAL_IS_NULL(v) && !HAS_FUNCTION_CLASS(JSVAL_TO_OBJECT(v)));
27542:         *(JSObject**)slot = JSVAL_TO_OBJECT(v);
29883:         debug_only_printf(LC_TMTracer,
29883:                           "object<%p:%s> ", (void*)JSVAL_TO_OBJECT(v),
27542:                           JSVAL_IS_NULL(v)
27542:                           ? "null"
29883:                           : STOBJ_GET_CLASS(JSVAL_TO_OBJECT(v))->name);
27542:         return;
29896:       case TT_INT32:
17482:         jsint i;
17482:         if (JSVAL_IS_INT(v))
17482:             *(jsint*)slot = JSVAL_TO_INT(v);
29896:         else if (tag == JSVAL_DOUBLE && JSDOUBLE_IS_INT(*JSVAL_TO_DOUBLE(v), i))
17469:             *(jsint*)slot = i;
21433:         else
21433:             JS_ASSERT(JSVAL_IS_INT(v));
29883:         debug_only_printf(LC_TMTracer, "int<%d> ", *(jsint*)slot);
21433:         return;
29896:       case TT_DOUBLE:
17482:         jsdouble d;
17482:         if (JSVAL_IS_INT(v))
17482:             d = JSVAL_TO_INT(v);
21433:         else
17482:             d = *JSVAL_TO_DOUBLE(v);
21433:         JS_ASSERT(JSVAL_IS_INT(v) || JSVAL_IS_DOUBLE(v));
17482:         *(jsdouble*)slot = d;
29883:         debug_only_printf(LC_TMTracer, "double<%g> ", d);
21433:         return;
29896:       case TT_JSVAL:
29896:         JS_NOT_REACHED("found jsval type in an entry type map");
27541:         return;
29896:       case TT_STRING:
27542:         JS_ASSERT(tag == JSVAL_STRING);
27542:         *(JSString**)slot = JSVAL_TO_STRING(v);
29883:         debug_only_printf(LC_TMTracer, "string<%p> ", (void*)(*(JSString**)slot));
27542:         return;
29896:       case TT_NULL:
27542:         JS_ASSERT(tag == JSVAL_OBJECT);
27542:         *(JSObject**)slot = NULL;
29883:         debug_only_print0(LC_TMTracer, "null ");
27542:         return;
29896:       case TT_PSEUDOBOOLEAN:
24846:         /* Watch out for pseudo-booleans. */
21433:         JS_ASSERT(tag == JSVAL_BOOLEAN);
24846:         *(JSBool*)slot = JSVAL_TO_PSEUDO_BOOLEAN(v);
29896:         debug_only_printf(LC_TMTracer, "pseudoboolean<%d> ", *(JSBool*)slot);
21433:         return;
29896:       case TT_FUNCTION: {
27541:         JS_ASSERT(tag == JSVAL_OBJECT);
27541:         JSObject* obj = JSVAL_TO_OBJECT(v);
27541:         *(JSObject**)slot = obj;
27541: #ifdef DEBUG
27541:         JSFunction* fun = GET_FUNCTION_PRIVATE(cx, obj);
29883:         debug_only_printf(LC_TMTracer,
29883:                           "function<%p:%s> ", (void*) obj,
27541:                           fun->atom
27541:                           ? JS_GetStringBytes(ATOM_TO_STRING(fun->atom))
29883:                           : "unnamed");
27541: #endif
27541:         return;
27541:       }
27542:     }
27542: 
27542:     JS_NOT_REACHED("unexpected type");
18296: }
17360: 
23449: /* We maintain an emergency pool of doubles so we can recover safely if a trace runs
19987:    out of memory (doubles or objects). */
19991: static jsval
23449: AllocateDoubleFromReservedPool(JSContext* cx)
19987: {
19987:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
23449:     JS_ASSERT(tm->reservedDoublePoolPtr > tm->reservedDoublePool);
23449:     return *--tm->reservedDoublePoolPtr;
19987: }
19987: 
19987: static bool
23449: js_ReplenishReservedPool(JSContext* cx, JSTraceMonitor* tm)
19991: {
19991:     /* We should not be called with a full pool. */
23449:     JS_ASSERT((size_t) (tm->reservedDoublePoolPtr - tm->reservedDoublePool) <
19991:               MAX_NATIVE_STACK_SLOTS);
19991: 
19991:     /*
19991:      * When the GC runs in js_NewDoubleInRootedValue, it resets
23449:      * tm->reservedDoublePoolPtr back to tm->reservedDoublePool.
19991:      */
19991:     JSRuntime* rt = cx->runtime;
19991:     uintN gcNumber = rt->gcNumber;
25490:     uintN lastgcNumber = gcNumber;
23449:     jsval* ptr = tm->reservedDoublePoolPtr;
23449:     while (ptr < tm->reservedDoublePool + MAX_NATIVE_STACK_SLOTS) {
21514:         if (!js_NewDoubleInRootedValue(cx, 0.0, ptr))
21514:             goto oom;
25490: 
25490:         /* Check if the last call to js_NewDoubleInRootedValue GC'd. */
25490:         if (rt->gcNumber != lastgcNumber) {
25490:             lastgcNumber = rt->gcNumber;
23449:             JS_ASSERT(tm->reservedDoublePoolPtr == tm->reservedDoublePool);
23449:             ptr = tm->reservedDoublePool;
25490: 
25490:             /*
25490:              * Have we GC'd more than once? We're probably running really
25490:              * low on memory, bail now.
25490:              */
21514:             if (uintN(rt->gcNumber - gcNumber) > uintN(1))
21514:                 goto oom;
21514:             continue;
21514:         }
21514:         ++ptr;
21514:     }
23449:     tm->reservedDoublePoolPtr = ptr;
21514:     return true;
21514: 
21514: oom:
21514:     /*
21514:      * Already massive GC pressure, no need to hold doubles back.
21514:      * We won't run any native code anyway.
21514:      */
23449:     tm->reservedDoublePoolPtr = tm->reservedDoublePool;
21514:     return false;
19987: }
19987: 
17387: /* Box a value from the native stack back into the jsval format. Integers
17387:    that are too large to fit into a jsval are automatically boxed into
17387:    heap-allocated doubles. */
23446: static void
29896: NativeToValue(JSContext* cx, jsval& v, JSTraceType type, double* slot)
17841: {
17483:     jsint i;
17469:     jsdouble d;
17494:     switch (type) {
29896:       case TT_OBJECT:
27542:         v = OBJECT_TO_JSVAL(*(JSObject**)slot);
27542:         JS_ASSERT(v != JSVAL_ERROR_COOKIE); /* don't leak JSVAL_ERROR_COOKIE */
29883:         debug_only_printf(LC_TMTracer,
29883:                           "object<%p:%s> ", (void*)JSVAL_TO_OBJECT(v),
27542:                           JSVAL_IS_NULL(v)
27542:                           ? "null"
29883:                           : STOBJ_GET_CLASS(JSVAL_TO_OBJECT(v))->name);
17393:         break;
29896:       case TT_INT32:
17483:         i = *(jsint*)slot;
29883:         debug_only_printf(LC_TMTracer, "int<%d> ", i);
17483:       store_int:
17469:         if (INT_FITS_IN_JSVAL(i)) {
17492:             v = INT_TO_JSVAL(i);
17393:             break;
17469:         }
17469:         d = (jsdouble)i;
17483:         goto store_double;
29896:       case TT_DOUBLE:
17469:         d = *slot;
29883:         debug_only_printf(LC_TMTracer, "double<%g> ", d);
17483:         if (JSDOUBLE_IS_INT(d, i))
17483:             goto store_int;
19987:       store_double: {
19987:         /* Its not safe to trigger the GC here, so use an emergency heap if we are out of
19987:            double boxes. */
19987:         if (cx->doubleFreeList) {
19987: #ifdef DEBUG
25628:             JSBool ok =
19987: #endif
19987:                 js_NewDoubleInRootedValue(cx, d, &v);
19987:             JS_ASSERT(ok);
23446:             return;
19987:         }
23449:         v = AllocateDoubleFromReservedPool(cx);
19991:         JS_ASSERT(JSVAL_IS_DOUBLE(v) && *JSVAL_TO_DOUBLE(v) == 0.0);
19991:         *JSVAL_TO_DOUBLE(v) = d;
23446:         return;
19987:       }
29896:       case TT_JSVAL:
27542:         v = *(jsval*)slot;
27542:         JS_ASSERT(v != JSVAL_ERROR_COOKIE); /* don't leak JSVAL_ERROR_COOKIE */
29883:         debug_only_printf(LC_TMTracer, "box<%p> ", (void*)v);
27542:         break;
29896:       case TT_STRING:
17492:         v = STRING_TO_JSVAL(*(JSString**)slot);
29883:         debug_only_printf(LC_TMTracer, "string<%p> ", (void*)(*(JSString**)slot));
17393:         break;
29896:       case TT_NULL:
23075:         JS_ASSERT(*(JSObject**)slot == NULL);
23075:         v = JSVAL_NULL;
29883:         debug_only_printf(LC_TMTracer, "null<%p> ", (void*)(*(JSObject**)slot));
23075:         break;
29896:       case TT_PSEUDOBOOLEAN:
27542:         /* Watch out for pseudo-booleans. */
27542:         v = PSEUDO_BOOLEAN_TO_JSVAL(*(JSBool*)slot);
29883:         debug_only_printf(LC_TMTracer, "boolean<%d> ", *(JSBool*)slot);
27542:         break;
29896:       case TT_FUNCTION: {
27541:         JS_ASSERT(HAS_FUNCTION_CLASS(*(JSObject**)slot));
27541:         v = OBJECT_TO_JSVAL(*(JSObject**)slot);
27541: #ifdef DEBUG
27541:         JSFunction* fun = GET_FUNCTION_PRIVATE(cx, JSVAL_TO_OBJECT(v));
29883:         debug_only_printf(LC_TMTracer,
29883:                           "function<%p:%s> ", (void*)JSVAL_TO_OBJECT(v),
27541:                           fun->atom
27541:                           ? JS_GetStringBytes(ATOM_TO_STRING(fun->atom))
29883:                           : "unnamed");
27541: #endif
27541:         break;
27541:       }
17393:     }
17361: }
17361: 
29882: class BuildNativeFrameVisitor : public SlotVisitorBase
29882: {
29880:     JSContext *mCx;
29896:     JSTraceType *mTypeMap;
29880:     double *mGlobal;
29880:     double *mStack;
29880: public:
29880:     BuildNativeFrameVisitor(JSContext *cx,
29896:                             JSTraceType *typemap,
29880:                             double *global,
29880:                             double *stack) :
29880:         mCx(cx),
29880:         mTypeMap(typemap),
29880:         mGlobal(global),
29880:         mStack(stack)
29880:     {}
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE void
29880:     visitGlobalSlot(jsval *vp, unsigned n, unsigned slot) {
29883:         debug_only_printf(LC_TMTracer, "global%d: ", n);
29880:         ValueToNative(mCx, *vp, *mTypeMap++, &mGlobal[slot]);
29880:     }
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
29880:     visitStackSlots(jsval *vp, int count, JSStackFrame* fp) {
29880:         for (int i = 0; i < count; ++i) {
29883:             debug_only_printf(LC_TMTracer, "%s%d: ", stackSlotKind(), i);
29880:             ValueToNative(mCx, *vp++, *mTypeMap++, mStack++);
29880:         }
29880:         return true;
29880:     }
29880: };
29880: 
22652: static JS_REQUIRES_STACK void
29880: BuildNativeFrame(JSContext *cx, JSObject *globalObj, unsigned callDepth,
29880:                  unsigned ngslots, uint16 *gslots,
29896:                  JSTraceType *typeMap, double *global, double *stack)
29880: {
29880:     BuildNativeFrameVisitor visitor(cx, typeMap, global, stack);
29882:     VisitSlots(visitor, cx, globalObj, callDepth, ngslots, gslots);
29883:     debug_only_print0(LC_TMTracer, "\n");
21433: }
21433: 
29882: class FlushNativeGlobalFrameVisitor : public SlotVisitorBase
29882: {
29880:     JSContext *mCx;
29896:     JSTraceType *mTypeMap;
29880:     double *mGlobal;
29880: public:
29880:     FlushNativeGlobalFrameVisitor(JSContext *cx,
29896:                                   JSTraceType *typeMap,
29880:                                   double *global) :
29880:         mCx(cx),
29880:         mTypeMap(typeMap),
29880:         mGlobal(global)
29880:     {}
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE void
29880:     visitGlobalSlot(jsval *vp, unsigned n, unsigned slot) {
29883:         debug_only_printf(LC_TMTracer, "global%d=", n);
29880:         NativeToValue(mCx, *vp, *mTypeMap++, &mGlobal[slot]);
29880:     }
29880: };
29880: 
29882: class FlushNativeStackFrameVisitor : public SlotVisitorBase
29882: {
29880:     JSContext *mCx;
29896:     JSTraceType *mTypeMap;
29880:     double *mStack;
29880:     jsval *mStop;
29880: public:
29880:     FlushNativeStackFrameVisitor(JSContext *cx,
29896:                                  JSTraceType *typeMap,
29880:                                  double *stack,
29880:                                  jsval *stop) :
29880:         mCx(cx),
29880:         mTypeMap(typeMap),
29880:         mStack(stack),
29880:         mStop(stop)
29880:     {}
29880: 
29896:     JSTraceType* getTypeMap()
29880:     {
29880:         return mTypeMap;
29880:     }
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
29880:     visitStackSlots(jsval *vp, size_t count, JSStackFrame* fp) {
29880:         for (size_t i = 0; i < count; ++i) {
29880:             if (vp == mStop)
29880:                 return false;
29883:             debug_only_printf(LC_TMTracer, "%s%u=", stackSlotKind(), unsigned(i));
29880:             NativeToValue(mCx, *vp++, *mTypeMap++, mStack++);
29880:         }
29880:         return true;
29880:     }
29880: };
29880: 
29880: /* Box the given native frame into a JS frame. This is infallible. */
22652: static JS_REQUIRES_STACK void
29880: FlushNativeGlobalFrame(JSContext *cx, double *global, unsigned ngslots,
29896:                        uint16 *gslots, JSTraceType *typemap)
29880: {
29880:     FlushNativeGlobalFrameVisitor visitor(cx, typemap, global);
29880:     JSObject *globalObj = JS_GetGlobalForObject(cx, cx->fp->scopeChain);
29882:     VisitGlobalSlots(visitor, cx, globalObj, ngslots, gslots);
29883:     debug_only_print0(LC_TMTracer, "\n");
17857: }
17857: 
28268: /*
29021:  * Generic function to read upvars on trace.
29021:  *     T   Traits type parameter. Must provide static functions:
29021:  *             interp_get(fp, slot)     Read the value out of an interpreter frame.
29021:  *             native_slot(argc, slot)  Return the position of the desired value in the on-trace
29021:  *                                      stack frame (with position 0 being callee).
29021:  *
29021:  *     upvarLevel  Static level of the function containing the upvar definition.
29021:  *     slot        Identifies the value to get. The meaning is defined by the traits type.
29021:  *     callDepth   Call depth of current point relative to trace entry
29021:  */
29021: template<typename T>
29896: JSTraceType JS_INLINE
29021: js_GetUpvarOnTrace(JSContext* cx, uint32 upvarLevel, int32 slot, uint32 callDepth, double* result)
29021: {
28268:     InterpState* state = cx->interpState;
28738:     FrameInfo** fip = state->rp + callDepth;
28738: 
28738:     /*
28738:      * First search the FrameInfo call stack for an entry containing
29021:      * our upvar, namely one with level == upvarLevel.
28738:      */
28738:     while (--fip >= state->callstackBase) {
28738:         FrameInfo* fi = *fip;
28738:         JSFunction* fun = GET_FUNCTION_PRIVATE(cx, fi->callee);
28738:         uintN calleeLevel = fun->u.i.script->staticLevel;
28738:         if (calleeLevel == upvarLevel) {
28738:             /*
28738:              * Now find the upvar's value in the native stack.
28738:              * nativeStackFramePos is the offset of the start of the
28738:              * activation record corresponding to *fip in the native
28738:              * stack.
28738:              */
28949:             int32 nativeStackFramePos = state->callstackBase[0]->spoffset;
28738:             for (FrameInfo** fip2 = state->callstackBase; fip2 <= fip; fip2++)
28887:                 nativeStackFramePos += (*fip2)->spdist;
28949:             nativeStackFramePos -= (2 + (*fip)->get_argc());
29021:             uint32 native_slot = T::native_slot((*fip)->get_argc(), slot);
29021:             *result = state->stackBase[nativeStackFramePos + native_slot];
29021:             return fi->get_typemap()[native_slot];
29021:         }
29021:     }
29021: 
29021:     // Next search the trace entry frame, which is not in the FrameInfo stack.
28911:     if (state->outermostTree->script->staticLevel == upvarLevel) {
29021:         uint32 argc = ((VMFragment*) state->outermostTree->fragment)->argc;
29021:         uint32 native_slot = T::native_slot(argc, slot);
29021:         *result = state->stackBase[native_slot];
29021:         return state->callstackBase[0]->get_typemap()[native_slot];
28738:     }
28738: 
28738:     /*
28738:      * If we did not find the upvar in the frames for the active traces,
28268:      * then we simply get the value from the interpreter state.
28268:      */
29021:     JS_ASSERT(upvarLevel < JS_DISPLAY_SIZE);
29021:     JSStackFrame* fp = cx->display[upvarLevel];
29021:     jsval v = T::interp_get(fp, slot);
29896:     JSTraceType type = getCoercedType(v);
28268:     ValueToNative(cx, v, type, result);
28268:     return type;
28268: }
28268: 
29021: // For this traits type, 'slot' is the argument index, which may be -2 for callee.
29021: struct UpvarArgTraits {
29021:     static jsval interp_get(JSStackFrame* fp, int32 slot) {
29021:         return fp->argv[slot];
29021:     }
29021: 
29021:     static uint32 native_slot(uint32 argc, int32 slot) {
29021:         return 2 /*callee,this*/ + slot;
29021:     }
29021: };
29021: 
29021: uint32 JS_FASTCALL
29021: js_GetUpvarArgOnTrace(JSContext* cx, uint32 upvarLevel, int32 slot, uint32 callDepth, double* result)
29021: {
29021:     return js_GetUpvarOnTrace<UpvarArgTraits>(cx, upvarLevel, slot, callDepth, result);
29021: }
29021: 
29021: // For this traits type, 'slot' is an index into the local slots array.
29021: struct UpvarVarTraits {
29021:     static jsval interp_get(JSStackFrame* fp, int32 slot) {
29021:         return fp->slots[slot];
29021:     }
29021: 
29021:     static uint32 native_slot(uint32 argc, int32 slot) {
30290:         return 3 /*callee,this,arguments*/ + argc + slot;
29021:     }
29021: };
29021: 
29021: uint32 JS_FASTCALL
29021: js_GetUpvarVarOnTrace(JSContext* cx, uint32 upvarLevel, int32 slot, uint32 callDepth, double* result)
29021: {
29021:     return js_GetUpvarOnTrace<UpvarVarTraits>(cx, upvarLevel, slot, callDepth, result);
29021: }
29021: 
29022: /*
29022:  * For this traits type, 'slot' is an index into the stack area (within slots, after nfixed)
29022:  * of a frame with no function. (On trace, the top-level frame is the only one that can have
29022:  * no function.)
29022:  */
29022: struct UpvarStackTraits {
29022:     static jsval interp_get(JSStackFrame* fp, int32 slot) {
29022:         return fp->slots[slot + fp->script->nfixed];
29022:     }
29022: 
29022:     static uint32 native_slot(uint32 argc, int32 slot) {
29022:         /*
29022:          * Locals are not imported by the tracer when the frame has no function, so
29022:          * we do not add fp->script->nfixed.
29022:          */
29022:         JS_ASSERT(argc == 0);
29022:         return slot;
29022:     }
29022: };
29022: 
29022: uint32 JS_FASTCALL
29022: js_GetUpvarStackOnTrace(JSContext* cx, uint32 upvarLevel, int32 slot, uint32 callDepth, double* result)
29022: {
29022:     return js_GetUpvarOnTrace<UpvarStackTraits>(cx, upvarLevel, slot, callDepth, result);
29022: }
29022: 
19076: /**
23446:  * Box the given native stack frame into the virtual machine stack. This
23446:  * is infallible.
19076:  *
19076:  * @param callDepth the distance between the entry frame into our trace and
19076:  *                  cx->fp when we make this call.  If this is not called as a
19076:  *                  result of a nested exit, callDepth is 0.
29896:  * @param mp an array of JSTraceTypes that indicate what the types of the things
29896:  *           on the stack are.
19076:  * @param np pointer to the native stack.  We want to copy values from here to
19076:  *           the JS stack as needed.
19076:  * @param stopFrame if non-null, this frame and everything above it should not
19076:  *                  be restored.
19076:  * @return the number of things we popped off of np.
19076:  */
22652: static JS_REQUIRES_STACK int
29896: FlushNativeStackFrame(JSContext* cx, unsigned callDepth, JSTraceType* mp, double* np,
19076:                       JSStackFrame* stopFrame)
19076: {
19076:     jsval* stopAt = stopFrame ? &stopFrame->argv[-2] : NULL;
29880: 
17857:     /* Root all string and object references first (we don't need to call the GC for this). */
29880:     FlushNativeStackFrameVisitor visitor(cx, mp, np, stopAt);
29882:     VisitStackSlots(visitor, cx, callDepth);
29880: 
17925:     // Restore thisp from the now-restored argv[-1] in each pending frame.
19076:     // Keep in mind that we didn't restore frames at stopFrame and above!
19076:     // Scope to keep |fp| from leaking into the macros we're using.
19076:     {
19076:         unsigned n = callDepth+1; // +1 to make sure we restore the entry frame
19076:         JSStackFrame* fp = cx->fp;
19076:         if (stopFrame) {
19076:             for (; fp != stopFrame; fp = fp->down) {
19076:                 JS_ASSERT(n != 0);
19076:                 --n;
19076:             }
19076:             // Skip over stopFrame itself.
19076:             JS_ASSERT(n != 0);
19076:             --n;
19076:             fp = fp->down;
19076:         }
19076:         for (; n != 0; fp = fp->down) {
19076:             --n;
27470:             if (fp->callee) {
30248:                 if (fp->argsobj)
30248:                     JS_SetPrivate(cx, JSVAL_TO_OBJECT(fp->argsobj), fp);
30248: 
28664:                 /*
28691:                  * We might return from trace with a different callee object, but it still
28691:                  * has to be the same JSFunction (FIXME: bug 471425, eliminate fp->callee).
28664:                  */
19076:                 JS_ASSERT(JSVAL_IS_OBJECT(fp->argv[-1]));
28664:                 JS_ASSERT(HAS_FUNCTION_CLASS(JSVAL_TO_OBJECT(fp->argv[-2])));
28664:                 JS_ASSERT(GET_FUNCTION_PRIVATE(cx, JSVAL_TO_OBJECT(fp->argv[-2])) ==
28664:                           GET_FUNCTION_PRIVATE(cx, fp->callee));
28691:                 JS_ASSERT(GET_FUNCTION_PRIVATE(cx, fp->callee) == fp->fun);
28664:                 fp->callee = JSVAL_TO_OBJECT(fp->argv[-2]);
28691: 
28664:                 /*
28664:                  * SynthesizeFrame sets scopeChain to NULL, because we can't calculate the
28664:                  * correct scope chain until we have the final callee. Calculate the real
28664:                  * scope object here.
28664:                  */
28691:                 if (!fp->scopeChain) {
28664:                     fp->scopeChain = OBJ_GET_PARENT(cx, fp->callee);
28691:                     if (fp->fun->flags & JSFUN_HEAVYWEIGHT) {
28691:                         /*
28691:                          * Set hookData to null because the failure case for js_GetCallObject
28691:                          * involves it calling the debugger hook.
28691:                          *
28691:                          * Allocating the Call object must not fail, so use an object
28691:                          * previously reserved by js_ExecuteTree if needed.
28691:                          */
28691:                         void* hookData = ((JSInlineFrame*)fp)->hookData;
28691:                         ((JSInlineFrame*)fp)->hookData = NULL;
28691:                         JS_ASSERT(!JS_TRACE_MONITOR(cx).useReservedObjects);
28691:                         JS_TRACE_MONITOR(cx).useReservedObjects = JS_TRUE;
28691: #ifdef DEBUG
28691:                         JSObject *obj =
28691: #endif
28691:                             js_GetCallObject(cx, fp);
28691:                         JS_ASSERT(obj);
28691:                         JS_TRACE_MONITOR(cx).useReservedObjects = JS_FALSE;
28691:                         ((JSInlineFrame*)fp)->hookData = hookData;
28691:                     }
28691:                 }
17949:                 fp->thisp = JSVAL_TO_OBJECT(fp->argv[-1]);
27470:                 if (fp->flags & JSFRAME_CONSTRUCTING) // constructors always compute 'this'
27470:                     fp->flags |= JSFRAME_COMPUTED_THIS;
19076:             }
19076:         }
19076:     }
29883:     debug_only_print0(LC_TMTracer, "\n");
29880:     return visitor.getTypeMap() - mp;
17361: }
17361: 
17363: /* Emit load instructions onto the trace that read the initial stack state. */
22652: JS_REQUIRES_STACK void
29896: TraceRecorder::import(LIns* base, ptrdiff_t offset, jsval* p, JSTraceType t,
18045:                       const char *prefix, uintN index, JSStackFrame *fp)
17319: {
17480:     LIns* ins;
29896:     if (t == TT_INT32) { /* demoted */
17482:         JS_ASSERT(isInt32(*p));
17480:         /* Ok, we have a valid demotion attempt pending, so insert an integer
17480:            read and promote it to double since all arithmetic operations expect
17480:            to see doubles on entry. The first op to use this slot will emit a
17480:            f2i cast which will cancel out the i2f we insert here. */
30238:         ins = lir->insLoad(LIR_ld, base, offset);
17803:         ins = lir->ins1(LIR_i2f, ins);
17480:     } else {
29896:         JS_ASSERT_IF(t != TT_JSVAL, isNumber(*p) == (t == TT_DOUBLE));
29896:         if (t == TT_DOUBLE) {
18232:             ins = lir->insLoad(LIR_ldq, base, offset);
29896:         } else if (t == TT_PSEUDOBOOLEAN) {
20393:             ins = lir->insLoad(LIR_ld, base, offset);
18232:         } else {
18232:             ins = lir->insLoad(LIR_ldp, base, offset);
18232:         }
17480:     }
24381:     checkForGlobalObjectReallocation();
17372:     tracker.set(p, ins);
26972: 
17372: #ifdef DEBUG
17737:     char name[64];
17372:     JS_ASSERT(strlen(prefix) < 10);
17925:     void* mark = NULL;
17925:     jsuword* localNames = NULL;
18011:     const char* funName = NULL;
17925:     if (*prefix == 'a' || *prefix == 'v') {
17925:         mark = JS_ARENA_MARK(&cx->tempPool);
28832:         if (fp->fun->hasLocalNames())
17925:             localNames = js_GetLocalNameArray(cx, fp->fun, &cx->tempPool);
18011:         funName = fp->fun->atom ? js_AtomToPrintableString(cx, fp->fun->atom) : "<anonymous>";
18011:     }
17737:     if (!strcmp(prefix, "argv")) {
18045:         if (index < fp->fun->nargs) {
17737:             JSAtom *atom = JS_LOCAL_NAME_TO_ATOM(localNames[index]);
18011:             JS_snprintf(name, sizeof name, "$%s.%s", funName, js_AtomToPrintableString(cx, atom));
18011:         } else {
18011:             JS_snprintf(name, sizeof name, "$%s.<arg%d>", funName, index);
18011:         }
17737:     } else if (!strcmp(prefix, "vars")) {
17925:         JSAtom *atom = JS_LOCAL_NAME_TO_ATOM(localNames[fp->fun->nargs + index]);
18011:         JS_snprintf(name, sizeof name, "$%s.%s", funName, js_AtomToPrintableString(cx, atom));
17737:     } else {
17379:         JS_snprintf(name, sizeof name, "$%s%d", prefix, index);
17737:     }
17925: 
17925:     if (mark)
17925:         JS_ARENA_RELEASE(&cx->tempPool, mark);
17721:     addName(ins, name);
17737: 
17587:     static const char* typestr[] = {
29896:         "object", "int", "double", "jsval", "string", "null", "boolean", "function"
17587:     };
29883:     debug_only_printf(LC_TMTracer, "import vp=%p name=%s type=%s flags=%d\n",
29883:                       (void*)p, name, typestr[t & 7], t >> 3);
17372: #endif
17317: }
17317: 
29882: class ImportGlobalSlotVisitor : public SlotVisitorBase
29882: {
29880:     TraceRecorder &mRecorder;
29880:     LIns *mBase;
29896:     JSTraceType *mTypemap;
29880: public:
29880:     ImportGlobalSlotVisitor(TraceRecorder &recorder,
29880:                             LIns *base,
29896:                             JSTraceType *typemap) :
29880:         mRecorder(recorder),
29880:         mBase(base),
29880:         mTypemap(typemap)
29880:     {}
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE void
29880:     visitGlobalSlot(jsval *vp, unsigned n, unsigned slot) {
29896:         JS_ASSERT(*mTypemap != TT_JSVAL);
29880:         mRecorder.import(mBase, mRecorder.nativeGlobalOffset(vp),
29880:                          vp, *mTypemap++, "global", n, NULL);
29880:     }
29880: };
29880: 
29882: class ImportBoxedStackSlotVisitor : public SlotVisitorBase
29882: {
29880:     TraceRecorder &mRecorder;
29880:     LIns *mBase;
29880:     ptrdiff_t mStackOffset;
29896:     JSTraceType *mTypemap;
29880:     JSStackFrame *mFp;
29880: public:
29880:     ImportBoxedStackSlotVisitor(TraceRecorder &recorder,
29880:                                 LIns *base,
29880:                                 ptrdiff_t stackOffset,
29896:                                 JSTraceType *typemap) :
29880:         mRecorder(recorder),
29880:         mBase(base),
29880:         mStackOffset(stackOffset),
29880:         mTypemap(typemap)
29880:     {}
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
29880:     visitStackSlots(jsval *vp, size_t count, JSStackFrame* fp) {
29880:         for (size_t i = 0; i < count; ++i) {
29896:             if (*mTypemap == TT_JSVAL) {
29896:                 mRecorder.import(mBase, mStackOffset, vp, TT_JSVAL,
29896:                                  "jsval", i, fp);
29880:                 LIns *vp_ins = mRecorder.get(vp);
29880:                 mRecorder.unbox_jsval(*vp, vp_ins,
29880:                                       mRecorder.copy(mRecorder.anchor));
29880:                 mRecorder.set(vp, vp_ins);
29880:             }
29880:             vp++;
29880:             mTypemap++;
29880:             mStackOffset += sizeof(double);
29880:         }
29880:         return true;
29880:     }
29880: };
29880: 
29882: class ImportUnboxedStackSlotVisitor : public SlotVisitorBase
29882: {
29880:     TraceRecorder &mRecorder;
29880:     LIns *mBase;
29880:     ptrdiff_t mStackOffset;
29896:     JSTraceType *mTypemap;
29880:     JSStackFrame *mFp;
29880: public:
29880:     ImportUnboxedStackSlotVisitor(TraceRecorder &recorder,
29880:                                   LIns *base,
29880:                                   ptrdiff_t stackOffset,
29896:                                   JSTraceType *typemap) :
29880:         mRecorder(recorder),
29880:         mBase(base),
29880:         mStackOffset(stackOffset),
29880:         mTypemap(typemap)
29880:     {}
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
29880:     visitStackSlots(jsval *vp, size_t count, JSStackFrame* fp) {
29880:         for (size_t i = 0; i < count; ++i) {
29896:             if (*mTypemap != TT_JSVAL) {
29880:                 mRecorder.import(mBase, mStackOffset, vp++, *mTypemap,
29882:                                  stackSlotKind(), i, fp);
29880:             }
29880:             mTypemap++;
29880:             mStackOffset += sizeof(double);
29880:         }
29880:         return true;
29880:     }
29880: };
29880: 
22652: JS_REQUIRES_STACK void
24246: TraceRecorder::import(TreeInfo* treeInfo, LIns* sp, unsigned stackSlots, unsigned ngslots,
29896:                       unsigned callDepth, JSTraceType* typeMap)
17997: {
28956:     /* If we get a partial list that doesn't have all the types (i.e. recording from a side
28956:        exit that was recorded but we added more global slots later), merge the missing types
28956:        from the entry type map. This is safe because at the loop edge we verify that we
28956:        have compatible types for all globals (entry type and loop edge type match). While
28956:        a different trace of the tree might have had a guard with a different type map for
28956:        these slots we just filled in here (the guard we continue from didn't know about them),
28956:        since we didn't take that particular guard the only way we could have ended up here
28956:        is if that other trace had at its end a compatible type distribution with the entry
28956:        map. Since thats exactly what we used to fill in the types our current side exit
28956:        didn't provide, this is always safe to do. */
28956: 
29896:     JSTraceType* globalTypeMap = typeMap + stackSlots;
28956:     unsigned length = treeInfo->nGlobalTypes();
28956: 
28956:     /*
28956:      * This is potentially the typemap of the side exit and thus shorter than the tree's
28956:      * global type map.
28956:      */
28956:     if (ngslots < length) {
28956:         mergeTypeMaps(&globalTypeMap/*out param*/, &ngslots/*out param*/,
28956:                       treeInfo->globalTypeMap(), length,
29896:                       (JSTraceType*)alloca(sizeof(JSTraceType) * length));
28956:     }
24491:     JS_ASSERT(ngslots == treeInfo->nGlobalTypes());
26972:     ptrdiff_t offset = -treeInfo->nativeStackBase;
29880: 
29880:     /*
29880:      * Check whether there are any values on the stack we have to unbox and
29880:      * do that first before we waste any time fetching the state from the
29880:      * stack.
29880:      */
29880:     ImportBoxedStackSlotVisitor boxedStackVisitor(*this, sp, offset, typeMap);
29882:     VisitStackSlots(boxedStackVisitor, cx, callDepth);
29880: 
29880:     ImportGlobalSlotVisitor globalVisitor(*this, lirbuf->state, globalTypeMap);
29882:     VisitGlobalSlots(globalVisitor, cx, globalObj, ngslots,
29880:                      treeInfo->globalSlots->data());
29880: 
29880:     ImportUnboxedStackSlotVisitor unboxedStackVisitor(*this, sp, offset,
29880:                                                       typeMap);
29882:     VisitStackSlots(unboxedStackVisitor, cx, callDepth);
17997: }
17997: 
25938: JS_REQUIRES_STACK bool
25938: TraceRecorder::isValidSlot(JSScope* scope, JSScopeProperty* sprop)
25938: {
25938:     uint32 setflags = (js_CodeSpec[*cx->fp->regs->pc].format & (JOF_SET | JOF_INCDEC | JOF_FOR));
25938: 
25938:     if (setflags) {
25938:         if (!SPROP_HAS_STUB_SETTER(sprop))
27933:             ABORT_TRACE_RV("non-stub setter", false);
25938:         if (sprop->attrs & JSPROP_READONLY)
27933:             ABORT_TRACE_RV("writing to a read-only property", false);
25938:     }
25938:     /* This check applies even when setflags == 0. */
25938:     if (setflags != JOF_SET && !SPROP_HAS_STUB_GETTER(sprop))
27933:         ABORT_TRACE_RV("non-stub getter", false);
25938: 
25938:     if (!SPROP_HAS_VALID_SLOT(sprop, scope))
27933:         ABORT_TRACE_RV("slotless obj property", false);
25938: 
25938:     return true;
25938: }
25938: 
17894: /* Lazily import a global slot if we don't already have it in the tracker. */
22652: JS_REQUIRES_STACK bool
17892: TraceRecorder::lazilyImportGlobalSlot(unsigned slot)
17891: {
18712:     if (slot != uint16(slot)) /* we use a table of 16-bit ints, bail out if that's not enough */
17891:         return false;
27065:     /*
27065:      * If the global object grows too large, alloca in js_ExecuteTree might fail, so
27065:      * abort tracing on global objects with unreasonably many slots.
27065:      */
27889:     if (STOBJ_NSLOTS(globalObj) > MAX_GLOBAL_SLOTS)
27065:         return false;
17891:     jsval* vp = &STOBJ_GET_SLOT(globalObj, slot);
24381:     if (known(vp))
17891:         return true; /* we already have it */
24491:     unsigned index = treeInfo->globalSlots->length();
18210:     /* Add the slot to the list of interned global slots. */
25491:     JS_ASSERT(treeInfo->nGlobalTypes() == treeInfo->globalSlots->length());
24491:     treeInfo->globalSlots->add(slot);
29896:     JSTraceType type = getCoercedType(*vp);
29896:     if (type == TT_INT32 && oracle.isGlobalSlotUndemotable(cx, slot))
29896:         type = TT_DOUBLE;
24246:     treeInfo->typeMap.add(type);
26282:     import(lirbuf->state, sizeof(struct InterpState) + slot*sizeof(double),
26282:            vp, type, "global", index, NULL);
29880:     specializeTreesToMissingGlobals(cx, globalObj, treeInfo);
17891:     return true;
17891: }
17891: 
18197: /* Write back a value onto the stack or global frames. */
18197: LIns*
18197: TraceRecorder::writeBack(LIns* i, LIns* base, ptrdiff_t offset)
18197: {
17792:     /* Sink all type casts targeting the stack into the side exit by simply storing the original
17792:        (uncasted) value. Each guard generates the side exit map based on the types of the
17792:        last stores to every stack location, so its safe to not perform them on-trace. */
17792:     if (isPromoteInt(i))
17792:         i = ::demote(lir, i);
18197:     return lir->insStorei(i, base, offset);
18197: }
18197: 
18197: /* Update the tracker, then issue a write back store. */
22652: JS_REQUIRES_STACK void
18197: TraceRecorder::set(jsval* p, LIns* i, bool initializing)
18197: {
26018:     JS_ASSERT(i != NULL);
24381:     JS_ASSERT(initializing || known(p));
24381:     checkForGlobalObjectReallocation();
18197:     tracker.set(p, i);
17803:     /* If we are writing to this location for the first time, calculate the offset into the
17803:        native frame manually, otherwise just look up the last load or store associated with
17803:        the same source address (p) and use the same offset/base. */
19068:     LIns* x = nativeFrameTracker.get(p);
19068:     if (!x) {
18197:         if (isGlobal(p))
26282:             x = writeBack(i, lirbuf->state, nativeGlobalOffset(p));
18197:         else
18197:             x = writeBack(i, lirbuf->sp, -treeInfo->nativeStackBase + nativeStackOffset(p));
17962:         nativeFrameTracker.set(p, x);
17803:     } else {
17815: #define ASSERT_VALID_CACHE_HIT(base, offset)                                  \
26282:     JS_ASSERT(base == lirbuf->sp || base == lirbuf->state);                   \
17815:     JS_ASSERT(offset == ((base == lirbuf->sp)                                 \
17962:         ? -treeInfo->nativeStackBase + nativeStackOffset(p)                   \
17815:         : nativeGlobalOffset(p)));                                            \
17815: 
17811:         JS_ASSERT(x->isop(LIR_sti) || x->isop(LIR_stqi));
30238:         ASSERT_VALID_CACHE_HIT(x->oprnd2(), x->disp());
30238:         writeBack(i, x->oprnd2(), x->disp());
17803:     }
17815: #undef ASSERT_VALID_CACHE_HIT
17320: }
17320: 
22652: JS_REQUIRES_STACK LIns*
24381: TraceRecorder::get(jsval* p)
24381: {
24381:     checkForGlobalObjectReallocation();
17320:     return tracker.get(p);
17320: }
17320: 
24381: JS_REQUIRES_STACK bool
24381: TraceRecorder::known(jsval* p)
24381: {
24381:     checkForGlobalObjectReallocation();
24381:     return tracker.has(p);
24381: }
24381: 
24381: /*
24381:  * The dslots of the global object are sometimes reallocated by the interpreter.
24381:  * This function check for that condition and re-maps the entries of the tracker
24381:  * accordingly.
24381:  */
24381: JS_REQUIRES_STACK void
24381: TraceRecorder::checkForGlobalObjectReallocation()
24381: {
24381:     if (global_dslots != globalObj->dslots) {
29883:         debug_only_print0(LC_TMTracer,
29883:                           "globalObj->dslots relocated, updating tracker\n");
24381:         jsval* src = global_dslots;
24381:         jsval* dst = globalObj->dslots;
24381:         jsuint length = globalObj->dslots[-1] - JS_INITIAL_NSLOTS;
27062:         LIns** map = (LIns**)alloca(sizeof(LIns*) * length);
24381:         for (jsuint n = 0; n < length; ++n) {
24381:             map[n] = tracker.get(src);
24381:             tracker.set(src++, NULL);
24381:         }
24381:         for (jsuint n = 0; n < length; ++n)
24381:             tracker.set(dst++, map[n]);
24381:         global_dslots = globalObj->dslots;
24381:     }
24381: }
24381: 
20416: /* Determine whether the current branch is a loop edge (taken or not taken). */
22652: static JS_REQUIRES_STACK bool
20416: js_IsLoopEdge(jsbytecode* pc, jsbytecode* header)
20416: {
20416:     switch (*pc) {
20416:       case JSOP_IFEQ:
20416:       case JSOP_IFNE:
20416:         return ((pc + GET_JUMP_OFFSET(pc)) == header);
20416:       case JSOP_IFEQX:
20416:       case JSOP_IFNEX:
20416:         return ((pc + GET_JUMPX_OFFSET(pc)) == header);
20416:       default:
20416:         JS_ASSERT((*pc == JSOP_AND) || (*pc == JSOP_ANDX) ||
20416:                   (*pc == JSOP_OR) || (*pc == JSOP_ORX));
20416:     }
20416:     return false;
20416: }
20416: 
29882: class AdjustCallerGlobalTypesVisitor : public SlotVisitorBase
29882: {
29880:     TraceRecorder &mRecorder;
29880:     JSContext *mCx;
29880:     nanojit::LirBuffer *mLirbuf;
29880:     nanojit::LirWriter *mLir;
29896:     JSTraceType *mTypeMap;
29880: public:
29880:     AdjustCallerGlobalTypesVisitor(TraceRecorder &recorder,
29896:                                    JSTraceType *typeMap) :
29880:         mRecorder(recorder),
29880:         mCx(mRecorder.cx),
29880:         mLirbuf(mRecorder.lirbuf),
29880:         mLir(mRecorder.lir),
29880:         mTypeMap(typeMap)
29880:     {}
29880: 
29896:     JSTraceType* getTypeMap()
29880:     {
29880:         return mTypeMap;
29880:     }
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE void
29880:     visitGlobalSlot(jsval *vp, unsigned n, unsigned slot) {
29880:         LIns *ins = mRecorder.get(vp);
29880:         bool isPromote = isPromoteInt(ins);
29896:         if (isPromote && *mTypeMap == TT_DOUBLE) {
29880:             mLir->insStorei(mRecorder.get(vp), mLirbuf->state,
29880:                             mRecorder.nativeGlobalOffset(vp));
29880:             /* Aggressively undo speculation so the inner tree will compile
29880:                if this fails. */
29880:             oracle.markGlobalSlotUndemotable(mCx, slot);
29880:         }
29896:         JS_ASSERT(!(!isPromote && *mTypeMap == TT_INT32));
29880:         ++mTypeMap;
29880:     }
29880: };
29880: 
29882: class AdjustCallerStackTypesVisitor : public SlotVisitorBase
29882: {
29880:     TraceRecorder &mRecorder;
29880:     JSContext *mCx;
29880:     nanojit::LirBuffer *mLirbuf;
29880:     nanojit::LirWriter *mLir;
29880:     unsigned mSlotnum;
29896:     JSTraceType *mTypeMap;
29880: public:
29880:     AdjustCallerStackTypesVisitor(TraceRecorder &recorder,
29896:                                   JSTraceType *typeMap) :
29880:         mRecorder(recorder),
29880:         mCx(mRecorder.cx),
29880:         mLirbuf(mRecorder.lirbuf),
29880:         mLir(mRecorder.lir),
29880:         mSlotnum(0),
29880:         mTypeMap(typeMap)
29880:     {}
29880: 
29896:     JSTraceType* getTypeMap()
29880:     {
29880:         return mTypeMap;
29880:     }
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
29880:     visitStackSlots(jsval *vp, size_t count, JSStackFrame* fp) {
29880:         for (size_t i = 0; i < count; ++i) {
29880:             LIns *ins = mRecorder.get(vp);
29880:             bool isPromote = isPromoteInt(ins);
29896:             if (isPromote && *mTypeMap == TT_DOUBLE) {
29880:                 mLir->insStorei(mRecorder.get(vp), mLirbuf->sp,
29880:                                 -mRecorder.treeInfo->nativeStackBase +
29880:                                 mRecorder.nativeStackOffset(vp));
29880:                 /* Aggressively undo speculation so the inner tree will
29880:                    compile if this fails. */
29880:                 oracle.markStackSlotUndemotable(mCx, mSlotnum);
29880:             }
29896:             JS_ASSERT(!(!isPromote && *mTypeMap == TT_INT32));
29880:             ++vp;
29880:             ++mTypeMap;
29880:             ++mSlotnum;
29880:         }
29880:         return true;
29880:     }
29880: };
29880: 
28239: /*
28239:  * Promote slots if necessary to match the called tree's type map. This function is
28239:  * infallible and must only be called if we are certain that it is possible to
28239:  * reconcile the types for each slot in the inner and outer trees.
28239:  */
28239: JS_REQUIRES_STACK void
24246: TraceRecorder::adjustCallerTypes(Fragment* f)
18250: {
24246:     TreeInfo* ti = (TreeInfo*)f->vmprivate;
29880: 
29880:     AdjustCallerGlobalTypesVisitor globalVisitor(*this, ti->globalTypeMap());
29882:     VisitGlobalSlots(globalVisitor, cx, *treeInfo->globalSlots);
29880: 
29880:     AdjustCallerStackTypesVisitor stackVisitor(*this, ti->stackTypeMap());
29882:     VisitStackSlots(stackVisitor, cx, 0);
29880: 
18650:     JS_ASSERT(f == f->root);
18250: }
18250: 
29896: JS_REQUIRES_STACK JSTraceType
24381: TraceRecorder::determineSlotType(jsval* vp)
19084: {
29896:     JSTraceType m;
19084:     LIns* i = get(vp);
27541:     if (isNumber(*vp)) {
29896:         m = isPromoteInt(i) ? TT_INT32 : TT_DOUBLE;
27541:     } else if (JSVAL_IS_OBJECT(*vp)) {
27541:         if (JSVAL_IS_NULL(*vp))
29896:             m = TT_NULL;
27541:         else if (HAS_FUNCTION_CLASS(JSVAL_TO_OBJECT(*vp)))
29896:             m = TT_FUNCTION;
27541:         else
29896:             m = TT_OBJECT;
27541:     } else {
29896:         JS_ASSERT(JSVAL_TAG(*vp) == JSVAL_STRING || JSVAL_TAG(*vp) == JSVAL_BOOLEAN);
29896:         JS_STATIC_ASSERT(TT_STRING == JSVAL_STRING);
29896:         JS_STATIC_ASSERT(TT_PSEUDOBOOLEAN == JSVAL_BOOLEAN);
29896:         m = JSTraceType(JSVAL_TAG(*vp));
29896:     }
29896:     JS_ASSERT(m != TT_INT32 || isInt32(*vp));
19084:     return m;
19084: }
19084: 
29882: class DetermineTypesVisitor : public SlotVisitorBase
29882: {
29880:     TraceRecorder &mRecorder;
29896:     JSTraceType *mTypeMap;
29880: public:
29880:     DetermineTypesVisitor(TraceRecorder &recorder,
29896:                           JSTraceType *typeMap) :
29880:         mRecorder(recorder),
29880:         mTypeMap(typeMap)
29880:     {}
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE void
29880:     visitGlobalSlot(jsval *vp, unsigned n, unsigned slot) {
29880:         *mTypeMap++ = mRecorder.determineSlotType(vp);
29880:     }
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
29880:     visitStackSlots(jsval *vp, size_t count, JSStackFrame* fp) {
29880:         for (size_t i = 0; i < count; ++i)
29880:             *mTypeMap++ = mRecorder.determineSlotType(vp++);
29880:         return true;
29880:     }
29880: 
29896:     JSTraceType* getTypeMap()
29880:     {
29880:         return mTypeMap;
29880:     }
29880: };
29880: 
27540: JS_REQUIRES_STACK VMSideExit*
17850: TraceRecorder::snapshot(ExitType exitType)
17850: {
17923:     JSStackFrame* fp = cx->fp;
20969:     JSFrameRegs* regs = fp->regs;
20969:     jsbytecode* pc = regs->pc;
20969: 
20969:     /* Check for a return-value opcode that needs to restart at the next instruction. */
20969:     const JSCodeSpec& cs = js_CodeSpec[*pc];
20969: 
26972:     /*
26972:      * When calling a _FAIL native, make the snapshot's pc point to the next
26972:      * instruction after the CALL or APPLY. Even on failure, a _FAIL native must not
26972:      * be called again from the interpreter.
26972:      */
20969:     bool resumeAfter = (pendingTraceableNative &&
24612:                         JSTN_ERRTYPE(pendingTraceableNative) == FAIL_STATUS);
20969:     if (resumeAfter) {
28086:         JS_ASSERT(*pc == JSOP_CALL || *pc == JSOP_APPLY || *pc == JSOP_NEW);
20969:         pc += cs.length;
20969:         regs->pc = pc;
21476:         MUST_FLOW_THROUGH("restore_pc");
20969:     }
20969: 
20969:     /* Generate the entry map for the (possibly advanced) pc and stash it in the trace. */
18425:     unsigned stackSlots = js_NativeStackSlots(cx, callDepth);
20969: 
18226:     /* It's sufficient to track the native stack use here since all stores above the
17962:        stack watermark defined by guards are killed. */
17962:     trackNativeStackUse(stackSlots + 1);
20969: 
20947:     /* Capture the type map into a temporary location. */
24491:     unsigned ngslots = treeInfo->globalSlots->length();
29896:     unsigned typemap_size = (stackSlots + ngslots) * sizeof(JSTraceType);
30435:     void *mark = JS_ARENA_MARK(&cx->tempPool);
30435:     JSTraceType* typemap;
30435:     JS_ARENA_ALLOCATE_CAST(typemap, JSTraceType*, &cx->tempPool, typemap_size);
29880: 
29880:     /*
29880:      * Determine the type of a store by looking at the current type of the
29880:      * actual value the interpreter is using. For numbers we have to check
29880:      * what kind of store we used last (integer or double) to figure out
29880:      * what the side exit show reflect in its typemap.
29880:      */
29880:     DetermineTypesVisitor detVisitor(*this, typemap);
29882:     VisitSlots(detVisitor, cx, callDepth, ngslots,
29880:                treeInfo->globalSlots->data());
29880:     JS_ASSERT(unsigned(detVisitor.getTypeMap() - typemap) ==
29880:               ngslots + stackSlots);
20969: 
26972:     /*
26972:      * If we are currently executing a traceable native or we are attaching a second trace
29896:      * to it, the value on top of the stack is a jsval. Make a note of this in the typemap.
26972:      */
26972:     if (pendingTraceableNative && (pendingTraceableNative->flags & JSTN_UNBOX_AFTER))
29896:         typemap[stackSlots - 1] = TT_JSVAL;
26972: 
26972:     /* Now restore the the original pc (after which early returns are ok). */
26286:     if (resumeAfter) {
20969:         MUST_FLOW_LABEL(restore_pc);
20969:         regs->pc = pc - cs.length;
20969:     } else {
20947:         /* If we take a snapshot on a goto, advance to the target address. This avoids inner
20947:            trees returning on a break goto, which the outer recorder then would confuse with
20947:            a break in the outer tree. */
20947:         if (*pc == JSOP_GOTO)
20947:             pc += GET_JUMP_OFFSET(pc);
20947:         else if (*pc == JSOP_GOTOX)
20947:             pc += GET_JUMPX_OFFSET(pc);
20969:     }
20969: 
26972:     /*
27540:      * Check if we already have a matching side exit; if so we can return that
27540:      * side exit instead of creating a new one.
26972:      */
21521:     VMSideExit** exits = treeInfo->sideExits.data();
20947:     unsigned nexits = treeInfo->sideExits.length();
20957:     if (exitType == LOOP_EXIT) {
20947:         for (unsigned n = 0; n < nexits; ++n) {
21521:             VMSideExit* e = exits[n];
25111:             if (e->pc == pc && e->imacpc == fp->imacpc &&
28996:                 ngslots == e->numGlobalSlots &&
24246:                 !memcmp(getFullTypeMap(exits[n]), typemap, typemap_size)) {
20947:                 AUDIT(mergedLoopExits);
30435:                 JS_ARENA_RELEASE(&cx->tempPool, mark);
27540:                 return e;
27540:             }
27540:         }
27540:     }
27540: 
29896:     if (sizeof(VMSideExit) + (stackSlots + ngslots) * sizeof(JSTraceType) >= NJ_MAX_SKIP_PAYLOAD_SZB) {
26972:         /*
23918:          * ::snapshot() is infallible in the sense that callers don't
23918:          * expect errors; but this is a trace-aborting error condition. So
23918:          * mangle the request to consume zero slots, and mark the tree as
23918:          * to-be-trashed. This should be safe as the trace will be aborted
23918:          * before assembly or execution due to the call to
23918:          * trackNativeStackUse above.
23918:          */
23918:         stackSlots = 0;
23918:         ngslots = 0;
30435:         typemap_size = 0;
23918:         trashSelf = true;
23918:     }
23918: 
27540:     /* We couldn't find a matching side exit, so create a new one. */
29896:     LIns* data = lir->insSkip(sizeof(VMSideExit) + (stackSlots + ngslots) * sizeof(JSTraceType));
27540:     VMSideExit* exit = (VMSideExit*) data->payload();
26972: 
26972:     /* Setup side exit structure. */
21521:     memset(exit, 0, sizeof(VMSideExit));
20931:     exit->from = fragment;
20931:     exit->calldepth = callDepth;
20931:     exit->numGlobalSlots = ngslots;
20931:     exit->numStackSlots = stackSlots;
20931:     exit->numStackSlotsBelowCurrentFrame = cx->fp->callee
18624:         ? nativeStackOffset(&cx->fp->argv[-2])/sizeof(double)
18624:         : 0;
20931:     exit->exitType = exitType;
22925:     exit->block = fp->blockChain;
25111:     exit->pc = pc;
25111:     exit->imacpc = fp->imacpc;
20931:     exit->sp_adj = (stackSlots * sizeof(double)) - treeInfo->nativeStackBase;
23262:     exit->rp_adj = exit->calldepth * sizeof(FrameInfo*);
28086:     exit->nativeCalleeWord = 0;
30034:     exit->lookupFlags = js_InferFlags(cx, 0);
24246:     memcpy(getFullTypeMap(exit), typemap, typemap_size);
30435: 
30435:     JS_ARENA_RELEASE(&cx->tempPool, mark);
27540:     return exit;
17381: }
17381: 
26972: JS_REQUIRES_STACK LIns*
27540: TraceRecorder::createGuardRecord(VMSideExit* exit)
27540: {
28182:     LIns* guardRec = lir->insSkip(sizeof(GuardRecord));
27540:     GuardRecord* gr = (GuardRecord*) guardRec->payload();
27540: 
27540:     memset(gr, 0, sizeof(GuardRecord));
27540:     gr->exit = exit;
27540:     exit->addGuard(gr);
27540: 
27540:     return guardRec;
27540: }
27540: 
27540: /*
27540:  * Emit a guard for condition (cond), expecting to evaluate to boolean result
27540:  * (expected) and using the supplied side exit if the conditon doesn't hold.
27540:  */
27540: JS_REQUIRES_STACK void
27540: TraceRecorder::guard(bool expected, LIns* cond, VMSideExit* exit)
27540: {
29883:     debug_only_printf(LC_TMRecorder,
29883:                       "    About to try emitting guard code for "
29893:                       "SideExit=%p exitType=%s\n",
29894:                       (void*)exit, getExitName(exit->exitType));
28837: 
27540:     LIns* guardRec = createGuardRecord(exit);
27540: 
27540:     /*
27540:      * BIG FAT WARNING: If compilation fails we don't reset the lirbuf, so it's
27540:      * safe to keep references to the side exits here. If we ever start
27540:      * rewinding those lirbufs, we have to make sure we purge the side exits
27540:      * that then no longer will be in valid memory.
27540:      */
26972:     if (exit->exitType == LOOP_EXIT)
26972:         treeInfo->sideExits.add(exit);
27540: 
26265:     if (!cond->isCond()) {
26265:         expected = !expected;
26265:         cond = lir->ins_eq0(cond);
26265:     }
27540: 
27540:     LIns* guardIns =
27540:         lir->insGuard(expected ? LIR_xf : LIR_xt, cond, guardRec);
28837:     if (!guardIns) {
29883:         debug_only_print0(LC_TMRecorder,
29883:                           "    redundant guard, eliminated, no codegen\n");
26972:     }
27540: }
27540: 
27540: JS_REQUIRES_STACK VMSideExit*
27540: TraceRecorder::copy(VMSideExit* copy)
27540: {
27540:     size_t typemap_size = copy->numGlobalSlots + copy->numStackSlots;
29896:     LIns* data = lir->insSkip(sizeof(VMSideExit) + typemap_size * sizeof(JSTraceType));
27540:     VMSideExit* exit = (VMSideExit*) data->payload();
27540: 
27540:     /* Copy side exit structure. */
29896:     memcpy(exit, copy, sizeof(VMSideExit) + typemap_size * sizeof(JSTraceType));
27540:     exit->guards = NULL;
27540:     exit->from = fragment;
27540:     exit->target = NULL;
27540: 
27540:     /*
27540:      * BIG FAT WARNING: If compilation fails we don't reset the lirbuf, so it's
27540:      * safe to keep references to the side exits here. If we ever start
27540:      * rewinding those lirbufs, we have to make sure we purge the side exits
27540:      * that then no longer will be in valid memory.
27540:      */
27540:     if (exit->exitType == LOOP_EXIT)
27540:         treeInfo->sideExits.add(exit);
27540:     return exit;
21083: }
21083: 
21083: /* Emit a guard for condition (cond), expecting to evaluate to boolean result (expected)
21083:    and generate a side exit with type exitType to jump to if the condition does not hold. */
26972: JS_REQUIRES_STACK void
17850: TraceRecorder::guard(bool expected, LIns* cond, ExitType exitType)
17323: {
26972:     guard(expected, cond, snapshot(exitType));
17336: }
17336: 
30295: /* Try to match the type of a slot to type t. checkType is used to verify that the type of
30295:  * values flowing into the loop edge is compatible with the type we expect in the loop header.
30295:  *
30295:  * @param v             Value.
30295:  * @param t             Typemap entry for value.
30295:  * @param stage_val     Outparam for set() address.
30295:  * @param stage_ins     Outparam for set() instruction.
30295:  * @param stage_count   Outparam for set() buffer count.
30295:  * @return              True if types are compatible, false otherwise.
30295:  */
30295: JS_REQUIRES_STACK bool
30295: TraceRecorder::checkType(jsval& v, JSTraceType t, jsval*& stage_val, LIns*& stage_ins,
30295:                          unsigned& stage_count)
30295: {
30295:     if (t == TT_INT32) { /* initially all whole numbers cause the slot to be demoted */
30295:         debug_only_printf(LC_TMTracer, "checkType(tag=1, t=%d, isnum=%d, i2f=%d) stage_count=%d\n",
30295:                           t,
30295:                           isNumber(v),
30295:                           isPromoteInt(get(&v)),
30295:                           stage_count);
30295:         if (!isNumber(v))
30295:             return false; /* not a number? type mismatch */
30295:         LIns* i = get(&v);
30295:         /* This is always a type mismatch, we can't close a double to an int. */
30295:         if (!isPromoteInt(i))
30295:             return false;
30295:         /* Looks good, slot is an int32, the last instruction should be promotable. */
30295:         JS_ASSERT(isInt32(v) && isPromoteInt(i));
30295:         /* Overwrite the value in this slot with the argument promoted back to an integer. */
30295:         stage_val = &v;
30295:         stage_ins = f2i(i);
30295:         stage_count++;
30295:         return true;
30295:     }
30295:     if (t == TT_DOUBLE) {
30295:         debug_only_printf(LC_TMTracer,
30295:                           "checkType(tag=2, t=%d, isnum=%d, promote=%d) stage_count=%d\n",
30295:                           t,
30295:                           isNumber(v),
30295:                           isPromoteInt(get(&v)),
30295:                           stage_count);
30295:         if (!isNumber(v))
30295:             return false; /* not a number? type mismatch */
30295:         LIns* i = get(&v);
30295:         /* We sink i2f conversions into the side exit, but at the loop edge we have to make
30295:            sure we promote back to double if at loop entry we want a double. */
30295:         if (isPromoteInt(i)) {
30295:             stage_val = &v;
30295:             stage_ins = lir->ins1(LIR_i2f, i);
30295:             stage_count++;
30295:         }
30295:         return true;
30295:     }
30295:     if (t == TT_NULL)
30295:         return JSVAL_IS_NULL(v);
30295:     if (t == TT_FUNCTION)
30295:         return !JSVAL_IS_PRIMITIVE(v) && HAS_FUNCTION_CLASS(JSVAL_TO_OBJECT(v));
30295:     if (t == TT_OBJECT)
30295:         return !JSVAL_IS_PRIMITIVE(v) && !HAS_FUNCTION_CLASS(JSVAL_TO_OBJECT(v));
30295: 
30295:     /* for non-number types we expect a precise match of the type */
30295:     JSTraceType vt = getCoercedType(v);
30295: #ifdef DEBUG
30295:     if (vt != t) {
30295:         debug_only_printf(LC_TMTracer, "Type mismatch: val %c, map %c ", typeChar[vt],
30295:                           typeChar[t]);
30295:     }
30295: #endif
30295:     debug_only_printf(LC_TMTracer, "checkType(vt=%d, t=%d) stage_count=%d\n",
30295:                       (int) vt, t, stage_count);
30295:     return vt == t;
30295: }
30295: 
30295: class SelfTypeStabilityVisitor : public SlotVisitorBase
30295: {
30295:     TraceRecorder &mRecorder;
30295:     JSTraceType *mTypeMap;
30295:     JSContext *mCx;
30295:     bool &mDemote;
30295:     jsval **&mStageVals;
30295:     LIns **&mStageIns;
30295:     unsigned &mStageCount;
30295:     unsigned mStackSlotNum;
30295:     bool mOk;
30295: 
30295: public:
30295: 
30295:     SelfTypeStabilityVisitor(TraceRecorder &recorder,
30295:                              JSTraceType *typeMap,
30295:                              bool &demote,
30295:                              jsval **&stageVals,
30295:                              LIns **&stageIns,
30295:                              unsigned &stageCount) :
30295:         mRecorder(recorder),
30295:         mTypeMap(typeMap),
30295:         mCx(mRecorder.cx),
30295:         mDemote(demote),
30295:         mStageVals(stageVals),
30295:         mStageIns(stageIns),
30295:         mStageCount(stageCount),
30295:         mStackSlotNum(0),
30295:         mOk(true)
30295:     {}
30295: 
30295:     JS_REQUIRES_STACK JS_ALWAYS_INLINE void
30295:     visitGlobalSlot(jsval *vp, unsigned n, unsigned slot) {
30295:         if (mOk) {
30295:             debug_only_printf(LC_TMTracer, "global%d ", n);
30295:             if (!mRecorder.checkType(*vp, *mTypeMap,
30295:                                      mStageVals[mStageCount],
30295:                                      mStageIns[mStageCount],
30295:                                      mStageCount)) {
30295:                 /* If the failure was an int->double, tell the oracle. */
30295:                 if (*mTypeMap == TT_INT32 && isNumber(*vp) &&
30295:                     !isPromoteInt(mRecorder.get(vp))) {
30295:                     oracle.markGlobalSlotUndemotable(mCx, slot);
30295:                     mDemote = true;
30295:                 } else {
30295:                     mOk = false;
30295:                 }
30295:             }
30295:             mTypeMap++;
30295:         }
30295:     }
30295: 
30295:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
30295:     visitStackSlots(jsval *vp, size_t count, JSStackFrame* fp) {
30295:         for (size_t i = 0; i < count; ++i) {
30295:             debug_only_printf(LC_TMTracer, "%s%u ", stackSlotKind(), unsigned(i));
30295:             if (!mRecorder.checkType(*vp, *mTypeMap,
30295:                                      mStageVals[mStageCount],
30295:                                      mStageIns[mStageCount],
30295:                                      mStageCount)) {
30295:                 if (*mTypeMap == TT_INT32 && isNumber(*vp) &&
30295:                     !isPromoteInt(mRecorder.get(vp))) {
30295:                     oracle.markStackSlotUndemotable(mCx, mStackSlotNum);
30295:                     mDemote = true;
30295:                 } else {
30295:                     mOk = false;
30295:                     break;
30295:                 }
30295:             }
30295:             vp++;
30295:             mTypeMap++;
30295:             mStackSlotNum++;
30295:         }
30295:         return mOk;
30295:     }
30295: 
30295:     bool isOk() {
30295:         return mOk;
30295:     }
30295: };
30295: 
30295: class PeerTypeStabilityVisitor : public SlotVisitorBase
30295: {
30295:     TraceRecorder &mRecorder;
30295:     JSTraceType *mTypeMap;
30295:     jsval **&mStageVals;
30295:     LIns **&mStageIns;
30295:     unsigned &mStageCount;
30295:     bool mOk;
30295: 
30295: public:
30295: 
30295:     PeerTypeStabilityVisitor(TraceRecorder &recorder,
30295:                               JSTraceType *typeMap,
30295:                               jsval **&stageVals,
30295:                               LIns **&stageIns,
30295:                               unsigned &stageCount) :
30295:         mRecorder(recorder),
30295:         mTypeMap(typeMap),
30295:         mStageVals(stageVals),
30295:         mStageIns(stageIns),
30295:         mStageCount(stageCount),
30295:         mOk(true)
30295:     {}
30295: 
30295:     JS_REQUIRES_STACK JS_ALWAYS_INLINE void check(jsval *vp) {
30295:         if (!mRecorder.checkType(*vp, *mTypeMap++,
30295:                                  mStageVals[mStageCount],
30295:                                  mStageIns[mStageCount],
30295:                                  mStageCount))
30295:             mOk = false;
30295:     }
30295: 
30295:     JS_REQUIRES_STACK JS_ALWAYS_INLINE void
30295:     visitGlobalSlot(jsval *vp, unsigned n, unsigned slot) {
30295:         if (mOk)
30295:             check(vp);
30295:     }
30295: 
30295:     JS_REQUIRES_STACK JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
30295:     visitStackSlots(jsval *vp, size_t count, JSStackFrame* fp) {
30295:         for (size_t i = 0; i < count; ++i) {
30295:             check(vp++);
30295:             if (!mOk)
30295:                 break;
30295:         }
30295:         return mOk;
30295:     }
30295: 
30295:     bool isOk() {
30295:         return mOk;
30295:     }
30295: };
30295: 
30295: class UndemoteVisitor : public SlotVisitorBase
30295: {
30295:     TraceRecorder &mRecorder;
30295:     JSContext *mCx;
30295:     JSTraceType *mTypeMap;
30295:     unsigned mStackSlotNum;
30295: public:
30295:     UndemoteVisitor(TraceRecorder &recorder,
30295:                     JSTraceType *typeMap) :
30295:         mRecorder(recorder),
30295:         mCx(mRecorder.cx),
30295:         mTypeMap(typeMap),
30295:         mStackSlotNum(0)
30295:     {}
30295: 
30295:     JS_REQUIRES_STACK JS_ALWAYS_INLINE void
30295:     visitGlobalSlot(jsval *vp, unsigned n, unsigned slot) {
30295:         if (*mTypeMap == TT_INT32) {
30295:             JS_ASSERT(isNumber(*vp));
30295:             if (!isPromoteInt(mRecorder.get(vp)))
30295:                 oracle.markGlobalSlotUndemotable(mCx, slot);
30295:         } else if (*mTypeMap == TT_DOUBLE) {
30295:             JS_ASSERT(isNumber(*vp));
30295:             oracle.markGlobalSlotUndemotable(mCx, slot);
30295:         } else {
30295:                 JS_ASSERT(*mTypeMap == TT_OBJECT
30295:                           ? !JSVAL_IS_PRIMITIVE(*vp) && !HAS_FUNCTION_CLASS(JSVAL_TO_OBJECT(*vp))
30295:                           : *mTypeMap == TT_STRING
30295:                           ? JSVAL_IS_STRING(*vp)
30295:                           : *mTypeMap == TT_PSEUDOBOOLEAN && JSVAL_TAG(*vp) == JSVAL_BOOLEAN);
30295:         }
30295:         mTypeMap++;
30295:     }
30295: 
30295:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
30295:     visitStackSlots(jsval *vp, size_t count, JSStackFrame* fp) {
30295:         for (size_t i = 0; i < count; ++i) {
30295:             if (*mTypeMap == TT_INT32) {
30295:                 JS_ASSERT(isNumber(*vp));
30295:                 if (!isPromoteInt(mRecorder.get(vp)))
30295:                     oracle.markStackSlotUndemotable(mCx, mStackSlotNum);
30295:             } else if (*mTypeMap == TT_DOUBLE) {
30295:                 JS_ASSERT(isNumber(*vp));
30295:                 oracle.markStackSlotUndemotable(mCx, mStackSlotNum);
30295:             } else {
30295:                 JS_ASSERT(*mTypeMap == TT_NULL
30295:                           ? JSVAL_IS_NULL(*vp)
30295:                           : *mTypeMap == TT_FUNCTION
30295:                           ? !JSVAL_IS_PRIMITIVE(*vp) && HAS_FUNCTION_CLASS(JSVAL_TO_OBJECT(*vp))
30295:                           : *mTypeMap == TT_OBJECT
30295:                           ? !JSVAL_IS_PRIMITIVE(*vp) && !HAS_FUNCTION_CLASS(JSVAL_TO_OBJECT(*vp))
30295:                           : *mTypeMap == TT_STRING
30295:                           ? JSVAL_IS_STRING(*vp)
30295:                           : *mTypeMap == TT_PSEUDOBOOLEAN && JSVAL_TAG(*vp) == JSVAL_BOOLEAN);
30295:             }
30295:             mStackSlotNum++;
30295:             mTypeMap++;
30295:             vp++;
30295:         }
30295:         return true;
30295:     }
30295: };
30295: 
30295: /**
30295:  * Make sure that the current values in the given stack frame and all stack frames
30295:  * up and including entryFrame are type-compatible with the entry map.
30295:  *
30295:  * @param root_peer         First fragment in peer list.
30295:  * @param stable_peer       Outparam for first type stable peer.
30295:  * @param demote            True if stability was achieved through demotion.
30295:  * @return                  True if type stable, false otherwise.
30295:  */
30295: JS_REQUIRES_STACK bool
30295: TraceRecorder::deduceTypeStability(Fragment* root_peer, Fragment** stable_peer, bool& demote)
30295: {
30295:     JS_ASSERT(treeInfo->globalSlots->length() ==
30295:               treeInfo->nGlobalTypes());
30295: 
30295:     if (stable_peer)
30295:         *stable_peer = NULL;
30295: 
30295:     /*
30295:      * Rather than calculate all of this stuff twice, it gets cached locally.  The "stage" buffers
30295:      * are for calls to set() that will change the exit types.
30295:      */
30295:     bool success;
30295:     unsigned stage_count;
30295:     jsval** stage_vals = (jsval**)alloca(sizeof(jsval*) * (treeInfo->typeMap.length()));
30295:     LIns** stage_ins = (LIns**)alloca(sizeof(LIns*) * (treeInfo->typeMap.length()));
30295: 
30295:     /* First run through and see if we can close ourselves - best case! */
30295:     stage_count = 0;
30295:     success = false;
30295: 
30295:     debug_only_printf(LC_TMTracer, "Checking type stability against self=%p\n", (void*)fragment);
30295:     SelfTypeStabilityVisitor selfVisitor(*this, treeInfo->stackTypeMap(), demote,
30295:                                          stage_vals, stage_ins, stage_count);
30295:     VisitSlots(selfVisitor, cx, 0, *treeInfo->globalSlots);
30295:     success = selfVisitor.isOk();
30295: 
30295:     /* If we got a success and we don't need to recompile, we should just close here. */
30295:     if (success && !demote) {
30295:         for (unsigned i = 0; i < stage_count; i++)
30295:             set(stage_vals[i], stage_ins[i]);
30295:         return true;
30295:     /* If we need to trash, don't bother checking peers. */
30295:     } else if (trashSelf) {
30295:         return false;
30295:     }
30295: 
30295:     demote = false;
30295: 
30295:     /* At this point the tree is about to be incomplete, so let's see if we can connect to any
30295:      * peer fragment that is type stable.
30295:      */
30295:     Fragment* f;
30295:     TreeInfo* ti;
30295:     for (f = root_peer; f != NULL; f = f->peer) {
30295:         debug_only_printf(LC_TMTracer,
30295:                           "Checking type stability against peer=%p (code=%p)\n",
30295:                           (void*)f, f->code());
30295:         if (!f->code())
30295:             continue;
30295:         ti = (TreeInfo*)f->vmprivate;
30295:         /* Don't allow varying stack depths */
30295:         if ((ti->nStackTypes != treeInfo->nStackTypes) ||
30295:             (ti->typeMap.length() != treeInfo->typeMap.length()) ||
30295:             (ti->globalSlots->length() != treeInfo->globalSlots->length()))
30295:             continue;
30295:         stage_count = 0;
30295:         success = false;
30295: 
30295:         PeerTypeStabilityVisitor peerVisitor(*this, ti->stackTypeMap(),
30295:                                              stage_vals, stage_ins, stage_count);
30295:         VisitSlots(peerVisitor, cx, 0, *treeInfo->globalSlots);
30295:         success = peerVisitor.isOk();
30295: 
30295:         if (success) {
30295:             /*
30295:              * There was a successful match.  We don't care about restoring the saved staging, but
30295:              * we do need to clear the original undemote list.
30295:              */
30295:             for (unsigned i = 0; i < stage_count; i++)
30295:                 set(stage_vals[i], stage_ins[i]);
30295:             if (stable_peer)
30295:                 *stable_peer = f;
30295:             demote = false;
30295:             return false;
30295:         }
30295:     }
30295: 
30295:     /*
30295:      * If this is a loop trace and it would be stable with demotions, build an undemote list
30295:      * and return true.  Our caller should sniff this and trash the tree, recording a new one
30295:      * that will assumedly stabilize.
30295:      */
30295:     if (demote && fragment->kind == LoopTrace) {
30295:         UndemoteVisitor visitor(*this, treeInfo->stackTypeMap());
30295:         VisitSlots(visitor, cx, 0, *treeInfo->globalSlots);
30295:         return true;
30295:     } else {
30295:         demote = false;
30295:     }
30295: 
30295:     return false;
30295: }
30295: 
27493: static JS_REQUIRES_STACK void
27493: FlushJITCache(JSContext* cx)
27493: {
27493:     if (!TRACING_ENABLED(cx))
27493:         return;
27493:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
29883:     debug_only_print0(LC_TMTracer, "Flushing cache.\n");
27493:     if (tm->recorder)
27493:         js_AbortRecording(cx, "flush cache");
27493:     TraceRecorder* tr;
27493:     while ((tr = tm->abortStack) != NULL) {
27493:         tr->removeFragmentoReferences();
27493:         tr->deepAbort();
27493:         tr->popAbortStack();
27493:     }
27493:     Fragmento* fragmento = tm->fragmento;
27493:     if (fragmento) {
27493:         if (tm->prohibitFlush) {
29883:             debug_only_print0(LC_TMTracer,
29883:                               "Deferring fragmento flush due to deep bail.\n");
27493:             tm->needFlush = JS_TRUE;
27493:             return;
27493:         }
27493: 
27493:         fragmento->clearFrags();
27493: #ifdef DEBUG
27493:         JS_ASSERT(fragmento->labels);
27493:         fragmento->labels->clear();
27493: #endif
27493:         tm->lirbuf->rewind();
27493:         for (size_t i = 0; i < FRAGMENT_TABLE_SIZE; ++i) {
27493:             VMFragment* f = tm->vmfragments[i];
27493:             while (f) {
27493:                 VMFragment* next = f->next;
27493:                 fragmento->clearFragment(f);
27493:                 f = next;
27493:             }
27493:             tm->vmfragments[i] = NULL;
27493:         }
27493:         for (size_t i = 0; i < MONITOR_N_GLOBAL_STATES; ++i) {
27493:             tm->globalStates[i].globalShape = -1;
27493:             tm->globalStates[i].globalSlots->clear();
27493:         }
27493:     }
27493:     tm->needFlush = JS_FALSE;
27493: }
27493: 
18606: /* Compile the current fragment. */
22652: JS_REQUIRES_STACK void
24307: TraceRecorder::compile(JSTraceMonitor* tm)
24307: {
29368: #ifdef MOZ_TRACEVIS
29368:     TraceVisStateObj tvso(S_COMPILE);
29368: #endif
29368: 
27493:     if (tm->needFlush) {
27493:         FlushJITCache(cx);
27493:         return;
27493:     }
24307:     Fragmento* fragmento = tm->fragmento;
18118:     if (treeInfo->maxNativeStackSlots >= MAX_NATIVE_STACK_SLOTS) {
29883:         debug_only_print0(LC_TMTracer, "Blacklist: excessive stack use.\n");
28105:         js_Blacklist((jsbytecode*) fragment->root->ip);
18118:         return;
18118:     }
25099:     if (anchor && anchor->exitType != CASE_EXIT)
18781:         ++treeInfo->branchCount;
22662:     if (lirbuf->outOMem()) {
21483:         fragmento->assm()->setError(nanojit::OutOMem);
21483:         return;
21483:     }
18606:     ::compile(fragmento->assm(), fragment);
24244:     if (fragmento->assm()->error() == nanojit::OutOMem)
24244:         return;
22633:     if (fragmento->assm()->error() != nanojit::None) {
29883:         debug_only_print0(LC_TMTracer, "Blacklisted: error during compilation\n");
28105:         js_Blacklist((jsbytecode*) fragment->root->ip);
21483:         return;
22633:     }
28105:     js_resetRecordingAttempts(cx, (jsbytecode*) fragment->ip);
28105:     js_resetRecordingAttempts(cx, (jsbytecode*) fragment->root->ip);
25099:     if (anchor) {
25099: #ifdef NANOJIT_IA32
25099:         if (anchor->exitType == CASE_EXIT)
25099:             fragmento->assm()->patch(anchor, anchor->switchInfo);
25099:         else
25099: #endif
24244:             fragmento->assm()->patch(anchor);
25099:     }
18211:     JS_ASSERT(fragment->code());
18211:     JS_ASSERT(!fragment->vmprivate);
18211:     if (fragment == fragment->root)
18211:         fragment->vmprivate = treeInfo;
17884:     /* :TODO: windows support */
17884: #if defined DEBUG && !defined WIN32
21717:     const char* filename = cx->fp->script->filename;
21717:     char* label = (char*)malloc((filename ? strlen(filename) : 7) + 16);
21717:     sprintf(label, "%s:%u", filename ? filename : "<stdin>",
21685:             js_FramePCToLineNumber(cx, cx->fp));
17731:     fragmento->labels->add(fragment, sizeof(Fragment), 0, label);
18056:     free(label);
17731: #endif
18697:     AUDIT(traceCompleted);
18334: }
18334: 
21433: static bool
21521: js_JoinPeersIfCompatible(Fragmento* frago, Fragment* stableFrag, TreeInfo* stableTree,
21521:                          VMSideExit* exit)
21433: {
24491:     JS_ASSERT(exit->numStackSlots == stableTree->nStackTypes);
24246: 
21433:     /* Must have a matching type unstable exit. */
24246:     if ((exit->numGlobalSlots + exit->numStackSlots != stableTree->typeMap.length()) ||
24246:         memcmp(getFullTypeMap(exit), stableTree->typeMap.data(), stableTree->typeMap.length())) {
21433:        return false;
21433:     }
21433: 
21433:     exit->target = stableFrag;
21433:     frago->assm()->patch(exit);
21433: 
21463:     stableTree->dependentTrees.addUnique(exit->from->root);
25491:     ((TreeInfo*)exit->from->root->vmprivate)->linkedTrees.addUnique(stableFrag);
21463: 
21433:     return true;
21433: }
21433: 
30295: /* Complete and compile a trace and link it to the existing tree if appropriate. */
30295: JS_REQUIRES_STACK void
30295: TraceRecorder::closeLoop(JSTraceMonitor* tm, bool& demote)
21433: {
26557:     /*
26557:      * We should have arrived back at the loop header, and hence we don't want to be in an imacro
26557:      * here and the opcode should be either JSOP_LOOP, or in case this loop was blacklisted in the
26557:      * meantime JSOP_NOP.
26557:      */
26557:     JS_ASSERT((*cx->fp->regs->pc == JSOP_LOOP || *cx->fp->regs->pc == JSOP_NOP) && !cx->fp->imacpc);
26557: 
30295:     bool stable;
30295:     Fragment* peer;
30295:     VMFragment* peer_root;
30295:     Fragmento* fragmento = tm->fragmento;
21433: 
21684:     if (callDepth != 0) {
29883:         debug_only_print0(LC_TMTracer,
29883:                           "Blacklisted: stack depth mismatch, possible recursion.\n");
28105:         js_Blacklist((jsbytecode*) fragment->root->ip);
22609:         trashSelf = true;
30295:         return;
30295:     }
30295: 
30295:     VMSideExit* exit = snapshot(UNSTABLE_LOOP_EXIT);
24491:     JS_ASSERT(exit->numStackSlots == treeInfo->nStackTypes);
24491: 
26818:     VMFragment* root = (VMFragment*)fragment->root;
30295:     peer_root = getLoop(traceMonitor, root->ip, root->globalObj, root->globalShape, root->argc);
30295:     JS_ASSERT(peer_root != NULL);
30295: 
30295:     stable = deduceTypeStability(peer_root, &peer, demote);
21433: 
21433: #if DEBUG
30295:     if (!stable)
18606:         AUDIT(unstableLoopVariable);
21433: #endif
21433: 
30295:     if (trashSelf) {
30295:         debug_only_print0(LC_TMTracer, "Trashing tree from type instability.\n");
30295:         return;
30295:     }
30295: 
30295:     if (stable && demote) {
30295:         JS_ASSERT(fragment->kind == LoopTrace);
30295:         return;
30295:     }
30295: 
30295:     if (!stable) {
30452:         fragment->lastIns = lir->insGuard(LIR_x, NULL, createGuardRecord(exit));
21433: 
30295:         /*
30295:          * If we didn't find a type stable peer, we compile the loop anyway and
30295:          * hope it becomes stable later.
30295:          */
21433:         if (!peer) {
21685:             /*
21685:              * If such a fragment does not exist, let's compile the loop ahead
21685:              * of time anyway.  Later, if the loop becomes type stable, we will
21685:              * connect these two fragments together.
21433:              */
29883:             debug_only_print0(LC_TMTracer,
29883:                               "Trace has unstable loop variable with no stable peer, "
29883:                               "compiling anyway.\n");
25102:             UnstableExit* uexit = new UnstableExit;
21433:             uexit->fragment = fragment;
21433:             uexit->exit = exit;
21433:             uexit->next = treeInfo->unstableExits;
21433:             treeInfo->unstableExits = uexit;
21433:         } else {
21433:             JS_ASSERT(peer->code());
21433:             exit->target = peer;
29883:             debug_only_printf(LC_TMTracer,
29883:                               "Joining type-unstable trace to target fragment %p.\n",
29883:                               (void*)peer);
30295:             stable = true;
21783:             ((TreeInfo*)peer->vmprivate)->dependentTrees.addUnique(fragment->root);
25491:             treeInfo->linkedTrees.addUnique(peer);
21433:         }
21433:     } else {
21433:         exit->target = fragment->root;
30452:         fragment->lastIns = lir->insGuard(LIR_loop, NULL, createGuardRecord(exit));
26557:     }
30295:     compile(tm);
26286: 
21483:     if (fragmento->assm()->error() != nanojit::None)
30295:         return;
30295: 
30295:     joinEdgesToEntry(fragmento, peer_root);
19588: 
29883:     debug_only_print0(LC_TMTracer,
29883:                       "updating specializations on dependent and linked trees\n");
25491:     if (fragment->root->vmprivate)
29880:         specializeTreesToMissingGlobals(cx, globalObj, (TreeInfo*)fragment->root->vmprivate);
25491: 
25627:     /*
25627:      * If this is a newly formed tree, and the outer tree has not been compiled yet, we
25627:      * should try to compile the outer tree again.
25627:      */
25937:     if (outer)
30295:         js_AttemptCompilation(cx, tm, globalObj, outer, outerArgc);
29883: #ifdef JS_JIT_SPEW
29883:     debug_only_printf(LC_TMMinimal,
29883:                       "recording completed at  %s:%u@%u via closeLoop\n",
21685:                       cx->fp->script->filename,
21685:                       js_FramePCToLineNumber(cx, cx->fp),
29883:                       FramePCOffset(cx->fp));
29883:     debug_only_print0(LC_TMMinimal, "\n");
29883: #endif
21433: }
21433: 
22652: JS_REQUIRES_STACK void
26818: TraceRecorder::joinEdgesToEntry(Fragmento* fragmento, VMFragment* peer_root)
21433: {
21433:     if (fragment->kind == LoopTrace) {
21433:         TreeInfo* ti;
21433:         Fragment* peer;
29896:         JSTraceType* t1;
29896:         JSTraceType* t2;
21433:         UnstableExit* uexit, **unext;
27062:         uint32* stackDemotes = (uint32*)alloca(sizeof(uint32) * treeInfo->nStackTypes);
27062:         uint32* globalDemotes = (uint32*)alloca(sizeof(uint32) * treeInfo->nGlobalTypes());
24246: 
30295:         for (peer = peer_root; peer != NULL; peer = peer->peer) {
21433:             if (!peer->code())
21433:                 continue;
21433:             ti = (TreeInfo*)peer->vmprivate;
21433:             uexit = ti->unstableExits;
21433:             unext = &ti->unstableExits;
21433:             while (uexit != NULL) {
21433:                 bool remove = js_JoinPeersIfCompatible(fragmento, fragment, treeInfo, uexit->exit);
21433:                 JS_ASSERT(!remove || fragment != peer);
29883:                 debug_only_stmt(
29883:                     if (remove) {
29883:                         debug_only_printf(LC_TMTracer,
29883:                                           "Joining type-stable trace to target exit %p->%p.\n",
29883:                                           (void*)uexit->fragment, (void*)uexit->exit);
29883:                     }
29883:                 )
21433:                 if (!remove) {
21433:                     /* See if this exit contains mismatch demotions, which imply trashing a tree.
21433:                        This is actually faster than trashing the original tree as soon as the
21433:                        instability is detected, since we could have compiled a fairly stable
21433:                        tree that ran faster with integers. */
24246:                     unsigned stackCount = 0;
24246:                     unsigned globalCount = 0;
24246:                     t1 = treeInfo->stackTypeMap();
24246:                     t2 = getStackTypeMap(uexit->exit);
21433:                     for (unsigned i = 0; i < uexit->exit->numStackSlots; i++) {
29896:                         if (t2[i] == TT_INT32 && t1[i] == TT_DOUBLE) {
24246:                             stackDemotes[stackCount++] = i;
21433:                         } else if (t2[i] != t1[i]) {
24246:                             stackCount = 0;
21433:                             break;
21433:                         }
21433:                     }
24246:                     t1 = treeInfo->globalTypeMap();
24246:                     t2 = getGlobalTypeMap(uexit->exit);
24246:                     for (unsigned i = 0; i < uexit->exit->numGlobalSlots; i++) {
29896:                         if (t2[i] == TT_INT32 && t1[i] == TT_DOUBLE) {
24246:                             globalDemotes[globalCount++] = i;
24246:                         } else if (t2[i] != t1[i]) {
24246:                             globalCount = 0;
24246:                             stackCount = 0;
24246:                             break;
24246:                         }
24246:                     }
24246:                     if (stackCount || globalCount) {
24246:                         for (unsigned i = 0; i < stackCount; i++)
24246:                             oracle.markStackSlotUndemotable(cx, stackDemotes[i]);
24246:                         for (unsigned i = 0; i < globalCount; i++)
29900:                             oracle.markGlobalSlotUndemotable(cx, ti->globalSlots->get(globalDemotes[i]));
22609:                         JS_ASSERT(peer == uexit->fragment->root);
22609:                         if (fragment == peer)
22609:                             trashSelf = true;
22609:                         else
22609:                             whichTreesToTrash.addUnique(uexit->fragment->root);
21433:                         break;
21433:                     }
21433:                 }
21433:                 if (remove) {
21433:                     *unext = uexit->next;
25102:                     delete uexit;
21433:                     uexit = *unext;
21433:                 } else {
21433:                     unext = &uexit->next;
21433:                     uexit = uexit->next;
21433:                 }
21433:             }
21433:         }
21433:     }
21433: 
29883:     debug_only_stmt(js_DumpPeerStability(traceMonitor, peer_root->ip, peer_root->globalObj,
28244:                              peer_root->globalShape, peer_root->argc);)
18606: }
18606: 
18606: /* Emit an always-exit guard and compile the tree (used for break statements. */
22652: JS_REQUIRES_STACK void
30295: TraceRecorder::endLoop(JSTraceMonitor* tm)
18606: {
21684:     if (callDepth != 0) {
29883:         debug_only_print0(LC_TMTracer, "Blacklisted: stack depth mismatch, possible recursion.\n");
28105:         js_Blacklist((jsbytecode*) fragment->root->ip);
22609:         trashSelf = true;
21684:         return;
21684:     }
21684: 
27540:     fragment->lastIns =
30452:         lir->insGuard(LIR_x, NULL, createGuardRecord(snapshot(LOOP_EXIT)));
30295:     compile(tm);
30295: 
30295:     if (tm->fragmento->assm()->error() != nanojit::None)
21483:         return;
21483: 
26818:     VMFragment* root = (VMFragment*)fragment->root;
30295:     joinEdgesToEntry(tm->fragmento, getLoop(tm, root->ip, root->globalObj, root->globalShape, root->argc));
21433: 
25491:     /* Note: this must always be done, in case we added new globals on trace and haven't yet
25491:        propagated those to linked and dependent trees. */
29883:     debug_only_print0(LC_TMTracer,
29883:                       "updating specializations on dependent and linked trees\n");
25491:     if (fragment->root->vmprivate)
29880:         specializeTreesToMissingGlobals(cx, globalObj, (TreeInfo*)fragment->root->vmprivate);
25491: 
25627:     /*
25627:      * If this is a newly formed tree, and the outer tree has not been compiled yet, we
25627:      * should try to compile the outer tree again.
25627:      */
25937:     if (outer)
30295:         js_AttemptCompilation(cx, tm, globalObj, outer, outerArgc);
29883: #ifdef JS_JIT_SPEW
29883:     debug_only_printf(LC_TMMinimal,
29883:                       "Recording completed at  %s:%u@%u via endLoop\n",
21685:                       cx->fp->script->filename,
21685:                       js_FramePCToLineNumber(cx, cx->fp),
29883:                       FramePCOffset(cx->fp));
29883:     debug_only_print0(LC_TMTracer, "\n");
29883: #endif
17334: }
17334: 
18241: /* Emit code to adjust the stack to match the inner tree's stack expectations. */
22652: JS_REQUIRES_STACK void
18250: TraceRecorder::prepareTreeCall(Fragment* inner)
17966: {
18121:     TreeInfo* ti = (TreeInfo*)inner->vmprivate;
18241:     inner_sp_ins = lirbuf->sp;
18215:     /* The inner tree expects to be called from the current frame. If the outer tree (this
18215:        trace) is currently inside a function inlining code (calldepth > 0), we have to advance
18007:        the native stack pointer such that we match what the inner trace expects to see. We
18007:        move it back when we come out of the inner tree call. */
18007:     if (callDepth > 0) {
18121:         /* Calculate the amount we have to lift the native stack pointer by to compensate for
18121:            any outer frames that the inner tree doesn't expect but the outer tree has. */
18280:         ptrdiff_t sp_adj = nativeStackOffset(&cx->fp->argv[-2]);
18133:         /* Calculate the amount we have to lift the call stack by */
23262:         ptrdiff_t rp_adj = callDepth * sizeof(FrameInfo*);
18121:         /* Guard that we have enough stack space for the tree we are trying to call on top
18121:            of the new value for sp. */
29883:         debug_only_printf(LC_TMTracer,
29883:                           "sp_adj=%d outer=%d inner=%d\n",
29883:                           sp_adj, treeInfo->nativeStackBase, ti->nativeStackBase);
18232:         LIns* sp_top = lir->ins2i(LIR_piadd, lirbuf->sp,
18184:                 - treeInfo->nativeStackBase /* rebase sp to beginning of outer tree's stack */
18184:                 + sp_adj /* adjust for stack in outer frame inner tree can't see */
18184:                 + ti->maxNativeStackSlots * sizeof(double)); /* plus the inner tree's stack */
18121:         guard(true, lir->ins2(LIR_lt, sp_top, eos_ins), OOM_EXIT);
18133:         /* Guard that we have enough call stack space. */
18232:         LIns* rp_top = lir->ins2i(LIR_piadd, lirbuf->rp, rp_adj +
23262:                 ti->maxCallDepth * sizeof(FrameInfo*));
18133:         guard(true, lir->ins2(LIR_lt, rp_top, eor_ins), OOM_EXIT);
18133:         /* We have enough space, so adjust sp and rp to their new level. */
18241:         lir->insStorei(inner_sp_ins = lir->ins2i(LIR_piadd, lirbuf->sp,
18184:                 - treeInfo->nativeStackBase /* rebase sp to beginning of outer tree's stack */
18184:                 + sp_adj /* adjust for stack in outer frame inner tree can't see */
18184:                 + ti->nativeStackBase), /* plus the inner tree's stack base */
18007:                 lirbuf->state, offsetof(InterpState, sp));
18232:         lir->insStorei(lir->ins2i(LIR_piadd, lirbuf->rp, rp_adj),
18133:                 lirbuf->state, offsetof(InterpState, rp));
18007:     }
18241: }
18241: 
18241: /* Record a call to an inner tree. */
22652: JS_REQUIRES_STACK void
21521: TraceRecorder::emitTreeCall(Fragment* inner, VMSideExit* exit)
18241: {
18241:     TreeInfo* ti = (TreeInfo*)inner->vmprivate;
26972: 
18007:     /* Invoke the inner tree. */
18712:     LIns* args[] = { INS_CONSTPTR(inner), lirbuf->state }; /* reverse order */
20915:     LIns* ret = lir->insCall(&js_CallTree_ci, args);
26972: 
18116:     /* Read back all registers, in case the called tree changed any of them. */
29896: #ifdef DEBUG
29896:     JSTraceType* map;
29896:     size_t i;
29896:     map = getGlobalTypeMap(exit);
29896:     for (i = 0; i < exit->numGlobalSlots; i++)
29896:         JS_ASSERT(map[i] != TT_JSVAL);
29896:     map = getStackTypeMap(exit);
29896:     for (i = 0; i < exit->numStackSlots; i++)
29896:         JS_ASSERT(map[i] != TT_JSVAL);
29896: #endif
30246:     /* bug 502604 - It is illegal to extend from the outer typemap without first extending from the
30246:      * inner. Make a new typemap here.
30246:      */
30246:     TypeMap fullMap;
30246:     fullMap.add(getStackTypeMap(exit), exit->numStackSlots);
30246:     fullMap.add(getGlobalTypeMap(exit), exit->numGlobalSlots);
30246:     TreeInfo* innerTree = (TreeInfo*)exit->from->root->vmprivate;
30246:     if (exit->numGlobalSlots < innerTree->nGlobalTypes()) {
30246:         fullMap.add(innerTree->globalTypeMap() + exit->numGlobalSlots,
30246:                     innerTree->nGlobalTypes() - exit->numGlobalSlots);
30246:     }
30246:     import(ti, inner_sp_ins, exit->numStackSlots, fullMap.length() - exit->numStackSlots,
30246:            exit->calldepth, fullMap.data());
26972: 
18159:     /* Restore sp and rp to their original values (we still have them in a register). */
18159:     if (callDepth > 0) {
18159:         lir->insStorei(lirbuf->sp, lirbuf->state, offsetof(InterpState, sp));
18159:         lir->insStorei(lirbuf->rp, lirbuf->state, offsetof(InterpState, rp));
18159:     }
26972: 
26972:     /*
26972:      * Guard that we come out of the inner tree along the same side exit we came out when
26972:      * we called the inner tree at recording time.
26972:      */
20931:     guard(true, lir->ins2(LIR_eq, ret, INS_CONSTPTR(exit)), NESTED_EXIT);
18650:     /* Register us as a dependent tree of the inner tree. */
18650:     ((TreeInfo*)inner->vmprivate)->dependentTrees.addUnique(fragment->root);
25491:     treeInfo->linkedTrees.addUnique(inner);
18334: }
18334: 
18694: /* Add a if/if-else control-flow merge point to the list of known merge points. */
22652: JS_REQUIRES_STACK void
18694: TraceRecorder::trackCfgMerges(jsbytecode* pc)
18694: {
18694:     /* If we hit the beginning of an if/if-else, then keep track of the merge point after it. */
18694:     JS_ASSERT((*pc == JSOP_IFEQ) || (*pc == JSOP_IFEQX));
18694:     jssrcnote* sn = js_GetSrcNote(cx->fp->script, pc);
18694:     if (sn != NULL) {
18694:         if (SN_TYPE(sn) == SRC_IF) {
18694:             cfgMerges.add((*pc == JSOP_IFEQ)
18694:                           ? pc + GET_JUMP_OFFSET(pc)
18694:                           : pc + GET_JUMPX_OFFSET(pc));
18694:         } else if (SN_TYPE(sn) == SRC_IF_ELSE)
18694:             cfgMerges.add(pc + js_GetSrcNoteOffset(sn, 0));
18694:     }
18694: }
18694: 
20416: /* Invert the direction of the guard if this is a loop edge that is not
20416:    taken (thin loop). */
22652: JS_REQUIRES_STACK void
26557: TraceRecorder::emitIf(jsbytecode* pc, bool cond, LIns* x)
26557: {
26557:     ExitType exitType;
20416:     if (js_IsLoopEdge(pc, (jsbytecode*)fragment->root->ip)) {
26557:         exitType = LOOP_EXIT;
26557: 
26557:         /*
26557:          * If we are about to walk out of the loop, generate code for the inverse loop
26557:          * condition, pretending we recorded the case that stays on trace.
26557:          */
26557:         if ((*pc == JSOP_IFEQ || *pc == JSOP_IFEQX) == cond) {
26557:             JS_ASSERT(*pc == JSOP_IFNE || *pc == JSOP_IFNEX || *pc == JSOP_IFEQ || *pc == JSOP_IFEQX);
29883:             debug_only_print0(LC_TMTracer,
29883:                               "Walking out of the loop, terminating it anyway.\n");
20416:             cond = !cond;
26557:         }
26557: 
26557:         /*
26557:          * Conditional guards do not have to be emitted if the condition is constant. We
26557:          * make a note whether the loop condition is true or false here, so we later know
26557:          * whether to emit a loop edge or a loop end.
21433:          */
26557:         if (x->isconst()) {
28182:             loop = (x->imm32() == cond);
26557:             return;
26557:         }
26557:     } else {
26557:         exitType = BRANCH_EXIT;
26557:     }
26557:     if (!x->isconst())
26557:         guard(cond, x, exitType);
20416: }
20416: 
18694: /* Emit code for a fused IFEQ/IFNE. */
22652: JS_REQUIRES_STACK void
18694: TraceRecorder::fuseIf(jsbytecode* pc, bool cond, LIns* x)
18694: {
26557:     if (*pc == JSOP_IFEQ || *pc == JSOP_IFNE) {
26557:         emitIf(pc, cond, x);
26557:         if (*pc == JSOP_IFEQ)
18694:             trackCfgMerges(pc);
26557:     }
26557: }
26557: 
26557: /* Check whether we have reached the end of the trace. */
27933: JS_REQUIRES_STACK JSRecordingStatus
26557: TraceRecorder::checkTraceEnd(jsbytecode *pc)
26557: {
26557:     if (js_IsLoopEdge(pc, (jsbytecode*)fragment->root->ip)) {
26557:         /*
26557:          * If we compile a loop, the trace should have a zero stack balance at the loop
26557:          * edge. Currently we are parked on a comparison op or IFNE/IFEQ, so advance
26557:          * pc to the loop header and adjust the stack pointer and pretend we have
26557:          * reached the loop header.
26557:          */
26557:         if (loop) {
26557:             JS_ASSERT(!cx->fp->imacpc && (pc == cx->fp->regs->pc || pc == cx->fp->regs->pc + 1));
26557:             bool fused = pc != cx->fp->regs->pc;
26557:             JSFrameRegs orig = *cx->fp->regs;
26557: 
26557:             cx->fp->regs->pc = (jsbytecode*)fragment->root->ip;
26557:             cx->fp->regs->sp -= fused ? 2 : 1;
26557: 
30295:             bool demote = false;
30295:             closeLoop(traceMonitor, demote);
26557: 
26557:             *cx->fp->regs = orig;
30295: 
30295:             /*
30295:              * If compiling this loop generated new oracle information which will likely
30295:              * lead to a different compilation result, immediately trigger another
30295:              * compiler run. This is guaranteed to converge since the oracle only
30295:              * accumulates adverse information but never drops it (except when we
30295:              * flush it during garbage collection.)
30295:              */
30295:             if (demote)
30295:                 js_AttemptCompilation(cx, traceMonitor, globalObj, outer, outerArgc);
26557:         } else {
30295:             endLoop(traceMonitor);
26557:         }
27933:         return JSRS_STOP;
27933:     }
27933:     return JSRS_CONTINUE;
17966: }
17966: 
21685: bool
21685: TraceRecorder::hasMethod(JSObject* obj, jsid id)
21685: {
21685:     if (!obj)
21685:         return false;
21685: 
21685:     JSObject* pobj;
21685:     JSProperty* prop;
21685:     int protoIndex = OBJ_LOOKUP_PROPERTY(cx, obj, id, &pobj, &prop);
21685:     if (protoIndex < 0 || !prop)
21685:         return false;
21685: 
21685:     bool found = false;
21685:     if (OBJ_IS_NATIVE(pobj)) {
21685:         JSScope* scope = OBJ_SCOPE(pobj);
21685:         JSScopeProperty* sprop = (JSScopeProperty*) prop;
21685: 
21685:         if (SPROP_HAS_STUB_GETTER(sprop) &&
21685:             SPROP_HAS_VALID_SLOT(sprop, scope)) {
21685:             jsval v = LOCKED_OBJ_GET_SLOT(pobj, sprop->slot);
21685:             if (VALUE_IS_FUNCTION(cx, v)) {
21685:                 found = true;
30258:                 if (!scope->branded()) {
30258:                     scope->brandingShapeChange(cx, sprop->slot, v);
30258:                     scope->setBranded();
21685:                 }
21685:             }
21685:         }
21685:     }
21685: 
21685:     OBJ_DROP_PROPERTY(cx, pobj, prop);
21685:     return found;
21685: }
21685: 
24299: JS_REQUIRES_STACK bool
21685: TraceRecorder::hasIteratorMethod(JSObject* obj)
21685: {
21685:     JS_ASSERT(cx->fp->regs->sp + 2 <= cx->fp->slots + cx->fp->script->nslots);
21685: 
21685:     return hasMethod(obj, ATOM_TO_JSID(cx->runtime->atomState.iteratorAtom));
21685: }
21685: 
17517: int
17517: nanojit::StackFilter::getTop(LInsp guard)
17517: {
21521:     VMSideExit* e = (VMSideExit*)guard->record()->exit;
20893:     if (sp == lirbuf->sp)
21521:         return e->sp_adj;
20893:     JS_ASSERT(sp == lirbuf->rp);
21521:     return e->rp_adj;
17517: }
17517: 
17517: #if defined NJ_VERBOSE
17517: void
17517: nanojit::LirNameMap::formatGuard(LIns *i, char *out)
17517: {
21521:     VMSideExit *x;
21521: 
21521:     x = (VMSideExit *)i->record()->exit;
17517:     sprintf(out,
25111:             "%s: %s %s -> pc=%p imacpc=%p sp%+ld rp%+ld",
17517:             formatRef(i),
17517:             lirNames[i->opcode()],
30452:             i->oprnd1() ? formatRef(i->oprnd1()) : "",
25111:             (void *)x->pc,
25111:             (void *)x->imacpc,
18612:             (long int)x->sp_adj,
21685:             (long int)x->rp_adj);
17517: }
17517: #endif
17517: 
25102: void
25102: nanojit::Fragment::onDestroy()
25102: {
25102:     delete (TreeInfo *)vmprivate;
25102: }
25102: 
24499: static JS_REQUIRES_STACK bool
17410: js_DeleteRecorder(JSContext* cx)
17293: {
18782:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
18782: 
18702:     /* Aborting and completing a trace end up here. */
17410:     delete tm->recorder;
17410:     tm->recorder = NULL;
24486: 
24486:     /*
24486:      * If we ran out of memory, flush the code cache.
24486:      */
27884:     if (JS_TRACE_MONITOR(cx).fragmento->assm()->error() == OutOMem ||
27884:         js_OverfullFragmento(tm, tm->fragmento)) {
27379:         FlushJITCache(cx);
24486:         return false;
24486:     }
24486: 
24486:     return true;
17410: }
17410: 
21514: /**
21688:  * Checks whether the shape of the global object has changed.
21514:  */
27493: static JS_REQUIRES_STACK bool
27493: CheckGlobalObjectShape(JSContext* cx, JSTraceMonitor* tm, JSObject* globalObj,
24491:                        uint32 *shape=NULL, SlotList** slots=NULL)
24491: {
27493:     if (tm->needFlush) {
27493:         FlushJITCache(cx);
27493:         return false;
27493:     }
24491: 
27889:     if (STOBJ_NSLOTS(globalObj) > MAX_GLOBAL_SLOTS)
27889:         return false;
27889: 
24491:     uint32 globalShape = OBJ_SHAPE(globalObj);
24491: 
24491:     if (tm->recorder) {
26818:         VMFragment* root = (VMFragment*)tm->recorder->getFragment()->root;
24491:         TreeInfo* ti = tm->recorder->getTreeInfo();
24491:         /* Check the global shape matches the recorder's treeinfo's shape. */
26819:         if (globalObj != root->globalObj || globalShape != root->globalShape) {
21514:             AUDIT(globalShapeMismatchAtEntry);
29883:             debug_only_printf(LC_TMTracer,
29883:                               "Global object/shape mismatch (%p/%u vs. %p/%u), flushing cache.\n",
26823:                               (void*)globalObj, globalShape, (void*)root->globalObj,
29883:                               root->globalShape);
28105:             js_Backoff(cx, (jsbytecode*) root->ip);
27493:             FlushJITCache(cx);
24491:             return false;
24491:         }
24491:         if (shape)
24491:             *shape = globalShape;
24491:         if (slots)
24491:             *slots = ti->globalSlots;
24491:         return true;
24491:     }
24491: 
24491:     /* No recorder, search for a tracked global-state (or allocate one). */
24491:     for (size_t i = 0; i < MONITOR_N_GLOBAL_STATES; ++i) {
24491:         GlobalState &state = tm->globalStates[i];
24491: 
27012:         if (state.globalShape == uint32(-1)) {
26819:             state.globalObj = globalObj;
24491:             state.globalShape = globalShape;
24491:             JS_ASSERT(state.globalSlots);
24491:             JS_ASSERT(state.globalSlots->length() == 0);
24491:         }
24491: 
26819:         if (state.globalObj == globalObj && state.globalShape == globalShape) {
24491:             if (shape)
24491:                 *shape = globalShape;
24491:             if (slots)
24491:                 *slots = state.globalSlots;
24491:             return true;
24491:         }
24491:     }
24491: 
24491:     /* No currently-tracked-global found and no room to allocate, abort. */
24491:     AUDIT(globalShapeMismatchAtEntry);
29883:     debug_only_printf(LC_TMTracer,
29883:                       "No global slotlist for global shape %u, flushing cache.\n",
29883:                       globalShape);
27493:     FlushJITCache(cx);
24491:     return false;
21514: }
21514: 
22652: static JS_REQUIRES_STACK bool
21521: js_StartRecorder(JSContext* cx, VMSideExit* anchor, Fragment* f, TreeInfo* ti,
29896:                  unsigned stackSlots, unsigned ngslots, JSTraceType* typeMap,
28244:                  VMSideExit* expectedInnerExit, jsbytecode* outer, uint32 outerArgc)
17731: {
18782:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
27493:     if (JS_TRACE_MONITOR(cx).needFlush) {
27493:         FlushJITCache(cx);
27493:         return false;
27493:     }
24613: 
28308:     JS_ASSERT(f->root != f || !cx->fp->imacpc);
28308: 
17731:     /* start recording if no exception during construction */
19987:     tm->recorder = new (&gc) TraceRecorder(cx, anchor, f, ti,
24246:                                            stackSlots, ngslots, typeMap,
28244:                                            expectedInnerExit, outer, outerArgc);
25627: 
17731:     if (cx->throwing) {
20422:         js_AbortRecording(cx, "setting up recorder failed");
17731:         return false;
17731:     }
17978:     /* clear any leftover error state */
19987:     tm->fragmento->assm()->setError(None);
17731:     return true;
17731: }
17731: 
17853: static void
17889: js_TrashTree(JSContext* cx, Fragment* f)
17889: {
18211:     JS_ASSERT((!f->code()) == (!f->vmprivate));
18650:     JS_ASSERT(f == f->root);
18211:     if (!f->code())
18211:         return;
17889:     AUDIT(treesTrashed);
29883:     debug_only_print0(LC_TMTracer, "Trashing tree info.\n");
18051:     Fragmento* fragmento = JS_TRACE_MONITOR(cx).fragmento;
18650:     TreeInfo* ti = (TreeInfo*)f->vmprivate;
17889:     f->vmprivate = NULL;
18051:     f->releaseCode(fragmento);
18650:     Fragment** data = ti->dependentTrees.data();
18650:     unsigned length = ti->dependentTrees.length();
18650:     for (unsigned n = 0; n < length; ++n)
18650:         js_TrashTree(cx, data[n]);
30458:     data = ti->linkedTrees.data();
30458:     length = ti->linkedTrees.length();
30458:     for (unsigned n = 0; n < length; ++n)
30458:         js_TrashTree(cx, data[n]);
25102:     delete ti;
18650:     JS_ASSERT(!f->code() && !f->vmprivate);
17853: }
17853: 
19663: static int
17923: js_SynthesizeFrame(JSContext* cx, const FrameInfo& fi)
17923: {
22652:     VOUCH_DOES_NOT_REQUIRE_STACK();
22652: 
17923:     JS_ASSERT(HAS_FUNCTION_CLASS(fi.callee));
17923: 
17923:     JSFunction* fun = GET_FUNCTION_PRIVATE(cx, fi.callee);
17923:     JS_ASSERT(FUN_INTERPRETED(fun));
17923: 
19662:     /* Assert that we have a correct sp distance from cx->fp->slots in fi. */
22925:     JSStackFrame* fp = cx->fp;
25111:     JS_ASSERT_IF(!fi.imacpc,
25111:                  js_ReconstructStackDepth(cx, fp->script, fi.pc)
28887:                  == uintN(fi.spdist - fp->script->nfixed));
19662: 
19662:     uintN nframeslots = JS_HOWMANY(sizeof(JSInlineFrame), sizeof(jsval));
19662:     JSScript* script = fun->u.i.script;
19662:     size_t nbytes = (nframeslots + script->nslots) * sizeof(jsval);
19662: 
19662:     /* Code duplicated from inline_call: case in js_Interpret (FIXME). */
17923:     JSArena* a = cx->stackPool.current;
17923:     void* newmark = (void*) a->avail;
28949:     uintN argc = fi.get_argc();
28887:     jsval* vp = fp->slots + fi.spdist - (2 + argc);
19662:     uintN missing = 0;
19662:     jsval* newsp;
19662: 
19662:     if (fun->nargs > argc) {
22925:         const JSFrameRegs& regs = *fp->regs;
19662: 
19662:         newsp = vp + 2 + fun->nargs;
19662:         JS_ASSERT(newsp > regs.sp);
19662:         if ((jsuword) newsp <= a->limit) {
19662:             if ((jsuword) newsp > a->avail)
19662:                 a->avail = (jsuword) newsp;
19662:             jsval* argsp = newsp;
19662:             do {
19662:                 *--argsp = JSVAL_VOID;
19662:             } while (argsp != regs.sp);
19662:             missing = 0;
19662:         } else {
19662:             missing = fun->nargs - argc;
19662:             nbytes += (2 + fun->nargs) * sizeof(jsval);
19662:         }
19662:     }
17923: 
17923:     /* Allocate the inline frame with its vars and operands. */
17923:     if (a->avail + nbytes <= a->limit) {
17923:         newsp = (jsval *) a->avail;
17923:         a->avail += nbytes;
19662:         JS_ASSERT(missing == 0);
17923:     } else {
23447:         /* This allocation is infallible: js_ExecuteTree reserved enough stack. */
17923:         JS_ARENA_ALLOCATE_CAST(newsp, jsval *, &cx->stackPool, nbytes);
23447:         JS_ASSERT(newsp);
19662: 
19662:         /*
19662:          * Move args if the missing ones overflow arena a, then push
19662:          * undefined for the missing args.
19662:          */
19662:         if (missing) {
19662:             memcpy(newsp, vp, (2 + argc) * sizeof(jsval));
19662:             vp = newsp;
19662:             newsp = vp + 2 + argc;
19662:             do {
19662:                 *newsp++ = JSVAL_VOID;
19662:             } while (--missing != 0);
19662:         }
17923:     }
17923: 
17923:     /* Claim space for the stack frame and initialize it. */
17923:     JSInlineFrame* newifp = (JSInlineFrame *) newsp;
17923:     newsp += nframeslots;
17923: 
17923:     newifp->frame.callobj = NULL;
17923:     newifp->frame.argsobj = NULL;
17923:     newifp->frame.varobj = NULL;
17923:     newifp->frame.script = script;
28664:     newifp->frame.callee = fi.callee; // Roll with a potentially stale callee for now.
17923:     newifp->frame.fun = fun;
17923: 
28949:     bool constructing = fi.is_constructing();
19577:     newifp->frame.argc = argc;
25111:     newifp->callerRegs.pc = fi.pc;
28887:     newifp->callerRegs.sp = fp->slots + fi.spdist;
25111:     fp->imacpc = fi.imacpc;
22925: 
22925: #ifdef DEBUG
22925:     if (fi.block != fp->blockChain) {
22925:         for (JSObject* obj = fi.block; obj != fp->blockChain; obj = STOBJ_GET_PARENT(obj))
22925:             JS_ASSERT(obj);
22925:     }
22925: #endif
22925:     fp->blockChain = fi.block;
21685: 
19981:     newifp->frame.argv = newifp->callerRegs.sp - argc;
19591:     JS_ASSERT(newifp->frame.argv);
19591: #ifdef DEBUG
19591:     // Initialize argv[-1] to a known-bogus value so we'll catch it if
19591:     // someone forgets to initialize it later.
19591:     newifp->frame.argv[-1] = JSVAL_HOLE;
19591: #endif
22925:     JS_ASSERT(newifp->frame.argv >= StackBase(fp) + 2);
17923: 
17923:     newifp->frame.rval = JSVAL_VOID;
22925:     newifp->frame.down = fp;
17923:     newifp->frame.annotation = NULL;
28664:     newifp->frame.scopeChain = NULL; // will be updated in FlushNativeStackFrame
17923:     newifp->frame.sharpDepth = 0;
17923:     newifp->frame.sharpArray = NULL;
19577:     newifp->frame.flags = constructing ? JSFRAME_CONSTRUCTING : 0;
17923:     newifp->frame.dormantNext = NULL;
17923:     newifp->frame.xmlNamespace = NULL;
17923:     newifp->frame.blockChain = NULL;
17923:     newifp->mark = newmark;
28656:     newifp->frame.thisp = NULL; // will be updated in FlushNativeStackFrame
17923: 
22925:     newifp->frame.regs = fp->regs;
17923:     newifp->frame.regs->pc = script->code;
17923:     newifp->frame.regs->sp = newsp + script->nfixed;
21685:     newifp->frame.imacpc = NULL;
17923:     newifp->frame.slots = newsp;
27012:     if (script->staticLevel < JS_DISPLAY_SIZE) {
27012:         JSStackFrame **disp = &cx->display[script->staticLevel];
18308:         newifp->frame.displaySave = *disp;
18308:         *disp = &newifp->frame;
18308:     }
17923: 
21141:     /*
22925:      * Note that fp->script is still the caller's script; set the callee
21141:      * inline frame's idea of caller version from its version.
21141:      */
22925:     newifp->callerVersion = (JSVersion) fp->script->version;
22925: 
22925:     // After this paragraph, fp and cx->fp point to the newly synthesized frame.
22925:     fp->regs = &newifp->callerRegs;
22925:     fp = cx->fp = &newifp->frame;
18226: 
21141:     /*
21141:      * If there's a call hook, invoke it to compute the hookData used by
21141:      * debuggers that cooperate with the interpreter.
21141:      */
21141:     JSInterpreterHook hook = cx->debugHooks->callHook;
21141:     if (hook) {
28691:         newifp->hookData = hook(cx, fp, JS_TRUE, 0, cx->debugHooks->callHookData);
21141:     } else {
21141:         newifp->hookData = NULL;
21141:     }
21141: 
30248:     /* Duplicate native stack layout computation: see VisitFrameSlots header comment. */
20404:     // FIXME? we must count stack slots from caller's operand stack up to (but not including)
18226:     // callee's, including missing arguments. Could we shift everything down to the caller's
18226:     // fp->slots (where vars start) and avoid some of the complexity?
28887:     return (fi.spdist - fp->down->script->nfixed) +
22925:            ((fun->nargs > fp->argc) ? fun->nargs - fp->argc : 0) +
30248:            script->nfixed + 1/*argsobj*/;
17923: }
17923: 
28086: static void
28086: SynthesizeSlowNativeFrame(JSContext *cx, VMSideExit *exit)
28086: {
28087:     VOUCH_DOES_NOT_REQUIRE_STACK();
28087: 
28086:     void *mark;
28086:     JSInlineFrame *ifp;
28086: 
28086:     /* This allocation is infallible: js_ExecuteTree reserved enough stack. */
28086:     mark = JS_ARENA_MARK(&cx->stackPool);
28086:     JS_ARENA_ALLOCATE_CAST(ifp, JSInlineFrame *, &cx->stackPool, sizeof(JSInlineFrame));
28086:     JS_ASSERT(ifp);
28086: 
28086:     JSStackFrame *fp = &ifp->frame;
28086:     fp->regs = NULL;
28086:     fp->imacpc = NULL;
28086:     fp->slots = NULL;
28086:     fp->callobj = NULL;
28086:     fp->argsobj = NULL;
28086:     fp->varobj = cx->fp->varobj;
28086:     fp->callee = exit->nativeCallee();
28086:     fp->script = NULL;
28086:     fp->fun = GET_FUNCTION_PRIVATE(cx, fp->callee);
28086:     // fp->thisp is really a jsval, so reinterpret_cast here, not JSVAL_TO_OBJECT.
28086:     fp->thisp = (JSObject *) cx->nativeVp[1];
28086:     fp->argc = cx->nativeVpLen - 2;
28086:     fp->argv = cx->nativeVp + 2;
28086:     fp->rval = JSVAL_VOID;
28086:     fp->down = cx->fp;
28086:     fp->annotation = NULL;
28086:     JS_ASSERT(cx->fp->scopeChain);
28086:     fp->scopeChain = cx->fp->scopeChain;
28086:     fp->blockChain = NULL;
28086:     fp->sharpDepth = 0;
28086:     fp->sharpArray = NULL;
28086:     fp->flags = exit->constructing() ? JSFRAME_CONSTRUCTING : 0;
28086:     fp->dormantNext = NULL;
28086:     fp->xmlNamespace = NULL;
28086:     fp->displaySave = NULL;
28086: 
28086:     ifp->mark = mark;
28086:     cx->fp = fp;
28086: }
28086: 
22652: JS_REQUIRES_STACK bool
25937: js_RecordTree(JSContext* cx, JSTraceMonitor* tm, Fragment* f, jsbytecode* outer,
28244:               uint32 outerArgc, JSObject* globalObj, uint32 globalShape,
28244:               SlotList* globalSlots, uint32 argc)
17939: {
23091:     JS_ASSERT(f->root == f);
21796: 
18239:     /* Make sure the global type map didn't change on us. */
28105:     if (!CheckGlobalObjectShape(cx, tm, globalObj)) {
28105:         js_Backoff(cx, (jsbytecode*) f->root->ip);
27493:         return false;
28105:     }
18239: 
17939:     AUDIT(recorderStarted);
18213: 
18213:     /* Try to find an unused peer fragment, or allocate a new one. */
18213:     while (f->code() && f->peer)
18213:         f = f->peer;
18213:     if (f->code())
28244:         f = getAnchor(&JS_TRACE_MONITOR(cx), f->root->ip, globalObj, globalShape, argc);
24307: 
24307:     if (!f) {
27379:         FlushJITCache(cx);
24307:         return false;
24307:     }
18213: 
17939:     f->root = f;
23264:     f->lirbuf = tm->lirbuf;
17981: 
27884:     if (f->lirbuf->outOMem() || js_OverfullFragmento(tm, tm->fragmento)) {
28105:         js_Backoff(cx, (jsbytecode*) f->root->ip);
27379:         FlushJITCache(cx);
29883:         debug_only_print0(LC_TMTracer,
29883:                           "Out of memory recording new tree, flushing cache.\n");
21483:         return false;
21483:     }
21483: 
18211:     JS_ASSERT(!f->code() && !f->vmprivate);
17981: 
17939:     /* setup the VM-private treeInfo structure for this fragment */
26818:     TreeInfo* ti = new (&gc) TreeInfo(f, globalSlots);
17939: 
24246:     /* capture the coerced type of each active slot in the type map */
29880:     ti->typeMap.captureTypes(cx, globalObj, *globalSlots, 0/*callDepth*/);
24491:     ti->nStackTypes = ti->typeMap.length() - globalSlots->length();
21433: 
21433: #ifdef DEBUG
28913:     ensureTreeIsUnique(tm, (VMFragment*)f, ti);
25627:     ti->treeFileName = cx->fp->script->filename;
25627:     ti->treeLineNumber = js_FramePCToLineNumber(cx, cx->fp);
25627:     ti->treePCOffset = FramePCOffset(cx->fp);
21433: #endif
21433: 
17939:     /* determine the native frame layout at the entry point */
24491:     unsigned entryNativeStackSlots = ti->nStackTypes;
18425:     JS_ASSERT(entryNativeStackSlots == js_NativeStackSlots(cx, 0/*callDepth*/));
17939:     ti->nativeStackBase = (entryNativeStackSlots -
17939:             (cx->fp->regs->sp - StackBase(cx->fp))) * sizeof(double);
17939:     ti->maxNativeStackSlots = entryNativeStackSlots;
17939:     ti->maxCallDepth = 0;
18595:     ti->script = cx->fp->script;
17939: 
17939:     /* recording primary trace */
21433:     if (!js_StartRecorder(cx, NULL, f, ti,
24491:                           ti->nStackTypes,
24491:                           ti->globalSlots->length(),
28244:                           ti->typeMap.data(), NULL, outer, outerArgc)) {
23918:         return false;
23918:     }
23918: 
23918:     return true;
23918: }
23918: 
30295: JS_REQUIRES_STACK static inline void
30295: markSlotUndemotable(JSContext* cx, TreeInfo* ti, unsigned slot)
30295: {
30295:     if (slot < ti->nStackTypes) {
30295:         oracle.markStackSlotUndemotable(cx, slot);
30295:         return;
30295:     }
30295: 
30295:     uint16* gslots = ti->globalSlots->data();
30295:     oracle.markGlobalSlotUndemotable(cx, gslots[slot - ti->nStackTypes]);
30295: }
30295: 
24491: JS_REQUIRES_STACK static inline bool
24491: isSlotUndemotable(JSContext* cx, TreeInfo* ti, unsigned slot)
24491: {
24491:     if (slot < ti->nStackTypes)
24295:         return oracle.isStackSlotUndemotable(cx, slot);
24295: 
24491:     uint16* gslots = ti->globalSlots->data();
24491:     return oracle.isGlobalSlotUndemotable(cx, gslots[slot - ti->nStackTypes]);
17939: }
17939: 
22652: JS_REQUIRES_STACK static bool
29880: js_AttemptToStabilizeTree(JSContext* cx, JSObject* globalObj, VMSideExit* exit,
29880:                           jsbytecode* outer, uint32 outerArgc)
21433: {
21433:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
28308:     if (tm->needFlush) {
28308:         FlushJITCache(cx);
28308:         return false;
28308:     }
28308: 
26818:     VMFragment* from = (VMFragment*)exit->from->root;
24491:     TreeInfo* from_ti = (TreeInfo*)from->vmprivate;
21433: 
21463:     JS_ASSERT(exit->from->root->code());
21463: 
28913:     /*
28913:      * The loop edge exit might not know about all types since the tree could have
28913:      * been further specialized since it was recorded. Fill in the missing types
28913:      * from the entry type map.
28913:      */
29896:     JSTraceType* m = getFullTypeMap(exit);
28913:     unsigned ngslots = exit->numGlobalSlots;
28913:     if (ngslots < from_ti->nGlobalTypes()) {
28097:         uint32 partial = exit->numStackSlots + exit->numGlobalSlots;
29896:         m = (JSTraceType*)alloca(from_ti->typeMap.length() * sizeof(JSTraceType));
28097:         memcpy(m, getFullTypeMap(exit), partial);
28097:         memcpy(m + partial, from_ti->globalTypeMap() + exit->numGlobalSlots,
28097:                from_ti->nGlobalTypes() - exit->numGlobalSlots);
28913:         ngslots = from_ti->nGlobalTypes();
28913:     }
28913:     JS_ASSERT(exit->numStackSlots + ngslots == from_ti->typeMap.length());
28913: 
28913:     /*
28913:      * If we see any doubles along the loop edge, mark those slots undemotable
28913:      * since we know now for a fact that they can contain doubles.
28913:      */
28913:     for (unsigned i = 0; i < from_ti->typeMap.length(); i++) {
29896:         if (m[i] == TT_DOUBLE)
28913:             markSlotUndemotable(cx, from_ti, i);
28097:     }
28097: 
24246:     bool bound = false;
24491:     for (Fragment* f = from->first; f != NULL; f = f->peer) {
24246:         if (!f->code())
24246:             continue;
24491:         TreeInfo* ti = (TreeInfo*)f->vmprivate;
24491:         JS_ASSERT(exit->numStackSlots == ti->nStackTypes);
24246:         /* Check the minimum number of slots that need to be compared. */
28097:         unsigned checkSlots = JS_MIN(from_ti->typeMap.length(), ti->typeMap.length());
29896:         JSTraceType* m2 = ti->typeMap.data();
24295:         /* Analyze the exit typemap against the peer typemap.
24295:          * Two conditions are important:
24295:          * 1) Typemaps are identical: these peers can be attached.
24295:          * 2) Typemaps do not match, but only contain I->D mismatches.
24295:          *    In this case, the original tree must be trashed because it
24295:          *    will never connect to any peer.
24295:          */
24491:         bool matched = true;
24491:         bool undemote = false;
24295:         for (uint32 i = 0; i < checkSlots; i++) {
24295:             /* If the types are equal we're okay. */
24295:             if (m[i] == m2[i])
24295:                 continue;
24295:             matched = false;
24295:             /* If there's an I->D that cannot be resolved, flag it.
24295:              * Otherwise, break and go to the next peer.
24295:              */
29896:             if (m[i] == TT_INT32 && m2[i] == TT_DOUBLE && isSlotUndemotable(cx, ti, i)) {
24295:                 undemote = true;
24295:             } else {
24295:                 undemote = false;
24295:                 break;
24295:             }
24295:         }
24295:         if (matched) {
24491:             JS_ASSERT(from_ti->globalSlots == ti->globalSlots);
24491:             JS_ASSERT(from_ti->nStackTypes == ti->nStackTypes);
24246:             /* Capture missing globals on both trees and link the fragments together. */
24246:             if (from != f) {
24246:                 ti->dependentTrees.addUnique(from);
25491:                 from_ti->linkedTrees.addUnique(f);
25491:             }
25491:             if (ti->nGlobalTypes() < ti->globalSlots->length())
29880:                 specializeTreesToMissingGlobals(cx, globalObj, ti);
24246:             exit->target = f;
24246:             tm->fragmento->assm()->patch(exit);
24246:             /* Now erase this exit from the unstable exit list. */
24491:             UnstableExit** tail = &from_ti->unstableExits;
24491:             for (UnstableExit* uexit = from_ti->unstableExits; uexit != NULL; uexit = uexit->next) {
24246:                 if (uexit->exit == exit) {
24246:                     *tail = uexit->next;
25102:                     delete uexit;
24246:                     bound = true;
24246:                     break;
24246:                 }
24246:                 tail = &uexit->next;
24246:             }
24246:             JS_ASSERT(bound);
29883:             debug_only_stmt( js_DumpPeerStability(tm, f->ip, from->globalObj, from->globalShape, from->argc); )
24283:             break;
24295:         } else if (undemote) {
24295:             /* The original tree is unconnectable, so trash it. */
24295:             js_TrashTree(cx, f);
24295:             /* We shouldn't attempt to record now, since we'll hit a duplicate. */
24295:             return false;
24246:         }
24246:     }
24246:     if (bound)
24246:         return false;
24246: 
26819:     VMFragment* root = (VMFragment*)from->root;
28244:     return js_RecordTree(cx, tm, from->first, outer, outerArgc, root->globalObj,
28244:                          root->globalShape, from_ti->globalSlots, cx->fp->argc);
21433: }
21433: 
22652: static JS_REQUIRES_STACK bool
29368: js_AttemptToExtendTree(JSContext* cx, VMSideExit* anchor, VMSideExit* exitedFrom, jsbytecode* outer
29368: #ifdef MOZ_TRACEVIS
29368:     , TraceVisStateObj* tvso = NULL
29368: #endif
29368:     )
18620: {
28308:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
28308:     if (tm->needFlush) {
28308:         FlushJITCache(cx);
29368: #ifdef MOZ_TRACEVIS
29368:         if (tvso) tvso->r = R_FAIL_EXTEND_FLUSH;
29368: #endif
28308:         return false;
28308:     }
28308: 
18620:     Fragment* f = anchor->from->root;
18619:     JS_ASSERT(f->vmprivate);
18781:     TreeInfo* ti = (TreeInfo*)f->vmprivate;
18781: 
18781:     /* Don't grow trees above a certain size to avoid code explosion due to tail duplication. */
29368:     if (ti->branchCount >= MAX_BRANCHES) {
29368: #ifdef MOZ_TRACEVIS
29368:         if (tvso) tvso->r = R_FAIL_EXTEND_MAX_BRANCHES;
29368: #endif
18781:         return false;
29368:     }
17939: 
17939:     Fragment* c;
18620:     if (!(c = anchor->target)) {
20931:         c = JS_TRACE_MONITOR(cx).fragmento->createBranch(anchor, cx->fp->regs->pc);
20931:         c->spawnedFrom = anchor;
17939:         c->parent = f;
18620:         anchor->target = c;
17939:         c->root = f;
17939:     }
17939: 
27290:     /*
27290:      * If we are recycling a fragment, it might have a different ip so reset it here. This
27290:      * can happen when attaching a branch to a NESTED_EXIT, which might extend along separate paths
27290:      * (i.e. after the loop edge, and after a return statement).
27290:      */
27290:     c->ip = cx->fp->regs->pc;
27290: 
29883:     debug_only_printf(LC_TMTracer,
29883:                       "trying to attach another branch to the tree (hits = %d)\n", c->hits());
25627: 
25627:     int32_t& hits = c->hits();
27163:     if (outer || (hits++ >= HOTEXIT && hits <= HOTEXIT+MAXEXIT)) {
17939:         /* start tracing secondary trace from this point */
17939:         c->lirbuf = f->lirbuf;
24246:         unsigned stackSlots;
18621:         unsigned ngslots;
29896:         JSTraceType* typeMap;
18621:         TypeMap fullMap;
18621:         if (exitedFrom == NULL) {
18621:             /* If we are coming straight from a simple side exit, just use that exit's type map
18621:                as starting point. */
20931:             ngslots = anchor->numGlobalSlots;
24246:             stackSlots = anchor->numStackSlots;
24246:             typeMap = getFullTypeMap(anchor);
18621:         } else {
18621:             /* If we side-exited on a loop exit and continue on a nesting guard, the nesting
18621:                guard (anchor) has the type information for everything below the current scope,
18621:                and the actual guard we exited from has the types for everything in the current
18621:                scope (and whatever it inlined). We have to merge those maps here. */
21521:             VMSideExit* e1 = anchor;
21521:             VMSideExit* e2 = exitedFrom;
24246:             fullMap.add(getStackTypeMap(e1), e1->numStackSlotsBelowCurrentFrame);
24246:             fullMap.add(getStackTypeMap(e2), e2->numStackSlots);
24246:             stackSlots = fullMap.length();
28981:             fullMap.add(getGlobalTypeMap(e2), e2->numGlobalSlots);
30246:             if (e2->numGlobalSlots < e1->numGlobalSlots) {
30246:                 /*
30246:                  * Watch out for an extremely rare case (bug 502714). The sequence of events is:
30246:                  * 
30246:                  * 1) Inner tree compiles not knowing about global X (which has type A).
30246:                  * 2) Inner tree learns about global X and specializes it to a different type
30246:                  *    (type B).
30246:                  * 3) Outer tree records inner tree with global X as type A, exiting as B.
30246:                  * 4) Outer tree now has a nesting guard with typeof(X)=B.
30246:                  * 5) Inner tree takes its original exit that does not know about X.
30246:                  * 
30246:                  * In this case, the nesting guard fails, and now it is illegal to use the nested
30246:                  * typemap entry for X. The correct entry is in the inner guard's TreeInfo,
30246:                  * analogous to the solution for bug 476653.
30246:                  */
30246:                 TreeInfo* innerTree = (TreeInfo*)e2->from->root->vmprivate;
30246:                 unsigned slots = e2->numGlobalSlots;
30246:                 if (innerTree->nGlobalTypes() > slots) {
30246:                     unsigned addSlots = JS_MIN(innerTree->nGlobalTypes() - slots,
30246:                                                e1->numGlobalSlots - slots);
30246:                     fullMap.add(innerTree->globalTypeMap() + e2->numGlobalSlots, addSlots);
30246:                     slots += addSlots;
30246:                 }
30246:                 if (slots < e1->numGlobalSlots)
30246:                     fullMap.add(getGlobalTypeMap(e1) + slots, e1->numGlobalSlots - slots);
30246:                 JS_ASSERT(slots == e1->numGlobalSlots);
28981:             }
28960:             ngslots = e1->numGlobalSlots;
24246:             typeMap = fullMap.data();
24246:         }
28960:         JS_ASSERT(ngslots >= anchor->numGlobalSlots);
29368:         bool rv = js_StartRecorder(cx, anchor, c, (TreeInfo*)f->vmprivate, stackSlots,
28244:                                    ngslots, typeMap, exitedFrom, outer, cx->fp->argc);
29368: #ifdef MOZ_TRACEVIS
29368:         if (!rv && tvso)
29368:             tvso->r = R_FAIL_EXTEND_START;
29368: #endif
29368:         return rv;
29368:     }
29368: #ifdef MOZ_TRACEVIS
29368:     if (tvso) tvso->r = R_FAIL_EXTEND_COLD;
29368: #endif
17939:     return false;
17939: }
17939: 
22652: static JS_REQUIRES_STACK VMSideExit*
21433: js_ExecuteTree(JSContext* cx, Fragment* f, uintN& inlineCallCount,
21521:                VMSideExit** innermostNestedGuardp);
17951: 
22652: JS_REQUIRES_STACK bool
20422: js_RecordLoopEdge(JSContext* cx, TraceRecorder* r, uintN& inlineCallCount)
17939: {
17939: #ifdef JS_THREADSAFE
17939:     if (OBJ_SCOPE(JS_GetGlobalForObject(cx, cx->fp->scopeChain))->title.ownercx != cx) {
20422:         js_AbortRecording(cx, "Global object not owned by this context");
17939:         return false; /* we stay away from shared global objects */
17939:     }
17939: #endif
26557: 
24307:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
26557: 
27493:     /* Process needFlush and deep abort requests. */
27493:     if (tm->needFlush) {
27493:         FlushJITCache(cx);
27493:         return false;
27493:     }
22632:     if (r->wasDeepAborted()) {
22632:         js_AbortRecording(cx, "deep abort requested");
22632:         return false;
22632:     }
26557: 
26557:     JS_ASSERT(r->getFragment() && !r->getFragment()->lastIns);
26818:     VMFragment* root = (VMFragment*)r->getFragment()->root;
26557: 
26557:     /* Does this branch go to an inner loop? */
28239:     Fragment* first = getLoop(&JS_TRACE_MONITOR(cx), cx->fp->regs->pc,
28244:                               root->globalObj, root->globalShape, cx->fp->argc);
28239:     if (!first) {
26557:         /* Not an inner loop we can call, abort trace. */
26557:         AUDIT(returnToDifferentLoopHeader);
26557:         JS_ASSERT(!cx->fp->imacpc);
29883:         debug_only_printf(LC_TMTracer,
29883:                           "loop edge to %d, header %d\n",
26557:                           cx->fp->regs->pc - cx->fp->script->code,
29883:                           (jsbytecode*)r->getFragment()->root->ip - cx->fp->script->code);
26557:         js_AbortRecording(cx, "Loop edge does not return to header");
26557:         return false;
26557:     }
21514: 
21514:     /* Make sure inner tree call will not run into an out-of-memory condition. */
23449:     if (tm->reservedDoublePoolPtr < (tm->reservedDoublePool + MAX_NATIVE_STACK_SLOTS) &&
23449:         !js_ReplenishReservedPool(cx, tm)) {
21514:         js_AbortRecording(cx, "Couldn't call inner tree (out of memory)");
21514:         return false;
21514:     }
21514: 
21514:     /* Make sure the shape of the global object still matches (this might flush
21514:        the JIT cache). */
21514:     JSObject* globalObj = JS_GetGlobalForObject(cx, cx->fp->scopeChain);
24491:     uint32 globalShape = -1;
24491:     SlotList* globalSlots = NULL;
27493:     if (!CheckGlobalObjectShape(cx, tm, globalObj, &globalShape, &globalSlots))
27493:         return false;
21433: 
29883:     debug_only_printf(LC_TMTracer,
29883:                       "Looking for type-compatible peer (%s:%d@%d)\n",
21433:                       cx->fp->script->filename,
21685:                       js_FramePCToLineNumber(cx, cx->fp),
29883:                       FramePCOffset(cx->fp));
21433: 
28239:     // Find a matching inner tree. If none can be found, compile one.
28239:     Fragment* f = r->findNestedCompatiblePeer(first);
28239:     if (!f || !f->code()) {
21433:         AUDIT(noCompatInnerTrees);
25937: 
28244:         VMFragment* outerFragment = (VMFragment*) tm->recorder->getFragment()->root;
28244:         jsbytecode* outer = (jsbytecode*) outerFragment->ip;
28244:         uint32 outerArgc = outerFragment->argc;
28244:         uint32 argc = cx->fp->argc;
21433:         js_AbortRecording(cx, "No compatible inner tree");
25627: 
28239:         // Find an empty fragment we can recycle, or allocate a new one.
28239:         for (f = first; f != NULL; f = f->peer) {
28239:             if (!f->code())
28239:                 break;
28239:         }
28239:         if (!f || f->code()) {
28244:             f = getAnchor(tm, cx->fp->regs->pc, globalObj, globalShape, argc);
24307:             if (!f) {
27379:                 FlushJITCache(cx);
24307:                 return false;
24307:             }
24307:         }
28244:         return js_RecordTree(cx, tm, f, outer, outerArgc, globalObj, globalShape, globalSlots, argc);
21433:     }
21433: 
28239:     r->adjustCallerTypes(f);
18250:     r->prepareTreeCall(f);
21521:     VMSideExit* innermostNestedGuard = NULL;
21521:     VMSideExit* lr = js_ExecuteTree(cx, f, inlineCallCount, &innermostNestedGuard);
25489:     if (!lr || r->wasDeepAborted()) {
25489:         if (!lr)
20422:             js_AbortRecording(cx, "Couldn't call inner tree");
18051:         return false;
18051:     }
26557: 
28244:     VMFragment* outerFragment = (VMFragment*) tm->recorder->getFragment()->root;
28244:     jsbytecode* outer = (jsbytecode*) outerFragment->ip;
20931:     switch (lr->exitType) {
18051:       case LOOP_EXIT:
18284:         /* If the inner tree exited on an unknown loop exit, grow the tree around it. */
18284:         if (innermostNestedGuard) {
25937:             js_AbortRecording(cx, "Inner tree took different side exit, abort current "
25937:                               "recording and grow nesting tree");
25937:             return js_AttemptToExtendTree(cx, innermostNestedGuard, lr, outer);
18284:         }
17997:         /* emit a call to the inner tree and continue recording the outer tree trace */
17997:         r->emitTreeCall(f, lr);
17997:         return true;
21433:       case UNSTABLE_LOOP_EXIT:
21433:         /* abort recording so the inner loop can become type stable. */
21433:         js_AbortRecording(cx, "Inner tree is trying to stabilize, abort outer recording");
29880:         return js_AttemptToStabilizeTree(cx, globalObj, lr, outer, outerFragment->argc);
29354:       case OVERFLOW_EXIT:
29354:         oracle.markInstructionUndemotable(cx->fp->regs->pc);
29354:         /* fall through */
18051:       case BRANCH_EXIT:
29367:       case CASE_EXIT:
18051:         /* abort recording the outer tree, extend the inner tree */
20422:         js_AbortRecording(cx, "Inner tree is trying to grow, abort outer recording");
25937:         return js_AttemptToExtendTree(cx, lr, NULL, outer);
18051:       default:
29894:         debug_only_printf(LC_TMTracer, "exit_type=%s\n", getExitName(lr->exitType));
20422:         js_AbortRecording(cx, "Inner tree not suitable for calling");
18051:         return false;
18051:     }
17988: }
21433: 
21433: static bool
29896: js_IsEntryTypeCompatible(jsval* vp, JSTraceType* m)
21433: {
21433:     unsigned tag = JSVAL_TAG(*vp);
21433: 
29883:     debug_only_printf(LC_TMTracer, "%c/%c ", tagChar[tag], typeChar[*m]);
21433: 
21433:     switch (*m) {
29896:       case TT_OBJECT:
27542:         if (tag == JSVAL_OBJECT && !JSVAL_IS_NULL(*vp) &&
27542:             !HAS_FUNCTION_CLASS(JSVAL_TO_OBJECT(*vp))) {
27542:             return true;
27542:         }
29883:         debug_only_printf(LC_TMTracer, "object != tag%u ", tag);
27542:         return false;
29896:       case TT_INT32:
21433:         jsint i;
21433:         if (JSVAL_IS_INT(*vp))
21433:             return true;
29896:         if (tag == JSVAL_DOUBLE && JSDOUBLE_IS_INT(*JSVAL_TO_DOUBLE(*vp), i))
21433:             return true;
29883:         debug_only_printf(LC_TMTracer, "int != tag%u(value=%lu) ", tag, (unsigned long)*vp);
21433:         return false;
29896:       case TT_DOUBLE:
21433:         if (JSVAL_IS_INT(*vp) || tag == JSVAL_DOUBLE)
21433:             return true;
29883:         debug_only_printf(LC_TMTracer, "double != tag%u ", tag);
21433:         return false;
29896:       case TT_JSVAL:
29896:         JS_NOT_REACHED("shouldn't see jsval type in entry");
27541:         return false;
29896:       case TT_STRING:
27542:         if (tag == JSVAL_STRING)
27542:             return true;
29883:         debug_only_printf(LC_TMTracer, "string != tag%u ", tag);
27542:         return false;
29896:       case TT_NULL:
27542:         if (JSVAL_IS_NULL(*vp))
27542:             return true;
29883:         debug_only_printf(LC_TMTracer, "null != tag%u ", tag);
27542:         return false;
29896:       case TT_PSEUDOBOOLEAN:
21433:         if (tag == JSVAL_BOOLEAN)
21433:             return true;
29883:         debug_only_printf(LC_TMTracer, "bool != tag%u ", tag);
21433:         return false;
27542:       default:
29896:         JS_ASSERT(*m == TT_FUNCTION);
27541:         if (tag == JSVAL_OBJECT && !JSVAL_IS_NULL(*vp) &&
27541:             HAS_FUNCTION_CLASS(JSVAL_TO_OBJECT(*vp))) {
27541:             return true;
27541:         }
29883:         debug_only_printf(LC_TMTracer, "fun != tag%u ", tag);
27541:         return false;
21433:     }
21433: }
21433: 
29882: class TypeCompatibilityVisitor : public SlotVisitorBase
29882: {
29880:     TraceRecorder &mRecorder;
29880:     JSContext *mCx;
29896:     JSTraceType *mTypeMap;
29880:     unsigned mStackSlotNum;
29880:     bool mOk;
29880: public:
29880:     TypeCompatibilityVisitor (TraceRecorder &recorder,
29896:                               JSTraceType *typeMap) :
29880:         mRecorder(recorder),
29880:         mCx(mRecorder.cx),
29880:         mTypeMap(typeMap),
29880:         mStackSlotNum(0),
29880:         mOk(true)
29880:     {}
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE void
29880:     visitGlobalSlot(jsval *vp, unsigned n, unsigned slot) {
29883:         debug_only_printf(LC_TMTracer, "global%d=", n);
29880:         if (!js_IsEntryTypeCompatible(vp, mTypeMap)) {
29880:             mOk = false;
29896:         } else if (!isPromoteInt(mRecorder.get(vp)) && *mTypeMap == TT_INT32) {
29880:             oracle.markGlobalSlotUndemotable(mCx, slot);
29880:             mOk = false;
29896:         } else if (JSVAL_IS_INT(*vp) && *mTypeMap == TT_DOUBLE) {
29880:             oracle.markGlobalSlotUndemotable(mCx, slot);
29880:         }
29880:         mTypeMap++;
29880:     }
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
29880:     visitStackSlots(jsval *vp, size_t count, JSStackFrame* fp) {
29880:         for (size_t i = 0; i < count; ++i) {
29883:             debug_only_printf(LC_TMTracer, "%s%u=", stackSlotKind(), unsigned(i));
29880:             if (!js_IsEntryTypeCompatible(vp, mTypeMap)) {
29880:                 mOk = false;
29896:             } else if (!isPromoteInt(mRecorder.get(vp)) && *mTypeMap == TT_INT32) {
29880:                 oracle.markStackSlotUndemotable(mCx, mStackSlotNum);
29880:                 mOk = false;
29896:             } else if (JSVAL_IS_INT(*vp) && *mTypeMap == TT_DOUBLE) {
29880:                 oracle.markStackSlotUndemotable(mCx, mStackSlotNum);
29880:             }
29880:             vp++;
29880:             mTypeMap++;
29880:             mStackSlotNum++;
29880:         }
29880:         return true;
29880:     }
29880: 
29880:     bool isOk() {
29880:         return mOk;
29880:     }
29880: };
29880: 
22652: JS_REQUIRES_STACK Fragment*
28239: TraceRecorder::findNestedCompatiblePeer(Fragment* f)
28239: {
21433:     JSTraceMonitor* tm;
21433: 
21433:     tm = &JS_TRACE_MONITOR(cx);
24491:     unsigned int ngslots = treeInfo->globalSlots->length();
21433: 
21433:     TreeInfo* ti;
21433:     for (; f != NULL; f = f->peer) {
28239:         if (!f->code())
21433:             continue;
28239: 
21433:         ti = (TreeInfo*)f->vmprivate;
21433: 
29883:         debug_only_printf(LC_TMTracer, "checking nested types %p: ", (void*)f);
21433: 
24491:         if (ngslots > ti->nGlobalTypes())
29880:             specializeTreesToMissingGlobals(cx, globalObj, ti);
28239: 
28239:         /*
28239:          * Determine whether the typemap of the inner tree matches the outer tree's
28239:          * current state. If the inner tree expects an integer, but the outer tree
28239:          * doesn't guarantee an integer for that slot, we mark the slot undemotable
28239:          * and mismatch here. This will force a new tree to be compiled that accepts
28239:          * a double for the slot. If the inner tree expects a double, but the outer
28239:          * tree has an integer, we can proceed, but we mark the location undemotable.
28239:          */
29880:         TypeCompatibilityVisitor visitor(*this, ti->typeMap.data());
29882:         VisitSlots(visitor, cx, 0, *treeInfo->globalSlots);
29880: 
29883:         debug_only_printf(LC_TMTracer, " %s\n", visitor.isOk() ? "match" : "");
29880:         if (visitor.isOk())
21433:             return f;
21433:     }
21433: 
21433:     return NULL;
21433: }
21433: 
29882: class CheckEntryTypeVisitor : public SlotVisitorBase
29882: {
29880:     bool mOk;
29896:     JSTraceType *mTypeMap;
29880: public:
29896:     CheckEntryTypeVisitor(JSTraceType *typeMap) :
29880:         mOk(true),
29880:         mTypeMap(typeMap)
29880:     {}
29880: 
29880:     JS_ALWAYS_INLINE void checkSlot(jsval *vp, char const *name, int i) {
29883:         debug_only_printf(LC_TMTracer, "%s%d=", name, i);
29896:         JS_ASSERT(*(uint8_t*)mTypeMap != 0xCD);
29880:         mOk = js_IsEntryTypeCompatible(vp, mTypeMap++);
29880:     }
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE void
29880:     visitGlobalSlot(jsval *vp, unsigned n, unsigned slot) {
29880:         if (mOk)
29880:             checkSlot(vp, "global", n);
29880:     }
29880: 
29880:     JS_REQUIRES_STACK JS_ALWAYS_INLINE bool
29880:     visitStackSlots(jsval *vp, size_t count, JSStackFrame* fp) {
29880:         for (size_t i = 0; i < count; ++i) {
29880:             if (!mOk)
29880:                 break;
29882:             checkSlot(vp++, stackSlotKind(), i);
29880:         }
29880:         return mOk;
29880:     }
29880: 
29880:     bool isOk() {
29880:         return mOk;
29880:     }
29880: };
29880: 
21433: /**
21433:  * Check if types are usable for trace execution.
21433:  *
21433:  * @param cx            Context.
21433:  * @param ti            Tree info of peer we're testing.
21433:  * @return              True if compatible (with or without demotions), false otherwise.
21433:  */
22652: static JS_REQUIRES_STACK bool
29880: js_CheckEntryTypes(JSContext* cx, JSObject* globalObj, TreeInfo* ti)
21433: {
24491:     unsigned int ngslots = ti->globalSlots->length();
24491: 
24491:     JS_ASSERT(ti->nStackTypes == js_NativeStackSlots(cx, 0));
24491: 
24491:     if (ngslots > ti->nGlobalTypes())
29880:         specializeTreesToMissingGlobals(cx, globalObj, ti);
24246: 
24246:     JS_ASSERT(ti->typeMap.length() == js_NativeStackSlots(cx, 0) + ngslots);
24491:     JS_ASSERT(ti->typeMap.length() == ti->nStackTypes + ngslots);
24491:     JS_ASSERT(ti->nGlobalTypes() == ngslots);
29880: 
29880:     CheckEntryTypeVisitor visitor(ti->typeMap.data());
29882:     VisitSlots(visitor, cx, 0, *ti->globalSlots);
21433: 
29883:     debug_only_print0(LC_TMTracer, "\n");
29880:     return visitor.isOk();
21433: }
21433: 
21433: /**
21433:  * Find an acceptable entry tree given a PC.
21433:  *
21433:  * @param cx            Context.
29880:  * @param globalObj     Global object.
21433:  * @param f             First peer fragment.
21433:  * @param nodemote      If true, will try to find a peer that does not require demotion.
25627:  * @out   count         Number of fragments consulted.
21433:  */
22652: static JS_REQUIRES_STACK Fragment*
29880: js_FindVMCompatiblePeer(JSContext* cx, JSObject* globalObj, Fragment* f, uintN& count)
25627: {
25627:     count = 0;
21433:     for (; f != NULL; f = f->peer) {
21433:         if (f->vmprivate == NULL)
21433:             continue;
29883:         debug_only_printf(LC_TMTracer,
29883:                           "checking vm types %p (ip: %p): ", (void*)f, f->ip);
29880:         if (js_CheckEntryTypes(cx, globalObj, (TreeInfo*)f->vmprivate))
21433:             return f;
25627:         ++count;
21433:     }
21433:     return NULL;
21433: }
21433: 
24612: static void
24612: LeaveTree(InterpState&, VMSideExit* lr);
24612: 
21433: /**
21514:  * Executes a tree.
21433:  */
24612: static JS_REQUIRES_STACK VMSideExit*
21433: js_ExecuteTree(JSContext* cx, Fragment* f, uintN& inlineCallCount,
21521:                VMSideExit** innermostNestedGuardp)
18209: {
29368: #ifdef MOZ_TRACEVIS
29368:     TraceVisStateObj tvso(S_EXECUTE);
29368: #endif
29368: 
26818:     JS_ASSERT(f->root == f && f->code() && f->vmprivate);
21433: 
21433:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
27062:     JSObject* globalObj = JS_GetGlobalForObject(cx, cx->fp->scopeChain);
18248:     TreeInfo* ti = (TreeInfo*)f->vmprivate;
24491:     unsigned ngslots = ti->globalSlots->length();
24491:     uint16* gslots = ti->globalSlots->data();
27062:     unsigned globalFrameSize = STOBJ_NSLOTS(globalObj);
21433: 
21433:     /* Make sure the global object is sane. */
26818:     JS_ASSERT(!ngslots || (OBJ_SHAPE(JS_GetGlobalForObject(cx, cx->fp->scopeChain)) ==
26818:               ((VMFragment*)f)->globalShape));
21433:     /* Make sure our caller replenished the double pool. */
23449:     JS_ASSERT(tm->reservedDoublePoolPtr >= tm->reservedDoublePool + MAX_NATIVE_STACK_SLOTS);
21433: 
23448:     /* Reserve objects and stack space now, to make leaving the tree infallible. */
23448:     if (!js_ReserveObjects(cx, MAX_CALL_STACK_ENTRIES))
23448:         return NULL;
24612: 
27882: #ifdef DEBUG
27882:     uintN savedProhibitFlush = JS_TRACE_MONITOR(cx).prohibitFlush;
27882: #endif
27882: 
26282:     /* Setup the interpreter state block, which is followed by the native global frame. */
27062:     InterpState* state = (InterpState*)alloca(sizeof(InterpState) + (globalFrameSize+1)*sizeof(double));
26282:     state->cx = cx;
26282:     state->inlineCallCountp = &inlineCallCount;
26282:     state->innermostNestedGuardp = innermostNestedGuardp;
26282:     state->outermostTree = ti;
26282:     state->lastTreeExitGuard = NULL;
26282:     state->lastTreeCallGuard = NULL;
26282:     state->rpAtLastTreeCall = NULL;
27166:     state->builtinStatus = 0;
26282: 
24612:     /* Setup the native global frame. */
26282:     double* global = (double*)(state+1);
24612: 
24612:     /* Setup the native stack frame. */
27062:     double stack_buffer[MAX_NATIVE_STACK_SLOTS];
26282:     state->stackBase = stack_buffer;
26282:     state->sp = stack_buffer + (ti->nativeStackBase/sizeof(double));
26282:     state->eos = stack_buffer + MAX_NATIVE_STACK_SLOTS;
24612: 
24612:     /* Setup the native call stack frame. */
27062:     FrameInfo* callstack_buffer[MAX_CALL_STACK_ENTRIES];
26282:     state->callstackBase = callstack_buffer;
26282:     state->rp = callstack_buffer;
26282:     state->eor = callstack_buffer + MAX_CALL_STACK_ENTRIES;
24612: 
24612:     void *reserve;
26282:     state->stackMark = JS_ARENA_MARK(&cx->stackPool);
23447:     JS_ARENA_ALLOCATE(reserve, &cx->stackPool, MAX_INTERP_STACK_BYTES);
23447:     if (!reserve)
23448:         return NULL;
23447: 
21514: #ifdef DEBUG
27062:     memset(stack_buffer, 0xCD, sizeof(stack_buffer));
27062:     memset(global, 0xCD, (globalFrameSize+1)*sizeof(double));
27889:     JS_ASSERT(globalFrameSize <= MAX_GLOBAL_SLOTS);
21514: #endif
21433: 
29883:     debug_only_stmt(*(uint64*)&global[globalFrameSize] = 0xdeadbeefdeadbeefLL;)
29883:     debug_only_printf(LC_TMTracer,
29883:                       "entering trace at %s:%u@%u, native stack slots: %u code: %p\n",
18260:                       cx->fp->script->filename,
21685:                       js_FramePCToLineNumber(cx, cx->fp),
21685:                       FramePCOffset(cx->fp),
21685:                       ti->maxNativeStackSlots,
29883:                       f->code());
18210: 
24491:     JS_ASSERT(ti->nGlobalTypes() == ngslots);
29880:     BuildNativeFrame(cx, globalObj, 0/*callDepth*/, ngslots, gslots,
29880:                      ti->typeMap.data(), global, stack_buffer);
24612: 
17397:     union { NIns *code; GuardRecord* (FASTCALL *func)(InterpState*, Fragment*); } u;
17397:     u.code = f->code();
17923: 
24612: #ifdef EXECUTE_TREE_TIMER
26282:     state->startTime = rdtsc();
18776: #endif
17923: 
27882:     JS_ASSERT(!tm->tracecx);
27882:     tm->tracecx = cx;
28245:     state->prev = cx->interpState;
26282:     cx->interpState = state;
18723: 
29883:     debug_only_stmt(fflush(NULL));
20931:     GuardRecord* rec;
29368:     // Note that the block scoping is crucial here for TraceVis;  the
29368:     // TraceVisStateObj constructors and destructors must run at the right times.
29368:     {
29368: #ifdef MOZ_TRACEVIS
29368:         TraceVisStateObj tvso_n(S_NATIVE);
29368: #endif
18723: #if defined(JS_NO_FASTCALL) && defined(NANOJIT_IA32)
26282:         SIMULATE_FASTCALL(rec, state, NULL, u.func);
18723: #else
26282:         rec = u.func(state, NULL);
18723: #endif
29368:     }
24376:     VMSideExit* lr = (VMSideExit*)rec->exit;
20931: 
21433:     AUDIT(traceTriggered);
21433: 
28245:     cx->interpState = state->prev;
27379: 
20931:     JS_ASSERT(lr->exitType != LOOP_EXIT || !lr->calldepth);
27882:     tm->tracecx = NULL;
26282:     LeaveTree(*state, lr);
27882:     JS_ASSERT(JS_TRACE_MONITOR(cx).prohibitFlush == savedProhibitFlush);
26282:     return state->innermost;
24612: }
24612: 
24612: static JS_FORCES_STACK void
24612: LeaveTree(InterpState& state, VMSideExit* lr)
24612: {
24612:     VOUCH_DOES_NOT_REQUIRE_STACK();
24612: 
24612:     JSContext* cx = state.cx;
24612:     FrameInfo** callstack = state.callstackBase;
24612:     double* stack = state.stackBase;
17923: 
20429:     /* Except if we find that this is a nested bailout, the guard the call returned is the
20429:        one we have to use to adjust pc and sp. */
21521:     VMSideExit* innermost = lr;
20429: 
19588:     /* While executing a tree we do not update state.sp and state.rp even if they grow. Instead,
19588:        guards tell us by how much sp and rp should be incremented in case of a side exit. When
19588:        calling a nested tree, however, we actively adjust sp and rp. If we have such frames
19588:        from outer trees on the stack, then rp will have been adjusted. Before we can process
19588:        the stack of the frames of the tree we directly exited from, we have to first work our
19588:        way through the outer frames and generate interpreter frames for them. Once the call
19588:        stack (rp) is empty, we can process the final frames (which again are not directly
19588:        visible and only the guard we exited on will tells us about). */
23262:     FrameInfo** rp = (FrameInfo**)state.rp;
20931:     if (lr->exitType == NESTED_EXIT) {
21521:         VMSideExit* nested = state.lastTreeCallGuard;
20429:         if (!nested) {
20429:             /* If lastTreeCallGuard is not set in state, we only have a single level of
20429:                nesting in this exit, so lr itself is the innermost and outermost nested
20429:                guard, and hence we set nested to lr. The calldepth of the innermost guard
20429:                is not added to state.rp, so we do it here manually. For a nesting depth
20429:                greater than 1 the CallTree builtin already added the innermost guard's
20429:                calldepth to state.rpAtLastTreeCall. */
20429:             nested = lr;
20429:             rp += lr->calldepth;
20429:         } else {
20429:             /* During unwinding state.rp gets overwritten at every step and we restore
20429:                it here to its state at the innermost nested guard. The builtin already
20429:                added the calldepth of that innermost guard to rpAtLastTreeCall. */
23262:             rp = (FrameInfo**)state.rpAtLastTreeCall;
20429:         }
20429:         innermost = state.lastTreeExitGuard;
24612:         if (state.innermostNestedGuardp)
24612:             *state.innermostNestedGuardp = nested;
20429:         JS_ASSERT(nested);
20931:         JS_ASSERT(nested->exitType == NESTED_EXIT);
20429:         JS_ASSERT(state.lastTreeExitGuard);
20931:         JS_ASSERT(state.lastTreeExitGuard->exitType != NESTED_EXIT);
19590:     }
21433: 
27166:     int32_t bs = state.builtinStatus;
24612:     bool bailed = innermost->exitType == STATUS_EXIT && (bs & JSBUILTIN_BAILED);
24870:     if (bailed) {
24612:         /*
24612:          * Deep-bail case.
24612:          *
24612:          * A _FAIL native already called LeaveTree. We already reconstructed
24612:          * the interpreter stack, in pre-call state, with pc pointing to the
24612:          * CALL/APPLY op, for correctness. Then we continued in native code.
28086:          *
28086:          * First, if we just returned from a slow native, pop its stack frame.
28086:          */
28086:         if (!cx->fp->script) {
28086:             JSStackFrame *fp = cx->fp;
28086:             JS_ASSERT(FUN_SLOW_NATIVE(GET_FUNCTION_PRIVATE(cx, fp->callee)));
28086:             JS_ASSERT(fp->regs == NULL);
28086:             JS_ASSERT(fp->down->regs != &((JSInlineFrame *) fp)->callerRegs);
28086:             cx->fp = fp->down;
28086:             JS_ARENA_RELEASE(&cx->stackPool, ((JSInlineFrame *) fp)->mark);
28086:         }
28086:         JS_ASSERT(cx->fp->script);
28086: 
24870:         if (!(bs & JSBUILTIN_ERROR)) {
24870:             /*
24612:              * The native succeeded (no exception or error). After it returned, the
24612:              * trace stored the return value (at the top of the native stack) and
27166:              * then immediately flunked the guard on state->builtinStatus.
24612:              *
24612:              * Now LeaveTree has been called again from the tail of
24612:              * js_ExecuteTree. We are about to return to the interpreter. Adjust
24612:              * the top stack frame to resume on the next op.
24612:              */
28086:             JS_ASSERT(*cx->fp->regs->pc == JSOP_CALL ||
28086:                       *cx->fp->regs->pc == JSOP_APPLY ||
28086:                       *cx->fp->regs->pc == JSOP_NEW);
24612:             uintN argc = GET_ARGC(cx->fp->regs->pc);
24612:             cx->fp->regs->pc += JSOP_CALL_LENGTH;
24612:             cx->fp->regs->sp -= argc + 1;
24612:             JS_ASSERT_IF(!cx->fp->imacpc,
24612:                          cx->fp->slots + cx->fp->script->nfixed +
24612:                          js_ReconstructStackDepth(cx, cx->fp->script, cx->fp->regs->pc) ==
24612:                          cx->fp->regs->sp);
24612: 
24612:             /*
24612:              * The return value was not available when we reconstructed the stack,
24612:              * but we have it now. Box it.
24612:              */
29896:             JSTraceType* typeMap = getStackTypeMap(innermost);
30287: 
30287:             /*
30287:              * If there's a tree call around the point that we deep exited at,
30287:              * then state.sp and state.rp were restored to their original
30287:              * values before the tree call and sp might be less than deepBailSp,
30287:              * which we sampled when we were told to deep bail.
30287:              */
30287:             JS_ASSERT(state.deepBailSp >= state.stackBase && state.sp <= state.deepBailSp);
24612:             NativeToValue(cx,
24612:                           cx->fp->regs->sp[-1],
24612:                           typeMap[innermost->numStackSlots - 1],
30287:                           (jsdouble *) state.deepBailSp + innermost->sp_adj / sizeof(jsdouble) - 1);
24870:         }
27379:         JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
27379:         if (tm->prohibitFlush && --tm->prohibitFlush == 0 && tm->needFlush)
27379:             FlushJITCache(cx);
24612:         return;
24612:     }
24612: 
24612:     JS_ARENA_RELEASE(&cx->stackPool, state.stackMark);
19588:     while (callstack < rp) {
19588:         /* Synthesize a stack frame and write out the values in it using the type map pointer
19588:            on the native call stack. */
23447:         js_SynthesizeFrame(cx, **callstack);
29896:         int slots = FlushNativeStackFrame(cx, 1 /* callDepth */, (JSTraceType*)(*callstack + 1),
29896:                                           stack, cx->fp);
19588: #ifdef DEBUG
19588:         JSStackFrame* fp = cx->fp;
29883:         debug_only_printf(LC_TMTracer,
29883:                           "synthesized deep frame for %s:%u@%u, slots=%d\n",
21685:                           fp->script->filename,
21685:                           js_FramePCToLineNumber(cx, fp),
21685:                           FramePCOffset(fp),
29883:                           slots);
19588: #endif
19588:         /* Keep track of the additional frames we put on the interpreter stack and the native
19588:            stack slots we consumed. */
24612:         ++*state.inlineCallCountp;
19588:         ++callstack;
18170:         stack += slots;
18164:     }
19588: 
18164:     /* We already synthesized the frames around the innermost guard. Here we just deal
18164:        with additional frames inside the tree we are bailing out from. */
19588:     JS_ASSERT(rp == callstack);
20429:     unsigned calldepth = innermost->calldepth;
18226:     unsigned calldepth_slots = 0;
19588:     for (unsigned n = 0; n < calldepth; ++n) {
23447:         calldepth_slots += js_SynthesizeFrame(cx, *callstack[n]);
24612:         ++*state.inlineCallCountp;
19588: #ifdef DEBUG
19588:         JSStackFrame* fp = cx->fp;
29883:         debug_only_printf(LC_TMTracer,
29883:                           "synthesized shallow frame for %s:%u@%u\n",
21685:                           fp->script->filename, js_FramePCToLineNumber(cx, fp),
29883:                           FramePCOffset(fp));
19588: #endif
19588:     }
17923: 
21685:     /* Adjust sp and pc relative to the tree we exited from (not the tree we entered into).
21685:        These are our final values for sp and pc since js_SynthesizeFrame has already taken
22925:        care of all frames in between. But first we recover fp->blockChain, which comes from
22925:        the side exit struct. */
17923:     JSStackFrame* fp = cx->fp;
18226: 
22925:     fp->blockChain = innermost->block;
22925: 
18193:     /* If we are not exiting from an inlined frame the state->sp is spbase, otherwise spbase
18193:        is whatever slots frames around us consume. */
25111:     fp->regs->pc = innermost->pc;
25111:     fp->imacpc = innermost->imacpc;
20931:     fp->regs->sp = StackBase(fp) + (innermost->sp_adj / sizeof(double)) - calldepth_slots;
21685:     JS_ASSERT_IF(!fp->imacpc,
21685:                  fp->slots + fp->script->nfixed +
18772:                  js_ReconstructStackDepth(cx, fp->script, fp->regs->pc) == fp->regs->sp);
17923: 
24612: #ifdef EXECUTE_TREE_TIMER
24612:     uint64 cycles = rdtsc() - state.startTime;
21459: #elif defined(JS_JIT_SPEW)
19040:     uint64 cycles = 0;
18788: #endif
18788: 
29883:     debug_only_printf(LC_TMTracer,
29893:                       "leaving trace at %s:%u@%u, op=%s, lr=%p, exitType=%s, sp=%d, "
19588:                       "calldepth=%d, cycles=%llu\n",
21685:                       fp->script->filename,
21685:                       js_FramePCToLineNumber(cx, fp),
21685:                       FramePCOffset(fp),
21685:                       js_CodeName[fp->imacpc ? *fp->imacpc : *fp->regs->pc],
25469:                       (void*)lr,
29894:                       getExitName(lr->exitType),
20931:                       fp->regs->sp - StackBase(fp),
19588:                       calldepth,
29883:                       cycles);
17923: 
18200:     /* If this trace is part of a tree, later branches might have added additional globals for
24246:        which we don't have any type information available in the side exit. We merge in this
18200:        information from the entry type-map. See also comment in the constructor of TraceRecorder
18200:        why this is always safe to do. */
24612:     TreeInfo* outermostTree = state.outermostTree;
24612:     uint16* gslots = outermostTree->globalSlots->data();
24612:     unsigned ngslots = outermostTree->globalSlots->length();
24612:     JS_ASSERT(ngslots == outermostTree->nGlobalTypes());
29896:     JSTraceType* globalTypeMap;
25491: 
25491:     /* Are there enough globals? This is the ideal fast path. */
25491:     if (innermost->numGlobalSlots == ngslots) {
25491:         globalTypeMap = getGlobalTypeMap(innermost);
25491:     /* Otherwise, merge the typemap of the innermost entry and exit together.  This should always
25491:        work because it is invalid for nested trees or linked trees to have incompatible types.
25491:        Thus, whenever a new global type is lazily added into a tree, all dependent and linked
25491:        trees are immediately specialized (see bug 476653). */
25491:     } else {
25491:         TreeInfo* ti = (TreeInfo*)innermost->from->root->vmprivate;
25491:         JS_ASSERT(ti->nGlobalTypes() == ngslots);
25491:         JS_ASSERT(ti->nGlobalTypes() > innermost->numGlobalSlots);
29896:         globalTypeMap = (JSTraceType*)alloca(ngslots * sizeof(JSTraceType));
25491:         memcpy(globalTypeMap, getGlobalTypeMap(innermost), innermost->numGlobalSlots);
25491:         memcpy(globalTypeMap + innermost->numGlobalSlots,
25491:                ti->globalTypeMap() + innermost->numGlobalSlots,
25491:                ti->nGlobalTypes() - innermost->numGlobalSlots);
25491:     }
18200: 
28654:     /* write back native stack frame */
28654: #ifdef DEBUG
28654:     int slots =
28654: #endif
28656:         FlushNativeStackFrame(cx, innermost->calldepth,
28654:                               getStackTypeMap(innermost),
28654:                               stack, NULL);
28654:     JS_ASSERT(unsigned(slots) == innermost->numStackSlots);
28654: 
28654:     if (innermost->nativeCalleeWord)
28654:         SynthesizeSlowNativeFrame(cx, innermost);
28654: 
28664:     /* write back interned globals */
28664:     double* global = (double*)(&state + 1);
29880:     FlushNativeGlobalFrame(cx, global,
29880:                            ngslots, gslots, globalTypeMap);
28664:     JS_ASSERT(*(uint64*)&global[STOBJ_NSLOTS(JS_GetGlobalForObject(cx, cx->fp->scopeChain))] ==
28664:               0xdeadbeefdeadbeefLL);
28664: 
28086:     cx->nativeVp = NULL;
28086: 
19591: #ifdef DEBUG
26268:     // Verify that our state restoration worked.
19591:     for (JSStackFrame* fp = cx->fp; fp; fp = fp->down) {
27012:         JS_ASSERT_IF(fp->callee, JSVAL_IS_OBJECT(fp->argv[-1]));
19591:     }
19591: #endif
23706: #ifdef JS_JIT_SPEW
23706:     if (innermost->exitType != TIMEOUT_EXIT)
17726:         AUDIT(sideExitIntoInterpreter);
23706:     else
23706:         AUDIT(timeoutIntoInterpreter);
23706: #endif
17397: 
24612:     state.innermost = innermost;
17772: }
17376: 
22652: JS_REQUIRES_STACK bool
20422: js_MonitorLoopEdge(JSContext* cx, uintN& inlineCallCount)
17939: {
29368: #ifdef MOZ_TRACEVIS
29368:     TraceVisStateObj tvso(S_MONITOR);
29368: #endif
29368: 
17939:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
17939: 
18672:     /* Is the recorder currently active? */
17954:     if (tm->recorder) {
24493:         jsbytecode* innerLoopHeaderPC = cx->fp->regs->pc;
24493: 
20422:         if (js_RecordLoopEdge(cx, tm->recorder, inlineCallCount))
17954:             return true;
24493: 
24493:         /*
24493:          * js_RecordLoopEdge will invoke an inner tree if we have a matching one. If we
24493:          * arrive here, that tree didn't run to completion and instead we mis-matched
24493:          * or the inner tree took a side exit other than the loop exit. We are thus
24493:          * no longer guaranteed to be parked on the same loop header js_MonitorLoopEdge
24493:          * was called for. In fact, this might not even be a loop header at all. Hence
24493:          * if the program counter no longer hovers over the inner loop header, return to
24493:          * the interpreter and do not attempt to trigger or record a new tree at this
24493:          * location.
24493:          */
29368:          if (innerLoopHeaderPC != cx->fp->regs->pc) {
29368: #ifdef MOZ_TRACEVIS
29368:              tvso.r = R_INNER_SIDE_EXIT;
29368: #endif
24493:              return false;
17954:          }
29368:     }
18317:     JS_ASSERT(!tm->recorder);
17939: 
23449:     /* Check the pool of reserved doubles (this might trigger a GC). */
23449:     if (tm->reservedDoublePoolPtr < (tm->reservedDoublePool + MAX_NATIVE_STACK_SLOTS) &&
23449:         !js_ReplenishReservedPool(cx, tm)) {
29368: #ifdef MOZ_TRACEVIS
29368:         tvso.r = R_DOUBLES;
29368: #endif
21514:         return false; /* Out of memory, don't try to record now. */
21514:     }
21514: 
21514:     /* Make sure the shape of the global object still matches (this might flush the JIT cache). */
21514:     JSObject* globalObj = JS_GetGlobalForObject(cx, cx->fp->scopeChain);
24491:     uint32 globalShape = -1;
24491:     SlotList* globalSlots = NULL;
24491: 
28105:     if (!CheckGlobalObjectShape(cx, tm, globalObj, &globalShape, &globalSlots)) {
28105:         js_Backoff(cx, cx->fp->regs->pc);
27493:         return false;
28105:     }
21514: 
25087:     /* Do not enter the JIT code with a pending operation callback. */
29368:     if (cx->operationCallbackFlag) {
29368: #ifdef MOZ_TRACEVIS
29368:         tvso.r = R_CALLBACK_PENDING;
29368: #endif
25087:         return false;
29368:     }
25087: 
17940:     jsbytecode* pc = cx->fp->regs->pc;
28244:     uint32 argc = cx->fp->argc;
28244: 
28244:     Fragment* f = getLoop(tm, pc, globalObj, globalShape, argc);
18204:     if (!f)
28244:         f = getAnchor(tm, pc, globalObj, globalShape, argc);
24307: 
24307:     if (!f) {
27379:         FlushJITCache(cx);
29368: #ifdef MOZ_TRACEVIS
29368:         tvso.r = R_OOM_GETANCHOR;
29368: #endif
24307:         return false;
24307:     }
21433: 
21433:     /* If we have no code in the anchor and no peers, we definitively won't be able to
24290:        activate any trees so, start compiling. */
21433:     if (!f->code() && !f->peer) {
25627:     record:
29368:         if (++f->hits() < HOTLOOP) {
29368: #ifdef MOZ_TRACEVIS
29368:             tvso.r = f->hits() < 1 ? R_BACKED_OFF : R_COLD;
29368: #endif
25627:             return false;
29368:         }
21433:         /* We can give RecordTree the root peer. If that peer is already taken, it will
21433:            walk the peer list and find us a free slot or allocate a new tree if needed. */
29368:         bool rv = js_RecordTree(cx, tm, f->first, NULL, 0, globalObj, globalShape,
28244:                                 globalSlots, argc);
29368: #ifdef MOZ_TRACEVIS
29368:         if (!rv)
29368:             tvso.r = R_FAIL_RECORD_TREE;
29368: #endif
29368:         return rv;
21433:     }
25627: 
29883:     debug_only_printf(LC_TMTracer,
29883:                       "Looking for compat peer %d@%d, from %p (ip: %p)\n",
21685:                       js_FramePCToLineNumber(cx, cx->fp),
29883:                       FramePCOffset(cx->fp), (void*)f, f->ip);
25627: 
25627:     uintN count;
29880:     Fragment* match = js_FindVMCompatiblePeer(cx, globalObj, f, count);
25627:     if (!match) {
25627:         if (count < MAXPEERS)
25627:             goto record;
25627:         /* If we hit the max peers ceiling, don't try to lookup fragments all the time. Thats
25627:            expensive. This must be a rather type-unstable loop. */
29883:         debug_only_print0(LC_TMTracer, "Blacklisted: too many peer trees.\n");
28105:         js_Blacklist((jsbytecode*) f->root->ip);
29368: #ifdef MOZ_TRACEVIS
29368:         tvso.r = R_MAX_PEERS;
29368: #endif
25627:         return false;
25627:     }
21433: 
21521:     VMSideExit* lr = NULL;
21521:     VMSideExit* innermostNestedGuard = NULL;
21433: 
21433:     lr = js_ExecuteTree(cx, match, inlineCallCount, &innermostNestedGuard);
29368:     if (!lr) {
29368: #ifdef MOZ_TRACEVIS
29368:         tvso.r = R_FAIL_EXECUTE_TREE;
29368: #endif
21433:         return false;
29368:     }
21433: 
18284:     /* If we exit on a branch, or on a tree call guard, try to grow the inner tree (in case
18284:        of a branch exit), or the tree nested around the tree we exited from (in case of the
18284:        tree call guard). */
29368:     bool rv;
20931:     switch (lr->exitType) {
21433:       case UNSTABLE_LOOP_EXIT:
29880:           rv = js_AttemptToStabilizeTree(cx, globalObj, lr, NULL, NULL);
29368: #ifdef MOZ_TRACEVIS
29368:           if (!rv)
29368:               tvso.r = R_FAIL_STABILIZE;
29368: #endif
29368:           return rv;
29354:       case OVERFLOW_EXIT:
29354:         oracle.markInstructionUndemotable(cx->fp->regs->pc);
29354:         /* fall through */
18284:       case BRANCH_EXIT:
25099:       case CASE_EXIT:
29368:           return js_AttemptToExtendTree(cx, lr, NULL, NULL
29368: #ifdef MOZ_TRACEVIS
29368:                                           , &tvso
29368: #endif
29368:                  );
18284:       case LOOP_EXIT:
18284:         if (innermostNestedGuard)
29368:             return js_AttemptToExtendTree(cx, innermostNestedGuard, lr, NULL
29368: #ifdef MOZ_TRACEVIS
29368:                                             , &tvso
29368: #endif
29368:                    );
29368: #ifdef MOZ_TRACEVIS
29368:         tvso.r = R_NO_EXTEND_OUTER;
29368: #endif
17951:         return false;
29368: #ifdef MOZ_TRACEVIS
29368:       case MISMATCH_EXIT:  tvso.r = R_MISMATCH_EXIT;  return false;
29368:       case OOM_EXIT:       tvso.r = R_OOM_EXIT;       return false;
29368:       case TIMEOUT_EXIT:   tvso.r = R_TIMEOUT_EXIT;   return false;
29368:       case DEEP_BAIL_EXIT: tvso.r = R_DEEP_BAIL_EXIT; return false;
29368:       case STATUS_EXIT:    tvso.r = R_STATUS_EXIT;    return false;
29368: #endif
18284:       default:
18284:         /* No, this was an unusual exit (i.e. out of memory/GC), so just resume interpretation. */
29368: #ifdef MOZ_TRACEVIS
29368:         tvso.r = R_OTHER_EXIT;
29368: #endif
18284:         return false;
18284:     }
17939: }
17939: 
27933: JS_REQUIRES_STACK JSRecordingStatus
23440: TraceRecorder::monitorRecording(JSContext* cx, TraceRecorder* tr, JSOp op)
23440: {
27493:     /* Process needFlush and deepAbort() requests now. */
27493:     if (JS_TRACE_MONITOR(cx).needFlush) {
27493:         FlushJITCache(cx);
27933:         return JSRS_STOP;
27493:     }
23440:     if (tr->wasDeepAborted()) {
22632:         js_AbortRecording(cx, "deep abort requested");
27933:         return JSRS_STOP;
23111:     }
26559:     JS_ASSERT(!tr->fragment->lastIns);
26559: 
26557:     /*
26557:      * Clear one-shot state used to communicate between record_JSOP_CALL and post-
28086:      * opcode-case-guts record hook (record_NativeCallComplete).
26557:      */
23440:     tr->pendingTraceableNative = NULL;
28086:     tr->newobj_ins = NULL;
19068: 
29883:     debug_only_stmt(
29883:         if (js_LogController.lcbits & LC_TMRecorder) {
29883:             js_Disassemble1(cx, cx->fp->script, cx->fp->regs->pc,
29883:                             cx->fp->imacpc
29883:                                 ? 0 : cx->fp->regs->pc - cx->fp->script->code,
29883:                             !cx->fp->imacpc, stdout);
29883:         }
29883:     )
19075: 
26011:     /* If op is not a break or a return from a loop, continue recording and follow the
26011:        trace. We check for imacro-calling bytecodes inside each switch case to resolve
26011:        the if (JSOP_IS_IMACOP(x)) conditions at compile time. */
26011: 
27933:     JSRecordingStatus status;
26563: #ifdef DEBUG
26563:     bool wasInImacro = (cx->fp->imacpc != NULL);
26563: #endif
23111:     switch (op) {
28008:       default:
28008:           status = JSRS_ERROR;
28008:           goto stop_recording;
23111: # define OPDEF(x,val,name,token,length,nuses,ndefs,prec,format)               \
23111:       case x:                                                                 \
27933:         status = tr->record_##x();                                            \
26011:         if (JSOP_IS_IMACOP(x))                                                \
23111:             goto imacro;                                                      \
23111:         break;
23111: # include "jsopcode.tbl"
23111: # undef OPDEF
23111:     }
23111: 
27933:     JS_ASSERT(status != JSRS_IMACRO);
26563:     JS_ASSERT_IF(!wasInImacro, cx->fp->imacpc == NULL);
26563: 
26557:     /* Process deepAbort() requests now. */
26557:     if (tr->wasDeepAborted()) {
26557:         js_AbortRecording(cx, "deep abort requested");
27933:         return JSRS_STOP;
26557:     }
26557: 
26557:     if (JS_TRACE_MONITOR(cx).fragmento->assm()->error()) {
26557:         js_AbortRecording(cx, "error during recording");
27933:         return JSRS_STOP;
26557:     }
26557: 
26826:     if (tr->lirbuf->outOMem() ||
27884:         js_OverfullFragmento(&JS_TRACE_MONITOR(cx), JS_TRACE_MONITOR(cx).fragmento)) {
26557:         js_AbortRecording(cx, "no more LIR memory");
27379:         FlushJITCache(cx);
27933:         return JSRS_STOP;
27933:     }
23111: 
23111:   imacro:
27933:     if (!STATUS_ABORTS_RECORDING(status))
27933:         return status;
23111: 
26557:   stop_recording:
26557:     /* If we recorded the end of the trace, destroy the recorder now. */
26557:     if (tr->fragment->lastIns) {
26557:         js_DeleteRecorder(cx);
27933:         return status;
26557:     }
26557: 
26557:     /* Looks like we encountered an error condition. Abort recording. */
23111:     js_AbortRecording(cx, js_CodeName[op]);
27933:     return status;
18683: }
18683: 
22652: JS_REQUIRES_STACK void
20422: js_AbortRecording(JSContext* cx, const char* reason)
17350: {
18614:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
18614:     JS_ASSERT(tm->recorder != NULL);
20965:     AUDIT(recorderAborted);
21685: 
18614:     /* Abort the trace and blacklist its starting point. */
20965:     Fragment* f = tm->recorder->getFragment();
26958: 
26958:     /*
26958:      * If the recorder already had its fragment disposed, or we actually finished
26958:      * recording and this recorder merely is passing through the deep abort state
26958:      * to the next recorder on the stack, just destroy the recorder. There is
26958:      * nothing to abort.
26958:      */
26958:     if (!f || f->lastIns) {
20965:         js_DeleteRecorder(cx);
20965:         return;
20965:     }
26958: 
20965:     JS_ASSERT(!f->vmprivate);
25627: #ifdef DEBUG
25627:     TreeInfo* ti = tm->recorder->getTreeInfo();
29883:     debug_only_printf(LC_TMAbort,
29883:                       "Abort recording of tree %s:%d@%d at %s:%d@%d: %s.\n",
25627:                       ti->treeFileName,
25627:                       ti->treeLineNumber,
25627:                       ti->treePCOffset,
25627:                       cx->fp->script->filename,
25627:                       js_FramePCToLineNumber(cx, cx->fp),
25627:                       FramePCOffset(cx->fp),
29883:                       reason);
25627: #endif
25627: 
28105:     js_Backoff(cx, (jsbytecode*) f->root->ip, f->root);
24486: 
24486:     /*
24486:      * If js_DeleteRecorder flushed the code cache, we can't rely on f any more.
24486:      */
24486:     if (!js_DeleteRecorder(cx))
24486:         return;
24486: 
24486:     /*
24486:      * If this is the primary trace and we didn't succeed compiling, trash the
24486:      * TreeInfo object.
24486:      */
18614:     if (!f->code() && (f->root == f))
17889:         js_TrashTree(cx, f);
17350: }
17350: 
18333: #if defined NANOJIT_IA32
18333: static bool
18333: js_CheckForSSE2()
18333: {
18333:     int features = 0;
18333: #if defined _MSC_VER
18333:     __asm
18333:     {
18333:         pushad
18333:         mov eax, 1
18333:         cpuid
18333:         mov features, edx
18333:         popad
18333:     }
18333: #elif defined __GNUC__
21475:     asm("xchg %%esi, %%ebx\n" /* we can't clobber ebx on gcc (PIC register) */
20953:         "mov $0x01, %%eax\n"
20953:         "cpuid\n"
20953:         "mov %%edx, %0\n"
21475:         "xchg %%esi, %%ebx\n"
20953:         : "=m" (features)
21475:         : /* We have no inputs */
21475:         : "%eax", "%esi", "%ecx", "%edx"
18788:        );
19058: #elif defined __SUNPRO_C || defined __SUNPRO_CC
19058:     asm("push %%ebx\n"
19058:         "mov $0x01, %%eax\n"
19058:         "cpuid\n"
19058:         "pop %%ebx\n"
19058:         : "=d" (features)
18333:         : /* We have no inputs */
19058:         : "%eax", "%ecx"
18333:        );
18333: #endif
18333:     return (features & (1<<26)) != 0;
18333: }
18333: #endif
18333: 
26545: #if defined(NANOJIT_ARM)
26545: 
26545: #if defined(_MSC_VER) && defined(WINCE)
26545: 
26545: // these come in from jswince.asm
28185: extern "C" int js_arm_try_thumb_op();
26545: extern "C" int js_arm_try_armv6t2_op();
28185: extern "C" int js_arm_try_armv5_op();
28185: extern "C" int js_arm_try_armv6_op();
28185: extern "C" int js_arm_try_armv7_op();
26545: extern "C" int js_arm_try_vfp_op();
26545: 
26545: static bool
28185: js_arm_check_thumb() {
28185:     bool ret = false;
28185:     __try {
28185:         js_arm_try_thumb_op();
28185:         ret = true;
28185:     } __except(GetExceptionCode() == EXCEPTION_ILLEGAL_INSTRUCTION) {
28185:         ret = false;
28185:     }
28185:     return ret;
28185: }
28185: 
28185: static bool
28185: js_arm_check_thumb2() {
26545:     bool ret = false;
26545:     __try {
26545:         js_arm_try_armv6t2_op();
26545:         ret = true;
26545:     } __except(GetExceptionCode() == EXCEPTION_ILLEGAL_INSTRUCTION) {
26545:         ret = false;
26545:     }
26545:     return ret;
26545: }
26545: 
28185: static unsigned int
28185: js_arm_check_arch() {
28185:     unsigned int arch = 4;
28185:     __try {
28185:         js_arm_try_armv5_op();
28185:         arch = 5;
28185:         js_arm_try_armv6_op();
28185:         arch = 6;
28185:         js_arm_try_armv7_op();
28185:         arch = 7;
28185:     } __except(GetExceptionCode() == EXCEPTION_ILLEGAL_INSTRUCTION) {
28185:     }
28185:     return arch;
28185: }
28185: 
26545: static bool
26545: js_arm_check_vfp() {
26545:     bool ret = false;
26545:     __try {
26545:         js_arm_try_vfp_op();
26545:         ret = true;
26545:     } __except(GetExceptionCode() == EXCEPTION_ILLEGAL_INSTRUCTION) {
26545:         ret = false;
26545:     }
26545:     return ret;
26545: }
26545: 
26545: #elif defined(__GNUC__) && defined(AVMPLUS_LINUX)
26545: 
26545: #include <stdlib.h>
26545: #include <unistd.h>
26545: #include <sys/types.h>
26545: #include <sys/stat.h>
26545: #include <sys/mman.h>
26545: #include <fcntl.h>
26545: #include <string.h>
26545: #include <elf.h>
26545: 
28185: // Assume ARMv4 by default.
28185: static unsigned int arm_arch = 4;
28185: static bool arm_has_thumb = false;
26545: static bool arm_has_vfp = false;
26545: static bool arm_has_neon = false;
26545: static bool arm_has_iwmmxt = false;
26545: static bool arm_tests_initialized = false;
26545: 
26545: static void
26545: arm_read_auxv() {
26545:     int fd;
26545:     Elf32_auxv_t aux;
26545: 
26545:     fd = open("/proc/self/auxv", O_RDONLY);
26545:     if (fd > 0) {
26545:         while (read(fd, &aux, sizeof(Elf32_auxv_t))) {
26545:             if (aux.a_type == AT_HWCAP) {
26545:                 uint32_t hwcap = aux.a_un.a_val;
26545:                 if (getenv("ARM_FORCE_HWCAP"))
26545:                     hwcap = strtoul(getenv("ARM_FORCE_HWCAP"), NULL, 0);
26545:                 // hardcode these values to avoid depending on specific versions
26545:                 // of the hwcap header, e.g. HWCAP_NEON
28185:                 arm_has_thumb = (hwcap & 4) != 0;
26545:                 arm_has_vfp = (hwcap & 64) != 0;
26545:                 arm_has_iwmmxt = (hwcap & 512) != 0;
26545:                 // this flag is only present on kernel 2.6.29
26545:                 arm_has_neon = (hwcap & 4096) != 0;
26545:             } else if (aux.a_type == AT_PLATFORM) {
26545:                 const char *plat = (const char*) aux.a_un.a_val;
26545:                 if (getenv("ARM_FORCE_PLATFORM"))
26545:                     plat = getenv("ARM_FORCE_PLATFORM");
28185:                 // The platform string has the form "v[0-9][lb]". The "l" or "b" indicate little-
28185:                 // or big-endian variants and the digit indicates the version of the platform.
28185:                 // We can only accept ARMv4 and above, but allow anything up to ARMv9 for future
28185:                 // processors. Architectures newer than ARMv7 are assumed to be
28185:                 // backwards-compatible with ARMv7.
28185:                 if ((plat[0] == 'v') &&
28185:                     (plat[1] >= '4') && (plat[1] <= '9') &&
28185:                     ((plat[2] == 'l') || (plat[2] == 'b')))
28185:                 {
28185:                     arm_arch = plat[1] - '0';
28185:                 }
28185:                 else
28185:                 {
28185:                     // For production code, ignore invalid (or unexpected) platform strings and
28185:                     // fall back to the default. For debug code, use an assertion to catch this.
28185:                     JS_ASSERT(false);
26545:                 }
26545:             }
26545:         }
26545:         close (fd);
26545: 
26545:         // if we don't have 2.6.29, we have to do this hack; set
26545:         // the env var to trust HWCAP.
28185:         if (!getenv("ARM_TRUST_HWCAP") && (arm_arch >= 7))
26545:             arm_has_neon = true;
26545:     }
26545: 
26545:     arm_tests_initialized = true;
26545: }
26545: 
26545: static bool
28185: js_arm_check_thumb() {
26545:     if (!arm_tests_initialized)
26545:         arm_read_auxv();
26545: 
28185:     return arm_has_thumb;
28185: }
28185: 
28185: static bool
28185: js_arm_check_thumb2() {
28185:     if (!arm_tests_initialized)
28185:         arm_read_auxv();
28185: 
28185:     // ARMv6T2 also supports Thumb2, but Linux doesn't provide an easy way to test for this as
28185:     // there is no associated bit in auxv. ARMv7 always supports Thumb2, and future architectures
28185:     // are assumed to be backwards-compatible.
28185:     return (arm_arch >= 7);
28185: }
28185: 
28185: static unsigned int
28185: js_arm_check_arch() {
28185:     if (!arm_tests_initialized)
28185:         arm_read_auxv();
28185: 
28185:     return arm_arch;
26545: }
26545: 
26545: static bool
26545: js_arm_check_vfp() {
26545:     if (!arm_tests_initialized)
26545:         arm_read_auxv();
26545: 
26545:     return arm_has_vfp;
26545: }
26545: 
26545: #else
28185: #warning Not sure how to check for architecture variant on your platform. Assuming ARMv4.
26545: static bool
28185: js_arm_check_thumb() { return false; }
28185: static bool
28185: js_arm_check_thumb2() { return false; }
28185: static unsigned int
28185: js_arm_check_arch() { return 4; }
26548: static bool
26545: js_arm_check_vfp() { return false; }
26545: #endif
26545: 
26545: #endif /* NANOJIT_ARM */
26545: 
27884: #define K *1024
27884: #define M K K
27884: #define G K M
27884: 
27884: void
27884: js_SetMaxCodeCacheBytes(JSContext* cx, uint32 bytes)
27884: {
27884:     JSTraceMonitor* tm = &JS_THREAD_DATA(cx)->traceMonitor;
27884:     JS_ASSERT(tm->fragmento && tm->reFragmento);
27884:     if (bytes > 1 G)
27884:         bytes = 1 G;
27884:     if (bytes < 128 K)
27884:         bytes = 128 K;
27884:     tm->maxCodeCacheBytes = bytes;
27884: }
27884: 
25940: void
18068: js_InitJIT(JSTraceMonitor *tm)
18068: {
29883: #if defined JS_JIT_SPEW
29883:     /* Set up debug logging */
29883:     if (!did_we_set_up_debug_logging) {
29883:         js_InitJITLogController();
29883:         did_we_set_up_debug_logging = true;
29883:     }
29883: #else
29883:     memset(&js_LogController, 0, sizeof(js_LogController));
29883: #endif
29883: 
26545:     if (!did_we_check_processor_features) {
18333: #if defined NANOJIT_IA32
22667:         avmplus::AvmCore::config.use_cmov =
22667:         avmplus::AvmCore::config.sse2 = js_CheckForSSE2();
18333: #endif
26545: #if defined NANOJIT_ARM
28185:         bool            arm_vfp     = js_arm_check_vfp();
28185:         bool            arm_thumb   = js_arm_check_thumb();
28185:         bool            arm_thumb2  = js_arm_check_thumb2();
28185:         unsigned int    arm_arch    = js_arm_check_arch();
28185: 
28185:         avmplus::AvmCore::config.vfp        = arm_vfp;
28185:         avmplus::AvmCore::config.soft_float = !arm_vfp;
28185:         avmplus::AvmCore::config.thumb      = arm_thumb;
28185:         avmplus::AvmCore::config.thumb2     = arm_thumb2;
28185:         avmplus::AvmCore::config.arch       = arm_arch;
28185: 
28185:         // Sanity-check the configuration detection.
28185:         //  * We don't understand architectures prior to ARMv4.
28185:         JS_ASSERT(arm_arch >= 4);
28185:         //  * All architectures support Thumb with the possible exception of ARMv4.
28185:         JS_ASSERT((arm_thumb) || (arm_arch == 4));
28185:         //  * Only ARMv6T2 and ARMv7(+) support Thumb2, but ARMv6 does not.
28185:         JS_ASSERT((arm_thumb2) || (arm_arch <= 6));
28185:         //  * All architectures that support Thumb2 also support Thumb.
28185:         JS_ASSERT((arm_thumb2 && arm_thumb) || (!arm_thumb2));
26545: #endif
26545:         did_we_check_processor_features = true;
26545:     }
26545: 
27884:     /*
27884:      * Set the default size for the code cache to 16MB.
27884:      */
27884:     tm->maxCodeCacheBytes = 16 M;
27884: 
28105:     if (!tm->recordAttempts.ops) {
28105:         JS_DHashTableInit(&tm->recordAttempts, JS_DHashGetStubOps(),
28105:                           NULL, sizeof(PCHashEntry),
28105:                           JS_DHASH_DEFAULT_CAPACITY(PC_HASH_COUNT));
28105:     }
28105: 
17442:     if (!tm->fragmento) {
24491:         JS_ASSERT(!tm->reservedDoublePool);
29883:         Fragmento* fragmento = new (&gc) Fragmento(core, &js_LogController, 32);
29860:         verbose_only(fragmento->labels = new (&gc) LabelMap(core);)
17442:         tm->fragmento = fragmento;
29856:         tm->lirbuf = new (&gc) LirBuffer(fragmento);
23264: #ifdef DEBUG
28546:         tm->lirbuf->names = new (&gc) LirNameMap(&gc, tm->fragmento->labels);
23264: #endif
24491:         for (size_t i = 0; i < MONITOR_N_GLOBAL_STATES; ++i) {
24491:             tm->globalStates[i].globalShape = -1;
24491:             JS_ASSERT(!tm->globalStates[i].globalSlots);
24491:             tm->globalStates[i].globalSlots = new (&gc) SlotList();
24491:         }
23449:         tm->reservedDoublePoolPtr = tm->reservedDoublePool = new jsval[MAX_NATIVE_STACK_SLOTS];
24307:         memset(tm->vmfragments, 0, sizeof(tm->vmfragments));
17442:     }
21491:     if (!tm->reFragmento) {
29883:         Fragmento* fragmento = new (&gc) Fragmento(core, &js_LogController, 32);
29860:         verbose_only(fragmento->labels = new (&gc) LabelMap(core);)
21491:         tm->reFragmento = fragmento;
29856:         tm->reLirBuf = new (&gc) LirBuffer(fragmento);
29915: #ifdef DEBUG
29915:         tm->reLirBuf->names = new (&gc) LirNameMap(&gc, fragmento->labels);
29915: #endif
21491:     }
17884: #if !defined XP_WIN
19623:     debug_only(memset(&jitstats, 0, sizeof(jitstats)));
17884: #endif
17726: }
17726: 
25940: void
18075: js_FinishJIT(JSTraceMonitor *tm)
18068: {
21459: #ifdef JS_JIT_SPEW
29883:     if (jitstats.recorderStarted) {
29883:         debug_only_printf(LC_TMStats,
29883:                           "recorder: started(%llu), aborted(%llu), completed(%llu), different header(%llu), "
18697:                           "trees trashed(%llu), slot promoted(%llu), unstable loop variable(%llu), "
29875:                           "breaks(%llu), returns(%llu), unstableInnerCalls(%llu), blacklisted(%llu)\n",
19623:                           jitstats.recorderStarted, jitstats.recorderAborted, jitstats.traceCompleted,
19623:                           jitstats.returnToDifferentLoopHeader, jitstats.treesTrashed, jitstats.slotPromoted,
21433:                           jitstats.unstableLoopVariable, jitstats.breakLoopExits, jitstats.returnLoopExits,
29875:                           jitstats.noCompatInnerTrees, jitstats.blacklisted);
29883:         debug_only_printf(LC_TMStats,
29883:                           "monitor: triggered(%llu), exits(%llu), type mismatch(%llu), "
19623:                           "global mismatch(%llu)\n", jitstats.traceTriggered, jitstats.sideExitIntoInterpreter,
19623:                           jitstats.typeMapMismatchAtEntry, jitstats.globalShapeMismatchAtEntry);
22616:     }
17726: #endif
18079:     if (tm->fragmento != NULL) {
24491:         JS_ASSERT(tm->reservedDoublePool);
18056:         verbose_only(delete tm->fragmento->labels;)
23264: #ifdef DEBUG
23264:         delete tm->lirbuf->names;
23264:         tm->lirbuf->names = NULL;
23264: #endif
23264:         delete tm->lirbuf;
23264:         tm->lirbuf = NULL;
28105: 
28105:         if (tm->recordAttempts.ops)
28105:             JS_DHashTableFinish(&tm->recordAttempts);
28105: 
25102:         for (size_t i = 0; i < FRAGMENT_TABLE_SIZE; ++i) {
25102:             VMFragment* f = tm->vmfragments[i];
25102:             while(f) {
25102:                 VMFragment* next = f->next;
25102:                 tm->fragmento->clearFragment(f);
25102:                 f = next;
25102:             }
25102:             tm->vmfragments[i] = NULL;
25102:         }
18056:         delete tm->fragmento;
18079:         tm->fragmento = NULL;
24491:         for (size_t i = 0; i < MONITOR_N_GLOBAL_STATES; ++i) {
24491:             JS_ASSERT(tm->globalStates[i].globalSlots);
24491:             delete tm->globalStates[i].globalSlots;
24491:         }
23449:         delete[] tm->reservedDoublePool;
23449:         tm->reservedDoublePool = tm->reservedDoublePoolPtr = NULL;
18079:     }
21502:     if (tm->reFragmento != NULL) {
23237:         delete tm->reLirBuf;
21502:         verbose_only(delete tm->reFragmento->labels;)
21502:         delete tm->reFragmento;
21502:     }
17442: }
17442: 
21723: void
21723: TraceRecorder::pushAbortStack()
21723: {
21723:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
21723: 
21723:     JS_ASSERT(tm->abortStack != this);
21723: 
21723:     nextRecorderToAbort = tm->abortStack;
21723:     tm->abortStack = this;
21723: }
21723: 
21723: void
21723: TraceRecorder::popAbortStack()
21723: {
21723:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
21723: 
21723:     JS_ASSERT(tm->abortStack == this);
21723: 
21723:     tm->abortStack = nextRecorderToAbort;
21723:     nextRecorderToAbort = NULL;
21723: }
21723: 
25940: void
26569: js_PurgeJITOracle()
26569: {
26569:     oracle.clear();
26569: }
26569: 
28105: static JSDHashOperator
28105: js_PurgeScriptRecordingAttempts(JSDHashTable *table,
28105:                                 JSDHashEntryHdr *hdr,
28105:                                 uint32 number, void *arg)
28105: {
28105:     PCHashEntry *e = (PCHashEntry *)hdr;
28105:     JSScript *script = (JSScript *)arg;
28105:     jsbytecode *pc = (jsbytecode *)e->key;
28105: 
28105:     if (JS_UPTRDIFF(pc, script->code) < script->length)
28105:         return JS_DHASH_REMOVE;
28105:     return JS_DHASH_NEXT;
28105: }
28105: 
28950: /*
28950:  * Call 'action' for each root fragment created for 'script'.
28950:  */
28950: template<typename FragmentAction>
28950: static void
28950: js_IterateScriptFragments(JSContext* cx, JSScript* script, FragmentAction action)
28950: {
24879:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
24879:     for (size_t i = 0; i < FRAGMENT_TABLE_SIZE; ++i) {
25089:         for (VMFragment **f = &(tm->vmfragments[i]); *f; ) {
28311:             VMFragment* frag = *f;
25089:             /* Disable future use of any script-associated VMFragment.*/
28311:             if (JS_UPTRDIFF(frag->ip, script->code) < script->length) {
28311:                 JS_ASSERT(frag->root == frag);
29859:                 VMFragment* next = frag->next;
29859:                 if (action(cx, tm, frag)) {
29883:                     debug_only_printf(LC_TMTracer,
29883:                                       "Disconnecting VMFragment %p "
25089:                                       "with ip %p, in range [%p,%p).\n",
28311:                                       (void*)frag, frag->ip, script->code,
29883:                                       script->code + script->length);
25102:                     *f = next;
25089:                 } else {
25089:                     f = &((*f)->next);
25089:                 }
29859:             } else {
29859:                 f = &((*f)->next);
29859:             }
29859:         }
29859:     }
29859: }
29859: 
29859: static bool
29859: trashTreeAction(JSContext* cx, JSTraceMonitor* tm, Fragment* frag)
28950: {
28950:     for (Fragment *p = frag; p; p = p->peer)
28950:         js_TrashTree(cx, p);
29859:     return false;
29859: }
29859: 
29859: static bool
29859: clearFragmentAction(JSContext* cx, JSTraceMonitor* tm, Fragment* frag)
28950: {
28950:     tm->fragmento->clearFragment(frag);
29859:     return true;
28950: }
28950: 
28950: JS_REQUIRES_STACK void
28950: js_PurgeScriptFragments(JSContext* cx, JSScript* script)
28950: {
28950:     if (!TRACING_ENABLED(cx))
28950:         return;
29883:     debug_only_printf(LC_TMTracer,
29883:                       "Purging fragments for JSScript %p.\n", (void*)script);
28950:     /*
28950:      * js_TrashTree trashes dependent trees recursively, so we must do all the trashing
28950:      * before clearing in order to avoid calling js_TrashTree with a deleted fragment.
28950:      */
28950:     js_IterateScriptFragments(cx, script, trashTreeAction);
28950:     js_IterateScriptFragments(cx, script, clearFragmentAction);
28950:     JSTraceMonitor* tm = &JS_TRACE_MONITOR(cx);
28105:     JS_DHashTableEnumerate(&(tm->recordAttempts),
28105:                            js_PurgeScriptRecordingAttempts, script);
28105: 
24879: }
24879: 
26826: bool
27884: js_OverfullFragmento(JSTraceMonitor* tm, Fragmento *fragmento)
26826: {
26826:     /*
26826:      * You might imagine the outOMem flag on the lirbuf is sufficient
26826:      * to model the notion of "running out of memory", but there are actually
26826:      * two separate issues involved:
26826:      *
26826:      *  1. The process truly running out of memory: malloc() or mmap()
26826:      *     failed.
26826:      *
26826:      *  2. The limit we put on the "intended size" of the tracemonkey code
26826:      *     cache, in pages, has been exceeded.
26826:      *
26826:      * Condition 1 doesn't happen very often, but we're obliged to try to
26826:      * safely shut down and signal the rest of spidermonkey when it
26826:      * does. Condition 2 happens quite regularly.
26826:      *
26826:      * Presently, the code in this file doesn't check the outOMem condition
26826:      * often enough, and frequently misuses the unchecked results of
26826:      * lirbuffer insertions on the asssumption that it will notice the
26826:      * outOMem flag "soon enough" when it returns to the monitorRecording
26826:      * function. This turns out to be a false assumption if we use outOMem
26826:      * to signal condition 2: we regularly provoke "passing our intended
26826:      * size" and regularly fail to notice it in time to prevent writing
26826:      * over the end of an artificially self-limited LIR buffer.
26826:      *
26826:      * To mitigate, though not completely solve, this problem, we're
26826:      * modeling the two forms of memory exhaustion *separately* for the
26826:      * time being: condition 1 is handled by the outOMem flag inside
26826:      * nanojit, and condition 2 is being handled independently *here*. So
26826:      * we construct our fragmentos to use all available memory they like,
26826:      * and only report outOMem to us when there is literally no OS memory
26826:      * left. Merely purging our cache when we hit our highwater mark is
26826:      * handled by the (few) callers of this function.
26826:      *
26826:      */
27884:     jsuint maxsz = tm->maxCodeCacheBytes;
27884:     if (fragmento == tm->fragmento) {
27884:         if (tm->prohibitFlush)
27884:             return false;
27884:     } else {
27884:         /*
27884:          * At the time of making the code cache size configurable, we were using
27884:          * 16 MB for the main code cache and 1 MB for the regular expression code
27884:          * cache. We will stick to this 16:1 ratio here until we unify the two
27884:          * code caches.
27884:          */
27884:         maxsz /= 16;
27884:     }
28355:     return (fragmento->cacheUsed() > maxsz);
26826: }
26826: 
25214: JS_FORCES_STACK JS_FRIEND_API(void)
25214: js_DeepBail(JSContext *cx)
25214: {
25214:     JS_ASSERT(JS_ON_TRACE(cx));
25214: 
27577:     /*
27577:      * Exactly one context on the current thread is on trace. Find out which
27577:      * one. (Most callers cannot guarantee that it's cx.)
27577:      */
27882:     JSTraceMonitor *tm = &JS_TRACE_MONITOR(cx);
27882:     JSContext *tracecx = tm->tracecx;
27577: 
24612:     /* It's a bug if a non-FAIL_STATUS builtin gets here. */
27577:     JS_ASSERT(tracecx->bailExit);
27577: 
27882:     tm->tracecx = NULL;
27882:     tm->prohibitFlush++;
29883:     debug_only_print0(LC_TMTracer, "Deep bail.\n");
27577:     LeaveTree(*tracecx->interpState, tracecx->bailExit);
27577:     tracecx->bailExit = NULL;
30287: 
30287:     InterpState* state = tracecx->interpState;
30287:     state->builtinStatus |= JSBUILTIN_BAILED;
30287:     state->deepBailSp = state->sp;
22652: }
22652: 
22652: JS_REQUIRES_STACK jsval&
17412: TraceRecorder::argval(unsigned n) const
17412: {
17799:     JS_ASSERT(n < cx->fp->fun->nargs);
17412:     return cx->fp->argv[n];
17412: }
17412: 
22652: JS_REQUIRES_STACK jsval&
17412: TraceRecorder::varval(unsigned n) const
17412: {
18137:     JS_ASSERT(n < cx->fp->script->nslots);
17807:     return cx->fp->slots[n];
17412: }
17412: 
22652: JS_REQUIRES_STACK jsval&
17412: TraceRecorder::stackval(int n) const
17412: {
17520:     jsval* sp = cx->fp->regs->sp;
17520:     return sp[n];
17412: }
17412: 
22652: JS_REQUIRES_STACK LIns*
18286: TraceRecorder::scopeChain() const
18286: {
18286:     return lir->insLoad(LIR_ldp,
18286:                         lir->insLoad(LIR_ldp, cx_ins, offsetof(JSContext, fp)),
18286:                         offsetof(JSStackFrame, scopeChain));
18286: }
18286: 
30248: /*
30248:  * Return the frame of a call object if that frame is part of the current trace. |depthp| is an
30248:  * optional outparam: if it is non-null, it will be filled in with the depth of the call object's
30248:  * frame relevant to cx->fp.
30248:  */
30248: JS_REQUIRES_STACK JSStackFrame*
30248: TraceRecorder::frameIfInRange(JSObject* obj, unsigned* depthp) const
30248: {
30248:     JSStackFrame* ofp = (JSStackFrame*) JS_GetPrivate(cx, obj);
30248:     JSStackFrame* fp = cx->fp;
30248:     for (unsigned depth = 0; depth <= callDepth; ++depth) {
30248:         if (fp == ofp) {
30248:             if (depthp)
30248:                 *depthp = depth;
30248:             return ofp;
30248:         }
18286:         if (!(fp = fp->down))
30248:             break;
30248:     }
30248:     return NULL;
18286: }
18286: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18286: TraceRecorder::activeCallOrGlobalSlot(JSObject* obj, jsval*& vp)
18286: {
23096:     // Lookup a name in the scope chain, arriving at a property either in the
23096:     // global object or some call object's fp->slots, and import that property
23096:     // into the trace's native stack frame. This could theoretically do *lookup*
23096:     // through the property cache, but there is little performance to be gained
23096:     // by doing so since at trace-execution time the underlying object (call
23096:     // object or global object) will not be consulted at all: the jsval*
23096:     // returned from this function will map (in the tracker) to a LIns* directly
23096:     // defining a slot in the trace's native stack.
23096: 
18286:     JS_ASSERT(obj != globalObj);
18286: 
18286:     JSAtom* atom = atoms[GET_INDEX(cx->fp->regs->pc)];
18286:     JSObject* obj2;
18286:     JSProperty* prop;
27933:     if (!js_FindProperty(cx, ATOM_TO_JSID(atom), &obj, &obj2, &prop))
27933:         ABORT_TRACE_ERROR("error in js_FindProperty");
27933:     if (!prop)
18286:         ABORT_TRACE("failed to find name in non-global scope chain");
18286: 
18286:     if (obj == globalObj) {
18286:         JSScopeProperty* sprop = (JSScopeProperty*) prop;
23096: 
25938:         if (obj2 != obj) {
18286:             OBJ_DROP_PROPERTY(cx, obj2, prop);
25938:             ABORT_TRACE("prototype property");
25938:         }
25938:         if (!isValidSlot(OBJ_SCOPE(obj), sprop)) {
25938:             OBJ_DROP_PROPERTY(cx, obj2, prop);
27933:             return JSRS_STOP;
25938:         }
25938:         if (!lazilyImportGlobalSlot(sprop->slot)) {
25938:             OBJ_DROP_PROPERTY(cx, obj2, prop);
18286:             ABORT_TRACE("lazy import of global slot failed");
25938:         }
18286:         vp = &STOBJ_GET_SLOT(obj, sprop->slot);
18286:         OBJ_DROP_PROPERTY(cx, obj2, prop);
27933:         return JSRS_CONTINUE;
18286:     }
18286: 
22618:     if (wasDeepAborted())
22618:         ABORT_TRACE("deep abort from property lookup");
22618: 
18286:     if (obj == obj2 && OBJ_GET_CLASS(cx, obj) == &js_CallClass) {
30248:         JSStackFrame* cfp = frameIfInRange(obj);
30248:         if (cfp) {
18286:             JSScopeProperty* sprop = (JSScopeProperty*) prop;
29359: 
29359:             uint32 setflags = (js_CodeSpec[*cx->fp->regs->pc].format & (JOF_SET | JOF_INCDEC | JOF_FOR));
29359:             if (setflags && (sprop->attrs & JSPROP_READONLY))
29359:                 ABORT_TRACE("writing to a read-only property");
29359: 
18286:             uintN slot = sprop->shortid;
18286: 
18426:             vp = NULL;
18286:             if (sprop->getter == js_GetCallArg) {
18286:                 JS_ASSERT(slot < cfp->fun->nargs);
18286:                 vp = &cfp->argv[slot];
18426:             } else if (sprop->getter == js_GetCallVar) {
18286:                 JS_ASSERT(slot < cfp->script->nslots);
18286:                 vp = &cfp->slots[slot];
18286:             }
18286:             OBJ_DROP_PROPERTY(cx, obj2, prop);
18426:             if (!vp)
18426:                 ABORT_TRACE("dynamic property of Call object");
27933:             return JSRS_CONTINUE;
18286:         }
18286:     }
18286: 
18286:     OBJ_DROP_PROPERTY(cx, obj2, prop);
18286:     ABORT_TRACE("fp->scopeChain is not global or active call object");
18286: }
18286: 
22652: JS_REQUIRES_STACK LIns*
17412: TraceRecorder::arg(unsigned n)
17412: {
17412:     return get(&argval(n));
17412: }
17412: 
22652: JS_REQUIRES_STACK void
17415: TraceRecorder::arg(unsigned n, LIns* i)
17415: {
17415:     set(&argval(n), i);
17415: }
17415: 
22652: JS_REQUIRES_STACK LIns*
17412: TraceRecorder::var(unsigned n)
17412: {
17412:     return get(&varval(n));
17412: }
17412: 
22652: JS_REQUIRES_STACK void
17415: TraceRecorder::var(unsigned n, LIns* i)
17415: {
17415:     set(&varval(n), i);
17415: }
17415: 
22652: JS_REQUIRES_STACK LIns*
17412: TraceRecorder::stack(int n)
17412: {
17412:     return get(&stackval(n));
17412: }
17412: 
22652: JS_REQUIRES_STACK void
17412: TraceRecorder::stack(int n, LIns* i)
17412: {
17788:     set(&stackval(n), i, n >= 0);
17412: }
17412: 
29354: extern jsdouble FASTCALL js_dmod(jsdouble a, jsdouble b);
29354: 
23456: JS_REQUIRES_STACK LIns*
21799: TraceRecorder::alu(LOpcode v, jsdouble v0, jsdouble v1, LIns* s0, LIns* s1)
21799: {
29354:     /*
29354:      * To even consider this operation for demotion, both operands have to be
29354:      * integers and the oracle must not give us a negative hint for the
29354:      * instruction.
29354:      */
29369:     if (oracle.isInstructionUndemotable(cx->fp->regs->pc) || !isPromoteInt(s0) || !isPromoteInt(s1)) {
29354:     out:
29354:         if (v == LIR_fmod) {
29354:             LIns* args[] = { s1, s0 };
29354:             return lir->insCall(&js_dmod_ci, args);
29354:         }
29354:         LIns* result = lir->ins2(v, s0, s1);
29354:         JS_ASSERT_IF(s0->isconstq() && s1->isconstq(), result->isconstq());
29354:         return result;
29354:     }
29354: 
21799:     jsdouble r;
29354:     switch (v) {
29354:     case LIR_fadd:
21799:         r = v0 + v1;
29354:         break;
29354:     case LIR_fsub:
21799:         r = v0 - v1;
29354:         break;
29354:     case LIR_fmul:
29354:         r = v0 * v1;
29373:         if (r == 0.0)
29373:             goto out;
29354:         break;
29354: #ifdef NANOJIT_IA32
29354:     case LIR_fdiv:
29354:         if (v1 == 0)
29354:             goto out;
29354:         r = v0 / v1;
29354:         break;
29354:     case LIR_fmod:
30260:         if (v0 < 0 || v1 == 0 || (s1->isconstq() && v1 < 0))
29354:             goto out;
29354:         r = js_dmod(v0, v1);
29354:         break;
29354: #endif
29354:     default:
29354:         goto out;
29354:     }
29354: 
29354:     /*
29354:      * The result must be an integer at record time, otherwise there is no
29354:      * point in trying to demote it.
29354:      */
29354:     if (jsint(r) != r || JSDOUBLE_IS_NEGZERO(r))
29354:         goto out;
29354: 
21799:     LIns* d0 = ::demote(lir, s0);
21799:     LIns* d1 = ::demote(lir, s1);
29354: 
29354:     /*
29354:      * Speculatively emit an integer operation, betting that at runtime we
29354:      * will get integer results again.
29354:      */
29354:     VMSideExit* exit;
29354:     LIns* result;
29354:     switch (v) {
29354: #ifdef NANOJIT_IA32
29354:       case LIR_fdiv:
21799:         if (d0->isconst() && d1->isconst())
21799:             return lir->ins1(LIR_i2f, lir->insImm(jsint(r)));
29354: 
29354:         exit = snapshot(OVERFLOW_EXIT);
29354: 
29354:         /*
29354:          * Make sure we don't trigger division by zero at runtime.
29354:          */
29354:         if (!d1->isconst())
29354:             guard(false, lir->ins_eq0(d1), exit);
29354:         result = lir->ins2(v = LIR_div, d0, d1);
29354: 
29354:         /*
29354:          * As long the modulus is zero, the result is an integer.
29354:          */
29354:         guard(true, lir->ins_eq0(lir->ins1(LIR_mod, result)), exit);
30260:         /* Don't lose a -0 */
30260:         guard(false, lir->ins_eq0(result), exit);
29354:         break;
29354:       case LIR_fmod: {
29354:         if (d0->isconst() && d1->isconst())
29354:             return lir->ins1(LIR_i2f, lir->insImm(jsint(r)));
29354: 
29354:         exit = snapshot(OVERFLOW_EXIT);
29354: 
29354:         /*
29354:          * Make sure we don't trigger division by zero at runtime.
29354:          */
29354:         if (!d1->isconst())
29354:             guard(false, lir->ins_eq0(d1), exit);
29354:         result = lir->ins1(v = LIR_mod, lir->ins2(LIR_div, d0, d1));
29354: 
29354:         /*
29354:          * If the result is not 0, it is always within the integer domain.
29354:          */
29354:         LIns* branch = lir->insBranch(LIR_jf, lir->ins_eq0(result), NULL);
29354: 
29354:         /*
29354:          * If the result is zero, we must exit if the lhs is negative since
29354:          * the result is -0 in this case, which is not in the integer domain.
29354:          */
30260:         guard(false, lir->ins2i(LIR_lt, d1, 0), exit);
29354:         branch->setTarget(lir->ins0(LIR_label));
29354:         break;
29354:       }
29354: #endif
29354:       default:
21799:         v = (LOpcode)((int)v & ~LIR64);
29354:         result = lir->ins2(v, d0, d1);
29354: 
29354:         /*
29354:          * If the operands guarantee that the result will be an integer (i.e.
29354:          * z = x + y with 0 <= (x|y) <= 0xffff guarantees z <= fffe0001), we
29354:          * don't have to guard against an overflow. Otherwise we emit a guard
29354:          * that will inform the oracle and cause a non-demoted trace to be
29354:          * attached that uses floating-point math for this operation.
29354:          */
29373:         if (!result->isconst() && (!overflowSafe(v, d0) || !overflowSafe(v, d1))) {
29373:             exit = snapshot(OVERFLOW_EXIT);
29373:             guard(false, lir->ins1(LIR_ov, result), exit);
29373:             if (v == LIR_mul) // make sure we don't lose a -0
29373:                 guard(false, lir->ins_eq0(result), exit);
29373:         }
29354:         break;
29354:     }
29354:     JS_ASSERT_IF(d0->isconst() && d1->isconst(),
29354:                  result->isconst() && result->imm32() == jsint(r));
21799:     return lir->ins1(LIR_i2f, result);
21799: }
21799: 
21799: LIns*
21799: TraceRecorder::f2i(LIns* f)
17469: {
20915:     return lir->insCall(&js_DoubleToInt32_ci, &f);
17469: }
17469: 
22652: JS_REQUIRES_STACK LIns*
21799: TraceRecorder::makeNumberInt32(LIns* f)
19979: {
19979:     JS_ASSERT(f->isQuad());
19979:     LIns* x;
19979:     if (!isPromote(f)) {
19979:         x = f2i(f);
19979:         guard(true, lir->ins2(LIR_feq, f, lir->ins1(LIR_i2f, x)), MISMATCH_EXIT);
19979:     } else {
19979:         x = ::demote(lir, f);
19979:     }
19979:     return x;
19979: }
19979: 
23456: JS_REQUIRES_STACK LIns*
21685: TraceRecorder::stringify(jsval& v)
21685: {
21685:     LIns* v_ins = get(&v);
21447:     if (JSVAL_IS_STRING(v))
21447:         return v_ins;
21447: 
21447:     LIns* args[] = { v_ins, cx_ins };
21447:     const CallInfo* ci;
21447:     if (JSVAL_IS_NUMBER(v)) {
21447:         ci = &js_NumberToString_ci;
21447:     } else if (JSVAL_TAG(v) == JSVAL_BOOLEAN) {
21447:         ci = &js_BooleanOrUndefinedToString_ci;
21447:     } else {
26036:         /*
26036:          * Callers must deal with non-primitive (non-null object) values by
26036:          * calling an imacro. We don't try to guess about which imacro, with
26036:          * what valueOf hint, here.
26036:          */
26036:         JS_ASSERT(JSVAL_IS_NULL(v));
26078:         return INS_CONSTPTR(ATOM_TO_STRING(cx->runtime->atomState.nullAtom));
26036:     }
26036: 
21447:     v_ins = lir->insCall(ci, args);
21447:     guard(false, lir->ins_eq0(v_ins), OOM_EXIT);
21447:     return v_ins;
21447: }
21447: 
27933: JS_REQUIRES_STACK JSRecordingStatus
21685: TraceRecorder::call_imacro(jsbytecode* imacro)
21685: {
21685:     JSStackFrame* fp = cx->fp;
21685:     JSFrameRegs* regs = fp->regs;
21685: 
27933:     // We can't nest imacros.
27933:     if (fp->imacpc)
27933:         return JSRS_STOP;
27933: 
21685:     fp->imacpc = regs->pc;
21685:     regs->pc = imacro;
21685:     atoms = COMMON_ATOMS_START(&cx->runtime->atomState);
27933:     return JSRS_IMACRO;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
20416: TraceRecorder::ifop()
17452: {
17452:     jsval& v = stackval(-1);
19604:     LIns* v_ins = get(&v);
20410:     bool cond;
20410:     LIns* x;
23075: 
26753:     if (JSVAL_IS_NULL(v)) {
26753:         cond = false;
26753:         x = lir->insImm(0);
26753:     } else if (!JSVAL_IS_PRIMITIVE(v)) {
26557:         cond = true;
26557:         x = lir->insImm(1);
26557:     } else if (JSVAL_TAG(v) == JSVAL_BOOLEAN) {
23075:         /* Test for boolean is true, negate later if we are testing for false. */
24846:         cond = JSVAL_TO_PSEUDO_BOOLEAN(v) == JS_TRUE;
20410:         x = lir->ins2i(LIR_eq, v_ins, 1);
17749:     } else if (isNumber(v)) {
17749:         jsdouble d = asNumber(v);
20411:         cond = !JSDOUBLE_IS_NaN(d) && d;
20410:         x = lir->ins2(LIR_and,
20410:                       lir->ins2(LIR_feq, v_ins, v_ins),
26265:                       lir->ins_eq0(lir->ins2(LIR_feq, v_ins, lir->insImmq(0))));
17749:     } else if (JSVAL_IS_STRING(v)) {
29366:         cond = JSVAL_TO_STRING(v)->length() != 0;
20411:         x = lir->ins2(LIR_piand,
18254:                       lir->insLoad(LIR_ldp,
19604:                                    v_ins,
29366:                                    (int)offsetof(JSString, mLength)),
29366:                       INS_CONSTWORD(JSString::LENGTH_MASK));
17452:     } else {
17927:         JS_NOT_REACHED("ifop");
27933:         return JSRS_STOP;
20410:     }
26557: 
26557:     jsbytecode* pc = cx->fp->regs->pc;
26557:     emitIf(pc, cond, x);
26557:     return checkTraceEnd(pc);
17452: }
17452: 
25099: #ifdef NANOJIT_IA32
26011: /* Record LIR for a tableswitch or tableswitchx op. We record LIR only the
26011:    "first" time we hit the op. Later, when we start traces after exiting that
26011:    trace, we just patch. */
25099: JS_REQUIRES_STACK LIns*
25099: TraceRecorder::tableswitch()
25099: {
25099:     jsval& v = stackval(-1);
25107:     if (!isNumber(v))
25107:         return NULL;
25107: 
25099:     /* no need to guard if condition is constant */
25107:     LIns* v_ins = f2i(get(&v));
25099:     if (v_ins->isconst() || v_ins->isconstq())
25099:         return NULL;
25099: 
25099:     jsbytecode* pc = cx->fp->regs->pc;
25099:     /* Starting a new trace after exiting a trace via switch. */
26011:     if (anchor &&
26011:         (anchor->exitType == CASE_EXIT || anchor->exitType == DEFAULT_EXIT) &&
26011:         fragment->ip == pc) {
25099:         return NULL;
26011:     }
25099: 
25099:     /* Decode jsop. */
25099:     jsint low, high;
25099:     if (*pc == JSOP_TABLESWITCH) {
25099:         pc += JUMP_OFFSET_LEN;
25099:         low = GET_JUMP_OFFSET(pc);
25099:         pc += JUMP_OFFSET_LEN;
25099:         high = GET_JUMP_OFFSET(pc);
25099:     } else {
25099:         pc += JUMPX_OFFSET_LEN;
25099:         low = GET_JUMPX_OFFSET(pc);
25099:         pc += JUMPX_OFFSET_LEN;
25099:         high = GET_JUMPX_OFFSET(pc);
25099:     }
25099: 
26011:     /* Really large tables won't fit in a page. This is a conservative check.
26011:        If it matters in practice we need to go off-page. */
25099:     if ((high + 1 - low) * sizeof(intptr_t*) + 128 > (unsigned) LARGEST_UNDERRUN_PROT) {
25099:         // This throws away the return value of switchop but it seems
25099:         // ok because switchop always returns true.
25099:         (void) switchop();
25099:         return NULL;
25099:     }
25099: 
25099:     /* Generate switch LIR. */
28182:     LIns* si_ins = lir_buf_writer->insSkip(sizeof(SwitchInfo));
25099:     SwitchInfo* si = (SwitchInfo*) si_ins->payload();
25099:     si->count = high + 1 - low;
25099:     si->table = 0;
25099:     si->index = (uint32) -1;
25107:     LIns* diff = lir->ins2(LIR_sub, v_ins, lir->insImm(low));
25099:     LIns* cmp = lir->ins2(LIR_ult, diff, lir->insImm(si->count));
27540:     lir->insGuard(LIR_xf, cmp, createGuardRecord(snapshot(DEFAULT_EXIT)));
28182:     lir->insStorei(diff, lir->insImmPtr(&si->index), 0);
27540:     VMSideExit* exit = snapshot(CASE_EXIT);
27540:     exit->switchInfo = si;
27540:     return lir->insGuard(LIR_xtbl, diff, createGuardRecord(exit));
25099: }
25099: #endif
25099: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18687: TraceRecorder::switchop()
18687: {
18687:     jsval& v = stackval(-1);
19604:     LIns* v_ins = get(&v);
19604:     /* no need to guard if condition is constant */
19604:     if (v_ins->isconst() || v_ins->isconstq())
27933:         return JSRS_CONTINUE;
18687:     if (isNumber(v)) {
18687:         jsdouble d = asNumber(v);
18687:         guard(true,
26265:               addName(lir->ins2(LIR_feq, v_ins, lir->insImmf(d)),
18687:                       "guard(switch on numeric)"),
18687:               BRANCH_EXIT);
18687:     } else if (JSVAL_IS_STRING(v)) {
19604:         LIns* args[] = { v_ins, INS_CONSTPTR(JSVAL_TO_STRING(v)) };
18687:         guard(true,
20915:               addName(lir->ins_eq0(lir->ins_eq0(lir->insCall(&js_EqualStrings_ci, args))),
18687:                       "guard(switch on string)"),
18687:               BRANCH_EXIT);
19995:     } else if (JSVAL_TAG(v) == JSVAL_BOOLEAN) {
18687:         guard(true,
24867:               addName(lir->ins2(LIR_eq, v_ins, lir->insImm(JSVAL_TO_PUBLIC_PSEUDO_BOOLEAN(v))),
18687:                       "guard(switch on boolean)"),
18687:               BRANCH_EXIT);
18687:     } else {
19995:         ABORT_TRACE("switch on object or null");
18687:     }
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17412: TraceRecorder::inc(jsval& v, jsint incr, bool pre)
17412: {
17782:     LIns* v_ins = get(&v);
27933:     CHECK_STATUS(inc(v, v_ins, incr, pre));
17782:     set(&v, v_ins);
27933:     return JSRS_CONTINUE;
17782: }
17782: 
17782: /*
17782:  * On exit, v_ins is the incremented unboxed value, and the appropriate
17782:  * value (pre- or post-increment as described by pre) is stacked.
17782:  */
27933: JS_REQUIRES_STACK JSRecordingStatus
17782: TraceRecorder::inc(jsval& v, LIns*& v_ins, jsint incr, bool pre)
17758: {
17758:     if (!isNumber(v))
17782:         ABORT_TRACE("can only inc numbers");
17758: 
26265:     LIns* v_after = alu(LIR_fadd, asNumber(v), incr, v_ins, lir->insImmf(incr));
17544: 
17544:     const JSCodeSpec& cs = js_CodeSpec[*cx->fp->regs->pc];
17544:     JS_ASSERT(cs.ndefs == 1);
17782:     stack(-cs.nuses, pre ? v_after : v_ins);
17782:     v_ins = v_after;
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17758: TraceRecorder::incProp(jsint incr, bool pre)
17758: {
17758:     jsval& l = stackval(-1);
17758:     if (JSVAL_IS_PRIMITIVE(l))
17758:         ABORT_TRACE("incProp on primitive");
17758: 
17758:     JSObject* obj = JSVAL_TO_OBJECT(l);
17758:     LIns* obj_ins = get(&l);
17758: 
17761:     uint32 slot;
17758:     LIns* v_ins;
27933:     CHECK_STATUS(prop(obj, obj_ins, slot, v_ins));
17761: 
18666:     if (slot == SPROP_INVALID_SLOT)
18666:         ABORT_TRACE("incProp on invalid slot");
18666: 
17761:     jsval& v = STOBJ_GET_SLOT(obj, slot);
27933:     CHECK_STATUS(inc(v, v_ins, incr, pre));
17761: 
23708:     box_jsval(v, v_ins);
17761: 
17761:     LIns* dslots_ins = NULL;
17782:     stobj_set_slot(obj_ins, slot, dslots_ins, v_ins);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17758: TraceRecorder::incElem(jsint incr, bool pre)
17758: {
17758:     jsval& r = stackval(-1);
17758:     jsval& l = stackval(-2);
17758:     jsval* vp;
17758:     LIns* v_ins;
17782:     LIns* addr_ins;
28411: 
28411:     if (!JSVAL_IS_OBJECT(l) || !JSVAL_IS_INT(r) ||
28411:         !guardDenseArray(JSVAL_TO_OBJECT(l), get(&l))) {
28411:         return JSRS_STOP;
28411:     }
28411: 
28411:     CHECK_STATUS(denseArrayElement(l, r, vp, v_ins, addr_ins));
20972:     if (!addr_ins) // if we read a hole, abort
27933:         return JSRS_STOP;
27933:     CHECK_STATUS(inc(*vp, v_ins, incr, pre));
23708:     box_jsval(*vp, v_ins);
17782:     lir->insStorei(v_ins, addr_ins, 0);
27933:     return JSRS_CONTINUE;
17412: }
17412: 
19576: static bool
19576: evalCmp(LOpcode op, double result)
19576: {
18017:     bool cond;
18017:     switch (op) {
19576:       case LIR_feq:
19576:         cond = (result == 0);
19576:         break;
18017:       case LIR_flt:
18017:         cond = result < 0;
18017:         break;
18017:       case LIR_fgt:
18017:         cond = result > 0;
18017:         break;
18017:       case LIR_fle:
18017:         cond = result <= 0;
18017:         break;
18017:       case LIR_fge:
18017:         cond = result >= 0;
18017:         break;
18017:       default:
19576:         JS_NOT_REACHED("unexpected comparison op");
19576:         return false;
19576:     }
19576:     return cond;
19576: }
19576: 
19576: static bool
19576: evalCmp(LOpcode op, double l, double r)
19576: {
19576:     return evalCmp(op, l - r);
19576: }
19576: 
19576: static bool
19576: evalCmp(LOpcode op, JSString* l, JSString* r)
19576: {
19576:     if (op == LIR_feq)
19576:         return js_EqualStrings(l, r);
19576:     return evalCmp(op, js_CompareStrings(l, r));
19576: }
19576: 
22705: JS_REQUIRES_STACK void
23093: TraceRecorder::strictEquality(bool equal, bool cmpCase)
22705: {
22705:     jsval& r = stackval(-1);
22705:     jsval& l = stackval(-2);
22705:     LIns* l_ins = get(&l);
22705:     LIns* r_ins = get(&r);
23115:     LIns* x;
23115:     bool cond;
22705: 
29896:     JSTraceType ltag = getPromotedType(l);
22705:     if (ltag != getPromotedType(r)) {
23115:         cond = !equal;
23115:         x = lir->insImm(cond);
29896:     } else if (ltag == TT_STRING) {
22705:         LIns* args[] = { r_ins, l_ins };
22705:         x = lir->ins2i(LIR_eq, lir->insCall(&js_EqualStrings_ci, args), equal);
23093:         cond = js_EqualStrings(JSVAL_TO_STRING(l), JSVAL_TO_STRING(r));
22705:     } else {
29896:         LOpcode op = (ltag != TT_DOUBLE) ? LIR_eq : LIR_feq;
22705:         x = lir->ins2(op, l_ins, r_ins);
22705:         if (!equal)
22705:             x = lir->ins_eq0(x);
29896:         cond = (ltag == TT_DOUBLE)
23118:                ? asNumber(l) == asNumber(r)
23117:                : l == r;
23093:     }
23093:     cond = (cond == equal);
23093: 
23093:     if (cmpCase) {
23093:         /* Only guard if the same path may not always be taken. */
23093:         if (!x->isconst())
23093:             guard(cond, x, BRANCH_EXIT);
23093:         return;
22705:     }
22705: 
22705:     set(&l, x);
22705: }
22705: 
27933: JS_REQUIRES_STACK JSRecordingStatus
23093: TraceRecorder::equality(bool negate, bool tryBranchAfterCond)
19576: {
23223:     jsval& rval = stackval(-1);
23223:     jsval& lval = stackval(-2);
23223:     LIns* l_ins = get(&lval);
23223:     LIns* r_ins = get(&rval);
23223: 
23223:     return equalityHelper(lval, rval, l_ins, r_ins, negate, tryBranchAfterCond, lval);
23223: }
23223: 
27933: JS_REQUIRES_STACK JSRecordingStatus
23223: TraceRecorder::equalityHelper(jsval l, jsval r, LIns* l_ins, LIns* r_ins,
23223:                               bool negate, bool tryBranchAfterCond,
23223:                               jsval& rval)
23223: {
23223:     bool fp = false;
19576:     bool cond;
23223:     LIns* args[] = { NULL, NULL };
23223: 
23223:     /*
23223:      * The if chain below closely mirrors that found in 11.9.3, in general
23223:      * deviating from that ordering of ifs only to account for SpiderMonkey's
23223:      * conflation of booleans and undefined and for the possibility of
23223:      * confusing objects and null.  Note carefully the spec-mandated recursion
23223:      * in the final else clause, which terminates because Number == T recurs
23223:      * only if T is Object, but that must recur again to convert Object to
23223:      * primitive, and ToPrimitive throws if the object cannot be converted to
23223:      * a primitive value (which would terminate recursion).
23223:      */
23223: 
23223:     if (getPromotedType(l) == getPromotedType(r)) {
23223:         if (JSVAL_TAG(l) == JSVAL_OBJECT || JSVAL_TAG(l) == JSVAL_BOOLEAN) {
23223:             cond = (l == r);
23223:         } else if (JSVAL_IS_STRING(l)) {
23223:             args[0] = r_ins, args[1] = l_ins;
23223:             l_ins = lir->insCall(&js_EqualStrings_ci, args);
23223:             r_ins = lir->insImm(1);
23223:             cond = js_EqualStrings(JSVAL_TO_STRING(l), JSVAL_TO_STRING(r));
21719:         } else {
23223:             JS_ASSERT(isNumber(l) && isNumber(r));
23223:             cond = (asNumber(l) == asNumber(r));
20392:             fp = true;
23223:         }
23223:     } else if (JSVAL_IS_NULL(l) && JSVAL_TAG(r) == JSVAL_BOOLEAN) {
24846:         l_ins = lir->insImm(JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID));
23223:         cond = (r == JSVAL_VOID);
23223:     } else if (JSVAL_TAG(l) == JSVAL_BOOLEAN && JSVAL_IS_NULL(r)) {
24846:         r_ins = lir->insImm(JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID));
23223:         cond = (l == JSVAL_VOID);
23223:     } else if (isNumber(l) && JSVAL_IS_STRING(r)) {
23223:         args[0] = r_ins, args[1] = cx_ins;
23223:         r_ins = lir->insCall(&js_StringToNumber_ci, args);
23223:         cond = (asNumber(l) == js_StringToNumber(cx, JSVAL_TO_STRING(r)));
23223:         fp = true;
23223:     } else if (JSVAL_IS_STRING(l) && isNumber(r)) {
23223:         args[0] = l_ins, args[1] = cx_ins;
20915:         l_ins = lir->insCall(&js_StringToNumber_ci, args);
23223:         cond = (js_StringToNumber(cx, JSVAL_TO_STRING(l)) == asNumber(r));
23223:         fp = true;
23223:     } else {
23223:         if (JSVAL_TAG(l) == JSVAL_BOOLEAN) {
25478:             bool isVoid = JSVAL_IS_VOID(l);
25478:             guard(isVoid,
25478:                   lir->ins2(LIR_eq, l_ins, INS_CONST(JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID))),
25478:                   BRANCH_EXIT);
25478:             if (!isVoid) {
23223:                 args[0] = l_ins, args[1] = cx_ins;
21447:                 l_ins = lir->insCall(&js_BooleanOrUndefinedToNumber_ci, args);
23223:                 l = (l == JSVAL_VOID)
24315:                     ? DOUBLE_TO_JSVAL(cx->runtime->jsNaN)
23223:                     : INT_TO_JSVAL(l == JSVAL_TRUE);
23223:                 return equalityHelper(l, r, l_ins, r_ins, negate,
23223:                                       tryBranchAfterCond, rval);
23223:             }
25478:         } else if (JSVAL_TAG(r) == JSVAL_BOOLEAN) {
25478:             bool isVoid = JSVAL_IS_VOID(r);
25478:             guard(isVoid,
25478:                   lir->ins2(LIR_eq, r_ins, INS_CONST(JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID))),
25478:                   BRANCH_EXIT);
25478:             if (!isVoid) {
23223:                 args[0] = r_ins, args[1] = cx_ins;
21447:                 r_ins = lir->insCall(&js_BooleanOrUndefinedToNumber_ci, args);
23223:                 r = (r == JSVAL_VOID)
24315:                     ? DOUBLE_TO_JSVAL(cx->runtime->jsNaN)
23223:                     : INT_TO_JSVAL(r == JSVAL_TRUE);
23223:                 return equalityHelper(l, r, l_ins, r_ins, negate,
23223:                                       tryBranchAfterCond, rval);
23223:             }
25478:         } else {
28175:             if ((JSVAL_IS_STRING(l) || isNumber(l)) && !JSVAL_IS_PRIMITIVE(r)) {
28175:                 ABORT_IF_XML(r);
23223:                 return call_imacro(equality_imacros.any_obj);
28175:             }
28175:             if (!JSVAL_IS_PRIMITIVE(l) && (JSVAL_IS_STRING(r) || isNumber(r))) {
28175:                 ABORT_IF_XML(l);
23223:                 return call_imacro(equality_imacros.obj_any);
25478:             }
28175:         }
23223: 
23223:         l_ins = lir->insImm(0);
23223:         r_ins = lir->insImm(1);
23223:         cond = false;
23223:     }
23223: 
23223:     /* If the operands aren't numbers, compare them as integers. */
22705:     LOpcode op = fp ? LIR_feq : LIR_eq;
23223:     LIns* x = lir->ins2(op, l_ins, r_ins);
22705:     if (negate) {
22705:         x = lir->ins_eq0(x);
22705:         cond = !cond;
22705:     }
22705: 
26557:     jsbytecode* pc = cx->fp->regs->pc;
26557: 
22705:     /*
26118:      * Don't guard if the same path is always taken.  If it isn't, we have to
26118:      * fuse comparisons and the following branch, because the interpreter does
26118:      * that.
22705:      */
26557:     if (tryBranchAfterCond)
26557:         fuseIf(pc + 1, cond, x);
26557: 
26557:     /*
26557:      * There is no need to write out the result of this comparison if the trace
26557:      * ends on this operation.
26557:      */
27933:     if (pc[1] == JSOP_IFNE || pc[1] == JSOP_IFEQ)
27933:         CHECK_STATUS(checkTraceEnd(pc + 1));
22705: 
22705:     /*
22705:      * We update the stack after the guard. This is safe since the guard bails
22705:      * out at the comparison and the interpreter will therefore re-execute the
22705:      * comparison. This way the value of the condition doesn't have to be
22705:      * calculated and saved on the stack in most cases.
22705:      */
23223:     set(&rval, x);
26557: 
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
23093: TraceRecorder::relational(LOpcode op, bool tryBranchAfterCond)
22705: {
22705:     jsval& r = stackval(-1);
22705:     jsval& l = stackval(-2);
22705:     LIns* x = NULL;
22705:     bool cond;
22705:     LIns* l_ins = get(&l);
22705:     LIns* r_ins = get(&r);
22705:     bool fp = false;
22705:     jsdouble lnum, rnum;
22705: 
22705:     /*
22705:      * 11.8.5 if either argument is an object with a function-valued valueOf
22705:      * property; if both arguments are objects with non-function-valued valueOf
22705:      * properties, abort.
22705:      */
23075:     if (!JSVAL_IS_PRIMITIVE(l)) {
28175:         ABORT_IF_XML(l);
28175:         if (!JSVAL_IS_PRIMITIVE(r)) {
28175:             ABORT_IF_XML(r);
22705:             return call_imacro(binary_imacros.obj_obj);
28175:         }
22705:         return call_imacro(binary_imacros.obj_any);
22705:     }
28175:     if (!JSVAL_IS_PRIMITIVE(r)) {
28175:         ABORT_IF_XML(r);
22705:         return call_imacro(binary_imacros.any_obj);
28175:     }
22705: 
22705:     /* 11.8.5 steps 3, 16-21. */
22705:     if (JSVAL_IS_STRING(l) && JSVAL_IS_STRING(r)) {
22705:         LIns* args[] = { r_ins, l_ins };
22705:         l_ins = lir->insCall(&js_CompareStrings_ci, args);
22705:         r_ins = lir->insImm(0);
22705:         cond = evalCmp(op, JSVAL_TO_STRING(l), JSVAL_TO_STRING(r));
22705:         goto do_comparison;
22705:     }
22705: 
22705:     /* 11.8.5 steps 4-5. */
22705:     if (!JSVAL_IS_NUMBER(l)) {
22705:         LIns* args[] = { l_ins, cx_ins };
22705:         switch (JSVAL_TAG(l)) {
22705:           case JSVAL_BOOLEAN:
22705:             l_ins = lir->insCall(&js_BooleanOrUndefinedToNumber_ci, args);
22705:             break;
22705:           case JSVAL_STRING:
22705:             l_ins = lir->insCall(&js_StringToNumber_ci, args);
22705:             break;
23075:           case JSVAL_OBJECT:
23075:             if (JSVAL_IS_NULL(l)) {
29354:                 l_ins = lir->insImmf(0.0);
23075:                 break;
23075:             }
23075:             // FALL THROUGH
22705:           case JSVAL_INT:
22705:           case JSVAL_DOUBLE:
22705:           default:
22705:             JS_NOT_REACHED("JSVAL_IS_NUMBER if int/double, objects should "
22705:                            "have been handled at start of method");
22705:             ABORT_TRACE("safety belt");
22705:         }
22705:     }
22705:     if (!JSVAL_IS_NUMBER(r)) {
22705:         LIns* args[] = { r_ins, cx_ins };
22705:         switch (JSVAL_TAG(r)) {
22705:           case JSVAL_BOOLEAN:
22705:             r_ins = lir->insCall(&js_BooleanOrUndefinedToNumber_ci, args);
22705:             break;
22705:           case JSVAL_STRING:
22705:             r_ins = lir->insCall(&js_StringToNumber_ci, args);
22705:             break;
23075:           case JSVAL_OBJECT:
23075:             if (JSVAL_IS_NULL(r)) {
29354:                 r_ins = lir->insImmf(0.0);
23075:                 break;
23075:             }
23075:             // FALL THROUGH
22705:           case JSVAL_INT:
22705:           case JSVAL_DOUBLE:
22705:           default:
22705:             JS_NOT_REACHED("JSVAL_IS_NUMBER if int/double, objects should "
22705:                            "have been handled at start of method");
22705:             ABORT_TRACE("safety belt");
22705:         }
22705:     }
22705:     {
22705:         jsval tmp = JSVAL_NULL;
22705:         JSAutoTempValueRooter tvr(cx, 1, &tmp);
22705: 
22705:         tmp = l;
22705:         lnum = js_ValueToNumber(cx, &tmp);
22705:         tmp = r;
22705:         rnum = js_ValueToNumber(cx, &tmp);
22705:     }
22705:     cond = evalCmp(op, lnum, rnum);
22705:     fp = true;
22705: 
22705:     /* 11.8.5 steps 6-15. */
22705:   do_comparison:
22705:     /* If the result is not a number or it's not a quad, we must use an integer compare. */
22651:     if (!fp) {
22651:         JS_ASSERT(op >= LIR_feq && op <= LIR_fge);
22651:         op = LOpcode(op + (LIR_eq - LIR_feq));
22651:     }
22650:     x = lir->ins2(op, l_ins, r_ins);
22705: 
26557:     jsbytecode* pc = cx->fp->regs->pc;
26557: 
22705:     /*
26118:      * Don't guard if the same path is always taken.  If it isn't, we have to
26118:      * fuse comparisons and the following branch, because the interpreter does
26118:      * that.
22705:      */
26557:     if (tryBranchAfterCond)
26557:         fuseIf(pc + 1, cond, x);
26557: 
26557:     /*
26557:      * There is no need to write out the result of this comparison if the trace
26557:      * ends on this operation.
26557:      */
27933:     if (pc[1] == JSOP_IFNE || pc[1] == JSOP_IFEQ)
27933:         CHECK_STATUS(checkTraceEnd(pc + 1));
22705: 
22705:     /*
22705:      * We update the stack after the guard. This is safe since the guard bails
22705:      * out at the comparison and the interpreter will therefore re-execute the
22705:      * comparison. This way the value of the condition doesn't have to be
22705:      * calculated and saved on the stack in most cases.
22705:      */
17413:     set(&l, x);
26557: 
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17467: TraceRecorder::unary(LOpcode op)
17467: {
17467:     jsval& v = stackval(-1);
17467:     bool intop = !(op & LIR64);
17467:     if (isNumber(v)) {
17467:         LIns* a = get(&v);
17467:         if (intop)
17469:             a = f2i(a);
17467:         a = lir->ins1(op, a);
17467:         if (intop)
17467:             a = lir->ins1(LIR_i2f, a);
17467:         set(&v, a);
27933:         return JSRS_CONTINUE;
27933:     }
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17467: TraceRecorder::binary(LOpcode op)
17467: {
17467:     jsval& r = stackval(-1);
17467:     jsval& l = stackval(-2);
21685: 
23075:     if (!JSVAL_IS_PRIMITIVE(l)) {
28175:         ABORT_IF_XML(l);
28175:         if (!JSVAL_IS_PRIMITIVE(r)) {
28175:             ABORT_IF_XML(r);
21685:             return call_imacro(binary_imacros.obj_obj);
28175:         }
21685:         return call_imacro(binary_imacros.obj_any);
21685:     }
28175:     if (!JSVAL_IS_PRIMITIVE(r)) {
28175:         ABORT_IF_XML(r);
21685:         return call_imacro(binary_imacros.any_obj);
28175:     }
21685: 
17467:     bool intop = !(op & LIR64);
17467:     LIns* a = get(&l);
17467:     LIns* b = get(&r);
21799: 
21799:     bool leftIsNumber = isNumber(l);
21799:     jsdouble lnum = leftIsNumber ? asNumber(l) : 0;
21799: 
21799:     bool rightIsNumber = isNumber(r);
21799:     jsdouble rnum = rightIsNumber ? asNumber(r) : 0;
21799: 
17910:     if ((op >= LIR_sub && op <= LIR_ush) ||  // sub, mul, (callh), or, xor, (not,) lsh, rsh, ush
29354:         (op >= LIR_fsub && op <= LIR_fmod)) { // fsub, fmul, fdiv, fmod
18693:         LIns* args[2];
17910:         if (JSVAL_IS_STRING(l)) {
17910:             args[0] = a;
18693:             args[1] = cx_ins;
20915:             a = lir->insCall(&js_StringToNumber_ci, args);
21799:             lnum = js_StringToNumber(cx, JSVAL_TO_STRING(l));
21799:             leftIsNumber = true;
17910:         }
17910:         if (JSVAL_IS_STRING(r)) {
17910:             args[0] = b;
18693:             args[1] = cx_ins;
20915:             b = lir->insCall(&js_StringToNumber_ci, args);
21799:             rnum = js_StringToNumber(cx, JSVAL_TO_STRING(r));
21799:             rightIsNumber = true;
17910:         }
17910:     }
21438:     if (JSVAL_TAG(l) == JSVAL_BOOLEAN) {
21438:         LIns* args[] = { a, cx_ins };
21447:         a = lir->insCall(&js_BooleanOrUndefinedToNumber_ci, args);
24846:         lnum = js_BooleanOrUndefinedToNumber(cx, JSVAL_TO_PSEUDO_BOOLEAN(l));
21799:         leftIsNumber = true;
20972:     }
21438:     if (JSVAL_TAG(r) == JSVAL_BOOLEAN) {
21438:         LIns* args[] = { b, cx_ins };
21447:         b = lir->insCall(&js_BooleanOrUndefinedToNumber_ci, args);
24846:         rnum = js_BooleanOrUndefinedToNumber(cx, JSVAL_TO_PSEUDO_BOOLEAN(r));
21799:         rightIsNumber = true;
21799:     }
21799:     if (leftIsNumber && rightIsNumber) {
17467:         if (intop) {
18693:             LIns *args[] = { a };
20915:             a = lir->insCall(op == LIR_ush ? &js_DoubleToUint32_ci : &js_DoubleToInt32_ci, args);
17469:             b = f2i(b);
17467:         }
21799:         a = alu(op, lnum, rnum, a, b);
17467:         if (intop)
17467:             a = lir->ins1(op == LIR_ush ? LIR_u2f : LIR_i2f, a);
17467:         set(&l, a);
27933:         return JSRS_CONTINUE;
27933:     }
27933:     return JSRS_STOP;
17467: }
17467: 
28353: JS_STATIC_ASSERT(offsetof(JSObjectOps, objectMap) == 0);
18026: 
30244: inline LIns*
30244: TraceRecorder::map(LIns *obj_ins)
30244: {
30244:     return addName(lir->insLoad(LIR_ldp, obj_ins, (int) offsetof(JSObject, map)), "map");
30244: }
30244: 
18026: bool
18026: TraceRecorder::map_is_native(JSObjectMap* map, LIns* map_ins, LIns*& ops_ins, size_t op_offset)
17899: {
28353:     JS_ASSERT(op_offset < sizeof(JSObjectOps));
28353:     JS_ASSERT(op_offset % sizeof(void *) == 0);
28353: 
28353: #define OP(ops) (*(void **) ((uint8 *) (ops) + op_offset))
28353:     void* ptr = OP(map->ops);
28353:     if (ptr != OP(&js_ObjectOps))
25636:         return false;
28353: #undef OP
28353: 
28353:     ops_ins = addName(lir->insLoad(LIR_ldp, map_ins, int(offsetof(JSObjectMap, ops))), "ops");
18230:     LIns* n = lir->insLoad(LIR_ldp, ops_ins, op_offset);
25636:     guard(true,
28353:           addName(lir->ins2(LIR_eq, n, INS_CONSTPTR(ptr)), "guard(native-map)"),
25636:           BRANCH_EXIT);
25636: 
25636:     return true;
17417: }
17417: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17746: TraceRecorder::test_property_cache(JSObject* obj, LIns* obj_ins, JSObject*& obj2, jsuword& pcval)
17459: {
19093:     jsbytecode* pc = cx->fp->regs->pc;
19093:     JS_ASSERT(*pc != JSOP_INITPROP && *pc != JSOP_SETNAME && *pc != JSOP_SETPROP);
19093: 
18439:     // Mimic the interpreter's special case for dense arrays by skipping up one
18439:     // hop along the proto chain when accessing a named (not indexed) property,
18439:     // typically to find Array.prototype methods.
18026:     JSObject* aobj = obj;
18034:     if (OBJ_IS_DENSE_ARRAY(cx, obj)) {
26754:         guardDenseArray(obj, obj_ins, BRANCH_EXIT);
18026:         aobj = OBJ_GET_PROTO(cx, obj);
18026:         obj_ins = stobj_get_fslot(obj_ins, JSSLOT_PROTO);
18026:     }
18026: 
30244:     LIns* map_ins = map(obj_ins);
17899:     LIns* ops_ins;
18026: 
18026:     // Interpreter calls to PROPERTY_CACHE_TEST guard on native object ops
28353:     // which is required to use native objects (those whose maps are scopes),
28353:     // or even more narrow conditions required because the cache miss case
28353:     // will call a particular object-op (js_GetProperty, js_SetProperty).
18026:     //
18026:     // We parameterize using offsetof and guard on match against the hook at
18026:     // the given offset in js_ObjectOps. TraceRecorder::record_JSOP_SETPROP
18026:     // guards the js_SetProperty case.
19093:     uint32 format = js_CodeSpec[*pc].format;
18026:     uint32 mode = JOF_MODE(format);
19090: 
19090:     // No need to guard native-ness of global object.
19090:     JS_ASSERT(OBJ_IS_NATIVE(globalObj));
19090:     if (aobj != globalObj) {
28353:         size_t op_offset = offsetof(JSObjectOps, objectMap);
18026:         if (mode == JOF_PROP || mode == JOF_VARPROP) {
18026:             JS_ASSERT(!(format & JOF_SET));
18026:             op_offset = offsetof(JSObjectOps, getProperty);
18026:         } else {
18026:             JS_ASSERT(mode == JOF_NAME);
18026:         }
18026: 
18026:         if (!map_is_native(aobj->map, map_ins, ops_ins, op_offset))
25636:             ABORT_TRACE("non-native map");
19090:     }
17459: 
17459:     JSAtom* atom;
17746:     JSPropCacheEntry* entry;
19093:     PROPERTY_CACHE_TEST(cx, pc, aobj, obj2, entry, atom);
24876:     if (!atom) {
24876:         // Null atom means that obj2 is locked and must now be unlocked.
24876:         JS_UNLOCK_OBJ(cx, obj2);
24876:     } else {
18439:         // Miss: pre-fill the cache for the interpreter, as well as for our needs.
18439:         jsid id = ATOM_TO_JSID(atom);
17747:         JSProperty* prop;
19093:         if (JOF_OPMODE(*pc) == JOF_NAME) {
18112:             JS_ASSERT(aobj == obj);
27575:             entry = js_FindPropertyHelper(cx, id, true, &obj, &obj2, &prop);
27575: 
27933:             if (!entry)
27933:                 ABORT_TRACE_ERROR("error in js_FindPropertyHelper");
27933:             if (entry == JS_NO_PROP_CACHE_FILL)
27933:                 ABORT_TRACE("cannot cache name");
17747:         } else {
19712:             int protoIndex = js_LookupPropertyWithFlags(cx, aobj, id,
19712:                                                         cx->resolveFlags,
19712:                                                         &obj2, &prop);
27575: 
17878:             if (protoIndex < 0)
27933:                 ABORT_TRACE_ERROR("error in js_LookupPropertyWithFlags");
17878: 
17998:             if (prop) {
26973:                 if (!OBJ_IS_NATIVE(obj2)) {
26973:                     OBJ_DROP_PROPERTY(cx, obj2, prop);
26973:                     ABORT_TRACE("property found on non-native object");
26973:                 }
27930:                 entry = js_FillPropertyCache(cx, aobj, 0, protoIndex, obj2,
27930:                                              (JSScopeProperty*) prop, false);
27575:                 JS_ASSERT(entry);
27575:                 if (entry == JS_NO_PROP_CACHE_FILL)
27575:                     entry = NULL;
17998:             }
17998:         }
17998: 
17998:         if (!prop) {
17998:             // Propagate obj from js_FindPropertyHelper to record_JSOP_BINDNAME
18712:             // via our obj2 out-parameter. If we are recording JSOP_SETNAME and
18712:             // the global it's assigning does not yet exist, create it.
17998:             obj2 = obj;
19093: 
18712:             // Use PCVAL_NULL to return "no such property" to our caller.
17998:             pcval = PCVAL_NULL;
27933:             return JSRS_CONTINUE;
17998:         }
17998: 
17998:         OBJ_DROP_PROPERTY(cx, obj2, prop);
18439:         if (!entry)
17878:             ABORT_TRACE("failed to fill property cache");
17998:     }
17878: 
22618:     if (wasDeepAborted())
22618:         ABORT_TRACE("deep abort from property lookup");
22618: 
18439: #ifdef JS_THREADSAFE
18439:     // There's a potential race in any JS_THREADSAFE embedding that's nuts
18439:     // enough to share mutable objects on the scope or proto chain, but we
18439:     // don't care about such insane embeddings. Anyway, the (scope, proto)
18439:     // entry->vcap coordinates must reach obj2 from aobj at this point.
18439:     JS_ASSERT(cx->requestDepth);
18439: #endif
18439: 
29902:     // Emit guard(s), common code for both hit and miss cases.
29902:     // Check for first-level cache hit and guard on kshape if possible.
29902:     // Otherwise guard on key object exact match.
29902:     if (PCVCAP_TAG(entry->vcap) <= 1) {
18112:         if (aobj != globalObj) {
18286:             LIns* shape_ins = addName(lir->insLoad(LIR_ld, map_ins, offsetof(JSScope, shape)),
18286:                                       "shape");
29902:             guard(true, addName(lir->ins2i(LIR_eq, shape_ins, entry->kshape), "guard(kshape)(test_property_cache)"),
23721:                   BRANCH_EXIT);
18112:         }
29902:     } else {
29902: #ifdef DEBUG
29902:         JSOp op = js_GetOpcode(cx, cx->fp->script, pc);
29902:         JSAtom *pcatom;
29902:         if (op == JSOP_LENGTH) {
29902:             pcatom = cx->runtime->atomState.lengthAtom;
29902:         } else {
29902:             ptrdiff_t pcoff = (JOF_TYPE(js_CodeSpec[op].format) == JOF_SLOTATOM) ? SLOTNO_LEN : 0;
29902:             GET_ATOM_FROM_BYTECODE(cx->fp->script, pc, pcoff, pcatom);
29902:         }
29902:         JS_ASSERT(entry->kpc == (jsbytecode *) pcatom);
29902:         JS_ASSERT(entry->kshape == jsuword(aobj));
29902: #endif
29902:         if (aobj != globalObj && !obj_ins->isconstp()) {
29902:             guard(true, addName(lir->ins2i(LIR_eq, obj_ins, entry->kshape), "guard(kobj)"),
29902:                   BRANCH_EXIT);
29902:         }
29902:     }
18439: 
21685:     // For any hit that goes up the scope and/or proto chains, we will need to
18439:     // guard on the shape of the object containing the property.
18061:     if (PCVCAP_TAG(entry->vcap) >= 1) {
18439:         jsuword vcap = entry->vcap;
18439:         uint32 vshape = PCVCAP_SHAPE(vcap);
19020:         JS_ASSERT(OBJ_SHAPE(obj2) == vshape);
18439: 
20979:         LIns* obj2_ins;
20979:         if (PCVCAP_TAG(entry->vcap) == 1) {
20979:             // Duplicate the special case in PROPERTY_CACHE_TEST.
20979:             obj2_ins = stobj_get_fslot(obj_ins, JSSLOT_PROTO);
23721:             guard(false, lir->ins_eq0(obj2_ins), BRANCH_EXIT);
20979:         } else {
20979:             obj2_ins = INS_CONSTPTR(obj2);
20979:         }
30244:         map_ins = map(obj2_ins);
18439:         if (!map_is_native(obj2->map, map_ins, ops_ins))
25636:             ABORT_TRACE("non-native map");
18439: 
18439:         LIns* shape_ins = addName(lir->insLoad(LIR_ld, map_ins, offsetof(JSScope, shape)),
18439:                                   "shape");
18086:         guard(true,
28837:               addName(lir->ins2i(LIR_eq, shape_ins, vshape), "guard(vshape)(test_property_cache)"),
23721:               BRANCH_EXIT);
17878:     }
17747: 
18439:     pcval = entry->vword;
27933:     return JSRS_CONTINUE;
17459: }
17459: 
17429: void
28554: TraceRecorder::stobj_set_fslot(LIns *obj_ins, unsigned slot, LIns* v_ins, const char *name)
28554: {
28554:     addName(lir->insStorei(v_ins, obj_ins, offsetof(JSObject, fslots) + slot * sizeof(jsval)),
28554:             name);
28554: }
28554: 
28554: void
22626: TraceRecorder::stobj_set_dslot(LIns *obj_ins, unsigned slot, LIns*& dslots_ins, LIns* v_ins,
22626:                                const char *name)
22626: {
22626:     if (!dslots_ins)
22626:         dslots_ins = lir->insLoad(LIR_ldp, obj_ins, offsetof(JSObject, dslots));
22626:     addName(lir->insStorei(v_ins, dslots_ins, slot * sizeof(jsval)), name);
22626: }
22626: 
22626: void
17429: TraceRecorder::stobj_set_slot(LIns* obj_ins, unsigned slot, LIns*& dslots_ins, LIns* v_ins)
17426: {
17487:     if (slot < JS_INITIAL_NSLOTS) {
28554:         stobj_set_fslot(obj_ins, slot, v_ins, "set_slot(fslots)");
17487:     } else {
22626:         stobj_set_dslot(obj_ins, slot - JS_INITIAL_NSLOTS, dslots_ins, v_ins,
22626:                         "set_slot(dslots)");
17429:     }
17426: }
17426: 
17459: LIns*
17899: TraceRecorder::stobj_get_fslot(LIns* obj_ins, unsigned slot)
17899: {
17899:     JS_ASSERT(slot < JS_INITIAL_NSLOTS);
18230:     return lir->insLoad(LIR_ldp, obj_ins, offsetof(JSObject, fslots) + slot * sizeof(jsval));
17899: }
17899: 
17899: LIns*
27012: TraceRecorder::stobj_get_dslot(LIns* obj_ins, unsigned index, LIns*& dslots_ins)
27012: {
27012:     if (!dslots_ins)
27012:         dslots_ins = lir->insLoad(LIR_ldp, obj_ins, offsetof(JSObject, dslots));
27012:     return lir->insLoad(LIR_ldp, dslots_ins, index * sizeof(jsval));
27012: }
27012: 
27012: LIns*
26970: TraceRecorder::stobj_get_slot(LIns* obj_ins, unsigned slot, LIns*& dslots_ins)
26970: {
26970:     if (slot < JS_INITIAL_NSLOTS)
26970:         return stobj_get_fslot(obj_ins, slot);
27012:     return stobj_get_dslot(obj_ins, slot - JS_INITIAL_NSLOTS, dslots_ins);
17426: }
17426: 
27933: JSRecordingStatus
17429: TraceRecorder::native_set(LIns* obj_ins, JSScopeProperty* sprop, LIns*& dslots_ins, LIns* v_ins)
17426: {
17426:     if (SPROP_HAS_STUB_SETTER(sprop) && sprop->slot != SPROP_INVALID_SLOT) {
17429:         stobj_set_slot(obj_ins, sprop->slot, dslots_ins, v_ins);
27933:         return JSRS_CONTINUE;
17426:     }
17721:     ABORT_TRACE("unallocated or non-stub sprop");
17426: }
17426: 
27933: JSRecordingStatus
17429: TraceRecorder::native_get(LIns* obj_ins, LIns* pobj_ins, JSScopeProperty* sprop,
17429:                           LIns*& dslots_ins, LIns*& v_ins)
17426: {
17459:     if (!SPROP_HAS_STUB_GETTER(sprop))
27933:         return JSRS_STOP;
17459: 
17426:     if (sprop->slot != SPROP_INVALID_SLOT)
17459:         v_ins = stobj_get_slot(pobj_ins, sprop->slot, dslots_ins);
17426:     else
24846:         v_ins = INS_CONST(JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID));
27933:     return JSRS_CONTINUE;
24846: }
18001: 
23708: JS_REQUIRES_STACK void
17468: TraceRecorder::box_jsval(jsval v, LIns*& v_ins)
17468: {
17470:     if (isNumber(v)) {
17477:         LIns* args[] = { v_ins, cx_ins };
20915:         v_ins = lir->insCall(&js_BoxDouble_ci, args);
18680:         guard(false, lir->ins2(LIR_eq, v_ins, INS_CONST(JSVAL_ERROR_COOKIE)),
17850:               OOM_EXIT);
23708:         return;
17468:     }
17470:     switch (JSVAL_TAG(v)) {
17470:       case JSVAL_BOOLEAN:
18645:         v_ins = lir->ins2i(LIR_pior, lir->ins2i(LIR_pilsh, v_ins, JSVAL_TAGBITS), JSVAL_BOOLEAN);
23708:         return;
18001:       case JSVAL_OBJECT:
23708:         return;
23708:       default:
23708:         JS_ASSERT(JSVAL_TAG(v) == JSVAL_STRING);
18645:         v_ins = lir->ins2(LIR_pior, v_ins, INS_CONST(JSVAL_STRING));
23708:         return;
23708:     }
17470: }
17468: 
23710: JS_REQUIRES_STACK void
27540: TraceRecorder::unbox_jsval(jsval v, LIns*& v_ins, VMSideExit* exit)
17460: {
17470:     if (isNumber(v)) {
17470:         // JSVAL_IS_NUMBER(v)
17758:         guard(false,
18645:               lir->ins_eq0(lir->ins2(LIR_pior,
18648:                                      lir->ins2(LIR_piand, v_ins, INS_CONST(JSVAL_INT)),
17758:                                      lir->ins2i(LIR_eq,
18232:                                                 lir->ins2(LIR_piand, v_ins,
18648:                                                           INS_CONST(JSVAL_TAGMASK)),
18095:                                                 JSVAL_DOUBLE))),
26972:               exit);
18693:         LIns* args[] = { v_ins };
20915:         v_ins = lir->insCall(&js_UnboxDouble_ci, args);
23710:         return;
17460:     }
17470:     switch (JSVAL_TAG(v)) {
17470:       case JSVAL_BOOLEAN:
17541:         guard(true,
17541:               lir->ins2i(LIR_eq,
18648:                          lir->ins2(LIR_piand, v_ins, INS_CONST(JSVAL_TAGMASK)),
18095:                          JSVAL_BOOLEAN),
26972:               exit);
17470:         v_ins = lir->ins2i(LIR_ush, v_ins, JSVAL_TAGBITS);
23710:         return;
17630:       case JSVAL_OBJECT:
23075:         if (JSVAL_IS_NULL(v)) {
29896:             // JSVAL_NULL maps to type TT_NULL, so insist that v_ins == 0 here.
26972:             guard(true, lir->ins_eq0(v_ins), exit);
23075:         } else {
27541:             guard(false, lir->ins_eq0(v_ins), exit);
17630:             guard(true,
17630:                   lir->ins2i(LIR_eq,
28086:                              lir->ins2(LIR_piand, v_ins, INS_CONSTWORD(JSVAL_TAGMASK)),
18095:                              JSVAL_OBJECT),
23075:                   exit);
27541:             guard(HAS_FUNCTION_CLASS(JSVAL_TO_OBJECT(v)),
27541:                   lir->ins2(LIR_eq,
27541:                             lir->ins2(LIR_piand,
27541:                                       lir->insLoad(LIR_ldp, v_ins, offsetof(JSObject, classword)),
28086:                                       INS_CONSTWORD(~JSSLOT_CLASS_MASK_BITS)),
27541:                             INS_CONSTPTR(&js_FunctionClass)),
27541:                   exit);
23075:         }
23710:         return;
23710:       default:
23710:         JS_ASSERT(JSVAL_TAG(v) == JSVAL_STRING);
17870:         guard(true,
17870:               lir->ins2i(LIR_eq,
18648:                         lir->ins2(LIR_piand, v_ins, INS_CONST(JSVAL_TAGMASK)),
18095:                         JSVAL_STRING),
26972:               exit);
18645:         v_ins = lir->ins2(LIR_piand, v_ins, INS_CONST(~JSVAL_TAGMASK));
23710:         return;
23710:     }
17470: }
17460: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17688: TraceRecorder::getThis(LIns*& this_ins)
17688: {
28841:     /*
28841:      * js_ComputeThisForFrame updates cx->fp->argv[-1], so sample it into 'original' first.
28841:      */
28914:     jsval original = JSVAL_NULL;
28914:     if (cx->fp->callee) {
28914:         original = cx->fp->argv[-1];
28914:         if (!JSVAL_IS_PRIMITIVE(original) &&
28914:             guardClass(JSVAL_TO_OBJECT(original), get(&cx->fp->argv[-1]), &js_WithClass, snapshot(MISMATCH_EXIT))) {
28914:             ABORT_TRACE("can't trace getThis on With object");
28914:         }
28914:     }
28914: 
28650:     JSObject* thisObj = js_ComputeThisForFrame(cx, cx->fp);
28650:     if (!thisObj)
28650:         ABORT_TRACE_ERROR("js_ComputeThisForName failed");
28650: 
27495:     /*
27495:      * In global code, bake in the global object as 'this' object.
27495:      */
27495:     if (!cx->fp->callee) {
27495:         JS_ASSERT(callDepth == 0);
27495:         this_ins = INS_CONSTPTR(thisObj);
27495: 
27470:         /*
27495:          * We don't have argv[-1] in global code, so we don't update the tracker here.
27495:          */
27933:         return JSRS_CONTINUE;
27495:     }
27495: 
28326:     jsval& thisv = cx->fp->argv[-1];
28961:     JS_ASSERT(JSVAL_IS_OBJECT(thisv));
28326: 
27495:     /*
27495:      * Traces type-specialize between null and objects, so if we currently see a null
27495:      * value in argv[-1], this trace will only match if we see null at runtime as well.
27495:      * Bake in the global object as 'this' object, updating the tracker as well. We
27495:      * can only detect this condition prior to calling js_ComputeThisForFrame, since it
27495:      * updates the interpreter's copy of argv[-1].
27495:      */
28962:     JSClass* clasp = NULL;;
28961:     if (JSVAL_IS_NULL(original) ||
28962:         (((clasp = STOBJ_GET_CLASS(JSVAL_TO_OBJECT(original))) == &js_CallClass) ||
28962:          (clasp == &js_BlockClass))) {
28962:         if (clasp)
28962:             guardClass(JSVAL_TO_OBJECT(original), get(&thisv), clasp, snapshot(BRANCH_EXIT));
28326:         JS_ASSERT(!JSVAL_IS_PRIMITIVE(thisv));
27548:         if (thisObj != globalObj)
27548:             ABORT_TRACE("global object was wrapped while recording");
27470:         this_ins = INS_CONSTPTR(thisObj);
28326:         set(&thisv, this_ins);
28326:         return JSRS_CONTINUE;
28326:     }
28326:     this_ins = get(&thisv);
28326: 
28908:     /*
28908:      * The only unwrapped object that needs to be wrapped that we can get here is the
28908:      * global object obtained throught the scope chain.
28908:      */
28908:     JSObject* obj = js_GetWrappedObject(cx, JSVAL_TO_OBJECT(thisv));
30291:     JSObject* inner = obj;
30291:     OBJ_TO_INNER_OBJECT(cx, inner);
28908:     if (!obj)
28908:         return JSRS_ERROR;
28908: 
30291:     JS_ASSERT(original == thisv ||
30291:               original == OBJECT_TO_JSVAL(inner) ||
30291:               original == OBJECT_TO_JSVAL(obj));
30291: 
30291:     // If the returned this object is the unwrapped inner or outer object,
30291:     // then we need to use the wrapped outer object.
30291:     LIns* is_inner = lir->ins2(LIR_eq, this_ins, INS_CONSTPTR(inner));
30291:     LIns* is_outer = lir->ins2(LIR_eq, this_ins, INS_CONSTPTR(obj));
30291:     LIns* wrapper = INS_CONSTPTR(JSVAL_TO_OBJECT(thisv));
30291: 
30291:     this_ins = lir->ins_choose(is_inner,
30291:                                wrapper,
30291:                                lir->ins_choose(is_outer,
30291:                                                wrapper,
30291:                                                this_ins));
28908: 
27933:     return JSRS_CONTINUE;
17688: }
17688: 
27637: 
27637: LIns*
27637: TraceRecorder::getStringLength(LIns* str_ins)
27637: {
29366:     LIns* len_ins = lir->insLoad(LIR_ldp, str_ins, (int)offsetof(JSString, mLength));
27637: 
27637:     LIns* masked_len_ins = lir->ins2(LIR_piand,
27637:                                      len_ins,
29366:                                      INS_CONSTWORD(JSString::LENGTH_MASK));
27637: 
27637:     return
27637:         lir->ins_choose(lir->ins_eq0(lir->ins2(LIR_piand,
27637:                                                len_ins,
29366:                                                INS_CONSTWORD(JSString::DEPENDENT))),
27637:                         masked_len_ins,
27637:                         lir->ins_choose(lir->ins_eq0(lir->ins2(LIR_piand,
27637:                                                                len_ins,
29366:                                                                INS_CONSTWORD(JSString::PREFIX))),
27637:                                         lir->ins2(LIR_piand,
27637:                                                   len_ins,
29366:                                                   INS_CONSTWORD(JSString::DEPENDENT_LENGTH_MASK)),
27637:                                         masked_len_ins));
27637: }
27637: 
22652: JS_REQUIRES_STACK bool
27540: TraceRecorder::guardClass(JSObject* obj, LIns* obj_ins, JSClass* clasp, VMSideExit* exit)
17899: {
20974:     bool cond = STOBJ_GET_CLASS(obj) == clasp;
17899: 
19020:     LIns* class_ins = lir->insLoad(LIR_ldp, obj_ins, offsetof(JSObject, classword));
27540:     class_ins = lir->ins2(LIR_piand, class_ins, lir->insImm(~JSSLOT_CLASS_MASK_BITS));
17899: 
17899:     char namebuf[32];
17899:     JS_snprintf(namebuf, sizeof namebuf, "guard(class is %s)", clasp->name);
25879:     guard(cond, addName(lir->ins2(LIR_eq, class_ins, INS_CONSTPTR(clasp)), namebuf), exit);
20974:     return cond;
17630: }
17630: 
22652: JS_REQUIRES_STACK bool
21685: TraceRecorder::guardDenseArray(JSObject* obj, LIns* obj_ins, ExitType exitType)
21685: {
25879:     return guardClass(obj, obj_ins, &js_ArrayClass, snapshot(exitType));
17899: }
17899: 
29513: JS_REQUIRES_STACK bool
29513: TraceRecorder::guardHasPrototype(JSObject* obj, LIns* obj_ins,
29513:                                  JSObject** pobj, LIns** pobj_ins,
29513:                                  VMSideExit* exit)
29513: {
29513:     *pobj = JSVAL_TO_OBJECT(obj->fslots[JSSLOT_PROTO]);
29513:     *pobj_ins = stobj_get_fslot(obj_ins, JSSLOT_PROTO);
29513: 
29513:     bool cond = *pobj == NULL;
29513:     guard(cond, addName(lir->ins_eq0(*pobj_ins), "guard(proto-not-null)"), exit);
29513:     return !cond;
29513: }
29513: 
27933: JS_REQUIRES_STACK JSRecordingStatus
27891: TraceRecorder::guardPrototypeHasNoIndexedProperties(JSObject* obj, LIns* obj_ins, ExitType exitType)
27891: {
27891:     /*
27891:      * Guard that no object along the prototype chain has any indexed properties which
27891:      * might become visible through holes in the array.
27891:      */
27540:     VMSideExit* exit = snapshot(exitType);
27891: 
27891:     if (js_PrototypeHasIndexedProperties(cx, obj))
27933:         return JSRS_STOP;
27891: 
29513:     while (guardHasPrototype(obj, obj_ins, &obj, &obj_ins, exit)) {
30244:         LIns* map_ins = map(obj_ins);
27891:         LIns* ops_ins;
27891:         if (!map_is_native(obj->map, map_ins, ops_ins))
27891:             ABORT_TRACE("non-native object involved along prototype chain");
27891: 
27891:         LIns* shape_ins = addName(lir->insLoad(LIR_ld, map_ins, offsetof(JSScope, shape)),
27891:                                   "shape");
26972:         guard(true,
27891:               addName(lir->ins2i(LIR_eq, shape_ins, OBJ_SHAPE(obj)), "guard(shape)"),
26972:               exit);
27891:     }
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JSRecordingStatus
26274: TraceRecorder::guardNotGlobalObject(JSObject* obj, LIns* obj_ins)
26274: {
26274:     if (obj == globalObj)
26274:         ABORT_TRACE("reference aliases global object");
26819:     guard(false, lir->ins2(LIR_eq, obj_ins, INS_CONSTPTR(globalObj)), MISMATCH_EXIT);
27933:     return JSRS_CONTINUE;
19983: }
19983: 
22652: JS_REQUIRES_STACK void
17818: TraceRecorder::clearFrameSlotsFromCache()
17818: {
17815:     /* Clear out all slots of this frame in the nativeFrameTracker. Different locations on the
17811:        VM stack might map to different locations on the native stack depending on the
17811:        number of arguments (i.e.) of the next call, so we have to make sure we map
17811:        those in to the cache with the right offsets. */
17811:     JSStackFrame* fp = cx->fp;
17811:     jsval* vp;
17811:     jsval* vpstop;
30248:     // Duplicate native stack layout computation: see VisitFrameSlots header comment.
30248:     // This doesn't do layout arithmetic, but it must clear out all the slots defined as
30248:     // imported by VisitFrameSlots.
18136:     if (fp->callee) {
18187:         vp = &fp->argv[-2];
28244:         vpstop = &fp->argv[argSlots(fp)];
18136:         while (vp < vpstop)
18136:             nativeFrameTracker.set(vp++, (LIns*)0);
30248:         nativeFrameTracker.set(&fp->argsobj, (LIns*)0);
18136:     }
18136:     vp = &fp->slots[0];
18136:     vpstop = &fp->slots[fp->script->nslots];
18136:     while (vp < vpstop)
18136:         nativeFrameTracker.set(vp++, (LIns*)0);
17818: }
17818: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17818: TraceRecorder::record_EnterFrame()
17818: {
19078:     JSStackFrame* fp = cx->fp;
19078: 
17852:     if (++callDepth >= MAX_CALLDEPTH)
17852:         ABORT_TRACE("exceeded maximum call depth");
20899:     // FIXME: Allow and attempt to inline a single level of recursion until we compile
20899:     //        recursive calls as independent trees (459301).
20899:     if (fp->script == fp->down->script && fp->down->down && fp->down->down->script == fp->script)
19078:         ABORT_TRACE("recursive call");
19078: 
29883:     debug_only_printf(LC_TMTracer, "EnterFrame %s, callDepth=%d\n",
18140:                       js_AtomToPrintableString(cx, cx->fp->fun->atom),
29883:                       callDepth);
29883:     debug_only_stmt(
29883:         if (js_LogController.lcbits & LC_TMRecorder) {
23450:             js_Disassemble(cx, cx->fp->script, JS_TRUE, stdout);
29883:             debug_only_print0(LC_TMTracer, "----\n");
29883:         }
29883:     )
30248:     LIns* void_ins = INS_VOID();
30248: 
30248:     // Duplicate native stack layout computation: see VisitFrameSlots header comment.
30248:     // This doesn't do layout arithmetic, but it must initialize in the tracker all the
30248:     // slots defined as imported by VisitFrameSlots.
18119:     jsval* vp = &fp->argv[fp->argc];
19567:     jsval* vpstop = vp + ptrdiff_t(fp->fun->nargs) - ptrdiff_t(fp->argc);
18119:     while (vp < vpstop) {
18119:         if (vp >= fp->down->regs->sp)
18119:             nativeFrameTracker.set(vp, (LIns*)0);
18119:         set(vp++, void_ins, true);
18119:     }
18119: 
18119:     vp = &fp->slots[0];
18119:     vpstop = vp + fp->script->nfixed;
18119:     while (vp < vpstop)
18119:         set(vp++, void_ins, true);
30248:     set(&fp->argsobj, INS_CONSTPTR(0), true);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17818: TraceRecorder::record_LeaveFrame()
17818: {
29883:     debug_only_stmt(
18142:         if (cx->fp->fun)
29883:             debug_only_printf(LC_TMTracer,
29883:                               "LeaveFrame (back to %s), callDepth=%d\n",
18140:                               js_AtomToPrintableString(cx, cx->fp->fun->atom),
18140:                               callDepth);
18260:         );
26118:     if (callDepth-- <= 0)
26118:         ABORT_TRACE("returned out of a loop we started tracing");
18001: 
18001:     // LeaveFrame gets called after the interpreter popped the frame and
18001:     // stored rval, so cx->fp not cx->fp->down, and -1 not 0.
24293:     atoms = FrameAtomBase(cx, cx->fp);
18150:     set(&stackval(-1), rval_ins, true);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_PUSH()
17409: {
24846:     stack(0, INS_CONST(JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID)));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_POPV()
17926: {
20907:     jsval& rval = stackval(-1);
20907:     LIns *rval_ins = get(&rval);
23708:     box_jsval(rval, rval_ins);
20907: 
20907:     // Store it in cx->fp->rval. NB: Tricky dependencies. cx->fp is the right
20907:     // frame because POPV appears only in global and eval code and we don't
20907:     // trace JSOP_EVAL or leaving the frame where tracing started.
20907:     LIns *fp_ins = lir->insLoad(LIR_ldp, cx_ins, offsetof(JSContext, fp));
20907:     lir->insStorei(rval_ins, fp_ins, offsetof(JSStackFrame, rval));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
21685: TraceRecorder::record_JSOP_ENTERWITH()
21685: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
21685: TraceRecorder::record_JSOP_LEAVEWITH()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_RETURN()
17409: {
26557:     /* A return from callDepth 0 terminates the current loop. */
26557:     if (callDepth == 0) {
26557:         AUDIT(returnLoopExits);
30295:         endLoop(traceMonitor);
27933:         return JSRS_STOP;
26557:     }
26557: 
30248:     // If we have created an |arguments| object for the frame, we must copy the argument
30248:     // values into the object as properties in case it is used after this frame returns.
30248:     if (cx->fp->argsobj) {
30248:         LIns* argsobj_ins = get(&cx->fp->argsobj);
30248:         LIns* length_ins = INS_CONST(cx->fp->argc);
30248:         LIns* callee_ins = get(&cx->fp->argv[-2]);
30248:         LIns* args_ins = lir->insAlloc(sizeof(jsval) * cx->fp->argc);
30248:         for (uintN i = 0; i < cx->fp->argc; ++i) {
30248:             LIns* arg_ins = get(&cx->fp->argv[i]);
30248:             box_jsval(cx->fp->argv[i], arg_ins);
30248:             lir->insStorei(arg_ins, args_ins, i * sizeof(jsval));
30248:         }
30248:         LIns* args[] = { args_ins, callee_ins, length_ins, argsobj_ins, cx_ins };
30248:         LIns* call_ins = lir->insCall(&js_PutArguments_ci, args);
30248:         guard(false, lir->ins_eq0(call_ins), STATUS_EXIT);
30248:     }
30248: 
26557:     /* If we inlined this function call, make the return value available to the caller code. */
18001:     jsval& rval = stackval(-1);
18661:     JSStackFrame *fp = cx->fp;
18785:     if ((cx->fp->flags & JSFRAME_CONSTRUCTING) && JSVAL_IS_PRIMITIVE(rval)) {
18661:         JS_ASSERT(OBJECT_TO_JSVAL(fp->thisp) == fp->argv[-1]);
18661:         rval_ins = get(&fp->argv[-1]);
18661:     } else {
18661:         rval_ins = get(&rval);
18001:     }
29883:     debug_only_printf(LC_TMTracer,
29883:                       "returning from %s\n",
29883:                       js_AtomToPrintableString(cx, cx->fp->fun->atom));
17818:     clearFrameSlotsFromCache();
26557: 
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_GOTO()
17409: {
26557:     /*
26557:      * If we hit a break, end the loop and generate an always taken loop exit guard.
26557:      * For other downward gotos (like if/else) continue recording.
26557:      */
26557:     jssrcnote* sn = js_GetSrcNote(cx->fp->script, cx->fp->regs->pc);
26557: 
26557:     if (sn && SN_TYPE(sn) == SRC_BREAK) {
26557:         AUDIT(breakLoopExits);
30295:         endLoop(traceMonitor);
27933:         return JSRS_STOP;
27933:     }
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_IFEQ()
17409: {
18694:     trackCfgMerges(cx->fp->regs->pc);
20416:     return ifop();
17409: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_IFNE()
17409: {
20416:     return ifop();
17409: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_ARGUMENTS()
17409: {
30248:     LIns* a_ins = get(&cx->fp->argsobj);
30248:     JSObject* global = JS_GetGlobalForObject(cx, cx->fp->scopeChain);
30248:     LIns* global_ins = INS_CONSTPTR(global);
30248:     LIns* args[] = { a_ins, global_ins, cx_ins };
30248:     a_ins = lir->insCall(&js_Arguments_ci, args);
19068:     guard(false, lir->ins_eq0(a_ins), OOM_EXIT);
19068:     stack(0, a_ins);
30248:     set(&cx->fp->argsobj, a_ins);
30248:     return JSRS_CONTINUE;
17409: }
17899: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_DUP()
17409: {
17448:     stack(0, get(&stackval(-1)));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_DUP2()
17409: {
17448:     stack(0, get(&stackval(-2)));
17448:     stack(1, get(&stackval(-1)));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
21685: TraceRecorder::record_JSOP_SWAP()
21685: {
21685:     jsval& l = stackval(-2);
21685:     jsval& r = stackval(-1);
21685:     LIns* l_ins = get(&l);
21685:     LIns* r_ins = get(&r);
21685:     set(&r, l_ins);
21685:     set(&l, r_ins);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
23097: TraceRecorder::record_JSOP_PICK()
23097: {
23097:     jsval* sp = cx->fp->regs->sp;
23102:     jsint n = cx->fp->regs->pc[1];
23097:     JS_ASSERT(sp - (n+1) >= StackBase(cx->fp));
24381:     LIns* top = get(sp - (n+1));
23097:     for (jsint i = 0; i < n; ++i)
24381:         set(sp - (n+1) + i, get(sp - n + i));
23097:     set(&sp[-1], top);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_SETCONST()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_BITOR()
17409: {
17469:     return binary(LIR_or);
17409: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_BITXOR()
17409: {
17469:     return binary(LIR_xor);
17409: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_BITAND()
17409: {
17469:     return binary(LIR_and);
17409: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_EQ()
17409: {
23093:     return equality(false, true);
18687: }
18687: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_NE()
17409: {
23093:     return equality(true, true);
17409: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_LT()
17409: {
23093:     return relational(LIR_flt, true);
17409: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_LE()
17409: {
23093:     return relational(LIR_fle, true);
17409: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_GT()
17409: {
23093:     return relational(LIR_fgt, true);
17409: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_GE()
17409: {
23093:     return relational(LIR_fge, true);
17409: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_LSH()
17409: {
17469:     return binary(LIR_lsh);
17409: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_RSH()
17409: {
17469:     return binary(LIR_rsh);
17409: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_URSH()
17409: {
17469:     return binary(LIR_ush);
17409: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_ADD()
17409: {
17872:     jsval& r = stackval(-1);
17872:     jsval& l = stackval(-2);
21685: 
23075:     if (!JSVAL_IS_PRIMITIVE(l)) {
28175:         ABORT_IF_XML(l);
28175:         if (!JSVAL_IS_PRIMITIVE(r)) {
28175:             ABORT_IF_XML(r);
21685:             return call_imacro(add_imacros.obj_obj);
28175:         }
21685:         return call_imacro(add_imacros.obj_any);
21685:     }
28175:     if (!JSVAL_IS_PRIMITIVE(r)) {
28175:         ABORT_IF_XML(r);
21685:         return call_imacro(add_imacros.any_obj);
28175:     }
21685: 
21447:     if (JSVAL_IS_STRING(l) || JSVAL_IS_STRING(r)) {
21685:         LIns* args[] = { stringify(r), stringify(l), cx_ins };
20915:         LIns* concat = lir->insCall(&js_ConcatStrings_ci, args);
17873:         guard(false, lir->ins_eq0(concat), OOM_EXIT);
17872:         set(&l, concat);
27933:         return JSRS_CONTINUE;
17872:     }
21685: 
17469:     return binary(LIR_fadd);
17409: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_SUB()
17409: {
17469:     return binary(LIR_fsub);
17409: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_MUL()
17409: {
17469:     return binary(LIR_fmul);
17409: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_DIV()
17409: {
17469:     return binary(LIR_fdiv);
17409: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_MOD()
17409: {
29354:     return binary(LIR_fmod);
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_NOT()
17409: {
17436:     jsval& v = stackval(-1);
19070:     if (JSVAL_TAG(v) == JSVAL_BOOLEAN) {
19070:         set(&v, lir->ins_eq0(lir->ins2i(LIR_eq, get(&v), 1)));
27933:         return JSRS_CONTINUE;
19070:     }
19554:     if (isNumber(v)) {
21776:         LIns* v_ins = get(&v);
21776:         set(&v, lir->ins2(LIR_or, lir->ins2(LIR_feq, v_ins, lir->insImmq(0)),
21776:                                   lir->ins_eq0(lir->ins2(LIR_feq, v_ins, v_ins))));
27933:         return JSRS_CONTINUE;
19554:     }
23075:     if (JSVAL_TAG(v) == JSVAL_OBJECT) {
19554:         set(&v, lir->ins_eq0(get(&v)));
27933:         return JSRS_CONTINUE;
18769:     }
20435:     JS_ASSERT(JSVAL_IS_STRING(v));
19580:     set(&v, lir->ins_eq0(lir->ins2(LIR_piand,
29366:                                    lir->insLoad(LIR_ldp, get(&v), (int)offsetof(JSString, mLength)),
29366:                                    INS_CONSTWORD(JSString::LENGTH_MASK))));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_BITNOT()
17409: {
17469:     return unary(LIR_not);
17409: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_NEG()
17409: {
18787:     jsval& v = stackval(-1);
23106: 
28175:     if (!JSVAL_IS_PRIMITIVE(v)) {
28175:         ABORT_IF_XML(v);
23106:         return call_imacro(unary_imacros.sign);
28175:     }
23106: 
18787:     if (isNumber(v)) {
18787:         LIns* a = get(&v);
18787: 
18787:         /* If we're a promoted integer, we have to watch out for 0s since -0 is a double.
18787:            Only follow this path if we're not an integer that's 0 and we're not a double
18787:            that's zero.
18787:          */
29354:         if (!oracle.isInstructionUndemotable(cx->fp->regs->pc) &&
29354:             isPromoteInt(a) &&
18787:             (!JSVAL_IS_INT(v) || JSVAL_TO_INT(v) != 0) &&
22610:             (!JSVAL_IS_DOUBLE(v) || !JSDOUBLE_IS_NEGZERO(*JSVAL_TO_DOUBLE(v))) &&
22610:             -asNumber(v) == (int)-asNumber(v)) {
18787:             a = lir->ins1(LIR_neg, ::demote(lir, a));
26117:             if (!a->isconst()) {
27540:                 VMSideExit* exit = snapshot(OVERFLOW_EXIT);
29354:                 guard(false, lir->ins1(LIR_ov, a), exit);
29354:                 guard(false, lir->ins2i(LIR_eq, a, 0), exit);
26117:             }
18787:             a = lir->ins1(LIR_i2f, a);
18787:         } else {
18787:             a = lir->ins1(LIR_fneg, a);
18787:         }
18787: 
18787:         set(&v, a);
27933:         return JSRS_CONTINUE;
18787:     }
23106: 
23106:     if (JSVAL_IS_NULL(v)) {
26265:         set(&v, lir->insImmf(-0.0));
27933:         return JSRS_CONTINUE;
23106:     }
23106: 
23225:     JS_ASSERT(JSVAL_TAG(v) == JSVAL_STRING || JSVAL_TAG(v) == JSVAL_BOOLEAN);
23225: 
23106:     LIns* args[] = { get(&v), cx_ins };
23225:     set(&v, lir->ins1(LIR_fneg,
23225:                       lir->insCall(JSVAL_IS_STRING(v)
23225:                                    ? &js_StringToNumber_ci
23225:                                    : &js_BooleanOrUndefinedToNumber_ci,
23225:                                    args)));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
23106: TraceRecorder::record_JSOP_POS()
23106: {
23225:     jsval& v = stackval(-1);
23225: 
28175:     if (!JSVAL_IS_PRIMITIVE(v)) {
28175:         ABORT_IF_XML(v);
23106:         return call_imacro(unary_imacros.sign);
28175:     }
23106: 
23225:     if (isNumber(v))
27933:         return JSRS_CONTINUE;
23225: 
23225:     if (JSVAL_IS_NULL(v)) {
23225:         set(&v, lir->insImmq(0));
27933:         return JSRS_CONTINUE;
23225:     }
23225: 
23225:     JS_ASSERT(JSVAL_TAG(v) == JSVAL_STRING || JSVAL_TAG(v) == JSVAL_BOOLEAN);
23225: 
23225:     LIns* args[] = { get(&v), cx_ins };
23225:     set(&v, lir->insCall(JSVAL_IS_STRING(v)
23106:                          ? &js_StringToNumber_ci
23106:                          : &js_BooleanOrUndefinedToNumber_ci,
23106:                          args));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
23106: TraceRecorder::record_JSOP_PRIMTOP()
23106: {
23106:     // Either this opcode does nothing or we couldn't have traced here, because
23106:     // we'd have thrown an exception -- so do nothing if we actually hit this.
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
24873: TraceRecorder::record_JSOP_OBJTOP()
24873: {
28175:     jsval& v = stackval(-1);
28175:     ABORT_IF_XML(v);
27933:     return JSRS_CONTINUE;
24873: }
24873: 
18300: JSBool
18300: js_Array(JSContext* cx, JSObject* obj, uintN argc, jsval* argv, jsval* rval);
18300: 
18712: JSBool
18712: js_Object(JSContext *cx, JSObject *obj, uintN argc, jsval *argv, jsval *rval);
18712: 
20402: JSBool
20402: js_Date(JSContext *cx, JSObject *obj, uintN argc, jsval *argv, jsval *rval);
20402: 
27933: JSRecordingStatus
22626: TraceRecorder::getClassPrototype(JSObject* ctor, LIns*& proto_ins)
22626: {
22626:     jsval pval;
22626: 
22626:     if (!OBJ_GET_PROPERTY(cx, ctor,
28086:                           ATOM_TO_JSID(cx->runtime->atomState.classPrototypeAtom),
22626:                           &pval)) {
27933:         ABORT_TRACE_ERROR("error getting prototype from constructor");
22626:     }
23075:     if (JSVAL_TAG(pval) != JSVAL_OBJECT)
22626:         ABORT_TRACE("got primitive prototype from constructor");
25887: #ifdef DEBUG
25887:     JSBool ok, found;
25887:     uintN attrs;
25887:     ok = JS_GetPropertyAttributes(cx, ctor, js_class_prototype_str, &attrs, &found);
25887:     JS_ASSERT(ok);
25887:     JS_ASSERT(found);
25887:     JS_ASSERT((~attrs & (JSPROP_READONLY | JSPROP_PERMANENT)) == 0);
25887: #endif
22626:     proto_ins = INS_CONSTPTR(JSVAL_TO_OBJECT(pval));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JSRecordingStatus
27012: TraceRecorder::getClassPrototype(JSProtoKey key, LIns*& proto_ins)
27012: {
27012:     JSObject* proto;
27012:     if (!js_GetClassPrototype(cx, globalObj, INT_TO_JSID(key), &proto))
27933:         ABORT_TRACE_ERROR("error in js_GetClassPrototype");
27012:     proto_ins = INS_CONSTPTR(proto);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
28086: #define IGNORE_NATIVE_CALL_COMPLETE_CALLBACK ((JSTraceableNative*)1)
28086: 
28086: JSRecordingStatus
28086: TraceRecorder::newString(JSObject* ctor, uint32 argc, jsval* argv, jsval* rval)
28086: {
28086:     JS_ASSERT(argc == 1);
28086: 
28175:     if (!JSVAL_IS_PRIMITIVE(argv[0])) {
28175:         ABORT_IF_XML(argv[0]);
28086:         return call_imacro(new_imacros.String);
28175:     }
28086: 
28086:     LIns* proto_ins;
28086:     CHECK_STATUS(getClassPrototype(ctor, proto_ins));
28086: 
28086:     LIns* args[] = { stringify(argv[0]), proto_ins, cx_ins };
28086:     LIns* obj_ins = lir->insCall(&js_String_tn_ci, args);
28086:     guard(false, lir->ins_eq0(obj_ins), OOM_EXIT);
28086: 
28086:     set(rval, obj_ins);
28086:     pendingTraceableNative = IGNORE_NATIVE_CALL_COMPLETE_CALLBACK;
28086:     return JSRS_CONTINUE;
28086: }
28086: 
27933: JSRecordingStatus
22626: TraceRecorder::newArray(JSObject* ctor, uint32 argc, jsval* argv, jsval* rval)
22626: {
28086:     LIns *proto_ins;
27933:     CHECK_STATUS(getClassPrototype(ctor, proto_ins));
22626: 
28086:     LIns *arr_ins;
28086:     if (argc == 0 || (argc == 1 && JSVAL_IS_NUMBER(argv[0]))) {
28086:         // arr_ins = js_NewEmptyArray(cx, Array.prototype)
22626:         LIns *args[] = { proto_ins, cx_ins };
28086:         arr_ins = lir->insCall(&js_NewEmptyArray_ci, args);
22626:         guard(false, lir->ins_eq0(arr_ins), OOM_EXIT);
28086:         if (argc == 1) {
28086:             // array_ins.fslots[JSSLOT_ARRAY_LENGTH] = length
28086:             lir->insStorei(f2i(get(argv)), // FIXME: is this 64-bit safe?
28086:                            arr_ins,
28086:                            offsetof(JSObject, fslots) + JSSLOT_ARRAY_LENGTH * sizeof(jsval));
28086:         }
22626:     } else {
22626:         // arr_ins = js_NewUninitializedArray(cx, Array.prototype, argc)
22626:         LIns *args[] = { INS_CONST(argc), proto_ins, cx_ins };
22626:         arr_ins = lir->insCall(&js_NewUninitializedArray_ci, args);
22626:         guard(false, lir->ins_eq0(arr_ins), OOM_EXIT);
22626: 
22626:         // arr->dslots[i] = box_jsval(vp[i]);  for i in 0..argc
22626:         LIns *dslots_ins = NULL;
26826:         for (uint32 i = 0; i < argc && !lirbuf->outOMem(); i++) {
22626:             LIns *elt_ins = get(argv + i);
23708:             box_jsval(argv[i], elt_ins);
22626:             stobj_set_dslot(arr_ins, i, dslots_ins, elt_ins, "set_array_elt");
22626:         }
28554: 
28554:         if (argc > 0)
28554:             stobj_set_fslot(arr_ins, JSSLOT_ARRAY_COUNT, INS_CONST(argc), "set_array_count");
22626:     }
28086: 
22626:     set(rval, arr_ins);
28086:     pendingTraceableNative = IGNORE_NATIVE_CALL_COMPLETE_CALLBACK;
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
26552: TraceRecorder::emitNativeCall(JSTraceableNative* known, uintN argc, LIns* args[])
26552: {
26552:     bool constructing = known->flags & JSTN_CONSTRUCTOR;
26552: 
26552:     if (JSTN_ERRTYPE(known) == FAIL_STATUS) {
26552:         // This needs to capture the pre-call state of the stack. So do not set
26552:         // pendingTraceableNative before taking this snapshot.
26552:         JS_ASSERT(!pendingTraceableNative);
26552: 
26552:         // Take snapshot for deep LeaveTree and store it in cx->bailExit.
28086:         // If we are calling a slow native, add information to the side exit
28086:         // for SynthesizeSlowNativeFrame.
27540:         VMSideExit* exit = snapshot(DEEP_BAIL_EXIT);
28086:         JSObject* funobj = JSVAL_TO_OBJECT(stackval(0 - (2 + argc)));
28086:         if (FUN_SLOW_NATIVE(GET_FUNCTION_PRIVATE(cx, funobj)))
28086:             exit->setNativeCallee(funobj, constructing);
27540:         lir->insStorei(INS_CONSTPTR(exit), cx_ins, offsetof(JSContext, bailExit));
26552: 
26552:         // Tell nanojit not to discard or defer stack writes before this call.
27540:         LIns* guardRec = createGuardRecord(exit);
30452:         lir->insGuard(LIR_xbarrier, NULL, guardRec);
26552:     }
26552: 
26552:     LIns* res_ins = lir->insCall(known->builtin, args);
26552:     rval_ins = res_ins;
27059:     switch (JSTN_ERRTYPE(known)) {
27059:       case FAIL_NULL:
26552:         guard(false, lir->ins_eq0(res_ins), OOM_EXIT);
27059:         break;
27059:       case FAIL_NEG:
27059:         res_ins = lir->ins1(LIR_i2f, res_ins);
27059:         guard(false, lir->ins2(LIR_flt, res_ins, lir->insImmq(0)), OOM_EXIT);
27059:         break;
27059:       case FAIL_VOID:
27059:         guard(false, lir->ins2i(LIR_eq, res_ins, JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID)), OOM_EXIT);
27059:         break;
27059:       case FAIL_COOKIE:
27059:         guard(false, lir->ins2(LIR_eq, res_ins, INS_CONST(JSVAL_ERROR_COOKIE)), OOM_EXIT);
27059:         break;
27059:       default:;
27059:     }
26552: 
26552:     set(&stackval(0 - (2 + argc)), res_ins);
26552: 
26552:     /*
28086:      * The return value will be processed by NativeCallComplete since
26552:      * we have to know the actual return value type for calls that return
26552:      * jsval (like Array_p_pop).
26552:      */
26552:     pendingTraceableNative = known;
26552: 
27933:     return JSRS_CONTINUE;
26552: }
26552: 
26552: /*
28086:  * Check whether we have a specialized implementation for this native invocation.
26552:  */
27933: JS_REQUIRES_STACK JSRecordingStatus
26552: TraceRecorder::callTraceableNative(JSFunction* fun, uintN argc, bool constructing)
26552: {
26552:     JSTraceableNative* known = FUN_TRCINFO(fun);
26552:     JS_ASSERT(known && (JSFastNative)fun->u.n.native == known->native);
26552: 
20431:     JSStackFrame* fp = cx->fp;
20431:     jsbytecode *pc = fp->regs->pc;
21511: 
18300:     jsval& fval = stackval(0 - (2 + argc));
26552:     jsval& tval = stackval(0 - (1 + argc));
26552: 
18641:     LIns* this_ins = get(&tval);
20431: 
26676:     LIns* args[nanojit::MAXARGS];
20431:     do {
25887:         if (((known->flags & JSTN_CONSTRUCTOR) != 0) != constructing)
17651:             continue;
17634: 
17871:         uintN knownargc = strlen(known->argtypes);
17870:         if (argc != knownargc)
18115:             continue;
17870: 
17870:         intN prefixc = strlen(known->prefix);
20431:         JS_ASSERT(prefixc <= 3);
17870:         LIns** argp = &args[argc + prefixc - 1];
17870:         char argtype;
17870: 
29880: #if defined DEBUG
18172:         memset(args, 0xCD, sizeof(args));
18172: #endif
18172: 
20431:         uintN i;
20431:         for (i = prefixc; i--; ) {
20431:             argtype = known->prefix[i];
20431:             if (argtype == 'C') {
20431:                 *argp = cx_ins;
20431:             } else if (argtype == 'T') {   /* this, as an object */
23075:                 if (JSVAL_IS_PRIMITIVE(tval))
20431:                     goto next_specialization;
20431:                 *argp = this_ins;
20431:             } else if (argtype == 'S') {   /* this, as a string */
23228:                 if (!JSVAL_IS_STRING(tval))
23228:                     goto next_specialization;
23226:                 *argp = this_ins;
20431:             } else if (argtype == 'f') {
20431:                 *argp = INS_CONSTPTR(JSVAL_TO_OBJECT(fval));
20431:             } else if (argtype == 'p') {
27933:                 CHECK_STATUS(getClassPrototype(JSVAL_TO_OBJECT(fval), *argp));
20431:             } else if (argtype == 'R') {
20431:                 *argp = INS_CONSTPTR(cx->runtime);
20431:             } else if (argtype == 'P') {
24600:                 // FIXME: Set pc to imacpc when recording JSOP_CALL inside the
24600:                 //        JSOP_GETELEM imacro (bug 476559).
24600:                 if (*pc == JSOP_CALL && fp->imacpc && *fp->imacpc == JSOP_GETELEM)
24600:                     *argp = INS_CONSTPTR(fp->imacpc);
24600:                 else
20431:                     *argp = INS_CONSTPTR(pc);
20431:             } else if (argtype == 'D') {  /* this, as a number */
22634:                 if (!isNumber(tval))
20431:                     goto next_specialization;
20431:                 *argp = this_ins;
20431:             } else {
20431:                 JS_NOT_REACHED("unknown prefix arg type");
20431:             }
20431:             argp--;
20431:         }
20431: 
20431:         for (i = knownargc; i--; ) {
22634:             jsval& arg = stackval(0 - (i + 1));
22634:             *argp = get(&arg);
20431: 
20431:             argtype = known->argtypes[i];
20431:             if (argtype == 'd' || argtype == 'i') {
20431:                 if (!isNumber(arg))
20431:                     goto next_specialization;
20431:                 if (argtype == 'i')
20431:                     *argp = f2i(*argp);
20431:             } else if (argtype == 'o') {
23075:                 if (JSVAL_IS_PRIMITIVE(arg))
20431:                     goto next_specialization;
20431:             } else if (argtype == 's') {
20431:                 if (!JSVAL_IS_STRING(arg))
20431:                     goto next_specialization;
20431:             } else if (argtype == 'r') {
20431:                 if (!VALUE_IS_REGEXP(cx, arg))
20431:                     goto next_specialization;
20431:             } else if (argtype == 'f') {
20431:                 if (!VALUE_IS_FUNCTION(cx, arg))
20431:                     goto next_specialization;
20431:             } else if (argtype == 'v') {
23708:                 box_jsval(arg, *argp);
20431:             } else {
20431:                 goto next_specialization;
20431:             }
20431:             argp--;
20431:         }
29880: #if defined DEBUG
18172:         JS_ASSERT(args[0] != (LIns *)0xcdcdcdcd);
18172: #endif
26552:         return emitNativeCall(known, argc, args);
28086: 
28086: next_specialization:;
28086:     } while ((known++)->flags & JSTN_MORE);
28086: 
28086:     return JSRS_STOP;
28086: }
28086: 
28086: JS_REQUIRES_STACK JSRecordingStatus
28086: TraceRecorder::callNative(uintN argc, JSOp mode)
28086: {
28086:     LIns* args[5];
28086: 
28086:     JS_ASSERT(mode == JSOP_CALL || mode == JSOP_NEW || mode == JSOP_APPLY);
28086: 
28086:     jsval* vp = &stackval(0 - (2 + argc));
28086:     JSObject* funobj = JSVAL_TO_OBJECT(vp[0]);
28086:     JSFunction* fun = GET_FUNCTION_PRIVATE(cx, funobj);
28086: 
26552:     if (fun->flags & JSFUN_TRACEABLE) {
28086:         JSRecordingStatus status;
28086:         if ((status = callTraceableNative(fun, argc, mode == JSOP_NEW)) != JSRS_STOP)
27933:             return status;
26552:     }
26552: 
28326:     JSFastNative native = (JSFastNative)fun->u.n.native;
28326:     if (native == js_fun_apply || native == js_fun_call)
28326:         ABORT_TRACE("trying to call native apply or call");
28326: 
28086:     // Allocate the vp vector and emit code to root it.
28086:     uintN vplen = 2 + JS_MAX(argc, FUN_MINARGS(fun)) + fun->u.n.extra;
26552:     if (!(fun->flags & JSFUN_FAST_NATIVE))
28086:         vplen++;  // slow native return value slot
28086:     lir->insStorei(INS_CONST(vplen), cx_ins, offsetof(JSContext, nativeVpLen));
28086:     LIns* invokevp_ins = lir->insAlloc(vplen * sizeof(jsval));
28086:     lir->insStorei(invokevp_ins, cx_ins, offsetof(JSContext, nativeVp));
28086: 
28086:     // vp[0] is the callee.
28086:     lir->insStorei(INS_CONSTWORD(OBJECT_TO_JSVAL(funobj)), invokevp_ins, 0);
28086: 
28086:     // Calculate |this|.
28086:     LIns* this_ins;
28086:     if (mode == JSOP_NEW) {
28086:         JSClass* clasp = fun->u.n.clasp;
28086:         JS_ASSERT(clasp != &js_SlowArrayClass);
28086:         if (!clasp)
28086:             clasp = &js_ObjectClass;
28086:         JS_ASSERT(((jsuword) clasp & 3) == 0);
28086: 
28086:         // Abort on |new Function|. js_NewInstance would allocate a regular-
28086:         // sized JSObject, not a Function-sized one. (The Function ctor would
28086:         // deep-bail anyway but let's not go there.)
28086:         if (clasp == &js_FunctionClass)
28086:             ABORT_TRACE("new Function");
28086: 
28086:         if (clasp->getObjectOps)
28086:             ABORT_TRACE("new with non-native ops");
28086: 
28086:         args[0] = INS_CONSTPTR(funobj);
28086:         args[1] = INS_CONSTPTR(clasp);
28086:         args[2] = cx_ins;
28086:         newobj_ins = lir->insCall(&js_NewInstance_ci, args);
28086:         guard(false, lir->ins_eq0(newobj_ins), OOM_EXIT);
28086:         this_ins = newobj_ins;  // boxing an object is a no-op
28086:     } else if (JSFUN_BOUND_METHOD_TEST(fun->flags)) {
28086:         this_ins = INS_CONSTWORD(OBJECT_TO_JSVAL(OBJ_GET_PARENT(cx, funobj)));
28086:     } else {
28086:         this_ins = get(&vp[1]);
28326:         /*
28326:          * For fast natives, 'null' or primitives are fine as as 'this' value.
28326:          * For slow natives we have to ensure the object is substituted for the
28326:          * appropriate global object or boxed object value. JSOP_NEW allocates its
29896:          * own object so it's guaranteed to have a valid 'this' value.
28326:          */
28326:         if (!(fun->flags & JSFUN_FAST_NATIVE)) {
28086:             if (JSVAL_IS_NULL(vp[1])) {
28326:                 JSObject* thisObj = js_ComputeThis(cx, JS_FALSE, vp + 2);
28326:                 if (!thisObj)
28326:                     ABORT_TRACE_ERROR("error in js_ComputeGlobalThis");
28326:                 this_ins = INS_CONSTPTR(thisObj);
28326:             } else if (!JSVAL_IS_OBJECT(vp[1])) {
28326:                 ABORT_TRACE("slow native(primitive, args)");
28086:             } else {
28326:                 if (guardClass(JSVAL_TO_OBJECT(vp[1]), this_ins, &js_WithClass, snapshot(MISMATCH_EXIT)))
28326:                     ABORT_TRACE("can't trace slow native invocation on With object");
28326: 
28326:                 this_ins = lir->ins_choose(lir->ins_eq0(stobj_get_fslot(this_ins, JSSLOT_PARENT)),
28326:                                            INS_CONSTPTR(globalObj),
28326:                                            this_ins);
28086:             }
28086:         }
28086:         box_jsval(vp[1], this_ins);
28086:     }
28086:     lir->insStorei(this_ins, invokevp_ins, 1 * sizeof(jsval));
28086: 
28086:     // Populate argv.
28086:     for (uintN n = 2; n < 2 + argc; n++) {
26552:         LIns* i = get(&vp[n]);
26552:         box_jsval(vp[n], i);
26552:         lir->insStorei(i, invokevp_ins, n * sizeof(jsval));
28086: 
28086:         // For a very long argument list we might run out of LIR space, so
28086:         // check inside the loop.
28086:         if (lirbuf->outOMem())
28086:             ABORT_TRACE("out of memory in argument list");
28086:     }
28086: 
28086:     // Populate extra slots, including the return value slot for a slow native.
28086:     if (2 + argc < vplen) {
28086:         LIns* undef_ins = INS_CONSTWORD(JSVAL_VOID);
28086:         for (uintN n = 2 + argc; n < vplen; n++) {
28086:             lir->insStorei(undef_ins, invokevp_ins, n * sizeof(jsval));
28086: 
28086:             if (lirbuf->outOMem())
28086:                 ABORT_TRACE("out of memory in extra slots");
28086:         }
28086:     }
28086: 
28086:     // Set up arguments for the JSNative or JSFastNative.
28086:     uint32 types;
28086:     if (fun->flags & JSFUN_FAST_NATIVE) {
28086:         if (mode == JSOP_NEW)
28086:             ABORT_TRACE("untraceable fast native constructor");
28086:         native_rval_ins = invokevp_ins;
28086:         args[0] = invokevp_ins;
28086:         args[1] = lir->insImm(argc);
28086:         args[2] = cx_ins;
28086:         types = ARGSIZE_LO | ARGSIZE_LO << 2 | ARGSIZE_LO << 4 | ARGSIZE_LO << 6;
28086:     } else {
28086:         native_rval_ins = lir->ins2i(LIR_piadd, invokevp_ins, int32_t((vplen - 1) * sizeof(jsval)));
28086:         args[0] = native_rval_ins;
28086:         args[1] = lir->ins2i(LIR_piadd, invokevp_ins, int32_t(2 * sizeof(jsval)));
28086:         args[2] = lir->insImm(argc);
28086:         args[3] = this_ins;
28086:         args[4] = cx_ins;
28086:         types = ARGSIZE_LO | ARGSIZE_LO << 2 | ARGSIZE_LO << 4 | ARGSIZE_LO << 6 |
28086:                 ARGSIZE_LO << 8 | ARGSIZE_LO << 10;
28086:     }
28086: 
28086:     // Generate CallInfo and a JSTraceableNative structure on the fly.  Do not
28086:     // use JSTN_UNBOX_AFTER for mode JSOP_NEW because record_NativeCallComplete
28086:     // unboxes the result specially.
26552: 
28182:     CallInfo* ci = (CallInfo*) lir->insSkip(sizeof(struct CallInfo))->payload();
26552:     ci->_address = uintptr_t(fun->u.n.native);
26552:     ci->_cse = ci->_fold = 0;
26552:     ci->_abi = ABI_CDECL;
28086:     ci->_argtypes = types;
26552: #ifdef DEBUG
28086:     ci->_name = JS_GetFunctionName(fun);
26552:  #endif
26552: 
26552:     // Generate a JSTraceableNative structure on the fly.
26552:     generatedTraceableNative->builtin = ci;
26552:     generatedTraceableNative->native = (JSFastNative)fun->u.n.native;
28086:     generatedTraceableNative->flags = FAIL_STATUS | ((mode == JSOP_NEW)
28241:                                                      ? JSTN_CONSTRUCTOR
28086:                                                      : JSTN_UNBOX_AFTER);
28086: 
26552:     generatedTraceableNative->prefix = generatedTraceableNative->argtypes = NULL;
26552: 
28086:     // argc is the original argc here. It is used to calculate where to place
28086:     // the return value.
28086:     JSRecordingStatus status;
28086:     if ((status = emitNativeCall(generatedTraceableNative, argc, args)) != JSRS_CONTINUE)
28086:         return status;
28086: 
28086:     // Unroot the vp.
28086:     lir->insStorei(INS_CONSTPTR(NULL), cx_ins, offsetof(JSContext, nativeVp));
28086:     return JSRS_CONTINUE;
28086: }
28086: 
28086: JS_REQUIRES_STACK JSRecordingStatus
28086: TraceRecorder::functionCall(uintN argc, JSOp mode)
26552: {
26552:     jsval& fval = stackval(0 - (2 + argc));
26552:     JS_ASSERT(&fval >= StackBase(cx->fp));
26552: 
26552:     if (!VALUE_IS_FUNCTION(cx, fval))
26552:         ABORT_TRACE("callee is not a function");
26552: 
26552:     jsval& tval = stackval(0 - (1 + argc));
26552: 
26552:     /*
26552:      * If callee is not constant, it's a shapeless call and we have to guard
26552:      * explicitly that we will get this callee again at runtime.
26552:      */
27933:     if (!get(&fval)->isconst())
27933:         CHECK_STATUS(guardCallee(fval));
26552: 
26552:     /*
26552:      * Require that the callee be a function object, to avoid guarding on its
26552:      * class here. We know if the callee and this were pushed by JSOP_CALLNAME
26552:      * or JSOP_CALLPROP that callee is a *particular* function, since these hit
26552:      * the property cache and guard on the object (this) in which the callee
26552:      * was found. So it's sufficient to test here that the particular function
26552:      * is interpreted, not guard on that condition.
26552:      *
26552:      * Bytecode sequences that push shapeless callees must guard on the callee
26552:      * class being Function and the function being interpreted.
26552:      */
26552:     JSFunction* fun = GET_FUNCTION_PRIVATE(cx, JSVAL_TO_OBJECT(fval));
26552: 
26552:     if (FUN_INTERPRETED(fun)) {
28086:         if (mode == JSOP_NEW) {
28086:             LIns* args[] = { get(&fval), INS_CONSTPTR(&js_ObjectClass), cx_ins };
26552:             LIns* tv_ins = lir->insCall(&js_NewInstance_ci, args);
26552:             guard(false, lir->ins_eq0(tv_ins), OOM_EXIT);
26552:             set(&tval, tv_ins);
26552:         }
28086:         return interpretedFunctionCall(fval, fun, argc, mode == JSOP_NEW);
26552:     }
26552: 
26552:     if (FUN_SLOW_NATIVE(fun)) {
26552:         JSNative native = fun->u.n.native;
28086:         jsval* argv = &tval + 1;
26552:         if (native == js_Array)
28086:             return newArray(JSVAL_TO_OBJECT(fval), argc, argv, &fval);
28240:         if (native == js_String && argc == 1) {
28086:             if (mode == JSOP_NEW)
28086:                 return newString(JSVAL_TO_OBJECT(fval), 1, argv, &fval);
28175:             if (!JSVAL_IS_PRIMITIVE(argv[0])) {
28175:                 ABORT_IF_XML(argv[0]);
26552:                 return call_imacro(call_imacros.String);
28175:             }
28086:             set(&fval, stringify(argv[0]));
28086:             pendingTraceableNative = IGNORE_NATIVE_CALL_COMPLETE_CALLBACK;
28086:             return JSRS_CONTINUE;
28086:         }
28086:     }
28086: 
28086:     return callNative(argc, mode);
20431: }
20431: 
27933: JS_REQUIRES_STACK JSRecordingStatus
20431: TraceRecorder::record_JSOP_NEW()
20431: {
27234:     uintN argc = GET_ARGC(cx->fp->regs->pc);
27234:     cx->fp->assertValidStackDepth(argc + 2);
28086:     return functionCall(argc, JSOP_NEW);
18300: }
18300: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18300: TraceRecorder::record_JSOP_DELNAME()
18300: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18300: TraceRecorder::record_JSOP_DELPROP()
18300: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18300: TraceRecorder::record_JSOP_DELELEM()
18300: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18300: TraceRecorder::record_JSOP_TYPEOF()
18300: {
18300:     jsval& r = stackval(-1);
18300:     LIns* type;
18300:     if (JSVAL_IS_STRING(r)) {
18663:         type = INS_CONSTPTR(ATOM_TO_STRING(cx->runtime->atomState.typeAtoms[JSTYPE_STRING]));
18300:     } else if (isNumber(r)) {
18663:         type = INS_CONSTPTR(ATOM_TO_STRING(cx->runtime->atomState.typeAtoms[JSTYPE_NUMBER]));
27572:     } else if (VALUE_IS_FUNCTION(cx, r)) {
27572:         type = INS_CONSTPTR(ATOM_TO_STRING(cx->runtime->atomState.typeAtoms[JSTYPE_FUNCTION]));
18300:     } else {
18300:         LIns* args[] = { get(&r), cx_ins };
18300:         if (JSVAL_TAG(r) == JSVAL_BOOLEAN) {
18300:             // We specialize identically for boolean and undefined. We must not have a hole here.
18300:             // Pass the unboxed type here, since TypeOfBoolean knows how to handle it.
24846:             JS_ASSERT(r == JSVAL_TRUE || r == JSVAL_FALSE || r == JSVAL_VOID);
20915:             type = lir->insCall(&js_TypeOfBoolean_ci, args);
18300:         } else {
23075:             JS_ASSERT(JSVAL_TAG(r) == JSVAL_OBJECT);
20915:             type = lir->insCall(&js_TypeOfObject_ci, args);
18300:         }
18300:     }
18300:     set(&r, type);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18300: TraceRecorder::record_JSOP_VOID()
18300: {
24846:     stack(-1, INS_CONST(JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID)));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18300: TraceRecorder::record_JSOP_INCNAME()
18300: {
18300:     return incName(1);
18300: }
18300: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18300: TraceRecorder::record_JSOP_INCPROP()
18300: {
18300:     return incProp(1);
18300: }
18300: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18300: TraceRecorder::record_JSOP_INCELEM()
18300: {
18300:     return incElem(1);
18300: }
18300: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18300: TraceRecorder::record_JSOP_DECNAME()
18300: {
18300:     return incName(-1);
18300: }
18300: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18300: TraceRecorder::record_JSOP_DECPROP()
18300: {
18300:     return incProp(-1);
18300: }
18300: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18300: TraceRecorder::record_JSOP_DECELEM()
18300: {
18300:     return incElem(-1);
18300: }
18300: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18300: TraceRecorder::incName(jsint incr, bool pre)
18300: {
18300:     jsval* vp;
27933:     CHECK_STATUS(name(vp));
18300:     LIns* v_ins = get(vp);
27933:     CHECK_STATUS(inc(*vp, v_ins, incr, pre));
18300:     set(vp, v_ins);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18300: TraceRecorder::record_JSOP_NAMEINC()
18300: {
18300:     return incName(1, false);
18300: }
18300: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18300: TraceRecorder::record_JSOP_PROPINC()
18300: {
18300:     return incProp(1, false);
18300: }
18300: 
18300: // XXX consolidate with record_JSOP_GETELEM code...
27933: JS_REQUIRES_STACK JSRecordingStatus
18300: TraceRecorder::record_JSOP_ELEMINC()
18300: {
18300:     return incElem(1, false);
18300: }
18300: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18300: TraceRecorder::record_JSOP_NAMEDEC()
18300: {
21805:     return incName(-1, false);
18300: }
18300: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18300: TraceRecorder::record_JSOP_PROPDEC()
18300: {
18300:     return incProp(-1, false);
18300: }
18300: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18300: TraceRecorder::record_JSOP_ELEMDEC()
18300: {
18300:     return incElem(-1, false);
18300: }
18300: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18300: TraceRecorder::record_JSOP_GETPROP()
18300: {
18300:     return getProp(stackval(-1));
18300: }
18300: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18300: TraceRecorder::record_JSOP_SETPROP()
18300: {
19093:     jsval& l = stackval(-2);
19093:     if (JSVAL_IS_PRIMITIVE(l))
19093:         ABORT_TRACE("primitive this for SETPROP");
19093: 
19093:     JSObject* obj = JSVAL_TO_OBJECT(l);
19093:     if (obj->map->ops->setProperty != js_SetProperty)
19093:         ABORT_TRACE("non-native JSObjectOps::setProperty");
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
19171: TraceRecorder::record_SetPropHit(JSPropCacheEntry* entry, JSScopeProperty* sprop)
19093: {
27932:     if (entry == JS_NO_PROP_CACHE_FILL)
27932:         ABORT_TRACE("can't trace uncacheable property set");
27932:     if (PCVCAP_TAG(entry->vcap) >= 1)
27932:         ABORT_TRACE("can't trace inherited property set");
27932: 
19093:     jsbytecode* pc = cx->fp->regs->pc;
27932:     JS_ASSERT(entry->kpc == pc);
27932: 
18300:     jsval& r = stackval(-1);
18300:     jsval& l = stackval(-2);
18300: 
19093:     JS_ASSERT(!JSVAL_IS_PRIMITIVE(l));
18300:     JSObject* obj = JSVAL_TO_OBJECT(l);
18300:     LIns* obj_ins = get(&l);
27932:     JSScope* scope = OBJ_SCOPE(obj);
27932: 
27932:     JS_ASSERT(scope->object == obj);
30258:     JS_ASSERT(scope->has(sprop));
27932: 
27932:     if (!isValidSlot(scope, sprop))
27933:         return JSRS_STOP;
25938: 
30249:     /*
30249:      * Setting a function-valued property might need to rebrand the object; we
30249:      * don't trace that case. There's no need to guard on that, though, because
30249:      * separating functions into the trace-time type TT_FUNCTION will save the
30249:      * day!
30249:      */
30258:     if (scope->branded() && VALUE_IS_FUNCTION(cx, r))
30255:         ABORT_TRACE("can't trace function-valued property set in branded scope");
30249: 
19093:     if (obj == globalObj) {
27932:         JS_ASSERT(SPROP_HAS_VALID_SLOT(sprop, scope));
19093:         uint32 slot = sprop->slot;
19093:         if (!lazilyImportGlobalSlot(slot))
19093:             ABORT_TRACE("lazy import of global slot failed");
19093: 
19093:         LIns* r_ins = get(&r);
19093:         set(&STOBJ_GET_SLOT(obj, slot), r_ins);
19093: 
19093:         JS_ASSERT(*pc != JSOP_INITPROP);
19093:         if (pc[JSOP_SETPROP_LENGTH] != JSOP_POP)
19093:             set(&l, r_ins);
27933:         return JSRS_CONTINUE;
19093:     }
19093: 
19093:     // The global object's shape is guarded at trace entry, all others need a guard here.
30244:     LIns* map_ins = map(obj_ins);
18300:     LIns* ops_ins;
18300:     if (!map_is_native(obj->map, map_ins, ops_ins, offsetof(JSObjectOps, setProperty)))
25636:         ABORT_TRACE("non-native map");
18300: 
18300:     LIns* shape_ins = addName(lir->insLoad(LIR_ld, map_ins, offsetof(JSScope, shape)), "shape");
28837:     guard(true, addName(lir->ins2i(LIR_eq, shape_ins, entry->kshape), "guard(kshape)(record_SetPropHit)"),
23721:           BRANCH_EXIT);
19171: 
28312:     uint32 vshape = PCVCAP_SHAPE(entry->vcap);
28312:     if (entry->kshape != vshape) {
28312:         LIns *vshape_ins = lir->insLoad(LIR_ld,
28312:                                         lir->insLoad(LIR_ldp, cx_ins, offsetof(JSContext, runtime)),
28312:                                         offsetof(JSRuntime, protoHazardShape));
28837:         guard(true, addName(lir->ins2i(LIR_eq, vshape_ins, vshape), "guard(vshape)(record_SetPropHit)"),
28312:               MISMATCH_EXIT);
28312: 
18712:         LIns* args[] = { INS_CONSTPTR(sprop), obj_ins, cx_ins };
20915:         LIns* ok_ins = lir->insCall(&js_AddProperty_ci, args);
19171:         guard(false, lir->ins_eq0(ok_ins), OOM_EXIT);
18300:     }
18300: 
18300:     LIns* dslots_ins = NULL;
18300:     LIns* v_ins = get(&r);
18300:     LIns* boxed_ins = v_ins;
23708:     box_jsval(r, boxed_ins);
27933:     CHECK_STATUS(native_set(obj_ins, sprop, dslots_ins, boxed_ins));
19093: 
19093:     if (*pc != JSOP_INITPROP && pc[JSOP_SETPROP_LENGTH] != JSOP_POP)
19093:         set(&l, v_ins);
27933:     return JSRS_CONTINUE;
19093: }
19093: 
24489: /* Functions used by JSOP_GETELEM. */
24489: 
24489: static JSBool
24489: GetProperty(JSContext *cx, uintN argc, jsval *vp)
24489: {
24489:     jsval *argv;
24489:     jsid id;
24489: 
25213:     JS_ASSERT_NOT_ON_TRACE(cx);
25213:     JS_ASSERT(cx->fp->imacpc && argc == 1);
24489:     argv = JS_ARGV(cx, vp);
24489:     JS_ASSERT(JSVAL_IS_STRING(argv[0]));
24489:     if (!js_ValueToStringId(cx, argv[0], &id))
24489:         return JS_FALSE;
24489:     argv[0] = ID_TO_VALUE(id);
24489:     return OBJ_GET_PROPERTY(cx, JS_THIS_OBJECT(cx, vp), id, &JS_RVAL(cx, vp));
24489: }
24489: 
24489: static jsval FASTCALL
24598: GetProperty_tn(JSContext *cx, jsbytecode *pc, JSObject *obj, JSString *name)
24489: {
25094:     JSAutoTempIdRooter idr(cx);
25094:     JSAutoTempValueRooter tvr(cx);
24489: 
25094:     if (!js_ValueToStringId(cx, STRING_TO_JSVAL(name), idr.addr()) ||
25094:         !OBJ_GET_PROPERTY(cx, obj, idr.id(), tvr.addr())) {
27166:         js_SetBuiltinError(cx);
25094:         *tvr.addr() = JSVAL_ERROR_COOKIE;
24600:     }
25094:     return tvr.value();
24489: }
24489: 
24489: static JSBool
24489: GetElement(JSContext *cx, uintN argc, jsval *vp)
24489: {
24489:     jsval *argv;
24489:     jsid id;
24489: 
25213:     JS_ASSERT_NOT_ON_TRACE(cx);
25213:     JS_ASSERT(cx->fp->imacpc && argc == 1);
24489:     argv = JS_ARGV(cx, vp);
24489:     JS_ASSERT(JSVAL_IS_NUMBER(argv[0]));
24489:     if (!JS_ValueToId(cx, argv[0], &id))
24489:         return JS_FALSE;
24489:     argv[0] = ID_TO_VALUE(id);
24489:     return OBJ_GET_PROPERTY(cx, JS_THIS_OBJECT(cx, vp), id, &JS_RVAL(cx, vp));
24489: }
24489: 
24489: static jsval FASTCALL
24598: GetElement_tn(JSContext* cx, jsbytecode *pc, JSObject* obj, int32 index)
24489: {
25094:     JSAutoTempValueRooter tvr(cx);
25094:     JSAutoTempIdRooter idr(cx);
25094: 
25094:     if (!js_Int32ToId(cx, index, idr.addr())) {
27166:         js_SetBuiltinError(cx);
24489:         return JSVAL_ERROR_COOKIE;
24612:     }
25094:     if (!OBJ_GET_PROPERTY(cx, obj, idr.id(), tvr.addr())) {
27166:         js_SetBuiltinError(cx);
25094:         *tvr.addr() = JSVAL_ERROR_COOKIE;
24612:     }
25094:     return tvr.value();
24489: }
24489: 
24489: JS_DEFINE_TRCINFO_1(GetProperty,
24598:     (4, (static, JSVAL_FAIL,    GetProperty_tn, CONTEXT, PC, THIS, STRING,      0, 0)))
24489: JS_DEFINE_TRCINFO_1(GetElement,
24598:     (4, (extern, JSVAL_FAIL,    GetElement_tn,  CONTEXT, PC, THIS, INT32,       0, 0)))
24489: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18300: TraceRecorder::record_JSOP_GETELEM()
18300: {
26552:     bool call = *cx->fp->regs->pc == JSOP_CALLELEM;
26551: 
19979:     jsval& idx = stackval(-1);
19983:     jsval& lval = stackval(-2);
19983: 
19983:     LIns* obj_ins = get(&lval);
19979:     LIns* idx_ins = get(&idx);
19979: 
26274:     // Special case for array-like access of strings.
26274:     if (JSVAL_IS_STRING(lval) && isInt32(idx)) {
26551:         if (call)
26551:             ABORT_TRACE("JSOP_CALLELEM on a string");
26274:         int i = asInt32(idx);
29366:         if (size_t(i) >= JSVAL_TO_STRING(lval)->length())
18692:             ABORT_TRACE("Invalid string index in JSOP_GETELEM");
19979:         idx_ins = makeNumberInt32(idx_ins);
19979:         LIns* args[] = { idx_ins, obj_ins, cx_ins };
20915:         LIns* unitstr_ins = lir->insCall(&js_String_getelem_ci, args);
18300:         guard(false, lir->ins_eq0(unitstr_ins), MISMATCH_EXIT);
19983:         set(&lval, unitstr_ins);
27933:         return JSRS_CONTINUE;
19983:     }
19983: 
19983:     if (JSVAL_IS_PRIMITIVE(lval))
19979:         ABORT_TRACE("JSOP_GETLEM on a primitive");
28175:     ABORT_IF_XML(lval);
19979: 
19983:     JSObject* obj = JSVAL_TO_OBJECT(lval);
19979:     jsval id;
19979:     LIns* v_ins;
19979: 
26274:     /* Property access using a string name or something we have to stringify. */
26274:     if (!JSVAL_IS_INT(idx)) {
26374:         if (!JSVAL_IS_PRIMITIVE(idx))
26374:             ABORT_TRACE("non-primitive index");
26274:         // If index is not a string, turn it into a string.
26274:         if (!js_InternNonIntElementId(cx, obj, idx, &id))
27933:             ABORT_TRACE_ERROR("failed to intern non-int element id");
26274:         set(&idx, stringify(idx));
26274: 
19983:         // Store the interned string to the stack to save the interpreter from redoing this work.
19983:         idx = ID_TO_VALUE(id);
26274: 
26274:         // The object is not guaranteed to be a dense array at this point, so it might be the
26274:         // global object, which we have to guard against.
27933:         CHECK_STATUS(guardNotGlobalObject(obj, obj_ins));
26274: 
26551:         return call_imacro(call ? callelem_imacros.callprop : getelem_imacros.getprop);
19979:     }
19979: 
30248:     if (STOBJ_GET_CLASS(obj) == &js_ArgumentsClass) {
30248:         guardClass(obj, obj_ins, &js_ArgumentsClass, snapshot(MISMATCH_EXIT));
30248: 
30248:         unsigned depth;
30248:         JSStackFrame* afp = frameIfInRange(obj, &depth);
30248:         if (afp) {
30248:             uintN int_idx = JSVAL_TO_INT(idx);
30248:             jsval* vp = &afp->argv[int_idx];
30248:             if (idx_ins->isconstq()) {
30248:                 if (int_idx >= 0 && int_idx < afp->argc)
30248:                     v_ins = get(vp);
30248:                 else
30248:                     v_ins = INS_VOID();
30248:             } else {
30248:                 // If the index is not a constant expression, we generate LIR to load the value from
30248:                 // the native stack area. The guard on js_ArgumentClass above ensures the up-to-date
30248:                 // value has been written back to the native stack area.
30248: 
30248:                 idx_ins = makeNumberInt32(idx_ins);
30248:                 if (int_idx >= 0 && int_idx < afp->argc) {
30248:                     JSTraceType type = getCoercedType(*vp);
30248: 
30248:                     // Guard that the argument has the same type on trace as during recording.
30248:                     LIns* typemap_ins;
30248:                     if (callDepth == depth) {
30248:                         // In this case, we are in the same frame where the arguments object was created.
30248:                         // The entry type map is not necessarily up-to-date, so we capture a new type map
30248:                         // for this point in the code.
30248:                         unsigned stackSlots = js_NativeStackSlots(cx, 0/*callDepth*/);
30248:                         if (stackSlots * sizeof(JSTraceType) > NJ_MAX_SKIP_PAYLOAD_SZB)
30248:                             ABORT_TRACE("|arguments| requires saving too much stack");
30248:                         JSTraceType* typemap = (JSTraceType*) lir->insSkip(stackSlots * sizeof(JSTraceType))->payload();
30248:                         DetermineTypesVisitor detVisitor(*this, typemap);
30248:                         VisitStackSlots(detVisitor, cx, 0);
30248:                         typemap_ins = INS_CONSTPTR(typemap + 2 /*callee,this*/);
30248:                     } else {
30248:                         // In this case, we are in a deeper frame from where the arguments object was
30248:                         // created. The type map at the point of the call out from the creation frame
30248:                         // is accurate.
30248:                         // Note: this relies on the assumption that we abort on setting an element of
30248:                         // an arguments object in any deeper frame.
30248:                         LIns* fip_ins = lir->insLoad(LIR_ldp, lirbuf->rp, (callDepth-depth)*sizeof(FrameInfo*));
30248:                         typemap_ins = lir->ins2(LIR_add, fip_ins, INS_CONST(sizeof(FrameInfo) + 2/*callee,this*/ * sizeof(JSTraceType)));
30248:                     }
30248: 
30248:                     LIns* typep_ins = lir->ins2(LIR_add, typemap_ins, 
30248:                                                 lir->ins2(LIR_mul, idx_ins, INS_CONST(sizeof(JSTraceType))));
30248:                     LIns* type_ins = lir->insLoad(LIR_ldcb, typep_ins, 0);
30248:                     guard(true,
30248:                           addName(lir->ins2(LIR_eq, type_ins, lir->insImm(type)),
30248:                                   "guard(type-stable upvar)"),
30248:                           BRANCH_EXIT);
30248: 
30248:                     // Read the value out of the native stack area.
30248:                     guard(true, lir->ins2(LIR_ult, idx_ins, INS_CONST(afp->argc)), 
30248:                           snapshot(BRANCH_EXIT));
30248:                     size_t stackOffset = -treeInfo->nativeStackBase + nativeStackOffset(&afp->argv[0]);
30248:                     LIns* args_addr_ins = lir->ins2(LIR_add, lirbuf->sp, INS_CONST(stackOffset));
30248:                     LIns* argi_addr_ins = lir->ins2(LIR_add, args_addr_ins,
30248:                                                     lir->ins2(LIR_mul, idx_ins, INS_CONST(sizeof(double))));
30248:                     v_ins = stackLoad(argi_addr_ins, type);
30248:                 } else {
30248:                     guard(false, lir->ins2(LIR_ult, idx_ins, INS_CONST(afp->argc)), 
30248:                           snapshot(BRANCH_EXIT));
30248:                     v_ins = INS_VOID();
30248:                 }
30248:             }
30248:             JS_ASSERT(v_ins);
30248:             set(&lval, v_ins);
30248:             return JSRS_CONTINUE;
30248:         }
30248:         ABORT_TRACE("can't reach arguments object's frame");
30248:     }
30248: 
28411:     if (!guardDenseArray(obj, obj_ins, BRANCH_EXIT)) {
27933:         CHECK_STATUS(guardNotGlobalObject(obj, obj_ins));
26274: 
26551:         return call_imacro(call ? callelem_imacros.callelem : getelem_imacros.getelem);
18300:     }
18300: 
28411:     // Fast path for dense arrays accessed with a integer index.
18300:     jsval* vp;
18300:     LIns* addr_ins;
28411:     CHECK_STATUS(denseArrayElement(lval, idx, vp, v_ins, addr_ins));
19983:     set(&lval, v_ins);
26551:     if (call)
26551:         set(&idx, obj_ins);
27933:     return JSRS_CONTINUE;
18300: }
18300: 
24489: /* Functions used by JSOP_SETELEM */
24489: 
24489: static JSBool
24489: SetProperty(JSContext *cx, uintN argc, jsval *vp)
24489: {
24489:     jsval *argv;
24489:     jsid id;
24489: 
24489:     JS_ASSERT(argc == 2);
24489:     argv = JS_ARGV(cx, vp);
24489:     JS_ASSERT(JSVAL_IS_STRING(argv[0]));
24489:     if (!js_ValueToStringId(cx, argv[0], &id))
24489:         return JS_FALSE;
24489:     argv[0] = ID_TO_VALUE(id);
24489:     if (!OBJ_SET_PROPERTY(cx, JS_THIS_OBJECT(cx, vp), id, &argv[1]))
24489:         return JS_FALSE;
24489:     JS_SET_RVAL(cx, vp, JSVAL_VOID);
24489:     return JS_TRUE;
24489: }
24489: 
26395: static JSBool FASTCALL
24489: SetProperty_tn(JSContext* cx, JSObject* obj, JSString* idstr, jsval v)
24489: {
25094:     JSAutoTempValueRooter tvr(cx, v);
25094:     JSAutoTempIdRooter idr(cx);
25094: 
25094:     if (!js_ValueToStringId(cx, STRING_TO_JSVAL(idstr), idr.addr()) ||
25094:         !OBJ_SET_PROPERTY(cx, obj, idr.id(), tvr.addr())) {
27166:         js_SetBuiltinError(cx);
24612:     }
24846:     return JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID);
24489: }
24489: 
24489: static JSBool
24489: SetElement(JSContext *cx, uintN argc, jsval *vp)
24489: {
24489:     jsval *argv;
24489:     jsid id;
24489: 
24489:     JS_ASSERT(argc == 2);
24489:     argv = JS_ARGV(cx, vp);
24489:     JS_ASSERT(JSVAL_IS_NUMBER(argv[0]));
24489:     if (!JS_ValueToId(cx, argv[0], &id))
24489:         return JS_FALSE;
24489:     argv[0] = ID_TO_VALUE(id);
24489:     if (!OBJ_SET_PROPERTY(cx, JS_THIS_OBJECT(cx, vp), id, &argv[1]))
24489:         return JS_FALSE;
24489:     JS_SET_RVAL(cx, vp, JSVAL_VOID);
24489:     return JS_TRUE;
24489: }
24489: 
26395: static JSBool FASTCALL
24489: SetElement_tn(JSContext* cx, JSObject* obj, int32 index, jsval v)
24489: {
25094:     JSAutoTempIdRooter idr(cx);
25094:     JSAutoTempValueRooter tvr(cx, v);
25094: 
25094:     if (!js_Int32ToId(cx, index, idr.addr()) ||
25094:         !OBJ_SET_PROPERTY(cx, obj, idr.id(), tvr.addr())) {
27166:         js_SetBuiltinError(cx);
25094:     }
24846:     return JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID);
24489: }
24489: 
24489: JS_DEFINE_TRCINFO_1(SetProperty,
24489:     (4, (extern, BOOL_FAIL,     SetProperty_tn, CONTEXT, THIS, STRING, JSVAL,   0, 0)))
24489: JS_DEFINE_TRCINFO_1(SetElement,
24489:     (4, (extern, BOOL_FAIL,     SetElement_tn,  CONTEXT, THIS, INT32, JSVAL,    0, 0)))
24489: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18300: TraceRecorder::record_JSOP_SETELEM()
18300: {
18300:     jsval& v = stackval(-1);
19979:     jsval& idx = stackval(-2);
19983:     jsval& lval = stackval(-3);
18300: 
18300:     /* no guards for type checks, trace specialized this already */
19983:     if (JSVAL_IS_PRIMITIVE(lval))
18300:         ABORT_TRACE("left JSOP_SETELEM operand is not an object");
28175:     ABORT_IF_XML(lval);
19979: 
19983:     JSObject* obj = JSVAL_TO_OBJECT(lval);
19983:     LIns* obj_ins = get(&lval);
19979:     LIns* idx_ins = get(&idx);
18300:     LIns* v_ins = get(&v);
20000:     jsid id;
19979: 
26274:     if (!JSVAL_IS_INT(idx)) {
27110:         if (!JSVAL_IS_PRIMITIVE(idx))
27110:             ABORT_TRACE("non-primitive index");
26274:         // If index is not a string, turn it into a string.
26274:         if (!js_InternNonIntElementId(cx, obj, idx, &id))
27933:             ABORT_TRACE_ERROR("failed to intern non-int element id");
26274:         set(&idx, stringify(idx));
26274: 
26274:         // Store the interned string to the stack to save the interpreter from redoing this work.
26274:         idx = ID_TO_VALUE(id);
26274: 
26274:         // The object is not guaranteed to be a dense array at this point, so it might be the
26274:         // global object, which we have to guard against.
27933:         CHECK_STATUS(guardNotGlobalObject(obj, obj_ins));
26274: 
26376:         return call_imacro((*cx->fp->regs->pc == JSOP_INITELEM)
26376:                            ? initelem_imacros.initprop
26376:                            : setelem_imacros.setprop);
26274:     }
26274: 
26274:     if (JSVAL_TO_INT(idx) < 0 || !OBJ_IS_DENSE_ARRAY(cx, obj)) {
27933:         CHECK_STATUS(guardNotGlobalObject(obj, obj_ins));
26274: 
26274:         return call_imacro((*cx->fp->regs->pc == JSOP_INITELEM)
26274:                            ? initelem_imacros.initelem
26274:                            : setelem_imacros.setelem);
26274:     }
26274: 
26274:     // Make sure the array is actually dense.
26274:     if (!guardDenseArray(obj, obj_ins, BRANCH_EXIT))
27933:         return JSRS_STOP;
26274: 
26274:     // Fast path for dense arrays accessed with a non-negative integer index. In case the trace
26274:     // calculated the index using the FPU, force it to be an integer.
26274:     idx_ins = makeNumberInt32(idx_ins);
26274: 
26274:     // Box the value so we can use one builtin instead of having to add one builtin for every
26274:     // storage type.
26012:     LIns* boxed_v_ins = v_ins;
26012:     box_jsval(v, boxed_v_ins);
26012: 
24489:     LIns* args[] = { boxed_v_ins, idx_ins, obj_ins, cx_ins };
24489:     LIns* res_ins = lir->insCall(&js_Array_dense_setelem_ci, args);
19993:     guard(false, lir->ins_eq0(res_ins), MISMATCH_EXIT);
18300: 
18300:     jsbytecode* pc = cx->fp->regs->pc;
18300:     if (*pc == JSOP_SETELEM && pc[JSOP_SETELEM_LENGTH] != JSOP_POP)
19983:         set(&lval, v_ins);
19993: 
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18300: TraceRecorder::record_JSOP_CALLNAME()
18300: {
18300:     JSObject* obj = cx->fp->scopeChain;
18300:     if (obj != globalObj) {
18300:         jsval* vp;
27933:         CHECK_STATUS(activeCallOrGlobalSlot(obj, vp));
18300:         stack(0, get(vp));
26819:         stack(1, INS_CONSTPTR(globalObj));
27933:         return JSRS_CONTINUE;
18300:     }
18300: 
18300:     LIns* obj_ins = scopeChain();
18300:     JSObject* obj2;
18300:     jsuword pcval;
25633: 
27933:     CHECK_STATUS(test_property_cache(obj, obj_ins, obj2, pcval));
18300: 
18300:     if (PCVAL_IS_NULL(pcval) || !PCVAL_IS_OBJECT(pcval))
18300:         ABORT_TRACE("callee is not an object");
25633: 
18300:     JS_ASSERT(HAS_FUNCTION_CLASS(PCVAL_TO_OBJECT(pcval)));
18300: 
18712:     stack(0, INS_CONSTPTR(PCVAL_TO_OBJECT(pcval)));
18300:     stack(1, obj_ins);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
29021: JS_DEFINE_CALLINFO_5(extern, UINT32, js_GetUpvarArgOnTrace, CONTEXT, UINT32, INT32, UINT32,
29021:                      DOUBLEPTR, 0, 0)
29021: JS_DEFINE_CALLINFO_5(extern, UINT32, js_GetUpvarVarOnTrace, CONTEXT, UINT32, INT32, UINT32,
28738:                      DOUBLEPTR, 0, 0)
29022: JS_DEFINE_CALLINFO_5(extern, UINT32, js_GetUpvarStackOnTrace, CONTEXT, UINT32, INT32, UINT32,
29022:                      DOUBLEPTR, 0, 0)
28268: 
28923: /*
28923:  * Record LIR to get the given upvar. Return the LIR instruction for
28923:  * the upvar value. NULL is returned only on a can't-happen condition
28923:  * with an invalid typemap. The value of the upvar is returned as v.
28923:  */
28923: JS_REQUIRES_STACK LIns*
28923: TraceRecorder::upvar(JSScript* script, JSUpvarArray* uva, uintN index, jsval& v)
28923: {
28275:     /*
28969:      * Try to find the upvar in the current trace's tracker. For &vr to be
28969:      * the address of the jsval found in js_GetUpvar, we must initialize
28969:      * vr directly with the result, so it is a reference to the same location.
28969:      * It does not work to assign the result to v, because v is an already
28969:      * existing reference that points to something else.
28969:      */
29021:     uint32 cookie = uva->vector[index];
29021:     jsval& vr = js_GetUpvar(cx, script->staticLevel, cookie);
28969:     v = vr;
28969:     LIns* upvar_ins = get(&vr);
28275:     if (upvar_ins) {
28923:         return upvar_ins;
28275:     }
28275: 
28275:     /*
28275:      * The upvar is not in the current trace, so get the upvar value
28275:      * exactly as the interpreter does and unbox.
28275:      */
29021:     uint32 level = script->staticLevel - UPVAR_FRAME_SKIP(cookie);
29021:     uint32 cookieSlot = UPVAR_FRAME_SLOT(cookie);
29021:     JSStackFrame* fp = cx->display[level];
29021:     const CallInfo* ci;
29021:     int32 slot;
29021:     if (!fp->fun) {
29022:         ci = &js_GetUpvarStackOnTrace_ci;
29022:         slot = cookieSlot;
29021:     } else if (cookieSlot < fp->fun->nargs) {
29021:         ci = &js_GetUpvarArgOnTrace_ci;
29021:         slot = cookieSlot;
29021:     } else if (cookieSlot == CALLEE_UPVAR_SLOT) {
29021:         ci = &js_GetUpvarArgOnTrace_ci;
29021:         slot = -2;
29021:     } else {
29021:         ci = &js_GetUpvarVarOnTrace_ci;
29021:         slot = cookieSlot - fp->fun->nargs;
29021:     }
29021: 
28268:     LIns* outp = lir->insAlloc(sizeof(double));
28268:     LIns* args[] = {
28268:         outp,
28738:         INS_CONST(callDepth),
29021:         INS_CONST(slot),
29021:         INS_CONST(level),
28268:         cx_ins
28268:     };
28268:     LIns* call_ins = lir->insCall(ci, args);
29896:     JSTraceType type = getCoercedType(v);
28268:     guard(true,
28268:           addName(lir->ins2(LIR_eq, call_ins, lir->insImm(type)),
28268:                   "guard(type-stable upvar)"),
28268:           BRANCH_EXIT);
30248:     return stackLoad(outp, type);
30248: }
30248: 
30248: /*
30248:  * Generate LIR to load a value from the native stack. This method ensures that the
30248:  * correct LIR load operator is used.
30248:  */
30248: LIns* TraceRecorder::stackLoad(LIns* base, uint8 type)
30248: {
28268:     LOpcode loadOp;
28268:     switch (type) {
29896:       case TT_DOUBLE:
28268:         loadOp = LIR_ldq;
28268:         break;
29896:       case TT_OBJECT:
29896:       case TT_STRING:
29896:       case TT_FUNCTION:
29896:       case TT_NULL:
28268:         loadOp = LIR_ldp;
28268:         break;
29896:       case TT_INT32:
29896:       case TT_PSEUDOBOOLEAN:
28268:         loadOp = LIR_ld;
28268:         break;
29896:       case TT_JSVAL:
28268:       default:
29896:         JS_NOT_REACHED("found jsval type in an upvar type map entry");
28923:         return NULL;
28268:     }
28268: 
30248:     LIns* result = lir->insLoad(loadOp, base, 0);
29896:     if (type == TT_INT32)
28268:         result = lir->ins1(LIR_i2f, result);
28923:     return result;
28923: }
28923: 
28923: JS_REQUIRES_STACK JSRecordingStatus
28923: TraceRecorder::record_JSOP_GETUPVAR()
28923: {
28923:     uintN index = GET_UINT16(cx->fp->regs->pc);
28923:     JSScript *script = cx->fp->script;
28923:     JSUpvarArray* uva = JS_SCRIPT_UPVARS(script);
28923:     JS_ASSERT(index < uva->length);
28923: 
28923:     jsval v;
28923:     LIns* upvar_ins = upvar(script, uva, index, v);
28923:     if (!upvar_ins)
28923:         return JSRS_STOP;
28923:     stack(0, upvar_ins);
28268:     return JSRS_CONTINUE;
28268: }
28268: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18308: TraceRecorder::record_JSOP_CALLUPVAR()
18308: {
27933:     CHECK_STATUS(record_JSOP_GETUPVAR());
27012:     stack(1, INS_CONSTPTR(NULL));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
27012: TraceRecorder::record_JSOP_GETDSLOT()
27012: {
27012:     JSObject* callee = cx->fp->callee;
28557:     LIns* callee_ins = get(&cx->fp->argv[-2]);
27012: 
27012:     unsigned index = GET_UINT16(cx->fp->regs->pc);
27012:     LIns* dslots_ins = NULL;
27012:     LIns* v_ins = stobj_get_dslot(callee_ins, index, dslots_ins);
27012: 
27012:     unbox_jsval(callee->dslots[index], v_ins, snapshot(BRANCH_EXIT));
27012:     stack(0, v_ins);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
27012: TraceRecorder::record_JSOP_CALLDSLOT()
27012: {
27933:     CHECK_STATUS(record_JSOP_GETDSLOT());
27012:     stack(1, INS_CONSTPTR(NULL));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
22634: TraceRecorder::guardCallee(jsval& callee)
21526: {
23712:     JS_ASSERT(VALUE_IS_FUNCTION(cx, callee));
23712: 
27540:     VMSideExit* branchExit = snapshot(BRANCH_EXIT);
22623:     JSObject* callee_obj = JSVAL_TO_OBJECT(callee);
22623:     LIns* callee_ins = get(&callee);
23074: 
27540:     guard(true,
27540:           lir->ins2(LIR_eq,
30248:                     stobj_get_private(callee_ins),
22623:                     INS_CONSTPTR(OBJ_GET_PRIVATE(cx, callee_obj))),
27540:           branchExit);
22623:     guard(true,
22623:           lir->ins2(LIR_eq,
22623:                     stobj_get_fslot(callee_ins, JSSLOT_PARENT),
22623:                     INS_CONSTPTR(OBJ_GET_PARENT(cx, callee_obj))),
27540:           branchExit);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
19577: TraceRecorder::interpretedFunctionCall(jsval& fval, JSFunction* fun, uintN argc, bool constructing)
18300: {
19149:     if (JS_GetGlobalForObject(cx, JSVAL_TO_OBJECT(fval)) != globalObj)
19149:         ABORT_TRACE("JSOP_CALL or JSOP_NEW crosses global scopes");
19149: 
18300:     JSStackFrame* fp = cx->fp;
18300: 
18300:     // TODO: track the copying via the tracker...
18300:     if (argc < fun->nargs &&
18300:         jsuword(fp->regs->sp + (fun->nargs - argc)) > cx->stackPool.current->limit) {
18300:         ABORT_TRACE("can't trace calls with too few args requiring argv move");
18300:     }
18300: 
19085:     // Generate a type map for the outgoing frame and stash it in the LIR
19085:     unsigned stackSlots = js_NativeStackSlots(cx, 0/*callDepth*/);
29896:     if (sizeof(FrameInfo) + stackSlots * sizeof(JSTraceType) > NJ_MAX_SKIP_PAYLOAD_SZB)
23918:         ABORT_TRACE("interpreted function call requires saving too much stack");
29896:     LIns* data = lir->insSkip(sizeof(FrameInfo) + stackSlots * sizeof(JSTraceType));
23262:     FrameInfo* fi = (FrameInfo*)data->payload();
29896:     JSTraceType* typemap = reinterpret_cast<JSTraceType *>(fi + 1);
29880: 
29880:     DetermineTypesVisitor detVisitor(*this, typemap);
29882:     VisitStackSlots(detVisitor, cx, 0);
19085: 
19577:     if (argc >= 0x8000)
19577:         ABORT_TRACE("too many arguments");
19577: 
23262:     fi->callee = JSVAL_TO_OBJECT(fval);
23262:     fi->block = fp->blockChain;
25111:     fi->pc = fp->regs->pc;
25111:     fi->imacpc = fp->imacpc;
28887:     fi->spdist = fp->regs->sp - fp->slots;
28949:     fi->set_argc(argc, constructing);
28840:     fi->spoffset = 2 /*callee,this*/ + fp->argc;
18300: 
18300:     unsigned callDepth = getCallDepth();
18300:     if (callDepth >= treeInfo->maxCallDepth)
18300:         treeInfo->maxCallDepth = callDepth + 1;
28840:     if (callDepth == 0)
28887:         fi->spoffset = 2 /*callee,this*/ + argc - fi->spdist;
18300: 
23262:     lir->insStorei(INS_CONSTPTR(fi), lirbuf->rp, callDepth * sizeof(FrameInfo*));
18300: 
18300:     atoms = fun->u.i.script->atomMap.vector;
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18300: TraceRecorder::record_JSOP_CALL()
18300: {
27234:     uintN argc = GET_ARGC(cx->fp->regs->pc);
27234:     cx->fp->assertValidStackDepth(argc + 2);
28086:     return functionCall(argc,
28086:                         (cx->fp->imacpc && *cx->fp->imacpc == JSOP_APPLY)
28086:                         ? JSOP_APPLY
28086:                         : JSOP_CALL);
19582: }
19582: 
23097: static jsbytecode* apply_imacro_table[] = {
23097:     apply_imacros.apply0,
23097:     apply_imacros.apply1,
23097:     apply_imacros.apply2,
23097:     apply_imacros.apply3,
23097:     apply_imacros.apply4,
23097:     apply_imacros.apply5,
23097:     apply_imacros.apply6,
23097:     apply_imacros.apply7,
23097:     apply_imacros.apply8
23097: };
23097: 
23097: static jsbytecode* call_imacro_table[] = {
23097:     apply_imacros.call0,
23097:     apply_imacros.call1,
23097:     apply_imacros.call2,
23097:     apply_imacros.call3,
23097:     apply_imacros.call4,
23097:     apply_imacros.call5,
23097:     apply_imacros.call6,
23097:     apply_imacros.call7,
23097:     apply_imacros.call8
23097: };
23097: 
27933: JS_REQUIRES_STACK JSRecordingStatus
21452: TraceRecorder::record_JSOP_APPLY()
21452: {
22634:     JSStackFrame* fp = cx->fp;
22634:     jsbytecode *pc = fp->regs->pc;
22634:     uintN argc = GET_ARGC(pc);
27234:     cx->fp->assertValidStackDepth(argc + 2);
27234: 
22634:     jsval* vp = fp->regs->sp - (argc + 2);
22634:     jsuint length = 0;
22634:     JSObject* aobj = NULL;
22634:     LIns* aobj_ins = NULL;
23097: 
23097:     JS_ASSERT(!fp->imacpc);
22634: 
22634:     if (!VALUE_IS_FUNCTION(cx, vp[0]))
22634:         return record_JSOP_CALL();
28175:     ABORT_IF_XML(vp[0]);
22634: 
22634:     JSObject* obj = JSVAL_TO_OBJECT(vp[0]);
22634:     JSFunction* fun = GET_FUNCTION_PRIVATE(cx, obj);
22634:     if (FUN_INTERPRETED(fun))
22634:         return record_JSOP_CALL();
22634: 
22634:     bool apply = (JSFastNative)fun->u.n.native == js_fun_apply;
22634:     if (!apply && (JSFastNative)fun->u.n.native != js_fun_call)
22634:         return record_JSOP_CALL();
22634: 
22634:     /*
23097:      * We don't trace apply and call with a primitive 'this', which is the
23097:      * first positional parameter.
22634:      */
23097:     if (argc > 0 && JSVAL_IS_PRIMITIVE(vp[2]))
23097:         return record_JSOP_CALL();
23097: 
23097:     /*
23097:      * Guard on the identity of this, which is the function we are applying.
23097:      */
23712:     if (!VALUE_IS_FUNCTION(cx, vp[1]))
23712:         ABORT_TRACE("callee is not a function");
27933:     CHECK_STATUS(guardCallee(vp[1]));
23097: 
22634:     if (apply && argc >= 2) {
23097:         if (argc != 2)
23097:             ABORT_TRACE("apply with excess arguments");
22634:         if (JSVAL_IS_PRIMITIVE(vp[3]))
22634:             ABORT_TRACE("arguments parameter of apply is primitive");
22634:         aobj = JSVAL_TO_OBJECT(vp[3]);
22634:         aobj_ins = get(&vp[3]);
22634: 
22634:         /* 
30248:          * We trace dense arrays and arguments objects. The code we generate for apply
30248:          * uses imacros to handle a specific number of arguments.
30248:          */
30248:         if (OBJ_IS_DENSE_ARRAY(cx, aobj)) {
30248:             guardDenseArray(aobj, aobj_ins);
23097:             length = jsuint(aobj->fslots[JSSLOT_ARRAY_LENGTH]);
23097:             guard(true,
23097:                   lir->ins2i(LIR_eq,
22634:                              stobj_get_fslot(aobj_ins, JSSLOT_ARRAY_LENGTH),
22634:                              length),
22634:                   BRANCH_EXIT);
30248:         } else if (OBJ_GET_CLASS(cx, aobj) == &js_ArgumentsClass) {
30248:             guardClass(aobj, aobj_ins, &js_ArgumentsClass, snapshot(MISMATCH_EXIT));
30248:             JSStackFrame* afp = frameIfInRange(aobj);
30248:             if (!afp)
30248:                 ABORT_TRACE("arguments object not in range");
30248:             length = afp->argc;
30248:         } else {
30248:             ABORT_TRACE("arguments parameter of apply is not a dense array or argments object");
30248:         }
30248: 
30248:         if (length >= JS_ARRAY_LENGTH(apply_imacro_table))
30248:             ABORT_TRACE("too many arguments to apply");
22634: 
23097:         return call_imacro(apply_imacro_table[length]);
23097:     }
23097: 
23097:     if (argc >= JS_ARRAY_LENGTH(call_imacro_table))
23097:         ABORT_TRACE("too many arguments to call");
23097: 
23097:     return call_imacro(call_imacro_table[argc]);
21452: }
21452: 
26752: static JSBool FASTCALL
26752: CatchStopIteration_tn(JSContext* cx, JSBool ok, jsval* vp)
26752: {
26752:     if (!ok && cx->throwing && js_ValueIsStopIteration(cx->exception)) {
26752:         cx->throwing = JS_FALSE;
26752:         cx->exception = JSVAL_VOID;
26752:         *vp = JSVAL_HOLE;
26752:         return JS_TRUE;
26752:     }
26752:     return ok;
26752: }
26752: 
26752: JS_DEFINE_TRCINFO_1(CatchStopIteration_tn,
26752:     (3, (static, BOOL, CatchStopIteration_tn, CONTEXT, BOOL, JSVALPTR, 0, 0)))
26752: 
27933: JS_REQUIRES_STACK JSRecordingStatus
28086: TraceRecorder::record_NativeCallComplete()
28086: {
28086:     if (pendingTraceableNative == IGNORE_NATIVE_CALL_COMPLETE_CALLBACK)
28086:         return JSRS_CONTINUE;
28086: 
28086:     jsbytecode* pc = cx->fp->regs->pc;
28086: 
20405:     JS_ASSERT(pendingTraceableNative);
28086:     JS_ASSERT(*pc == JSOP_CALL || *pc == JSOP_APPLY || *pc == JSOP_NEW);
26552: 
26552:     jsval& v = stackval(-1);
26552:     LIns* v_ins = get(&v);
26552: 
20405:     /* At this point the generated code has already called the native function
20405:        and we can no longer fail back to the original pc location (JSOP_CALL)
20405:        because that would cause the interpreter to re-execute the native
20969:        function, which might have side effects.
20969: 
26972:        Instead, the snapshot() call below sees that we are currently parked on
26972:        a traceable native's JSOP_CALL instruction, and it will advance the pc
26972:        to restore by the length of the current opcode.  If the native's return
26972:        type is jsval, snapshot() will also indicate in the type map that the
26972:        element on top of the stack is a boxed value which doesn't need to be
26972:        boxed if the type guard generated by unbox_jsval() fails. */
24612: 
24612:     if (JSTN_ERRTYPE(pendingTraceableNative) == FAIL_STATUS) {
24612:         // Keep cx->bailExit null when it's invalid.
24612:         lir->insStorei(INS_CONSTPTR(NULL), cx_ins, (int) offsetof(JSContext, bailExit));
27060: 
27166:         LIns* status = lir->insLoad(LIR_ld, lirbuf->state, (int) offsetof(InterpState, builtinStatus));
26552:         if (pendingTraceableNative == generatedTraceableNative) {
26552:             LIns* ok_ins = v_ins;
26552: 
26552:             /*
26752:              * Custom implementations of Iterator.next() throw a StopIteration exception.
26752:              * Catch and clear it and set the return value to JSVAL_HOLE in this case.
26752:              */
28086:             if (uintptr_t(pc - nextiter_imacros.custom_iter_next) <
26752:                 sizeof(nextiter_imacros.custom_iter_next)) {
28086:                 LIns* args[] = { native_rval_ins, ok_ins, cx_ins }; /* reverse order */
26752:                 ok_ins = lir->insCall(&CatchStopIteration_tn_ci, args);
26752:             }
26752: 
26752:             /*
26552:              * If we run a generic traceable native, the return value is in the argument
28086:              * vector for native function calls. The actual return value of the native is a JSBool
28086:              * indicating the error status.
28086:              */
28086:             v_ins = lir->insLoad(LIR_ld, native_rval_ins, 0);
28086:             if (*pc == JSOP_NEW) {
28086:                 LIns* x = lir->ins_eq0(lir->ins2i(LIR_piand, v_ins, JSVAL_TAGMASK));
28086:                 x = lir->ins_choose(x, v_ins, INS_CONST(0));
28086:                 v_ins = lir->ins_choose(lir->ins_eq0(x), newobj_ins, x);
28086:             }
26552:             set(&v, v_ins);
26552: 
26552:             /*
26552:              * If this is a generic traceable native invocation, propagate the boolean return
28086:              * value of the native into builtinStatus. If the return value (v_ins)
26552:              * is true, status' == status. Otherwise status' = status | JSBUILTIN_ERROR.
26552:              * We calculate (rval&1)^1, which is 1 if rval is JS_FALSE (error), and then
26552:              * shift that by 1 which is JSBUILTIN_ERROR.
26552:              */
26552:             JS_STATIC_ASSERT((1 - JS_TRUE) << 1 == 0);
26552:             JS_STATIC_ASSERT((1 - JS_FALSE) << 1 == JSBUILTIN_ERROR);
26552:             status = lir->ins2(LIR_or,
26552:                                status,
26552:                                lir->ins2i(LIR_lsh,
26552:                                           lir->ins2i(LIR_xor,
26552:                                                      lir->ins2i(LIR_and, ok_ins, 1),
26552:                                                      1),
26552:                                           1));
27166:             lir->insStorei(status, lirbuf->state, (int) offsetof(InterpState, builtinStatus));
26552:         }
24612:         guard(true,
26552:               lir->ins_eq0(status),
26286:               STATUS_EXIT);
24612:     }
24612: 
27933:     JSRecordingStatus ok = JSRS_CONTINUE;
24612:     if (pendingTraceableNative->flags & JSTN_UNBOX_AFTER) {
26972:         /*
26972:          * If we side exit on the unboxing code due to a type change, make sure that the boxed
26972:          * value is actually currently associated with that location, and that we are talking
26972:          * about the top of the stack here, which is where we expected boxed values.
26972:          */
26972:         JS_ASSERT(&v == &cx->fp->regs->sp[-1] && get(&v) == v_ins);
26972:         unbox_jsval(v, v_ins, snapshot(BRANCH_EXIT));
20405:         set(&v, v_ins);
27059:     } else if (JSTN_ERRTYPE(pendingTraceableNative) == FAIL_NEG) {
27059:         /* Already added i2f in functionCall. */
27059:         JS_ASSERT(JSVAL_IS_NUMBER(v));
24612:     } else {
20966:         /* Convert the result to double if the builtin returns int32. */
20966:         if (JSVAL_IS_NUMBER(v) &&
20966:             (pendingTraceableNative->builtin->_argtypes & 3) == nanojit::ARGSIZE_LO) {
20966:             set(&v, lir->ins1(LIR_i2f, v_ins));
20966:         }
20405:     }
20405: 
23111:     // We'll null pendingTraceableNative in monitorRecording, on the next op cycle.
20969:     // There must be a next op since the stack is non-empty.
20405:     return ok;
20405: }
20405: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18096: TraceRecorder::name(jsval*& vp)
18096: {
18096:     JSObject* obj = cx->fp->scopeChain;
18096:     if (obj != globalObj)
18286:         return activeCallOrGlobalSlot(obj, vp);
18115: 
18115:     /* Can't use prop here, because we don't want unboxing from global slots. */
18286:     LIns* obj_ins = scopeChain();
18096:     uint32 slot;
27897: 
27897:     JSObject* obj2;
27897:     jsuword pcval;
27897: 
27897:     /*
27897:      * Property cache ensures that we are dealing with an existing property,
27897:      * and guards the shape for us.
27897:      */
27933:     CHECK_STATUS(test_property_cache(obj, obj_ins, obj2, pcval));
27897: 
27897:     /*
27897:      * Abort if property doesn't exist (interpreter will report an error.)
27897:      */
27897:     if (PCVAL_IS_NULL(pcval))
25633:         ABORT_TRACE("named property not found");
18096: 
27897:     /*
27897:      * Insist on obj being the directly addressed object.
27897:      */
27897:     if (obj2 != obj)
27897:         ABORT_TRACE("name() hit prototype chain");
27897: 
27897:     /* Don't trace getter or setter calls, our caller wants a direct slot. */
27897:     if (PCVAL_IS_SPROP(pcval)) {
27897:         JSScopeProperty* sprop = PCVAL_TO_SPROP(pcval);
27897:         if (!isValidSlot(OBJ_SCOPE(obj), sprop))
27897:             ABORT_TRACE("name() not accessing a valid slot");
27897:         slot = sprop->slot;
27897:     } else {
27897:         if (!PCVAL_IS_SLOT(pcval))
27897:             ABORT_TRACE("PCE is not a slot");
27897:         slot = PCVAL_TO_SLOT(pcval);
27897:     }
27897: 
18096:     if (!lazilyImportGlobalSlot(slot))
18096:         ABORT_TRACE("lazy import of global slot failed");
18096: 
18096:     vp = &STOBJ_GET_SLOT(obj, slot);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17761: TraceRecorder::prop(JSObject* obj, LIns* obj_ins, uint32& slot, LIns*& v_ins)
17758: {
17758:     /*
17758:      * Can't specialize to assert obj != global, must guard to avoid aliasing
17758:      * stale homes of stacked global variables.
17758:      */
27933:     CHECK_STATUS(guardNotGlobalObject(obj, obj_ins));
17758: 
18115:     /*
18115:      * Property cache ensures that we are dealing with an existing property,
18115:      * and guards the shape for us.
18115:      */
18115:     JSObject* obj2;
18115:     jsuword pcval;
27933:     CHECK_STATUS(test_property_cache(obj, obj_ins, obj2, pcval));
17665: 
17998:     /* Check for non-existent property reference, which results in undefined. */
17998:     const JSCodeSpec& cs = js_CodeSpec[*cx->fp->regs->pc];
18115:     if (PCVAL_IS_NULL(pcval)) {
25633:         /*
29872:          * We could specialize to guard on just JSClass.getProperty, but a mere
29872:          * class guard is simpler and slightly faster.
29872:          */
29872:         if (OBJ_GET_CLASS(cx, obj)->getProperty != JS_PropertyStub) {
29872:             ABORT_TRACE("can't trace through access to undefined property if "
29872:                         "JSClass.getProperty hook isn't stubbed");
29872:         }
29872:         guardClass(obj, obj_ins, OBJ_GET_CLASS(cx, obj), snapshot(MISMATCH_EXIT));
29872: 
29872:         /*
25633:          * This trace will be valid as long as neither the object nor any object
29872:          * on its prototype chain changes shape.
29874:          *
29874:          * FIXME: This loop can become a single shape guard once bug 497789 has
29874:          * been fixed.
25633:          */
27540:         VMSideExit* exit = snapshot(BRANCH_EXIT);
29513:         do {
30244:             LIns* map_ins = map(obj_ins);
25633:             LIns* ops_ins;
25636:             if (map_is_native(obj->map, map_ins, ops_ins)) {
25633:                 LIns* shape_ins = addName(lir->insLoad(LIR_ld, map_ins, offsetof(JSScope, shape)),
25633:                                           "shape");
25633:                 guard(true,
25633:                       addName(lir->ins2i(LIR_eq, shape_ins, OBJ_SHAPE(obj)), "guard(shape)"),
25886:                       exit);
25636:             } else if (!guardDenseArray(obj, obj_ins, BRANCH_EXIT))
25636:                 ABORT_TRACE("non-native object involved in undefined property access");
29513:         } while (guardHasPrototype(obj, obj_ins, &obj, &obj_ins, exit));
25633: 
24846:         v_ins = INS_CONST(JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID));
18666:         slot = SPROP_INVALID_SLOT;
27933:         return JSRS_CONTINUE;
17998:     }
17998: 
18115:     /* Insist if setting on obj being the directly addressed object. */
23085:     uint32 setflags = (cs.format & (JOF_SET | JOF_INCDEC | JOF_FOR));
18143:     LIns* dslots_ins = NULL;
18143: 
18115:     /* Don't trace getter or setter calls, our caller wants a direct slot. */
18115:     if (PCVAL_IS_SPROP(pcval)) {
18115:         JSScopeProperty* sprop = PCVAL_TO_SPROP(pcval);
18115: 
18115:         if (setflags && !SPROP_HAS_STUB_SETTER(sprop))
18115:             ABORT_TRACE("non-stub setter");
23085:         if (setflags && (sprop->attrs & JSPROP_READONLY))
23085:             ABORT_TRACE("writing to a readonly property");
18115:         if (setflags != JOF_SET && !SPROP_HAS_STUB_GETTER(sprop)) {
18115:             // FIXME 450335: generalize this away from regexp built-in getters.
18115:             if (setflags == 0 &&
18115:                 sprop->getter == js_RegExpClass.getProperty &&
18115:                 sprop->shortid < 0) {
19986:                 if (sprop->shortid == REGEXP_LAST_INDEX)
27637:                     ABORT_TRACE("can't trace RegExp.lastIndex yet");
18115:                 LIns* args[] = { INS_CONSTPTR(sprop), obj_ins, cx_ins };
20915:                 v_ins = lir->insCall(&js_CallGetter_ci, args);
18766:                 guard(false, lir->ins2(LIR_eq, v_ins, INS_CONST(JSVAL_ERROR_COOKIE)), OOM_EXIT);
26972:                 /*
26972:                  * BIG FAT WARNING: This snapshot cannot be a BRANCH_EXIT, since
26972:                  * the value to the top of the stack is not the value we unbox.
26972:                  */
23710:                 unbox_jsval((sprop->shortid == REGEXP_SOURCE) ? JSVAL_STRING : JSVAL_BOOLEAN,
26972:                             v_ins,
26972:                             snapshot(MISMATCH_EXIT));
27933:                 return JSRS_CONTINUE;
18115:             }
27637:             if (setflags == 0 &&
27637:                 sprop->getter == js_StringClass.getProperty &&
27637:                 sprop->id == ATOM_KEY(cx->runtime->atomState.lengthAtom)) {
27637:                 if (!guardClass(obj, obj_ins, &js_StringClass, snapshot(MISMATCH_EXIT)))
27637:                     ABORT_TRACE("can't trace String.length on non-String objects");
30248:                 LIns* str_ins = stobj_get_private(obj_ins, JSVAL_TAGMASK);
27637:                 v_ins = lir->ins1(LIR_i2f, getStringLength(str_ins));
27933:                 return JSRS_CONTINUE;
27637:             }
18115:             ABORT_TRACE("non-stub getter");
18115:         }
28909:         if (!SPROP_HAS_VALID_SLOT(sprop, OBJ_SCOPE(obj2)))
18115:             ABORT_TRACE("no valid slot");
18115:         slot = sprop->slot;
18115:     } else {
18115:         if (!PCVAL_IS_SLOT(pcval))
18115:             ABORT_TRACE("PCE is not a slot");
18115:         slot = PCVAL_TO_SLOT(pcval);
18115:     }
18115: 
25092:     if (obj2 != obj) {
25092:         if (setflags)
25092:             ABORT_TRACE("JOF_SET opcode hit prototype chain");
25092: 
25092:         /*
25092:          * We're getting a proto-property. Walk up the prototype chain emitting
25092:          * proto slot loads, updating obj as we go, leaving obj set to obj2 with
25092:          * obj_ins the last proto-load.
25092:          */
25092:         while (obj != obj2) {
25092:             obj_ins = stobj_get_slot(obj_ins, JSSLOT_PROTO, dslots_ins);
25092:             obj = STOBJ_GET_PROTO(obj);
25092:         }
25092:     }
25092: 
17758:     v_ins = stobj_get_slot(obj_ins, slot, dslots_ins);
26972:     unbox_jsval(STOBJ_GET_SLOT(obj, slot), v_ins, snapshot(BRANCH_EXIT));
26972: 
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
28411: TraceRecorder::denseArrayElement(jsval& oval, jsval& ival, jsval*& vp, LIns*& v_ins,
28411:                                  LIns*& addr_ins)
28411: {
28411:     JS_ASSERT(JSVAL_IS_OBJECT(oval) && JSVAL_IS_INT(ival));
19983: 
19983:     JSObject* obj = JSVAL_TO_OBJECT(oval);
19983:     LIns* obj_ins = get(&oval);
27891:     jsint idx = JSVAL_TO_INT(ival);
27891:     LIns* idx_ins = makeNumberInt32(get(&ival));
17758: 
27891:     VMSideExit* exit = snapshot(BRANCH_EXIT);
27891: 
17758:     /* check that the index is within bounds */
18230:     LIns* dslots_ins = lir->insLoad(LIR_ldp, obj_ins, offsetof(JSObject, dslots));
27891:     jsuint capacity = js_DenseArrayCapacity(obj);
27891:     bool within = (jsuint(idx) < jsuint(obj->fslots[JSSLOT_ARRAY_LENGTH]) && jsuint(idx) < capacity);
27891:     if (!within) {
27891:         /* If idx < 0, stay on trace (and read value as undefined, since this is a dense array). */
27891:         LIns* br1 = NULL;
27892:         if (MAX_DSLOTS_LENGTH > JS_BITMASK(30) && !idx_ins->isconst()) {
27891:             JS_ASSERT(sizeof(jsval) == 8); // Only 64-bit machines support large enough arrays for this.
27891:             br1 = lir->insBranch(LIR_jt,
27891:                                  lir->ins2i(LIR_lt, idx_ins, 0),
27891:                                  NULL);
27891:         }
27891: 
27891:         /* If not idx < length, stay on trace (and read value as undefined). */
27891:         LIns* br2 = lir->insBranch(LIR_jf,
27891:                                    lir->ins2(LIR_ult,
27891:                                              idx_ins,
27891:                                              stobj_get_fslot(obj_ins, JSSLOT_ARRAY_LENGTH)),
27891:                                    NULL);
27891: 
27891:         /* If dslots is NULL, stay on trace (and read value as undefined). */
27891:         LIns* br3 = lir->insBranch(LIR_jt, lir->ins_eq0(dslots_ins), NULL);
27891: 
27891:         /* If not idx < capacity, stay on trace (and read value as undefined). */
27891:         LIns* br4 = lir->insBranch(LIR_jf,
27891:                                    lir->ins2(LIR_ult,
27891:                                              idx_ins,
27891:                                              lir->insLoad(LIR_ldp,
27891:                                                           dslots_ins,
27891:                                                           -(int)sizeof(jsval))),
27891:                                    NULL);
30452:         lir->insGuard(LIR_x, NULL, createGuardRecord(exit));
27891:         LIns* label = lir->ins0(LIR_label);
27891:         if (br1)
28182:             br1->setTarget(label);
28182:         br2->setTarget(label);
28182:         br3->setTarget(label);
28182:         br4->setTarget(label);
27891: 
27933:         CHECK_STATUS(guardPrototypeHasNoIndexedProperties(obj, obj_ins, MISMATCH_EXIT));
25883: 
20972:         // Return undefined and indicate that we didn't actually read this (addr_ins).
24846:         v_ins = lir->insImm(JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID));
20972:         addr_ins = NULL;
27933:         return JSRS_CONTINUE;
20972:     }
20404: 
27891:     /* Guard against negative index */
27892:     if (MAX_DSLOTS_LENGTH > JS_BITMASK(30) && !idx_ins->isconst()) {
27891:         JS_ASSERT(sizeof(jsval) == 8); // Only 64-bit machines support large enough arrays for this.
27891:         guard(false,
27891:               lir->ins2i(LIR_lt, idx_ins, 0),
27891:               exit);
27891:     }
27891: 
27891:     /* Guard array length */
27891:     guard(true,
27891:           lir->ins2(LIR_ult, idx_ins, stobj_get_fslot(obj_ins, JSSLOT_ARRAY_LENGTH)),
27891:           exit);
27891: 
27891:     /* dslots must not be NULL */
27891:     guard(false,
27891:           lir->ins_eq0(dslots_ins),
27891:           exit);
27891: 
27891:     /* Guard array capacity */
27891:     guard(true,
27891:           lir->ins2(LIR_ult,
27891:                     idx_ins,
27891:                     lir->insLoad(LIR_ldp, dslots_ins, 0 - (int)sizeof(jsval))),
27891:           exit);
27891: 
27891:     /* Load the value and guard on its type to unbox it. */
27891:     vp = &obj->dslots[jsuint(idx)];
18232:     addr_ins = lir->ins2(LIR_piadd, dslots_ins,
18232:                          lir->ins2i(LIR_pilsh, idx_ins, (sizeof(jsval) == 4) ? 2 : 3));
18230:     v_ins = lir->insLoad(LIR_ldp, addr_ins, 0);
27891:     unbox_jsval(*vp, v_ins, exit);
20404: 
19053:     if (JSVAL_TAG(*vp) == JSVAL_BOOLEAN) {
27891:         /*
27891:          * If we read a hole from the array, convert it to undefined and guard that there
27891:          * are no indexed properties along the prototype chain.
27891:          */
27891:         LIns* br = lir->insBranch(LIR_jf,
27891:                                   lir->ins2i(LIR_eq, v_ins, JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_HOLE)),
27891:                                   NULL);
27933:         CHECK_STATUS(guardPrototypeHasNoIndexedProperties(obj, obj_ins, MISMATCH_EXIT));
28182:         br->setTarget(lir->ins0(LIR_label));
27891: 
27891:         /*
27891:          * Don't let the hole value escape. Turn it into an undefined.
27891:          */
27891:         v_ins = lir->ins2i(LIR_and, v_ins, ~(JSVAL_HOLE_FLAG >> JSVAL_TAGBITS));
19053:     }
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17758: TraceRecorder::getProp(JSObject* obj, LIns* obj_ins)
17758: {
17761:     uint32 slot;
17758:     LIns* v_ins;
27933:     CHECK_STATUS(prop(obj, obj_ins, slot, v_ins));
17758: 
17758:     const JSCodeSpec& cs = js_CodeSpec[*cx->fp->regs->pc];
17758:     JS_ASSERT(cs.ndefs == 1);
17758:     stack(-cs.nuses, v_ins);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17688: TraceRecorder::getProp(jsval& v)
17665: {
17665:     if (JSVAL_IS_PRIMITIVE(v))
17665:         ABORT_TRACE("primitive lhs");
17665: 
17688:     return getProp(JSVAL_TO_OBJECT(v), get(&v));
17665: }
17665: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_NAME()
17409: {
18096:     jsval* vp;
27933:     CHECK_STATUS(name(vp));
18096:     stack(0, get(vp));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_DOUBLE()
17409: {
17611:     jsval v = jsval(atoms[GET_INDEX(cx->fp->regs->pc)]);
26265:     stack(0, lir->insImmf(*JSVAL_TO_DOUBLE(v)));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_STRING()
17409: {
17902:     JSAtom* atom = atoms[GET_INDEX(cx->fp->regs->pc)];
17902:     JS_ASSERT(ATOM_IS_STRING(atom));
18712:     stack(0, INS_CONSTPTR(ATOM_TO_STRING(atom)));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_ZERO()
17409: {
26265:     stack(0, lir->insImmq(0));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_ONE()
17409: {
26265:     stack(0, lir->insImmf(1));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_NULL()
17409: {
18712:     stack(0, INS_CONSTPTR(NULL));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_THIS()
17409: {
17688:     LIns* this_ins;
27933:     CHECK_STATUS(getThis(this_ins));
17688:     stack(0, this_ins);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_FALSE()
17409: {
17418:     stack(0, lir->insImm(0));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_TRUE()
17409: {
17418:     stack(0, lir->insImm(1));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_OR()
17409: {
20416:     return ifop();
17409: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_AND()
17409: {
20416:     return ifop();
17409: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_TABLESWITCH()
17409: {
25099: #ifdef NANOJIT_IA32
26557:     /* Handle tableswitches specially -- prepare a jump table if needed. */
26557:     LIns* guardIns = tableswitch();
26557:     if (guardIns) {
26557:         fragment->lastIns = guardIns;
26557:         compile(&JS_TRACE_MONITOR(cx));
26557:     }
27933:     return JSRS_STOP;
25099: #else
18687:     return switchop();
25099: #endif
17409: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_LOOKUPSWITCH()
17409: {
18687:     return switchop();
17409: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_STRICTEQ()
17409: {
23093:     strictEquality(true, false);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_STRICTNE()
17409: {
23093:     strictEquality(false, false);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_OBJECT()
17409: {
18027:     JSStackFrame* fp = cx->fp;
18027:     JSScript* script = fp->script;
18027:     unsigned index = atoms - script->atomMap.vector + GET_INDEX(fp->regs->pc);
18027: 
18027:     JSObject* obj;
18027:     JS_GET_SCRIPT_OBJECT(script, index, obj);
18712:     stack(0, INS_CONSTPTR(obj));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_POP()
17409: {
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_TRAP()
17899: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_GETARG()
17409: {
17412:     stack(0, arg(GET_ARGNO(cx->fp->regs->pc)));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_SETARG()
17409: {
17415:     arg(GET_ARGNO(cx->fp->regs->pc), stack(-1));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18005: TraceRecorder::record_JSOP_GETLOCAL()
17409: {
17807:     stack(0, var(GET_SLOTNO(cx->fp->regs->pc)));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18005: TraceRecorder::record_JSOP_SETLOCAL()
17409: {
17807:     var(GET_SLOTNO(cx->fp->regs->pc), stack(-1));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_UINT16()
17409: {
26265:     stack(0, lir->insImmf(GET_UINT16(cx->fp->regs->pc)));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_NEWINIT()
17409: {
18036:     JSProtoKey key = JSProtoKey(GET_INT8(cx->fp->regs->pc));
27012:     LIns *proto_ins;
27933:     CHECK_STATUS(getClassPrototype(key, proto_ins));
27012: 
27012:     LIns* args[] = { proto_ins, cx_ins };
28086:     const CallInfo *ci = (key == JSProto_Array) ? &js_NewEmptyArray_ci : &js_Object_tn_ci;
20408:     LIns* v_ins = lir->insCall(ci, args);
18036:     guard(false, lir->ins_eq0(v_ins), OOM_EXIT);
18036:     stack(0, v_ins);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_ENDINIT()
17409: {
24198: #ifdef DEBUG
18712:     jsval& v = stackval(-1);
18712:     JS_ASSERT(!JSVAL_IS_PRIMITIVE(v));
24198: #endif
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_INITPROP()
17409: {
19093:     // All the action is in record_SetPropHit.
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_INITELEM()
17409: {
18036:     return record_JSOP_SETELEM();
17409: }
17899: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_DEFSHARP()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_USESHARP()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_INCARG()
17409: {
17544:     return inc(argval(GET_ARGNO(cx->fp->regs->pc)), 1);
17409: }
17899: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18005: TraceRecorder::record_JSOP_INCLOCAL()
17409: {
17807:     return inc(varval(GET_SLOTNO(cx->fp->regs->pc)), 1);
17409: }
17899: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_DECARG()
17409: {
17544:     return inc(argval(GET_ARGNO(cx->fp->regs->pc)), -1);
17409: }
17899: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18005: TraceRecorder::record_JSOP_DECLOCAL()
17409: {
17807:     return inc(varval(GET_SLOTNO(cx->fp->regs->pc)), -1);
17409: }
17899: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_ARGINC()
17409: {
17412:     return inc(argval(GET_ARGNO(cx->fp->regs->pc)), 1, false);
17409: }
17899: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18005: TraceRecorder::record_JSOP_LOCALINC()
17409: {
17807:     return inc(varval(GET_SLOTNO(cx->fp->regs->pc)), 1, false);
17409: }
17899: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_ARGDEC()
17409: {
17412:     return inc(argval(GET_ARGNO(cx->fp->regs->pc)), -1, false);
17409: }
17899: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18005: TraceRecorder::record_JSOP_LOCALDEC()
17409: {
17807:     return inc(varval(GET_SLOTNO(cx->fp->regs->pc)), -1, false);
17409: }
17899: 
27933: JS_REQUIRES_STACK JSRecordingStatus
21685: TraceRecorder::record_JSOP_IMACOP()
21685: {
21685:     JS_ASSERT(cx->fp->imacpc);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_ITER()
17409: {
17958:     jsval& v = stackval(-1);
24384:     if (JSVAL_IS_PRIMITIVE(v))
24384:         ABORT_TRACE("for-in on a primitive value");
28175:     ABORT_IF_XML(v);
24384: 
18136:     jsuint flags = cx->fp->regs->pc[1];
21685: 
24384:     if (hasIteratorMethod(JSVAL_TO_OBJECT(v))) {
21685:         if (flags == JSITER_ENUMERATE)
21685:             return call_imacro(iter_imacros.for_in);
21686:         if (flags == (JSITER_ENUMERATE | JSITER_FOREACH))
21685:             return call_imacro(iter_imacros.for_each);
24384:     } else {
24384:         if (flags == JSITER_ENUMERATE)
24384:             return call_imacro(iter_imacros.for_in_native);
24384:         if (flags == (JSITER_ENUMERATE | JSITER_FOREACH))
24384:             return call_imacro(iter_imacros.for_each_native);
24384:     }
21685:     ABORT_TRACE("unimplemented JSITER_* flags");
21685: }
21685: 
27933: JS_REQUIRES_STACK JSRecordingStatus
24384: TraceRecorder::record_JSOP_NEXTITER()
24384: {
24384:     jsval& iterobj_val = stackval(-2);
24384:     if (JSVAL_IS_PRIMITIVE(iterobj_val))
24310:         ABORT_TRACE("for-in on a primitive value");
28175:     ABORT_IF_XML(iterobj_val);
26752:     JSObject* iterobj = JSVAL_TO_OBJECT(iterobj_val);
26752:     JSClass* clasp = STOBJ_GET_CLASS(iterobj);
24310:     LIns* iterobj_ins = get(&iterobj_val);
26752:     if (clasp == &js_IteratorClass || clasp == &js_GeneratorClass) {
26752:         guardClass(iterobj, iterobj_ins, clasp, snapshot(BRANCH_EXIT));
24384:         return call_imacro(nextiter_imacros.native_iter_next);
25879:     }
24384:     return call_imacro(nextiter_imacros.custom_iter_next);
21441: }
21441: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18085: TraceRecorder::record_JSOP_ENDITER()
18085: {
21441:     LIns* args[] = { stack(-2), cx_ins };
20915:     LIns* ok_ins = lir->insCall(&js_CloseIterator_ci, args);
18085:     guard(false, lir->ins_eq0(ok_ins), MISMATCH_EXIT);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18085: TraceRecorder::record_JSOP_FORNAME()
18085: {
18136:     jsval* vp;
27933:     CHECK_STATUS(name(vp));
21441:     set(vp, stack(-1));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18136: TraceRecorder::record_JSOP_FORPROP()
18136: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18136: TraceRecorder::record_JSOP_FORELEM()
18136: {
21441:     return record_JSOP_DUP();
18085: }
18085: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18085: TraceRecorder::record_JSOP_FORARG()
18085: {
21441:     return record_JSOP_SETARG();
18085: }
18085: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18085: TraceRecorder::record_JSOP_FORLOCAL()
18085: {
21441:     return record_JSOP_SETLOCAL();
17899: }
17899: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_POPN()
17409: {
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_BINDNAME()
17409: {
27638:     JSStackFrame *fp = cx->fp;
27881:     JSObject *obj;
27638: 
27638:     if (fp->fun) {
27638:         // We can't trace BINDNAME in functions that contain direct
27638:         // calls to eval, as they might add bindings which
27638:         // previously-traced references would have to see.
27638:         if (JSFUN_HEAVYWEIGHT_TEST(fp->fun->flags))
27638:             ABORT_TRACE("Can't trace JSOP_BINDNAME in heavyweight functions.");
27638: 
27638:         // In non-heavyweight functions, we can safely skip the call
27638:         // object, if any.
27881:         obj = OBJ_GET_PARENT(cx, fp->callee);
27638:     } else {
27881:         obj = fp->scopeChain;
27638: 
27638:         // In global code, fp->scopeChain can only contain blocks
27638:         // whose values are still on the stack.  We never use BINDNAME
27638:         // to refer to these.
27881:         while (OBJ_GET_CLASS(cx, obj) == &js_BlockClass) {
27638:             // The block's values are still on the stack.
27881:             JS_ASSERT(OBJ_GET_PRIVATE(cx, obj) == fp);
27881: 
27881:             obj = OBJ_GET_PARENT(cx, obj);
27638: 
27638:             // Blocks always have parents.
27881:             JS_ASSERT(obj);
27881:         }
27881:     }
27881: 
27881:     if (obj != globalObj)
27638:         ABORT_TRACE("JSOP_BINDNAME must return global object on trace");
27638: 
27638:     // The trace is specialized to this global object.  Furthermore,
27638:     // we know it is the sole 'global' object on the scope chain: we
27638:     // set globalObj to the scope chain element with no parent, and we
27638:     // reached it starting from the function closure or the current
27638:     // scopeChain, so there is nothing inner to it.  So this must be
27638:     // the right base object.
27638:     stack(0, INS_CONSTPTR(globalObj));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_SETNAME()
17409: {
17545:     jsval& l = stackval(-2);
18085:     JS_ASSERT(!JSVAL_IS_PRIMITIVE(l));
17519: 
17541:     /*
17541:      * Trace cases that are global code or in lightweight functions scoped by
17541:      * the global object only.
17541:      */
17541:     JSObject* obj = JSVAL_TO_OBJECT(l);
17657:     if (obj != cx->fp->scopeChain || obj != globalObj)
19093:         ABORT_TRACE("JSOP_SETNAME left operand is not the global object");
19093: 
19093:     // The rest of the work is in record_SetPropHit.
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_THROW()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_IN()
17409: {
18495:     jsval& rval = stackval(-1);
21784:     jsval& lval = stackval(-2);
21784: 
18495:     if (JSVAL_IS_PRIMITIVE(rval))
18495:         ABORT_TRACE("JSOP_IN on non-object right operand");
21784:     JSObject* obj = JSVAL_TO_OBJECT(rval);
21784:     LIns* obj_ins = get(&rval);
18495: 
18495:     jsid id;
21784:     LIns* x;
18495:     if (JSVAL_IS_INT(lval)) {
18495:         id = INT_JSVAL_TO_JSID(lval);
21784:         LIns* args[] = { makeNumberInt32(get(&lval)), obj_ins, cx_ins };
21784:         x = lir->insCall(&js_HasNamedPropertyInt32_ci, args);
21784:     } else if (JSVAL_IS_STRING(lval)) {
21784:         if (!js_ValueToStringId(cx, lval, &id))
27933:             ABORT_TRACE_ERROR("left operand of JSOP_IN didn't convert to a string-id");
21784:         LIns* args[] = { get(&lval), obj_ins, cx_ins };
21784:         x = lir->insCall(&js_HasNamedProperty_ci, args);
18495:     } else {
21784:         ABORT_TRACE("string or integer expected");
21784:     }
21784: 
24846:     guard(false, lir->ins2i(LIR_eq, x, JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID)), OOM_EXIT);
21784:     x = lir->ins2i(LIR_eq, x, 1);
18495: 
18495:     JSObject* obj2;
18495:     JSProperty* prop;
18495:     if (!OBJ_LOOKUP_PROPERTY(cx, obj, id, &obj2, &prop))
27933:         ABORT_TRACE_ERROR("OBJ_LOOKUP_PROPERTY failed in JSOP_IN");
21784:     bool cond = prop != NULL;
18495:     if (prop)
18495:         OBJ_DROP_PROPERTY(cx, obj2, prop);
29906:     if (wasDeepAborted())
29906:         ABORT_TRACE("deep abort from property lookup");
18495: 
18495:     /* The interpreter fuses comparisons and the following branch,
18495:        so we have to do that here as well. */
18694:     fuseIf(cx->fp->regs->pc + 1, cond, x);
18495: 
18495:     /* We update the stack after the guard. This is safe since
18495:        the guard bails out at the comparison and the interpreter
18680:        will therefore re-execute the comparison. This way the
18495:        value of the condition doesn't have to be calculated and
18495:        saved on the stack in most cases. */
18495:     set(&lval, x);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
29363: static JSBool
29363: HasInstance(JSContext *cx, uintN argc, jsval *vp)
29363: {
29363:     jsval *argv;
29363:     JS_ASSERT(argc == 1);
29363:     argv = JS_ARGV(cx, vp);
29363:     JSBool result = JS_FALSE;
29363:     JSObject* obj = JS_THIS_OBJECT(cx, vp);
29363:     if (!obj->map->ops->hasInstance(cx, obj, argv[0], &result))
29363:         return JS_FALSE;
29363:     JS_SET_RVAL(cx, vp, BOOLEAN_TO_JSVAL(result));
29363:     return JS_TRUE;
29363: }
29363: 
29363: static JSBool FASTCALL
29363: HasInstance_tn(JSContext* cx, JSObject* obj, jsval v)
29363: {
29363:     JSBool result = JS_FALSE;
29363:     if (!obj->map->ops->hasInstance(cx, obj, v, &result))
29363:         js_SetBuiltinError(cx);
29363:     return result;
29363: }
29363: 
29363: JS_DEFINE_TRCINFO_1(HasInstance,
29363:     (3, (extern, BOOL_FAIL, HasInstance_tn, CONTEXT, THIS, JSVAL, 0, 0)))
29363: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_INSTANCEOF()
17409: {
29363:     jsval& r = stackval(-1);
29363: 
29363:     if (!JSVAL_IS_PRIMITIVE(r))
29363:         return call_imacro(instanceof_imacros.instanceof);
29363: 
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_DEBUGGER()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_GOSUB()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_RETSUB()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_EXCEPTION()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_LINENO()
17409: {
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_CONDSWITCH()
17409: {
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_CASE()
17409: {
23093:     strictEquality(true, true);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_DEFAULT()
17409: {
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_EVAL()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_ENUMELEM()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_GETTER()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_SETTER()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_DEFFUN()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
27012: TraceRecorder::record_JSOP_DEFFUN_FC()
27012: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_DEFCONST()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_DEFVAR()
17409: {
27933:     return JSRS_STOP;
17409: }
17926: 
27012: jsatomid
27012: TraceRecorder::getFullIndex(ptrdiff_t pcoff)
27012: {
27012:     jsatomid index = GET_INDEX(cx->fp->regs->pc + pcoff);
27012:     index += atoms - cx->fp->script->atomMap.vector;
27012:     return index;
27012: }
27012: 
27933: JS_REQUIRES_STACK JSRecordingStatus
27012: TraceRecorder::record_JSOP_LAMBDA()
17409: {
17763:     JSFunction* fun;
27012:     JS_GET_SCRIPT_FUNCTION(cx->fp->script, getFullIndex(), fun);
27012: 
27012:     if (FUN_NULL_CLOSURE(fun) && OBJ_GET_PARENT(cx, FUN_OBJECT(fun)) == globalObj) {
27012:         LIns *proto_ins;
27933:         CHECK_STATUS(getClassPrototype(JSProto_Function, proto_ins));
27012: 
27012:         LIns* args[] = { INS_CONSTPTR(globalObj), proto_ins, INS_CONSTPTR(fun), cx_ins };
27012:         LIns* x = lir->insCall(&js_NewNullClosure_ci, args);
27012:         stack(0, x);
27933:         return JSRS_CONTINUE;
27933:     }
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
27012: TraceRecorder::record_JSOP_LAMBDA_FC()
27012: {
28923:     JSFunction* fun;
28923:     JS_GET_SCRIPT_FUNCTION(cx->fp->script, getFullIndex(), fun);
28923: 
28923:     LIns* scopeChain_ins = get(&cx->fp->argv[-2]);
28923:     JS_ASSERT(scopeChain_ins);
28923: 
28923:     LIns* args[] = {
28923:         scopeChain_ins,
28923:         INS_CONSTPTR(fun),
28923:         cx_ins
28923:     };
28923:     LIns* call_ins = lir->insCall(&js_AllocFlatClosure_ci, args);
28923:     guard(false,
28923:           addName(lir->ins2(LIR_eq, call_ins, INS_CONSTPTR(0)),
28923:                   "guard(js_AllocFlatClosure)"),
28923:           OOM_EXIT);
28923:     stack(0, call_ins);
28923: 
28923:     JSUpvarArray *uva = JS_SCRIPT_UPVARS(fun->u.i.script);
28923:     for (uint32 i = 0, n = uva->length; i < n; i++) {
28923:         jsval v;
28923:         LIns* upvar_ins = upvar(fun->u.i.script, uva, i, v);
28923:         if (!upvar_ins)
27933:             return JSRS_STOP;
28923:         box_jsval(v, upvar_ins);
28923:         LIns* dslots_ins = NULL;
28923:         stobj_set_dslot(call_ins, i, dslots_ins, upvar_ins, "fc upvar");
28923:     }
28923: 
28923:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
27012: TraceRecorder::record_JSOP_CALLEE()
27012: {
28557:     stack(0, get(&cx->fp->argv[-2]));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18020: TraceRecorder::record_JSOP_SETLOCALPOP()
18020: {
20394:     var(GET_SLOTNO(cx->fp->regs->pc), stack(-1));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
23075: TraceRecorder::record_JSOP_IFPRIMTOP()
23075: {
23075:     // Traces are type-specialized, including null vs. object, so we need do
23075:     // nothing here. The upstream unbox_jsval called after valueOf or toString
23075:     // from an imacro (e.g.) will fork the trace for us, allowing us to just
23075:     // follow along mindlessly :-).
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18020: TraceRecorder::record_JSOP_SETCALL()
18020: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18020: TraceRecorder::record_JSOP_TRY()
18020: {
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18020: TraceRecorder::record_JSOP_FINALLY()
18020: {
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18020: TraceRecorder::record_JSOP_NOP()
18020: {
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18020: TraceRecorder::record_JSOP_ARGSUB()
18020: {
19068:     JSStackFrame* fp = cx->fp;
19068:     if (!(fp->fun->flags & JSFUN_HEAVYWEIGHT)) {
19068:         uintN slot = GET_ARGNO(fp->regs->pc);
30248:         if (slot < fp->argc)
19068:             stack(0, get(&cx->fp->argv[slot]));
30248:         else
30248:             stack(0, INS_VOID());
30248:         return JSRS_CONTINUE;
19068:     }
19068:     ABORT_TRACE("can't trace JSOP_ARGSUB hard case");
18020: }
18020: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18020: TraceRecorder::record_JSOP_ARGCNT()
18020: {
19068:     if (!(cx->fp->fun->flags & JSFUN_HEAVYWEIGHT)) {
26265:         stack(0, lir->insImmf(cx->fp->argc));
27933:         return JSRS_CONTINUE;
19068:     }
19068:     ABORT_TRACE("can't trace heavyweight JSOP_ARGCNT");
18020: }
18020: 
27933: JS_REQUIRES_STACK JSRecordingStatus
19970: TraceRecorder::record_DefLocalFunSetSlot(uint32 slot, JSObject* obj)
19970: {
27012:     JSFunction* fun = GET_FUNCTION_PRIVATE(cx, obj);
27012: 
27012:     if (FUN_NULL_CLOSURE(fun) && OBJ_GET_PARENT(cx, FUN_OBJECT(fun)) == globalObj) {
27012:         LIns *proto_ins;
27933:         CHECK_STATUS(getClassPrototype(JSProto_Function, proto_ins));
27012: 
27012:         LIns* args[] = { INS_CONSTPTR(globalObj), proto_ins, INS_CONSTPTR(fun), cx_ins };
27012:         LIns* x = lir->insCall(&js_NewNullClosure_ci, args);
27012:         var(slot, x);
27933:         return JSRS_CONTINUE;
27933:     }
27933: 
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18020: TraceRecorder::record_JSOP_DEFLOCALFUN()
18020: {
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
27012: TraceRecorder::record_JSOP_DEFLOCALFUN_FC()
27012: {
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_GOTOX()
17409: {
26557:     return record_JSOP_GOTO();
17409: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_IFEQX()
17409: {
17611:     return record_JSOP_IFEQ();
17611: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_IFNEX()
17409: {
17611:     return record_JSOP_IFNE();
17611: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_ORX()
17409: {
17611:     return record_JSOP_OR();
17611: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_ANDX()
17409: {
17611:     return record_JSOP_AND();
17611: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_GOSUBX()
17409: {
17611:     return record_JSOP_GOSUB();
17611: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_CASEX()
17409: {
23093:     strictEquality(true, true);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_DEFAULTX()
17409: {
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_TABLESWITCHX()
17409: {
26557:     return record_JSOP_TABLESWITCH();
17611: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_LOOKUPSWITCHX()
17409: {
18687:     return switchop();
17611: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_BACKPATCH()
17409: {
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_BACKPATCH_POP()
17409: {
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_THROWING()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_SETRVAL()
17409: {
17844:     // If we implement this, we need to update JSOP_STOP.
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_RETRVAL()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_GETGVAR()
17409: {
17807:     jsval slotval = cx->fp->slots[GET_SLOTNO(cx->fp->regs->pc)];
17468:     if (JSVAL_IS_NULL(slotval))
27933:         return JSRS_CONTINUE; // We will see JSOP_NAME from the interpreter's jump, so no-op here.
17545: 
17468:     uint32 slot = JSVAL_TO_INT(slotval);
17891: 
17892:     if (!lazilyImportGlobalSlot(slot))
17891:          ABORT_TRACE("lazy import of global slot failed");
17891: 
19973:     stack(0, get(&STOBJ_GET_SLOT(globalObj, slot)));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_SETGVAR()
17409: {
17807:     jsval slotval = cx->fp->slots[GET_SLOTNO(cx->fp->regs->pc)];
17494:     if (JSVAL_IS_NULL(slotval))
27933:         return JSRS_CONTINUE; // We will see JSOP_NAME from the interpreter's jump, so no-op here.
17545: 
17494:     uint32 slot = JSVAL_TO_INT(slotval);
17891: 
17892:     if (!lazilyImportGlobalSlot(slot))
17891:          ABORT_TRACE("lazy import of global slot failed");
17891: 
19973:     set(&STOBJ_GET_SLOT(globalObj, slot), stack(-1));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_INCGVAR()
17409: {
17807:     jsval slotval = cx->fp->slots[GET_SLOTNO(cx->fp->regs->pc)];
17468:     if (JSVAL_IS_NULL(slotval))
27933:         // We will see JSOP_INCNAME from the interpreter's jump, so no-op here.
27933:         return JSRS_CONTINUE;
17545: 
17468:     uint32 slot = JSVAL_TO_INT(slotval);
17891: 
17892:     if (!lazilyImportGlobalSlot(slot))
17891:          ABORT_TRACE("lazy import of global slot failed");
17891: 
19973:     return inc(STOBJ_GET_SLOT(globalObj, slot), 1);
17409: }
17545: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_DECGVAR()
17409: {
17807:     jsval slotval = cx->fp->slots[GET_SLOTNO(cx->fp->regs->pc)];
17494:     if (JSVAL_IS_NULL(slotval))
27933:         // We will see JSOP_INCNAME from the interpreter's jump, so no-op here.
27933:         return JSRS_CONTINUE;
17545: 
17494:     uint32 slot = JSVAL_TO_INT(slotval);
17891: 
17892:     if (!lazilyImportGlobalSlot(slot))
17891:          ABORT_TRACE("lazy import of global slot failed");
17891: 
19973:     return inc(STOBJ_GET_SLOT(globalObj, slot), -1);
17409: }
17545: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_GVARINC()
17409: {
17807:     jsval slotval = cx->fp->slots[GET_SLOTNO(cx->fp->regs->pc)];
17494:     if (JSVAL_IS_NULL(slotval))
27933:         // We will see JSOP_INCNAME from the interpreter's jump, so no-op here.
27933:         return JSRS_CONTINUE;
17545: 
17494:     uint32 slot = JSVAL_TO_INT(slotval);
17891: 
17892:     if (!lazilyImportGlobalSlot(slot))
17891:          ABORT_TRACE("lazy import of global slot failed");
17891: 
19973:     return inc(STOBJ_GET_SLOT(globalObj, slot), 1, false);
17409: }
17545: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_GVARDEC()
17409: {
17807:     jsval slotval = cx->fp->slots[GET_SLOTNO(cx->fp->regs->pc)];
17494:     if (JSVAL_IS_NULL(slotval))
27933:         // We will see JSOP_INCNAME from the interpreter's jump, so no-op here.
27933:         return JSRS_CONTINUE;
17545: 
17494:     uint32 slot = JSVAL_TO_INT(slotval);
17891: 
17892:     if (!lazilyImportGlobalSlot(slot))
17891:          ABORT_TRACE("lazy import of global slot failed");
17891: 
19973:     return inc(STOBJ_GET_SLOT(globalObj, slot), -1, false);
17409: }
17545: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_REGEXP()
17409: {
27933:     return JSRS_STOP;
17409: }
17926: 
17926: // begin JS_HAS_XML_SUPPORT
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_DEFXMLNS()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_ANYNAME()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_QNAMEPART()
17409: {
24625:     return record_JSOP_STRING();
17409: }
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_QNAMECONST()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_QNAME()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_TOATTRNAME()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_TOATTRVAL()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_ADDATTRNAME()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_ADDATTRVAL()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_BINDXMLNAME()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_SETXMLNAME()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_XMLNAME()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_DESCENDANTS()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_FILTER()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_ENDFILTER()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_TOXML()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_TOXMLLIST()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_XMLTAGEXPR()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_XMLELTEXPR()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_XMLOBJECT()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_XMLCDATA()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_XMLCOMMENT()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_XMLPI()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_GETFUNNS()
17926: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_STARTXML()
17926: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_STARTXMLEXPR()
17926: {
27933:     return JSRS_STOP;
17926: }
17926: 
17926: // end JS_HAS_XML_SUPPORT
17926: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_CALLPROP()
17409: {
17630:     jsval& l = stackval(-1);
17870:     JSObject* obj;
17870:     LIns* obj_ins;
20427:     LIns* this_ins;
17870:     if (!JSVAL_IS_PRIMITIVE(l)) {
17870:         obj = JSVAL_TO_OBJECT(l);
17870:         obj_ins = get(&l);
20427:         this_ins = obj_ins; // |this| for subsequent call
17870:     } else {
17870:         jsint i;
29883:         debug_only_stmt(const char* protoname = NULL;)
17870:         if (JSVAL_IS_STRING(l)) {
17870:             i = JSProto_String;
29883:             debug_only_stmt(protoname = "String.prototype";)
17870:         } else if (JSVAL_IS_NUMBER(l)) {
17870:             i = JSProto_Number;
29883:             debug_only_stmt(protoname = "Number.prototype";)
19995:         } else if (JSVAL_TAG(l) == JSVAL_BOOLEAN) {
19995:             if (l == JSVAL_VOID)
19995:                 ABORT_TRACE("callprop on void");
24846:             guard(false, lir->ins2i(LIR_eq, get(&l), JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID)), MISMATCH_EXIT);
17870:             i = JSProto_Boolean;
29883:             debug_only_stmt(protoname = "Boolean.prototype";)
17870:         } else {
17870:             JS_ASSERT(JSVAL_IS_NULL(l) || JSVAL_IS_VOID(l));
17870:             ABORT_TRACE("callprop on null or void");
17870:         }
17870: 
17870:         if (!js_GetClassPrototype(cx, NULL, INT_TO_JSID(i), &obj))
27933:             ABORT_TRACE_ERROR("GetClassPrototype failed!");
17870: 
18712:         obj_ins = INS_CONSTPTR(obj);
29883:         debug_only_stmt(obj_ins = addName(obj_ins, protoname);)
20427:         this_ins = get(&l); // use primitive as |this|
17870:     }
17870: 
17632:     JSObject* obj2;
17746:     jsuword pcval;
27933:     CHECK_STATUS(test_property_cache(obj, obj_ins, obj2, pcval));
17998: 
17998:     if (PCVAL_IS_NULL(pcval) || !PCVAL_IS_OBJECT(pcval))
17998:         ABORT_TRACE("callee is not an object");
17998:     JS_ASSERT(HAS_FUNCTION_CLASS(PCVAL_TO_OBJECT(pcval)));
17630: 
19054:     if (JSVAL_IS_PRIMITIVE(l)) {
19054:         JSFunction* fun = GET_FUNCTION_PRIVATE(cx, PCVAL_TO_OBJECT(pcval));
19054:         if (!PRIMITIVE_THIS_TEST(fun, l))
19054:             ABORT_TRACE("callee does not accept primitive |this|");
19054:     }
19054: 
20427:     stack(0, this_ins);
18712:     stack(-1, INS_CONSTPTR(PCVAL_TO_OBJECT(pcval)));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_DELDESC()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_UINT24()
17409: {
26265:     stack(0, lir->insImmf(GET_UINT24(cx->fp->regs->pc)));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_INDEXBASE()
17611: {
17611:     atoms += GET_INDEXBASE(cx->fp->regs->pc);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_RESETBASE()
17611: {
17611:     atoms = cx->fp->script->atomMap.vector;
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_RESETBASE0()
17611: {
17611:     atoms = cx->fp->script->atomMap.vector;
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_CALLELEM()
17409: {
26551:     return record_JSOP_GETELEM();
17409: }
17611: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_STOP()
17611: {
21685:     JSStackFrame *fp = cx->fp;
21685: 
21685:     if (fp->imacpc) {
21685:         // End of imacro, so return true to the interpreter immediately. The
21685:         // interpreter's JSOP_STOP case will return from the imacro, back to
21685:         // the pc after the calling op, still in the same JSStackFrame.
21721:         atoms = fp->script->atomMap.vector;
27933:         return JSRS_CONTINUE;
21685:     }
21685: 
18001:     /*
18001:      * We know falling off the end of a constructor returns the new object that
18001:      * was passed in via fp->argv[-1], while falling off the end of a function
18001:      * returns undefined.
18001:      *
18001:      * NB: we do not support script rval (eval, API users who want the result
18001:      * of the last expression-statement, debugger API calls).
18001:      */
18001:     if (fp->flags & JSFRAME_CONSTRUCTING) {
18001:         JS_ASSERT(OBJECT_TO_JSVAL(fp->thisp) == fp->argv[-1]);
18001:         rval_ins = get(&fp->argv[-1]);
18001:     } else {
24846:         rval_ins = INS_CONST(JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID));
18001:     }
17818:     clearFrameSlotsFromCache();
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_GETXPROP()
17409: {
17686:     jsval& l = stackval(-1);
17686:     if (JSVAL_IS_PRIMITIVE(l))
17686:         ABORT_TRACE("primitive-this for GETXPROP?");
17686: 
17686:     JSObject* obj = JSVAL_TO_OBJECT(l);
18096:     if (obj != cx->fp->scopeChain || obj != globalObj)
27933:         return JSRS_STOP;
18096: 
18096:     jsval* vp;
27933:     CHECK_STATUS(name(vp));
18096:     stack(-1, get(vp));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_CALLXMLNAME()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_TYPEOFEXPR()
17409: {
18019:     return record_JSOP_TYPEOF();
17409: }
17611: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_ENTERBLOCK()
17409: {
22907:     JSObject* obj;
27012:     JS_GET_SCRIPT_OBJECT(cx->fp->script, getFullIndex(0), obj);
22907: 
24846:     LIns* void_ins = INS_CONST(JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_VOID));
22907:     for (int i = 0, n = OBJ_BLOCK_COUNT(cx, obj); i < n; i++)
22907:         stack(i, void_ins);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_LEAVEBLOCK()
17409: {
26238:     /* We mustn't exit the lexical block we began recording in.  */
27933:     if (cx->fp->blockChain != lexicalBlock)
27933:         return JSRS_CONTINUE;
27933:     else
27933:         return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18005: TraceRecorder::record_JSOP_GENERATOR()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18005: TraceRecorder::record_JSOP_YIELD()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_ARRAYPUSH()
17409: {
24861:     uint32_t slot = GET_UINT16(cx->fp->regs->pc);
24861:     JS_ASSERT(cx->fp->script->nfixed <= slot);
24861:     JS_ASSERT(cx->fp->slots + slot < cx->fp->regs->sp - 1);
24861:     jsval &arrayval = cx->fp->slots[slot];
24861:     JS_ASSERT(JSVAL_IS_OBJECT(arrayval));
24861:     JS_ASSERT(OBJ_IS_DENSE_ARRAY(cx, JSVAL_TO_OBJECT(arrayval)));
24861:     LIns *array_ins = get(&arrayval);
24861:     jsval &elt = stackval(-1);
24861:     LIns *elt_ins = get(&elt);
24861:     box_jsval(elt, elt_ins);
24861: 
24861:     LIns *args[] = { elt_ins, array_ins, cx_ins };
24861:     LIns *ok_ins = lir->insCall(&js_ArrayCompPush_ci, args);
24861:     guard(false, lir->ins_eq0(ok_ins), OOM_EXIT);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_ENUMCONSTELEM()
17409: {
27933:     return JSRS_STOP;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_LEAVEBLOCKEXPR()
17409: {
22907:     LIns* v_ins = stack(-1);
22907:     int n = -1 - GET_UINT16(cx->fp->regs->pc);
22907:     stack(n, v_ins);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_GETTHISPROP()
17409: {
17688:     LIns* this_ins;
17758: 
27933:     CHECK_STATUS(getThis(this_ins));
28326:     /*
28326:      * It's safe to just use cx->fp->thisp here because getThis() returns JSRS_STOP if thisp
28326:      * is not available.
28326:      */
27933:     CHECK_STATUS(getProp(cx->fp->thisp, this_ins));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_GETARGPROP()
17409: {
17688:     return getProp(argval(GET_ARGNO(cx->fp->regs->pc)));
17409: }
17611: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18005: TraceRecorder::record_JSOP_GETLOCALPROP()
17409: {
17807:     return getProp(varval(GET_SLOTNO(cx->fp->regs->pc)));
17409: }
17611: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_INDEXBASE1()
17611: {
17611:     atoms += 1 << 16;
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_INDEXBASE2()
17611: {
17611:     atoms += 2 << 16;
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_INDEXBASE3()
17611: {
17611:     atoms += 3 << 16;
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_CALLGVAR()
17409: {
18003:     jsval slotval = cx->fp->slots[GET_SLOTNO(cx->fp->regs->pc)];
18003:     if (JSVAL_IS_NULL(slotval))
27933:         // We will see JSOP_CALLNAME from the interpreter's jump, so no-op here.
27933:         return JSRS_CONTINUE;
18003: 
18003:     uint32 slot = JSVAL_TO_INT(slotval);
18003: 
18003:     if (!lazilyImportGlobalSlot(slot))
18003:          ABORT_TRACE("lazy import of global slot failed");
18003: 
23731:     jsval& v = STOBJ_GET_SLOT(globalObj, slot);
18031:     stack(0, get(&v));
18712:     stack(1, INS_CONSTPTR(NULL));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
18005: TraceRecorder::record_JSOP_CALLLOCAL()
17409: {
18003:     uintN slot = GET_SLOTNO(cx->fp->regs->pc);
18003:     stack(0, var(slot));
18712:     stack(1, INS_CONSTPTR(NULL));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_CALLARG()
17409: {
18003:     uintN slot = GET_ARGNO(cx->fp->regs->pc);
18003:     stack(0, arg(slot));
18712:     stack(1, INS_CONSTPTR(NULL));
27933:     return JSRS_CONTINUE;
17409: }
17611: 
24384: /* Functions for use with JSOP_CALLBUILTIN. */
24384: 
24384: static JSBool
24384: ObjectToIterator(JSContext *cx, uintN argc, jsval *vp)
24384: {
24384:     jsval *argv = JS_ARGV(cx, vp);
24384:     JS_ASSERT(JSVAL_IS_INT(argv[0]));
24384:     JS_SET_RVAL(cx, vp, JS_THIS(cx, vp));
24384:     return js_ValueToIterator(cx, JSVAL_TO_INT(argv[0]), &JS_RVAL(cx, vp));
24384: }
24384: 
24384: static JSObject* FASTCALL
24612: ObjectToIterator_tn(JSContext* cx, jsbytecode* pc, JSObject *obj, int32 flags)
24384: {
24384:     jsval v = OBJECT_TO_JSVAL(obj);
25628:     JSBool ok = js_ValueToIterator(cx, flags, &v);
24612: 
24612:     if (!ok) {
27166:         js_SetBuiltinError(cx);
24384:         return NULL;
24612:     }
24384:     return JSVAL_TO_OBJECT(v);
24384: }
24384: 
24384: static JSBool
24384: CallIteratorNext(JSContext *cx, uintN argc, jsval *vp)
24384: {
24384:     return js_CallIteratorNext(cx, JS_THIS_OBJECT(cx, vp), &JS_RVAL(cx, vp));
24384: }
24384: 
24384: static jsval FASTCALL
24612: CallIteratorNext_tn(JSContext* cx, jsbytecode* pc, JSObject* iterobj)
24384: {
25094:     JSAutoTempValueRooter tvr(cx);
25628:     JSBool ok = js_CallIteratorNext(cx, iterobj, tvr.addr());
24612: 
24612:     if (!ok) {
27166:         js_SetBuiltinError(cx);
24384:         return JSVAL_ERROR_COOKIE;
24612:     }
25094:     return tvr.value();
24384: }
24384: 
24384: JS_DEFINE_TRCINFO_1(ObjectToIterator,
24612:     (4, (static, OBJECT_FAIL, ObjectToIterator_tn, CONTEXT, PC, THIS, INT32, 0, 0)))
24384: JS_DEFINE_TRCINFO_1(CallIteratorNext,
24612:     (3, (static, JSVAL_FAIL,  CallIteratorNext_tn, CONTEXT, PC, THIS,        0, 0)))
24384: 
24384: static const struct BuiltinFunctionInfo {
24384:     JSTraceableNative *tn;
24384:     int nargs;
24384: } builtinFunctionInfo[JSBUILTIN_LIMIT] = {
24384:     {ObjectToIterator_trcinfo,   1},
24489:     {CallIteratorNext_trcinfo,   0},
24489:     {GetProperty_trcinfo,        1},
24489:     {GetElement_trcinfo,         1},
24489:     {SetProperty_trcinfo,        2},
29363:     {SetElement_trcinfo,         2},
29363:     {HasInstance_trcinfo,        1}
24384: };
24384: 
24384: JSObject *
24384: js_GetBuiltinFunction(JSContext *cx, uintN index)
24384: {
24384:     JSRuntime *rt = cx->runtime;
24384:     JSObject *funobj = rt->builtinFunctions[index];
24384: 
24384:     if (!funobj) {
24384:         /* Use NULL parent and atom. Builtin functions never escape to scripts. */
25218:         JS_ASSERT(index < JS_ARRAY_LENGTH(builtinFunctionInfo));
25218:         const BuiltinFunctionInfo *bfi = &builtinFunctionInfo[index];
24384:         JSFunction *fun = js_NewFunction(cx,
24384:                                          NULL,
25218:                                          JS_DATA_TO_FUNC_PTR(JSNative, bfi->tn),
25218:                                          bfi->nargs,
24384:                                          JSFUN_FAST_NATIVE | JSFUN_TRACEABLE,
24384:                                          NULL,
24384:                                          NULL);
24384:         if (fun) {
24384:             funobj = FUN_OBJECT(fun);
24384:             STOBJ_CLEAR_PROTO(funobj);
24384:             STOBJ_CLEAR_PARENT(funobj);
24384: 
24384:             JS_LOCK_GC(rt);
24384:             if (!rt->builtinFunctions[index])  /* retest now that the lock is held */
24384:                 rt->builtinFunctions[index] = funobj;
24384:             else
24384:                 funobj = rt->builtinFunctions[index];
24384:             JS_UNLOCK_GC(rt);
24384:         }
24384:     }
24384:     return funobj;
24384: }
24384: 
27933: JS_REQUIRES_STACK JSRecordingStatus
24384: TraceRecorder::record_JSOP_CALLBUILTIN()
24384: {
24384:     JSObject *obj = js_GetBuiltinFunction(cx, GET_INDEX(cx->fp->regs->pc));
24384:     if (!obj)
27933:         ABORT_TRACE_ERROR("error in js_GetBuiltinFunction");
24384: 
24384:     stack(0, get(&stackval(-1)));
24384:     stack(-1, INS_CONSTPTR(obj));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_INT8()
17409: {
26265:     stack(0, lir->insImmf(GET_INT8(cx->fp->regs->pc)));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_INT32()
17409: {
26265:     stack(0, lir->insImmf(GET_INT32(cx->fp->regs->pc)));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_LENGTH()
17409: {
17715:     jsval& l = stackval(-1);
17869:     if (JSVAL_IS_PRIMITIVE(l)) {
17869:         if (!JSVAL_IS_STRING(l))
24625:             ABORT_TRACE("non-string primitive JSOP_LENGTH unsupported");
27637:         set(&l, lir->ins1(LIR_i2f, getStringLength(get(&l))));
27933:         return JSRS_CONTINUE;
17869:     }
17869: 
17869:     JSObject* obj = JSVAL_TO_OBJECT(l);
26285:     LIns* obj_ins = get(&l);
30248: 
30248:     if (STOBJ_GET_CLASS(obj) == &js_ArgumentsClass) {
30248:         guardClass(obj, obj_ins, &js_ArgumentsClass, snapshot(MISMATCH_EXIT));
30248:         JSStackFrame* afp = frameIfInRange(obj);
30248:         if (afp) {
30248:             LIns* v_ins = lir->ins1(LIR_i2f, INS_CONST(afp->argc));
30248:             set(&l, v_ins);
30248:             return JSRS_CONTINUE;
30248:         }
30248:     }
30248: 
26285:     LIns* v_ins;
26285:     if (OBJ_IS_ARRAY(cx, obj)) {
26285:         if (OBJ_IS_DENSE_ARRAY(cx, obj)) {
26565:             if (!guardDenseArray(obj, obj_ins, BRANCH_EXIT)) {
26285:                 JS_NOT_REACHED("OBJ_IS_DENSE_ARRAY but not?!?");
27933:                 return JSRS_STOP;
26565:             }
26285:         } else {
26285:             if (!guardClass(obj, obj_ins, &js_SlowArrayClass, snapshot(BRANCH_EXIT)))
26285:                 ABORT_TRACE("can't trace length property access on non-array");
26285:         }
26285:         v_ins = lir->ins1(LIR_i2f, stobj_get_fslot(obj_ins, JSSLOT_ARRAY_LENGTH));
26285:     } else {
26285:         if (!OBJ_IS_NATIVE(obj))
26285:             ABORT_TRACE("can't trace length property access on non-array, non-native object");
26285:         return getProp(obj, obj_ins);
26285:     }
17715:     set(&l, v_ins);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_NEWARRAY()
17409: {
27012:     LIns *proto_ins;
27933:     CHECK_STATUS(getClassPrototype(JSProto_Array, proto_ins));
23708: 
27234:     uint32 len = GET_UINT16(cx->fp->regs->pc);
27234:     cx->fp->assertValidStackDepth(len);
27234: 
27012:     LIns* args[] = { lir->insImm(len), proto_ins, cx_ins };
27012:     LIns* v_ins = lir->insCall(&js_NewUninitializedArray_ci, args);
23708:     guard(false, lir->ins_eq0(v_ins), OOM_EXIT);
23708: 
23708:     LIns* dslots_ins = NULL;
28274:     uint32 count = 0;
23708:     for (uint32 i = 0; i < len; i++) {
25628:         jsval& v = stackval(int(i) - int(len));
28274:         if (v != JSVAL_HOLE)
28274:             count++;
23708:         LIns* elt_ins = get(&v);
23708:         box_jsval(v, elt_ins);
23708:         stobj_set_dslot(v_ins, i, dslots_ins, elt_ins, "set_array_elt");
23708:     }
23708: 
28274:     if (count > 0)
28554:         stobj_set_fslot(v_ins, JSSLOT_ARRAY_COUNT, INS_CONST(count), "set_array_count");
28274: 
25628:     stack(-int(len), v_ins);
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JS_REQUIRES_STACK JSRecordingStatus
17926: TraceRecorder::record_JSOP_HOLE()
17409: {
24846:     stack(0, INS_CONST(JSVAL_TO_PSEUDO_BOOLEAN(JSVAL_HOLE)));
27933:     return JSRS_CONTINUE;
27933: }
27933: 
27933: JSRecordingStatus
25627: TraceRecorder::record_JSOP_LOOP()
25627: {
27933:     return JSRS_CONTINUE;
25627: }
25627: 
28952: #define DBG_STUB(OP)                                                          \
28952:     JS_REQUIRES_STACK JSRecordingStatus                                       \
28952:     TraceRecorder::record_##OP()                                              \
28952:     {                                                                         \
28952:         ABORT_TRACE("can't trace " #OP);                                      \
28952:     }
28952: 
28952: DBG_STUB(JSOP_GETUPVAR_DBG)
28952: DBG_STUB(JSOP_CALLUPVAR_DBG)
28952: DBG_STUB(JSOP_DEFFUN_DBGFC)
28952: DBG_STUB(JSOP_DEFLOCALFUN_DBGFC)
28952: DBG_STUB(JSOP_LAMBDA_DBGFC)
28952: 
21459: #ifdef JS_JIT_SPEW
21433: /* Prints information about entry typemaps and unstable exits for all peers at a PC */
21433: void
28244: js_DumpPeerStability(JSTraceMonitor* tm, const void* ip, JSObject* globalObj, uint32 globalShape,
28244:                      uint32 argc)
21433: {
21433:     Fragment* f;
21433:     TreeInfo* ti;
21456:     bool looped = false;
21685:     unsigned length = 0;
21433: 
28244:     for (f = getLoop(tm, ip, globalObj, globalShape, argc); f != NULL; f = f->peer) {
21433:         if (!f->vmprivate)
21433:             continue;
29883:         debug_only_printf(LC_TMStats, "fragment %p:\nENTRY: ", (void*)f);
21433:         ti = (TreeInfo*)f->vmprivate;
21456:         if (looped)
24491:             JS_ASSERT(ti->nStackTypes == length);
24491:         for (unsigned i = 0; i < ti->nStackTypes; i++)
29883:             debug_only_printf(LC_TMStats, "S%d ", ti->stackTypeMap()[i]);
24491:         for (unsigned i = 0; i < ti->nGlobalTypes(); i++)
29883:             debug_only_printf(LC_TMStats, "G%d ", ti->globalTypeMap()[i]);
29883:         debug_only_print0(LC_TMStats, "\n");
21433:         UnstableExit* uexit = ti->unstableExits;
21433:         while (uexit != NULL) {
29883:             debug_only_print0(LC_TMStats, "EXIT:  ");
29896:             JSTraceType* m = getFullTypeMap(uexit->exit);
21433:             for (unsigned i = 0; i < uexit->exit->numStackSlots; i++)
29883:                 debug_only_printf(LC_TMStats, "S%d ", m[i]);
24246:             for (unsigned i = 0; i < uexit->exit->numGlobalSlots; i++)
29883:                 debug_only_printf(LC_TMStats, "G%d ", m[uexit->exit->numStackSlots + i]);
29883:             debug_only_print0(LC_TMStats, "\n");
21433:             uexit = uexit->next;
21433:         }
24491:         length = ti->nStackTypes;
21456:         looped = true;
21433:     }
21433: }
21433: #endif
21433: 
29368: #ifdef MOZ_TRACEVIS
29368: 
29368: FILE* traceVisLogFile = NULL;
29368: 
29368: JS_FRIEND_API(bool)
29368: JS_StartTraceVis(const char* filename = "tracevis.dat")
29368: {
29368:     if (traceVisLogFile) {
29368:         // If we're currently recording, first we must stop.
29368:         JS_StopTraceVis();
29368:     }
29368: 
29368:     traceVisLogFile = fopen(filename, "wb");
29368:     if (!traceVisLogFile)
29368:         return false;
29368: 
29368:     return true;
29368: }
29368: 
29368: JS_FRIEND_API(JSBool)
29368: js_StartTraceVis(JSContext *cx, JSObject *obj,
29368:                  uintN argc, jsval *argv, jsval *rval)
29368: {
29368:     JSBool ok;
29368: 
29368:     if (argc > 0 && JSVAL_IS_STRING(argv[0])) {
29368:         JSString *str = JSVAL_TO_STRING(argv[0]);
29368:         char *filename = js_DeflateString(cx, str->chars(), str->length());
29368:         if (!filename)
29368:             goto error;
29368:         ok = JS_StartTraceVis(filename);
29368:         JS_free(cx, filename);
29368:     } else {
29368:         ok = JS_StartTraceVis();
29368:     }
29368: 
29368:     if (ok) {
29368:         fprintf(stderr, "started TraceVis recording\n");
29368:         return JS_TRUE;
29368:     }
29368: 
29368:   error:
29368:     JS_ReportError(cx, "failed to start TraceVis recording");
29368:     return JS_FALSE;
29368: }
29368: 
29368: JS_FRIEND_API(bool)
29368: JS_StopTraceVis()
29368: {
29368:     if (!traceVisLogFile)
29368:         return false;
29368: 
29368:     fclose(traceVisLogFile);    // not worth checking the result
29368:     traceVisLogFile = NULL;
29368: 
29368:     return true;
29368: }
29368: 
29368: JS_FRIEND_API(JSBool)
29368: js_StopTraceVis(JSContext *cx, JSObject *obj,
29368:                 uintN argc, jsval *argv, jsval *rval)
29368: {
29368:     JSBool ok = JS_StopTraceVis();
29368: 
29368:     if (ok)
29368:         fprintf(stderr, "stopped TraceVis recording\n");
29368:     else
29368:         JS_ReportError(cx, "TraceVis isn't running");
29368: 
29368:     return ok;
29368: }
29368: 
29368: #endif /* MOZ_TRACEVIS */
29368: 
23106: #define UNUSED(n)                                                             \
23106:     JS_REQUIRES_STACK bool                                                    \
23106:     TraceRecorder::record_JSOP_UNUSED##n() {                                  \
23106:         JS_NOT_REACHED("JSOP_UNUSED" # n);                                    \
23106:         return false;                                                         \
23106:     }
