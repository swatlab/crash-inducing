29366: /* -*- Mode: C++; tab-width: 8; indent-tabs-mode: nil; c-basic-offset: 4 -*-
    1:  * vim: set ts=8 sw=4 et tw=78:
    1:  *
    1:  * ***** BEGIN LICENSE BLOCK *****
    1:  * Version: MPL 1.1/GPL 2.0/LGPL 2.1
    1:  *
    1:  * The contents of this file are subject to the Mozilla Public License Version
    1:  * 1.1 (the "License"); you may not use this file except in compliance with
    1:  * the License. You may obtain a copy of the License at
    1:  * http://www.mozilla.org/MPL/
    1:  *
    1:  * Software distributed under the License is distributed on an "AS IS" basis,
    1:  * WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
    1:  * for the specific language governing rights and limitations under the
    1:  * License.
    1:  *
    1:  * The Original Code is Mozilla Communicator client code, released
    1:  * March 31, 1998.
    1:  *
    1:  * The Initial Developer of the Original Code is
    1:  * Netscape Communications Corporation.
    1:  * Portions created by the Initial Developer are Copyright (C) 1998
    1:  * the Initial Developer. All Rights Reserved.
    1:  *
    1:  * Contributor(s):
    1:  *
    1:  * Alternatively, the contents of this file may be used under the terms of
    1:  * either of the GNU General Public License Version 2 or later (the "GPL"),
    1:  * or the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
    1:  * in which case the provisions of the GPL or the LGPL are applicable instead
    1:  * of those above. If you wish to allow use of your version of this file only
    1:  * under the terms of either the GPL or the LGPL, and not to allow others to
    1:  * use your version of this file under the terms of the MPL, indicate your
    1:  * decision by deleting the provisions above and replace them with the notice
    1:  * and other provisions required by the GPL or the LGPL. If you do not delete
    1:  * the provisions above, a recipient may use your version of this file under
    1:  * the terms of any one of the MPL, the GPL or the LGPL.
    1:  *
    1:  * ***** END LICENSE BLOCK ***** */
    1: 
    1: /*
    1:  * JS Mark-and-Sweep Garbage Collector.
    1:  *
    1:  * This GC allocates fixed-sized things with sizes up to GC_NBYTES_MAX (see
    1:  * jsgc.h). It allocates from a special GC arena pool with each arena allocated
    1:  * using malloc. It uses an ideally parallel array of flag bytes to hold the
    1:  * mark bit, finalizer type index, etc.
    1:  *
    1:  * XXX swizzle page to freelist for better locality of reference
    1:  */
    1: #include <stdlib.h>     /* for free */
17182: #include <math.h>
    1: #include <string.h>     /* for memset used when DEBUG */
    1: #include "jstypes.h"
26316: #include "jsstdint.h"
    1: #include "jsutil.h" /* Added by JSIFY */
    1: #include "jshash.h" /* Added by JSIFY */
17182: #include "jsbit.h"
17182: #include "jsclist.h"
17182: #include "jsprf.h"
    1: #include "jsapi.h"
    1: #include "jsatom.h"
    1: #include "jscntxt.h"
18863: #include "jsversion.h"
    1: #include "jsdbgapi.h"
    1: #include "jsexn.h"
    1: #include "jsfun.h"
    1: #include "jsgc.h"
    1: #include "jsinterp.h"
    1: #include "jsiter.h"
    1: #include "jslock.h"
    1: #include "jsnum.h"
    1: #include "jsobj.h"
 3235: #include "jsparse.h"
    1: #include "jsscope.h"
    1: #include "jsscript.h"
24499: #include "jsstaticcheck.h"
    1: #include "jsstr.h"
30851: #include "jstask.h"
17976: #include "jstracer.h"
    1: 
    1: #if JS_HAS_XML_SUPPORT
    1: #include "jsxml.h"
    1: #endif
    1: 
30285: #ifdef INCLUDE_MOZILLA_DTRACE
30285: #include "jsdtracef.h"
30285: #endif
30285: 
    1: /*
34300:  * Include the headers for mmap.
    1:  */
34300: #if defined(XP_WIN)
34300: # include <windows.h>
12478: #endif
32823: #if defined(XP_UNIX) || defined(XP_BEOS)
32823: # include <unistd.h>
34300: # include <sys/mman.h>
32823: #endif
32823: /* On Mac OS X MAP_ANONYMOUS is not defined. */
32823: #if !defined(MAP_ANONYMOUS) && defined(MAP_ANON)
32823: # define MAP_ANONYMOUS MAP_ANON
32823: #endif
32823: #if !defined(MAP_ANONYMOUS)
32823: # define MAP_ANONYMOUS 0
32823: #endif
32823: 
32823: /*
25901:  * Check JSTempValueUnion has the size of jsval and void * so we can
25901:  * reinterpret jsval as void* GC-thing pointer and use JSTVU_SINGLE for
25901:  * different GC-things.
25901:  */
25901: JS_STATIC_ASSERT(sizeof(JSTempValueUnion) == sizeof(jsval));
25901: JS_STATIC_ASSERT(sizeof(JSTempValueUnion) == sizeof(void *));
25901: 
25901: /*
25901:  * Check that JSTRACE_XML follows JSTRACE_OBJECT, JSTRACE_DOUBLE and
25901:  * JSTRACE_STRING.
25901:  */
25901: JS_STATIC_ASSERT(JSTRACE_OBJECT == 0);
25901: JS_STATIC_ASSERT(JSTRACE_DOUBLE == 1);
25901: JS_STATIC_ASSERT(JSTRACE_STRING == 2);
25901: JS_STATIC_ASSERT(JSTRACE_XML    == 3);
25901: 
25901: /*
25901:  * JS_IS_VALID_TRACE_KIND assumes that JSTRACE_STRING is the last non-xml
25901:  * trace kind when JS_HAS_XML_SUPPORT is false.
25901:  */
25901: JS_STATIC_ASSERT(JSTRACE_STRING + 1 == JSTRACE_XML);
25901: 
25901: /*
25901:  * Check that we can use memset(p, 0, ...) to implement JS_CLEAR_WEAK_ROOTS.
25901:  */
25901: JS_STATIC_ASSERT(JSVAL_NULL == 0);
25901: 
33582: /*
33582:  * Check consistency of external string constants from JSFinalizeGCThingKind.
33582:  */
33582: JS_STATIC_ASSERT(FINALIZE_EXTERNAL_STRING_LAST - FINALIZE_EXTERNAL_STRING0 ==
33582:                  JS_EXTERNAL_STRING_LIMIT - 1);
25901: 
25901: /*
12282:  * A GC arena contains a fixed number of flag bits for each thing in its heap,
12282:  * and supports O(1) lookup of a flag given its thing's address.
    1:  *
 5917:  * To implement this, we allocate things of the same size from a GC arena
 5917:  * containing GC_ARENA_SIZE bytes aligned on GC_ARENA_SIZE boundary. The
 5917:  * following picture shows arena's layout:
    1:  *
 5917:  *  +------------------------------+--------------------+---------------+
 5917:  *  | allocation area for GC thing | flags of GC things | JSGCArenaInfo |
 5917:  *  +------------------------------+--------------------+---------------+
    1:  *
12282:  * To find the flag bits for the thing we calculate the thing index counting
12282:  * from arena's start using:
    1:  *
12282:  *   thingIndex = (thingAddress & GC_ARENA_MASK) / thingSize
12282:  *
12282:  * The details of flag's lookup depend on thing's kind. For all GC things
12282:  * except doubles we use one byte of flags where the 4 bits determine thing's
12282:  * type and the rest is used to implement GC marking, finalization and
12282:  * locking. We calculate the address of flag's byte using:
12282:  *
 5917:  *   flagByteAddress =
12282:  *       (thingAddress | GC_ARENA_MASK) - sizeof(JSGCArenaInfo) - thingIndex
12282:  *
 5917:  * where
12282:  *
 5917:  *   (thingAddress | GC_ARENA_MASK) - sizeof(JSGCArenaInfo)
    1:  *
12282:  * is the last byte of flags' area.
12282:  *
12282:  * This implies that the things are allocated from the start of their area and
12282:  * flags are allocated from the end. This arrangement avoids a relatively
12282:  * expensive calculation of the location of the boundary separating things and
12282:  * flags. The boundary's offset from the start of the arena is given by:
12282:  *
12282:  *   thingsPerArena * thingSize
12282:  *
12282:  * where thingsPerArena is the number of things that the arena can hold:
12282:  *
12282:  *   (GC_ARENA_SIZE - sizeof(JSGCArenaInfo)) / (thingSize + 1).
12282:  *
12282:  * To allocate doubles we use a specialized arena. It can contain only numbers
12282:  * so we do not need the type bits. Moreover, since the doubles do not require
12282:  * a finalizer and very few of them are locked via js_LockGCThing API, we use
12282:  * just one bit of flags per double to denote if it was marked during the
12282:  * marking phase of the GC. The locking is implemented via a hash table. Thus
12282:  * for doubles the flag area becomes a bitmap.
    1:  */
34300: 
34300: static const jsuword GC_ARENAS_PER_CHUNK = 16;
34300: static const jsuword GC_ARENA_SHIFT = 12;
34300: static const jsuword GC_ARENA_MASK = JS_BITMASK(GC_ARENA_SHIFT);
34300: static const jsuword GC_ARENA_SIZE = JS_BIT(GC_ARENA_SHIFT);
32823: 
 5917: struct JSGCArenaInfo {
 5917:     /*
12282:      * Allocation list for the arena or NULL if the arena holds double values.
 5917:      */
 5917:     JSGCArenaList   *list;
 5917: 
 5917:     /*
 5917:      * Pointer to the previous arena in a linked list. The arena can either
 5917:      * belong to one of JSContext.gcArenaList lists or, when it does not have
 5917:      * any allocated GC things, to the list of free arenas in the chunk with
 5917:      * head stored in JSGCChunkInfo.lastFreeArena.
 5917:      */
 5917:     JSGCArenaInfo   *prev;
 5917: 
32823:     /*
36410:      * A link field for the list of arenas with marked things that haven't yet
36410:      * been scanned for live children. The field is encoded as arena's page to
36410:      * to hold only the high-order arena-counting bits to share the space with
36410:      * firstArena and arenaIndex fields. For details see comments before
36410:      * DelayMarkingChildren.
32823:      */
36410:     jsuword         prevUnmarkedPage :  JS_BITS_PER_WORD - GC_ARENA_SHIFT;
32823: 
32823:     /*
32823:      * When firstArena is false, the index of arena in the chunk. When
32823:      * firstArena is true, the index of a free arena holding JSGCChunkInfo or
32823:      * NO_FREE_ARENAS if there are no free arenas in the chunk.
32823:      *
32823:      * GET_ARENA_INDEX and GET_CHUNK_INFO_INDEX are convenience macros to
32823:      * access either of indexes.
32823:      */
32823:     jsuword         arenaIndex :        GC_ARENA_SHIFT - 1;
32823: 
32823:     /* Flag indicating if the arena is the first in the chunk. */
32823:     jsuword         firstArena :        1;
    1: 
36410:     JSGCThing       *freeList;
36410: 
12282:     union {
36410:         /* See comments before DelayMarkingChildren. */
36410:         jsuword     unmarkedChildren;
36410: 
36410:         /* The arena has marked doubles. */
36410:         bool        hasMarkedDoubles;
33952:     };
    1: };
    1: 
33952: /* GC flag definitions, must fit in 8 bits. */
33582: const uint8 GCF_MARK        = JS_BIT(0);
33952: const uint8 GCF_LOCK        = JS_BIT(1); /* lock request bit in API */
33582: 
33582: /*
33582:  * The private JSGCThing struct, which describes a JSRuntime.gcFreeList element.
33582:  */
33582: struct JSGCThing {
33952:     JSGCThing   *link;
33582: };
33582: 
 5917: /*
12478:  * Macros to convert between JSGCArenaInfo, the start address of the arena and
12478:  * arena's page defined as (start address) >> GC_ARENA_SHIFT.
12478:  */
12478: #define ARENA_INFO_OFFSET (GC_ARENA_SIZE - (uint32) sizeof(JSGCArenaInfo))
12478: 
12478: #define IS_ARENA_INFO_ADDRESS(arena)                                          \
12478:     (((jsuword) (arena) & GC_ARENA_MASK) == ARENA_INFO_OFFSET)
12478: 
12478: #define ARENA_START_TO_INFO(arenaStart)                                       \
12478:     (JS_ASSERT(((arenaStart) & (jsuword) GC_ARENA_MASK) == 0),                \
12478:      (JSGCArenaInfo *) ((arenaStart) + (jsuword) ARENA_INFO_OFFSET))
12478: 
12478: #define ARENA_INFO_TO_START(arena)                                            \
12478:     (JS_ASSERT(IS_ARENA_INFO_ADDRESS(arena)),                                 \
12478:      (jsuword) (arena) & ~(jsuword) GC_ARENA_MASK)
12478: 
12478: #define ARENA_PAGE_TO_INFO(arenaPage)                                         \
12478:     (JS_ASSERT(arenaPage != 0),                                               \
12478:      JS_ASSERT(!((jsuword)(arenaPage) >> (JS_BITS_PER_WORD-GC_ARENA_SHIFT))), \
12478:      ARENA_START_TO_INFO((arenaPage) << GC_ARENA_SHIFT))
12478: 
12478: #define ARENA_INFO_TO_PAGE(arena)                                             \
12478:     (JS_ASSERT(IS_ARENA_INFO_ADDRESS(arena)),                                 \
12478:      ((jsuword) (arena) >> GC_ARENA_SHIFT))
12478: 
12478: #define GET_ARENA_INFO(chunk, index)                                          \
34300:     (JS_ASSERT((index) < GC_ARENAS_PER_CHUNK),                                \
12478:      ARENA_START_TO_INFO(chunk + ((index) << GC_ARENA_SHIFT)))
12478: 
32823: /*
32823:  * Definitions for allocating arenas in chunks.
32823:  *
32823:  * All chunks that have at least one free arena are put on the doubly-linked
32823:  * list with the head stored in JSRuntime.gcChunkList. JSGCChunkInfo contains
32823:  * the head of the chunk's free arena list together with the link fields for
32823:  * gcChunkList.
32823:  *
32823:  * Structure stored in one of chunk's free arenas. GET_CHUNK_INFO_INDEX gives
32823:  * the index of this arena. When all arenas in the chunk are used, it is
32823:  * removed from the list and the index is set to NO_FREE_ARENAS indicating
32823:  * that the chunk is not on gcChunkList and has no JSGCChunkInfo available.
32823:  */
32823: 
32823: struct JSGCChunkInfo {
32823:     JSGCChunkInfo   **prevp;
32823:     JSGCChunkInfo   *next;
32823:     JSGCArenaInfo   *lastFreeArena;
32823:     uint32          numFreeArenas;
32823: };
32823: 
32823: #define NO_FREE_ARENAS              JS_BITMASK(GC_ARENA_SHIFT - 1)
32823: 
34300: JS_STATIC_ASSERT(1 <= GC_ARENAS_PER_CHUNK &&
34300:                  GC_ARENAS_PER_CHUNK <= NO_FREE_ARENAS);
32823: 
32823: #define GET_ARENA_CHUNK(arena, index)                                         \
32823:     (JS_ASSERT(GET_ARENA_INDEX(arena) == index),                              \
32823:      ARENA_INFO_TO_START(arena) - ((index) << GC_ARENA_SHIFT))
32823: 
32823: #define GET_ARENA_INDEX(arena)                                                \
32823:     ((arena)->firstArena ? 0 : (uint32) (arena)->arenaIndex)
32823: 
32823: #define GET_CHUNK_INFO_INDEX(chunk)                                           \
32823:     ((uint32) ARENA_START_TO_INFO(chunk)->arenaIndex)
32823: 
32823: #define SET_CHUNK_INFO_INDEX(chunk, index)                                    \
34300:     (JS_ASSERT((index) < GC_ARENAS_PER_CHUNK || (index) == NO_FREE_ARENAS),   \
32823:      (void) (ARENA_START_TO_INFO(chunk)->arenaIndex = (jsuword) (index)))
32823: 
32823: #define GET_CHUNK_INFO(chunk, infoIndex)                                      \
32823:     (JS_ASSERT(GET_CHUNK_INFO_INDEX(chunk) == (infoIndex)),                   \
34300:      JS_ASSERT((uint32) (infoIndex) < GC_ARENAS_PER_CHUNK),                   \
32823:      (JSGCChunkInfo *) ((chunk) + ((infoIndex) << GC_ARENA_SHIFT)))
32823: 
32823: #define CHUNK_INFO_TO_INDEX(ci)                                               \
32823:     GET_ARENA_INDEX(ARENA_START_TO_INFO((jsuword)ci))
32823: 
 5917: /*
 5917:  * Macros for GC-thing operations.
 5917:  */
 5917: #define THINGS_PER_ARENA(thingSize)                                           \
 5917:     ((GC_ARENA_SIZE - (uint32) sizeof(JSGCArenaInfo)) / ((thingSize) + 1U))
 5917: 
 5917: #define THING_TO_ARENA(thing)                                                 \
32734:     (JS_ASSERT(!JSString::isStatic(thing)),                                   \
32686:      (JSGCArenaInfo *)(((jsuword) (thing) | GC_ARENA_MASK)                    \
32686:                        + 1 - sizeof(JSGCArenaInfo)))
 5917: 
 5917: #define THING_TO_INDEX(thing, thingSize)                                      \
 5917:     ((uint32) ((jsuword) (thing) & GC_ARENA_MASK) / (uint32) (thingSize))
 5917: 
 5917: #define THING_FLAGP(arena, thingIndex)                                        \
 5917:     (JS_ASSERT((jsuword) (thingIndex)                                         \
 5917:                < (jsuword) THINGS_PER_ARENA((arena)->list->thingSize)),       \
 5917:      (uint8 *)(arena) - 1 - (thingIndex))
 5917: 
 5917: #define THING_TO_FLAGP(thing, thingSize)                                      \
 5917:     THING_FLAGP(THING_TO_ARENA(thing), THING_TO_INDEX(thing, thingSize))
 5917: 
 5917: #define FLAGP_TO_ARENA(flagp) THING_TO_ARENA(flagp)
 5917: 
 5917: #define FLAGP_TO_INDEX(flagp)                                                 \
 5917:     (JS_ASSERT(((jsuword) (flagp) & GC_ARENA_MASK) < ARENA_INFO_OFFSET),      \
 5917:      (ARENA_INFO_OFFSET - 1 - (uint32) ((jsuword) (flagp) & GC_ARENA_MASK)))
 5917: 
 5917: #define FLAGP_TO_THING(flagp, thingSize)                                      \
 5917:     (JS_ASSERT(((jsuword) (flagp) & GC_ARENA_MASK) >=                         \
 5917:                (ARENA_INFO_OFFSET - THINGS_PER_ARENA(thingSize))),            \
 6155:      (JSGCThing *)(((jsuword) (flagp) & ~GC_ARENA_MASK) +                     \
 5917:                    (thingSize) * FLAGP_TO_INDEX(flagp)))
    1: 
33952: static inline JSGCThing *
33952: NextThing(JSGCThing *thing, size_t thingSize)
33952: {
33952:     return reinterpret_cast<JSGCThing *>(reinterpret_cast<jsuword>(thing) +
33952:                                          thingSize);
33952: }
33952: 
33952: static inline JSGCThing *
33952: MakeNewArenaFreeList(JSGCArenaInfo *a, unsigned thingSize, size_t nthings)
33952: {
33952:     JS_ASSERT(nthings * thingSize < GC_ARENA_SIZE - sizeof(JSGCArenaInfo));
33952: 
33952:     jsuword thingsStart = ARENA_INFO_TO_START(a);
33952:     JSGCThing *first = reinterpret_cast<JSGCThing *>(thingsStart);
33952:     JSGCThing *last = reinterpret_cast<JSGCThing *>(thingsStart +
33952:                                                     (nthings - 1) * thingSize);
33952:     for (JSGCThing *thing = first; thing != last;) {
33952:         JSGCThing *next = NextThing(thing, thingSize);
33952:         thing->link = next;
33952:         thing = next;
33952:     }
33952:     last->link = NULL;
33952:     return first;
33952: }
33952: 
12282: /*
12282:  * Macros for the specialized arena for doubles.
    1:  *
12282:  * DOUBLES_PER_ARENA defines the maximum number of doubles that the arena can
12282:  * hold. We find it as the following. Let n be the number of doubles in the
12282:  * arena. Together with the bitmap of flags and JSGCArenaInfo they should fit
12282:  * the arena. Hence DOUBLES_PER_ARENA or n_max is the maximum value of n for
12282:  * which the following holds:
12282:  *
12282:  *   n*s + ceil(n/B) <= M                                               (1)
12282:  *
12282:  * where "/" denotes normal real division,
12282:  *       ceil(r) gives the least integer not smaller than the number r,
13029:  *       s is the number of words in jsdouble,
13029:  *       B is number of bits per word or B == JS_BITS_PER_WORD
13029:  *       M is the number of words in the arena before JSGCArenaInfo or
13029:  *       M == (GC_ARENA_SIZE - sizeof(JSGCArenaInfo)) / sizeof(jsuword).
13029:  *       M == ARENA_INFO_OFFSET / sizeof(jsuword)
12282:  *
12282:  * We rewrite the inequality as
12282:  *
12282:  *   n*B*s/B + ceil(n/B) <= M,
12282:  *   ceil(n*B*s/B + n/B) <= M,
12282:  *   ceil(n*(B*s + 1)/B) <= M                                           (2)
12282:  *
12282:  * We define a helper function e(n, s, B),
12282:  *
12282:  *   e(n, s, B) := ceil(n*(B*s + 1)/B) - n*(B*s + 1)/B, 0 <= e(n, s, B) < 1.
12282:  *
12282:  * It gives:
12282:  *
12282:  *   n*(B*s + 1)/B + e(n, s, B) <= M,
12282:  *   n + e*B/(B*s + 1) <= M*B/(B*s + 1)
12282:  *
12282:  * We apply the floor function to both sides of the last equation, where
12282:  * floor(r) gives the biggest integer not greater than r. As a consequence we
12282:  * have:
12282:  *
12282:  *   floor(n + e*B/(B*s + 1)) <= floor(M*B/(B*s + 1)),
12282:  *   n + floor(e*B/(B*s + 1)) <= floor(M*B/(B*s + 1)),
12282:  *   n <= floor(M*B/(B*s + 1)),                                         (3)
12282:  *
12282:  * where floor(e*B/(B*s + 1)) is zero as e*B/(B*s + 1) < B/(B*s + 1) < 1.
12282:  * Thus any n that satisfies the original constraint (1) or its equivalent (2),
12282:  * must also satisfy (3). That is, we got an upper estimate for the maximum
12282:  * value of n. Lets show that this upper estimate,
12282:  *
12282:  *   floor(M*B/(B*s + 1)),                                              (4)
12282:  *
12282:  * also satisfies (1) and, as such, gives the required maximum value.
12282:  * Substituting it into (2) gives:
12282:  *
12282:  *   ceil(floor(M*B/(B*s + 1))*(B*s + 1)/B) == ceil(floor(M/X)*X)
12282:  *
12282:  * where X == (B*s + 1)/B > 1. But then floor(M/X)*X <= M/X*X == M and
12282:  *
12282:  *   ceil(floor(M/X)*X) <= ceil(M) == M.
12282:  *
12282:  * Thus the value of (4) gives the maximum n satisfying (1).
13029:  *
13029:  * For the final result we observe that in (4)
13029:  *
13029:  *    M*B == ARENA_INFO_OFFSET / sizeof(jsuword) * JS_BITS_PER_WORD
13029:  *        == ARENA_INFO_OFFSET * JS_BITS_PER_BYTE
13029:  *
13029:  *  and
13029:  *
13029:  *    B*s == JS_BITS_PER_WORD * sizeof(jsdouble) / sizeof(jsuword)
13029:  *        == JS_BITS_PER_DOUBLE.
    1:  */
33747: const size_t DOUBLES_PER_ARENA =
33747:     (ARENA_INFO_OFFSET * JS_BITS_PER_BYTE) / (JS_BITS_PER_DOUBLE + 1);
    1: 
    1: /*
13029:  * Check that  ARENA_INFO_OFFSET and sizeof(jsdouble) divides sizeof(jsuword).
12282:  */
13029: JS_STATIC_ASSERT(ARENA_INFO_OFFSET % sizeof(jsuword) == 0);
13029: JS_STATIC_ASSERT(sizeof(jsdouble) % sizeof(jsuword) == 0);
13029: JS_STATIC_ASSERT(sizeof(jsbitmap) == sizeof(jsuword));
13029: 
33747: const size_t DOUBLES_ARENA_BITMAP_WORDS =
33747:     JS_HOWMANY(DOUBLES_PER_ARENA, JS_BITS_PER_WORD);
13029: 
13029: /* Check that DOUBLES_PER_ARENA indeed maximises (1). */
12282: JS_STATIC_ASSERT(DOUBLES_PER_ARENA * sizeof(jsdouble) +
13029:                  DOUBLES_ARENA_BITMAP_WORDS * sizeof(jsuword) <=
12282:                  ARENA_INFO_OFFSET);
12282: 
12282: JS_STATIC_ASSERT((DOUBLES_PER_ARENA + 1) * sizeof(jsdouble) +
13029:                  sizeof(jsuword) *
13029:                  JS_HOWMANY((DOUBLES_PER_ARENA + 1), JS_BITS_PER_WORD) >
12282:                  ARENA_INFO_OFFSET);
12282: 
12282: /*
13029:  * When DOUBLES_PER_ARENA % BITS_PER_DOUBLE_FLAG_UNIT != 0, some bits in the
13029:  * last byte of the occupation bitmap are unused.
12282:  */
33747: const size_t UNUSED_DOUBLE_BITMAP_BITS =
33747:     DOUBLES_ARENA_BITMAP_WORDS * JS_BITS_PER_WORD - DOUBLES_PER_ARENA;
13029: 
13029: JS_STATIC_ASSERT(UNUSED_DOUBLE_BITMAP_BITS < JS_BITS_PER_WORD);
12282: 
33747: const size_t DOUBLES_ARENA_BITMAP_OFFSET =
33747:     ARENA_INFO_OFFSET - DOUBLES_ARENA_BITMAP_WORDS * sizeof(jsuword);
12282: 
12282: #define CHECK_DOUBLE_ARENA_INFO(arenaInfo)                                    \
12282:     (JS_ASSERT(IS_ARENA_INFO_ADDRESS(arenaInfo)),                             \
12282:      JS_ASSERT(!(arenaInfo)->list))                                           \
12282: 
12282: /*
12282:  * Get the start of the bitmap area containing double mark flags in the arena.
12282:  * To access the flag the code uses
    1:  *
13029:  *   JS_TEST_BIT(bitmapStart, index)
12282:  *
12282:  * That is, compared with the case of arenas with non-double things, we count
12282:  * flags from the start of the bitmap area, not from the end.
    1:  */
12282: #define DOUBLE_ARENA_BITMAP(arenaInfo)                                        \
12282:     (CHECK_DOUBLE_ARENA_INFO(arenaInfo),                                      \
13029:      (jsbitmap *) arenaInfo - DOUBLES_ARENA_BITMAP_WORDS)
12282: 
33747: #define DOUBLE_ARENA_BITMAP_END(arenaInfo)                                    \
33747:     (CHECK_DOUBLE_ARENA_INFO(arenaInfo), (jsbitmap *) (arenaInfo))
33747: 
12282: #define DOUBLE_THING_TO_INDEX(thing)                                          \
12282:     (CHECK_DOUBLE_ARENA_INFO(THING_TO_ARENA(thing)),                          \
12282:      JS_ASSERT(((jsuword) (thing) & GC_ARENA_MASK) <                          \
12282:                DOUBLES_ARENA_BITMAP_OFFSET),                                  \
12282:      ((uint32) (((jsuword) (thing) & GC_ARENA_MASK) / sizeof(jsdouble))))
12282: 
12282: static void
12282: ClearDoubleArenaFlags(JSGCArenaInfo *a)
12282: {
12282:     /*
12282:      * When some high bits in the last byte of the double occupation bitmap
33747:      * are unused, we must set them. Otherwise TurnUsedArenaIntoDoubleList
33747:      * will assume that they corresponds to some free cells and tries to
33747:      * allocate them.
12282:      *
12282:      * Note that the code works correctly with UNUSED_DOUBLE_BITMAP_BITS == 0.
12282:      */
33747:     jsbitmap *bitmap = DOUBLE_ARENA_BITMAP(a);
13029:     memset(bitmap, 0, (DOUBLES_ARENA_BITMAP_WORDS - 1) * sizeof *bitmap);
33747:     jsbitmap mask = ((jsbitmap) 1 << UNUSED_DOUBLE_BITMAP_BITS) - 1;
33747:     size_t nused = JS_BITS_PER_WORD - UNUSED_DOUBLE_BITMAP_BITS;
13029:     bitmap[DOUBLES_ARENA_BITMAP_WORDS - 1] = mask << nused;
12282: }
12282: 
16284: static JS_ALWAYS_INLINE JSBool
12282: IsMarkedDouble(JSGCArenaInfo *a, uint32 index)
12282: {
13029:     jsbitmap *bitmap;
12282: 
33952:     JS_ASSERT(a->hasMarkedDoubles);
13029:     bitmap = DOUBLE_ARENA_BITMAP(a);
13029:     return JS_TEST_BIT(bitmap, index);
12282: }
12282: 
    1: JS_STATIC_ASSERT(sizeof(JSStackHeader) >= 2 * sizeof(jsval));
    1: 
33952: JS_STATIC_ASSERT(sizeof(JSGCThing) <= sizeof(JSString));
33952: JS_STATIC_ASSERT(sizeof(JSGCThing) <= sizeof(jsdouble));
    1: 
    1: /* We want to use all the available GC thing space for object's slots. */
    1: JS_STATIC_ASSERT(sizeof(JSObject) % sizeof(JSGCThing) == 0);
    1: 
    1: #ifdef JS_GCMETER
10954: # define METER(x)               ((void) (x))
10954: # define METER_IF(condition, x) ((void) ((condition) && (x)))
    1: #else
    1: # define METER(x)               ((void) 0)
10954: # define METER_IF(condition, x) ((void) 0)
    1: #endif
    1: 
10954: #define METER_UPDATE_MAX(maxLval, rval)                                       \
10954:     METER_IF((maxLval) < (rval), (maxLval) = (rval))
10954: 
32823: static jsuword
32823: NewGCChunk(void)
32823: {
32823:     void *p;
32823: 
32823: #if defined(XP_WIN)
34300:     p = VirtualAlloc(NULL, GC_ARENAS_PER_CHUNK << GC_ARENA_SHIFT,
32823:                      MEM_COMMIT | MEM_RESERVE, PAGE_READWRITE);
32823:     return (jsuword) p;
34613: #elif defined(XP_OS2)
34613:     if (DosAllocMem(&p, GC_ARENAS_PER_CHUNK << GC_ARENA_SHIFT,
34613:                     OBJ_ANY | PAG_COMMIT | PAG_READ | PAG_WRITE)) {
34613:         if (DosAllocMem(&p, GC_ARENAS_PER_CHUNK << GC_ARENA_SHIFT,
34613:                         PAG_COMMIT | PAG_READ | PAG_WRITE)) {
34613:             return 0;
34613:         }
34613:     }
34613:     return (jsuword) p;
32823: #else
34300:     p = mmap(NULL, GC_ARENAS_PER_CHUNK << GC_ARENA_SHIFT,
32823:              PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
32823:     return (p == MAP_FAILED) ? 0 : (jsuword) p;
32823: #endif
32823: }
32823: 
32823: static void
32823: DestroyGCChunk(jsuword chunk)
32823: {
32823:     JS_ASSERT((chunk & GC_ARENA_MASK) == 0);
32823: #if defined(XP_WIN)
32823:     VirtualFree((void *) chunk, 0, MEM_RELEASE);
34613: #elif defined(XP_OS2)
34613:     DosFreeMem((void *) chunk);
32823: #elif defined(SOLARIS)
34300:     munmap((char *) chunk, GC_ARENAS_PER_CHUNK << GC_ARENA_SHIFT);
32823: #else
34300:     munmap((void *) chunk, GC_ARENAS_PER_CHUNK << GC_ARENA_SHIFT);
32823: #endif
32823: }
32823: 
32823: static void
32823: AddChunkToList(JSRuntime *rt, JSGCChunkInfo *ci)
32823: {
32823:     ci->prevp = &rt->gcChunkList;
32823:     ci->next = rt->gcChunkList;
32823:     if (rt->gcChunkList) {
32823:         JS_ASSERT(rt->gcChunkList->prevp == &rt->gcChunkList);
32823:         rt->gcChunkList->prevp = &ci->next;
32823:     }
32823:     rt->gcChunkList = ci;
32823: }
32823: 
32823: static void
32823: RemoveChunkFromList(JSRuntime *rt, JSGCChunkInfo *ci)
32823: {
32823:     *ci->prevp = ci->next;
32823:     if (ci->next) {
32823:         JS_ASSERT(ci->next->prevp == &ci->next);
32823:         ci->next->prevp = ci->prevp;
32823:     }
32823: }
32823: 
 5917: static JSGCArenaInfo *
34329: NewGCArena(JSContext *cx)
 5917: {
32823:     jsuword chunk;
12478:     JSGCArenaInfo *a;
 5917: 
34329:     JSRuntime *rt = cx->runtime;
35078:     if (!JS_THREAD_DATA(cx)->waiveGCQuota && rt->gcBytes >= rt->gcMaxBytes) {
34329:         /*
34329:          * FIXME bug 524051 We cannot run a last-ditch GC on trace for now, so
35078:          * just pretend we are out of memory which will throw us off trace and
35078:          * we will re-try this code path from the interpreter.
34329:          */
34329:         if (!JS_ON_TRACE(cx))
32823:             return NULL;
34329:         js_TriggerGC(cx, true);
34329:     }
32823: 
32823:     JSGCChunkInfo *ci;
32823:     uint32 i;
32823:     JSGCArenaInfo *aprev;
32823: 
32823:     ci = rt->gcChunkList;
32823:     if (!ci) {
32823:         chunk = NewGCChunk();
32823:         if (chunk == 0)
12282:             return NULL;
32823:         JS_ASSERT((chunk & GC_ARENA_MASK) == 0);
32823:         a = GET_ARENA_INFO(chunk, 0);
32823:         a->firstArena = JS_TRUE;
32823:         a->arenaIndex = 0;
32823:         aprev = NULL;
32823:         i = 0;
32823:         do {
32823:             a->prev = aprev;
32823:             aprev = a;
32823:             ++i;
32823:             a = GET_ARENA_INFO(chunk, i);
32823:             a->firstArena = JS_FALSE;
32823:             a->arenaIndex = i;
34300:         } while (i != GC_ARENAS_PER_CHUNK - 1);
32823:         ci = GET_CHUNK_INFO(chunk, 0);
32823:         ci->lastFreeArena = aprev;
34300:         ci->numFreeArenas = GC_ARENAS_PER_CHUNK - 1;
32823:         AddChunkToList(rt, ci);
32823:     } else {
32823:         JS_ASSERT(ci->prevp == &rt->gcChunkList);
32823:         a = ci->lastFreeArena;
32823:         aprev = a->prev;
32823:         if (!aprev) {
32823:             JS_ASSERT(ci->numFreeArenas == 1);
32823:             JS_ASSERT(ARENA_INFO_TO_START(a) == (jsuword) ci);
32823:             RemoveChunkFromList(rt, ci);
32823:             chunk = GET_ARENA_CHUNK(a, GET_ARENA_INDEX(a));
32823:             SET_CHUNK_INFO_INDEX(chunk, NO_FREE_ARENAS);
32823:         } else {
32823:             JS_ASSERT(ci->numFreeArenas >= 2);
32823:             JS_ASSERT(ARENA_INFO_TO_START(a) != (jsuword) ci);
32823:             ci->lastFreeArena = aprev;
32823:             ci->numFreeArenas--;
32823:         }
32823:     }
32823: 
32783:     rt->gcBytes += GC_ARENA_SIZE;
36410:     a->prevUnmarkedPage = 0;
32823: 
 5917:     return a;
    1: }
    1: 
    1: static void
32823: DestroyGCArenas(JSRuntime *rt, JSGCArenaInfo *last)
32823: {
32823:     JSGCArenaInfo *a;
32823: 
32823:     while (last) {
32823:         a = last;
32823:         last = last->prev;
32823: 
32823:         METER(rt->gcStats.afree++);
32823:         JS_ASSERT(rt->gcBytes >= GC_ARENA_SIZE);
32823:         rt->gcBytes -= GC_ARENA_SIZE;
32823: 
32823:         uint32 arenaIndex;
32823:         jsuword chunk;
32823:         uint32 chunkInfoIndex;
32823:         JSGCChunkInfo *ci;
32823: #ifdef DEBUG
32823:         jsuword firstArena;
32823: 
32823:         firstArena = a->firstArena;
32823:         arenaIndex = a->arenaIndex;
34300:         memset((void *) ARENA_INFO_TO_START(a), JS_FREE_PATTERN, GC_ARENA_SIZE);
32823:         a->firstArena = firstArena;
32823:         a->arenaIndex = arenaIndex;
32823: #endif
32823:         arenaIndex = GET_ARENA_INDEX(a);
32823:         chunk = GET_ARENA_CHUNK(a, arenaIndex);
32823:         chunkInfoIndex = GET_CHUNK_INFO_INDEX(chunk);
32823:         if (chunkInfoIndex == NO_FREE_ARENAS) {
32823:             chunkInfoIndex = arenaIndex;
32823:             SET_CHUNK_INFO_INDEX(chunk, arenaIndex);
32823:             ci = GET_CHUNK_INFO(chunk, chunkInfoIndex);
32823:             a->prev = NULL;
32823:             ci->lastFreeArena = a;
32823:             ci->numFreeArenas = 1;
32823:             AddChunkToList(rt, ci);
32823:         } else {
32823:             JS_ASSERT(chunkInfoIndex != arenaIndex);
32823:             ci = GET_CHUNK_INFO(chunk, chunkInfoIndex);
32823:             JS_ASSERT(ci->numFreeArenas != 0);
32823:             JS_ASSERT(ci->lastFreeArena);
32823:             JS_ASSERT(a != ci->lastFreeArena);
34300:             if (ci->numFreeArenas == GC_ARENAS_PER_CHUNK - 1) {
32823:                 RemoveChunkFromList(rt, ci);
32823:                 DestroyGCChunk(chunk);
32823:             } else {
32823:                 ++ci->numFreeArenas;
32823:                 a->prev = ci->lastFreeArena;
32823:                 ci->lastFreeArena = a;
32823:             }
32823:         }
32823:     }
32823: }
32823: 
33582: static inline size_t
33582: GetFinalizableThingSize(unsigned thingKind)
33582: {
33582:     JS_STATIC_ASSERT(JS_EXTERNAL_STRING_LIMIT == 8);
33582: 
33582:     static const uint8 map[FINALIZE_LIMIT] = {
33582:         sizeof(JSObject),   /* FINALIZE_OBJECT */
33582:         sizeof(JSFunction), /* FINALIZE_FUNCTION */
33582: #if JS_HAS_XML_SUPPORT
33582:         sizeof(JSXML),      /* FINALIZE_XML */
33582: #endif
33582:         sizeof(JSString),   /* FINALIZE_STRING */
33582:         sizeof(JSString),   /* FINALIZE_EXTERNAL_STRING0 */
33582:         sizeof(JSString),   /* FINALIZE_EXTERNAL_STRING1 */
33582:         sizeof(JSString),   /* FINALIZE_EXTERNAL_STRING2 */
33582:         sizeof(JSString),   /* FINALIZE_EXTERNAL_STRING3 */
33582:         sizeof(JSString),   /* FINALIZE_EXTERNAL_STRING4 */
33582:         sizeof(JSString),   /* FINALIZE_EXTERNAL_STRING5 */
33582:         sizeof(JSString),   /* FINALIZE_EXTERNAL_STRING6 */
33582:         sizeof(JSString),   /* FINALIZE_EXTERNAL_STRING7 */
33582:     };
33582: 
33582:     JS_ASSERT(thingKind < FINALIZE_LIMIT);
33582:     return map[thingKind];
33582: }
33582: 
33582: static inline size_t
33952: GetFinalizableTraceKind(size_t thingKind)
33582: {
33582:     JS_STATIC_ASSERT(JS_EXTERNAL_STRING_LIMIT == 8);
33582: 
33582:     static const uint8 map[FINALIZE_LIMIT] = {
33582:         JSTRACE_OBJECT,     /* FINALIZE_OBJECT */
33582:         JSTRACE_OBJECT,     /* FINALIZE_FUNCTION */
33582: #if JS_HAS_XML_SUPPORT      /* FINALIZE_XML */
33582:         JSTRACE_XML,
33582: #endif                      /* FINALIZE_STRING */
33582:         JSTRACE_STRING,
33582:         JSTRACE_STRING,     /* FINALIZE_EXTERNAL_STRING0 */
33582:         JSTRACE_STRING,     /* FINALIZE_EXTERNAL_STRING1 */
33582:         JSTRACE_STRING,     /* FINALIZE_EXTERNAL_STRING2 */
33582:         JSTRACE_STRING,     /* FINALIZE_EXTERNAL_STRING3 */
33582:         JSTRACE_STRING,     /* FINALIZE_EXTERNAL_STRING4 */
33582:         JSTRACE_STRING,     /* FINALIZE_EXTERNAL_STRING5 */
33582:         JSTRACE_STRING,     /* FINALIZE_EXTERNAL_STRING6 */
33582:         JSTRACE_STRING,     /* FINALIZE_EXTERNAL_STRING7 */
33582:     };
33582: 
33952:     JS_ASSERT(thingKind < FINALIZE_LIMIT);
33952:     return map[thingKind];
33952: }
33952: 
33952: static inline size_t
33952: GetFinalizableArenaTraceKind(JSGCArenaInfo *a)
33952: {
33952:     JS_ASSERT(a->list);
33952:     return GetFinalizableTraceKind(a->list->thingKind);
33582: }
33582: 
32823: static void
    1: InitGCArenaLists(JSRuntime *rt)
    1: {
33582:     for (unsigned i = 0; i != FINALIZE_LIMIT; ++i) {
33582:         JSGCArenaList *arenaList = &rt->gcArenaList[i];
33952:         arenaList->head = NULL;
33952:         arenaList->cursor = NULL;
33952:         arenaList->thingKind = i;
33947:         arenaList->thingSize = GetFinalizableThingSize(i);
    1:     }
33952:     rt->gcDoubleArenaList.head = NULL;
33747:     rt->gcDoubleArenaList.cursor = NULL;
    1: }
    1: 
    1: static void
    1: FinishGCArenaLists(JSRuntime *rt)
    1: {
33582:     for (unsigned i = 0; i < FINALIZE_LIMIT; i++) {
33582:         JSGCArenaList *arenaList = &rt->gcArenaList[i];
33952:         DestroyGCArenas(rt, arenaList->head);
33952:         arenaList->head = NULL;
33952:         arenaList->cursor = NULL;
    1:     }
33952:     DestroyGCArenas(rt, rt->gcDoubleArenaList.head);
33952:     rt->gcDoubleArenaList.head = NULL;
33747:     rt->gcDoubleArenaList.cursor = NULL;
12282: 
 5917:     rt->gcBytes = 0;
32823:     JS_ASSERT(rt->gcChunkList == 0);
    1: }
    1: 
12282: /*
12282:  * This function must not be called when thing is jsdouble.
12282:  */
 8005: static uint8 *
 8005: GetGCThingFlags(void *thing)
    1: {
 5917:     JSGCArenaInfo *a;
 5917:     uint32 index;
    1: 
 5917:     a = THING_TO_ARENA(thing);
 5917:     index = THING_TO_INDEX(thing, a->list->thingSize);
 5917:     return THING_FLAGP(a, index);
    1: }
    1: 
 8005: intN
 8005: js_GetExternalStringGCType(JSString *str)
 8005: {
33582:     JS_STATIC_ASSERT(FINALIZE_STRING + 1 == FINALIZE_EXTERNAL_STRING0);
32686:     JS_ASSERT(!JSString::isStatic(str));
32686: 
33582:     unsigned thingKind = THING_TO_ARENA(str)->list->thingKind;
33582:     JS_ASSERT(IsFinalizableStringKind(thingKind));
33582:     return intN(thingKind) - intN(FINALIZE_EXTERNAL_STRING0);
 8005: }
 8005: 
 8005: JS_FRIEND_API(uint32)
 8005: js_GetGCThingTraceKind(void *thing)
 8005: {
32734:     if (JSString::isStatic(thing))
32734:         return JSTRACE_STRING;
32734: 
33582:     JSGCArenaInfo *a = THING_TO_ARENA(thing);
12282:     if (!a->list)
12282:         return JSTRACE_DOUBLE;
33582:     return GetFinalizableArenaTraceKind(a);
    1: }
    1: 
    1: JSRuntime*
    1: js_GetGCStringRuntime(JSString *str)
    1: {
33582:     JSGCArenaList *list = THING_TO_ARENA(str)->list;
33582:     JS_ASSERT(list->thingSize == sizeof(JSString));
33582: 
33582:     unsigned i = list->thingKind;
33582:     JS_ASSERT(i == FINALIZE_STRING ||
33582:               (FINALIZE_EXTERNAL_STRING0 <= i &&
33582:                i < FINALIZE_EXTERNAL_STRING0 + JS_EXTERNAL_STRING_LIMIT));
33582:     return (JSRuntime *)((uint8 *)(list - i) -
33582:                          offsetof(JSRuntime, gcArenaList));
    1: }
    1: 
36410: bool
36410: js_IsAboutToBeFinalized(void *thing)
    1: {
12282:     JSGCArenaInfo *a;
12282:     uint32 index, flags;
12282: 
36660:     if (JSString::isStatic(thing))
36660:         return false;
36660: 
12282:     a = THING_TO_ARENA(thing);
12282:     if (!a->list) {
12282:         /*
12282:          * Check if arena has no marked doubles. In that case the bitmap with
12282:          * the mark flags contains all garbage as it is initialized only when
12282:          * marking the first double in the arena.
12282:          */
33952:         if (!a->hasMarkedDoubles)
12282:             return JS_TRUE;
12282:         index = DOUBLE_THING_TO_INDEX(thing);
12282:         return !IsMarkedDouble(a, index);
12282:     }
12282:     index = THING_TO_INDEX(thing, a->list->thingSize);
12282:     flags = *THING_FLAGP(a, index);
33952:     return !(flags & (GCF_MARK | GCF_LOCK));
    1: }
    1: 
    1: /* This is compatible with JSDHashEntryStub. */
    1: typedef struct JSGCRootHashEntry {
    1:     JSDHashEntryHdr hdr;
    1:     void            *root;
    1:     const char      *name;
    1: } JSGCRootHashEntry;
    1: 
    1: /* Initial size of the gcRootsHash table (SWAG, small enough to amortize). */
    1: #define GC_ROOTS_SIZE   256
 5917: 
32823: /*
32823:  * For a CPU with extremely large pages using them for GC things wastes
32823:  * too much memory.
32823:  */
32823: #define GC_ARENAS_PER_CPU_PAGE_LIMIT JS_BIT(18 - GC_ARENA_SHIFT)
32823: 
32823: JS_STATIC_ASSERT(GC_ARENAS_PER_CPU_PAGE_LIMIT <= NO_FREE_ARENAS);
32823: 
    1: JSBool
32823: js_InitGC(JSRuntime *rt, uint32 maxbytes)
    1: {
    1:     InitGCArenaLists(rt);
    1:     if (!JS_DHashTableInit(&rt->gcRootsHash, JS_DHashGetStubOps(), NULL,
    1:                            sizeof(JSGCRootHashEntry), GC_ROOTS_SIZE)) {
    1:         rt->gcRootsHash.ops = NULL;
    1:         return JS_FALSE;
    1:     }
    1:     rt->gcLocksHash = NULL;     /* create lazily */
    1: 
32553:     /*
32553:      * Separate gcMaxMallocBytes from gcMaxBytes but initialize to maxbytes
32553:      * for default backward API compatibility.
32553:      */
34288:     rt->gcMaxBytes = maxbytes;
34288:     rt->setGCMaxMallocBytes(maxbytes);
34288: 
32543:     rt->gcEmptyArenaPoolLifespan = 30000;
32543: 
31888:     /*
32553:      * By default the trigger factor gets maximum possible value. This
32553:      * means that GC will not be triggered by growth of GC memory (gcBytes).
31888:      */
32553:     rt->setGCTriggerFactor((uint32) -1);
32553: 
32553:     /*
32553:      * The assigned value prevents GC from running when GC memory is too low
32553:      * (during JS engine start).
32553:      */
32553:     rt->setGCLastBytes(8192);
24313: 
10954:     METER(memset(&rt->gcStats, 0, sizeof rt->gcStats));
    1:     return JS_TRUE;
    1: }
    1: 
    1: #ifdef JS_GCMETER
12282: 
12282: static void
12282: UpdateArenaStats(JSGCArenaStats *st, uint32 nlivearenas, uint32 nkilledArenas,
12282:                  uint32 nthings)
12282: {
12282:     size_t narenas;
12282: 
12282:     narenas = nlivearenas + nkilledArenas;
12282:     JS_ASSERT(narenas >= st->livearenas);
12282: 
12282:     st->newarenas = narenas - st->livearenas;
12282:     st->narenas = narenas;
12282:     st->livearenas = nlivearenas;
12282:     if (st->maxarenas < narenas)
12282:         st->maxarenas = narenas;
12282:     st->totalarenas += narenas;
12282: 
12282:     st->nthings = nthings;
12282:     if (st->maxthings < nthings)
12282:         st->maxthings = nthings;
12282:     st->totalthings += nthings;
12282: }
12282: 
    1: JS_FRIEND_API(void)
    1: js_DumpGCStats(JSRuntime *rt, FILE *fp)
    1: {
12282:     int i;
    1:     size_t sumArenas, sumTotalArenas;
12282:     size_t sumThings, sumMaxThings;
12282:     size_t sumThingSize, sumTotalThingSize;
12282:     size_t sumArenaCapacity, sumTotalArenaCapacity;
12282:     JSGCArenaStats *st;
12282:     size_t thingSize, thingsPerArena;
12282:     size_t sumAlloc, sumLocalAlloc, sumFail, sumRetry;
    1: 
    1:     fprintf(fp, "\nGC allocation statistics:\n");
    1: 
    1: #define UL(x)       ((unsigned long)(x))
    1: #define ULSTAT(x)   UL(rt->gcStats.x)
12282: #define PERCENT(x,y)  (100.0 * (double) (x) / (double) (y))
12282: 
    1:     sumArenas = 0;
    1:     sumTotalArenas = 0;
12282:     sumThings = 0;
12282:     sumMaxThings = 0;
12282:     sumThingSize = 0;
12282:     sumTotalThingSize = 0;
12282:     sumArenaCapacity = 0;
12282:     sumTotalArenaCapacity = 0;
12282:     sumAlloc = 0;
12282:     sumLocalAlloc = 0;
12282:     sumFail = 0;
12282:     sumRetry = 0;
12282:     for (i = -1; i < (int) GC_NUM_FREELISTS; i++) {
12282:         if (i == -1) {
12282:             thingSize = sizeof(jsdouble);
12282:             thingsPerArena = DOUBLES_PER_ARENA;
12282:             st = &rt->gcStats.doubleArenaStats;
12282:             fprintf(fp,
12282:                     "Arena list for double values (%lu doubles per arena):",
12282:                     UL(thingsPerArena));
12282:         } else {
12282:             thingSize = rt->gcArenaList[i].thingSize;
12282:             thingsPerArena = THINGS_PER_ARENA(thingSize);
12282:             st = &rt->gcStats.arenaStats[i];
12282:             fprintf(fp,
12282:                     "Arena list %d (thing size %lu, %lu things per arena):",
12282:                     i, UL(GC_FREELIST_NBYTES(i)), UL(thingsPerArena));
12282:         }
12282:         if (st->maxarenas == 0) {
12282:             fputs(" NEVER USED\n", fp);
    1:             continue;
    1:         }
12282:         putc('\n', fp);
12282:         fprintf(fp, "           arenas before GC: %lu\n", UL(st->narenas));
12282:         fprintf(fp, "       new arenas before GC: %lu (%.1f%%)\n",
12282:                 UL(st->newarenas), PERCENT(st->newarenas, st->narenas));
12282:         fprintf(fp, "            arenas after GC: %lu (%.1f%%)\n",
12282:                 UL(st->livearenas), PERCENT(st->livearenas, st->narenas));
12282:         fprintf(fp, "                 max arenas: %lu\n", UL(st->maxarenas));
12282:         fprintf(fp, "                     things: %lu\n", UL(st->nthings));
12282:         fprintf(fp, "        GC cell utilization: %.1f%%\n",
12282:                 PERCENT(st->nthings, thingsPerArena * st->narenas));
12282:         fprintf(fp, "   average cell utilization: %.1f%%\n",
12282:                 PERCENT(st->totalthings, thingsPerArena * st->totalarenas));
12282:         fprintf(fp, "                 max things: %lu\n", UL(st->maxthings));
12282:         fprintf(fp, "             alloc attempts: %lu\n", UL(st->alloc));
12282:         fprintf(fp, "        alloc without locks: %1u  (%.1f%%)\n",
12282:                 UL(st->localalloc), PERCENT(st->localalloc, st->alloc));
12282:         sumArenas += st->narenas;
12282:         sumTotalArenas += st->totalarenas;
12282:         sumThings += st->nthings;
12282:         sumMaxThings += st->maxthings;
12282:         sumThingSize += thingSize * st->nthings;
12282:         sumTotalThingSize += thingSize * st->totalthings;
12282:         sumArenaCapacity += thingSize * thingsPerArena * st->narenas;
12282:         sumTotalArenaCapacity += thingSize * thingsPerArena * st->totalarenas;
12282:         sumAlloc += st->alloc;
12282:         sumLocalAlloc += st->localalloc;
12282:         sumFail += st->fail;
12282:         sumRetry += st->retry;
    1:     }
    1:     fprintf(fp, "TOTAL STATS:\n");
    1:     fprintf(fp, "            bytes allocated: %lu\n", UL(rt->gcBytes));
    1:     fprintf(fp, "            total GC arenas: %lu\n", UL(sumArenas));
12282:     fprintf(fp, "            total GC things: %lu\n", UL(sumThings));
12282:     fprintf(fp, "        max total GC things: %lu\n", UL(sumMaxThings));
12282:     fprintf(fp, "        GC cell utilization: %.1f%%\n",
12282:             PERCENT(sumThingSize, sumArenaCapacity));
12282:     fprintf(fp, "   average cell utilization: %.1f%%\n",
12282:             PERCENT(sumTotalThingSize, sumTotalArenaCapacity));
12282:     fprintf(fp, "allocation retries after GC: %lu\n", UL(sumRetry));
12282:     fprintf(fp, "             alloc attempts: %lu\n", UL(sumAlloc));
12282:     fprintf(fp, "        alloc without locks: %1u  (%.1f%%)\n",
12282:             UL(sumLocalAlloc), PERCENT(sumLocalAlloc, sumAlloc));
12282:     fprintf(fp, "        allocation failures: %lu\n", UL(sumFail));
    1:     fprintf(fp, "         things born locked: %lu\n", ULSTAT(lockborn));
    1:     fprintf(fp, "           valid lock calls: %lu\n", ULSTAT(lock));
    1:     fprintf(fp, "         valid unlock calls: %lu\n", ULSTAT(unlock));
    1:     fprintf(fp, "       mark recursion depth: %lu\n", ULSTAT(depth));
    1:     fprintf(fp, "     maximum mark recursion: %lu\n", ULSTAT(maxdepth));
    1:     fprintf(fp, "     mark C recursion depth: %lu\n", ULSTAT(cdepth));
    1:     fprintf(fp, "   maximum mark C recursion: %lu\n", ULSTAT(maxcdepth));
36410:     fprintf(fp, "      delayed tracing calls: %lu\n", ULSTAT(unmarked));
    1: #ifdef DEBUG
36410:     fprintf(fp, "      max trace later count: %lu\n", ULSTAT(maxunmarked));
    1: #endif
    1:     fprintf(fp, "   maximum GC nesting level: %lu\n", ULSTAT(maxlevel));
    1:     fprintf(fp, "potentially useful GC calls: %lu\n", ULSTAT(poke));
32823:     fprintf(fp, "  thing arenas freed so far: %lu\n", ULSTAT(afree));
    1:     fprintf(fp, "     stack segments scanned: %lu\n", ULSTAT(stackseg));
    1:     fprintf(fp, "stack segment slots scanned: %lu\n", ULSTAT(segslots));
    1:     fprintf(fp, "reachable closeable objects: %lu\n", ULSTAT(nclose));
    1:     fprintf(fp, "    max reachable closeable: %lu\n", ULSTAT(maxnclose));
    1:     fprintf(fp, "      scheduled close hooks: %lu\n", ULSTAT(closelater));
    1:     fprintf(fp, "  max scheduled close hooks: %lu\n", ULSTAT(maxcloselater));
12282: 
    1: #undef UL
12282: #undef ULSTAT
12282: #undef PERCENT
    1: 
    1: #ifdef JS_ARENAMETER
    1:     JS_DumpArenaStats(fp);
    1: #endif
    1: }
    1: #endif
    1: 
    1: #ifdef DEBUG
    1: static void
    1: CheckLeakedRoots(JSRuntime *rt);
    1: #endif
    1: 
    1: void
    1: js_FinishGC(JSRuntime *rt)
    1: {
    1: #ifdef JS_ARENAMETER
    1:     JS_DumpArenaStats(stdout);
    1: #endif
    1: #ifdef JS_GCMETER
    1:     js_DumpGCStats(rt, stdout);
    1: #endif
    1: 
34299:     rt->gcIteratorTable.clear();
    1:     FinishGCArenaLists(rt);
    1: 
    1:     if (rt->gcRootsHash.ops) {
    1: #ifdef DEBUG
    1:         CheckLeakedRoots(rt);
    1: #endif
    1:         JS_DHashTableFinish(&rt->gcRootsHash);
    1:         rt->gcRootsHash.ops = NULL;
    1:     }
    1:     if (rt->gcLocksHash) {
    1:         JS_DHashTableDestroy(rt->gcLocksHash);
    1:         rt->gcLocksHash = NULL;
    1:     }
    1: }
    1: 
    1: JSBool
    1: js_AddRoot(JSContext *cx, void *rp, const char *name)
    1: {
    1:     JSBool ok = js_AddRootRT(cx->runtime, rp, name);
    1:     if (!ok)
    1:         JS_ReportOutOfMemory(cx);
    1:     return ok;
    1: }
    1: 
    1: JSBool
    1: js_AddRootRT(JSRuntime *rt, void *rp, const char *name)
    1: {
    1:     JSBool ok;
    1:     JSGCRootHashEntry *rhe;
    1: 
    1:     /*
    1:      * Due to the long-standing, but now removed, use of rt->gcLock across the
    1:      * bulk of js_GC, API users have come to depend on JS_AddRoot etc. locking
    1:      * properly with a racing GC, without calling JS_AddRoot from a request.
    1:      * We have to preserve API compatibility here, now that we avoid holding
    1:      * rt->gcLock across the mark phase (including the root hashtable mark).
    1:      */
    1:     JS_LOCK_GC(rt);
24871:     js_WaitForGC(rt);
    1:     rhe = (JSGCRootHashEntry *)
    1:           JS_DHashTableOperate(&rt->gcRootsHash, rp, JS_DHASH_ADD);
    1:     if (rhe) {
    1:         rhe->root = rp;
    1:         rhe->name = name;
    1:         ok = JS_TRUE;
    1:     } else {
    1:         ok = JS_FALSE;
    1:     }
    1:     JS_UNLOCK_GC(rt);
    1:     return ok;
    1: }
    1: 
    1: JSBool
    1: js_RemoveRoot(JSRuntime *rt, void *rp)
    1: {
    1:     /*
    1:      * Due to the JS_RemoveRootRT API, we may be called outside of a request.
    1:      * Same synchronization drill as above in js_AddRoot.
    1:      */
    1:     JS_LOCK_GC(rt);
24871:     js_WaitForGC(rt);
    1:     (void) JS_DHashTableOperate(&rt->gcRootsHash, rp, JS_DHASH_REMOVE);
    1:     rt->gcPoke = JS_TRUE;
    1:     JS_UNLOCK_GC(rt);
    1:     return JS_TRUE;
    1: }
    1: 
    1: #ifdef DEBUG
    1: 
18907: static JSDHashOperator
    1: js_root_printer(JSDHashTable *table, JSDHashEntryHdr *hdr, uint32 i, void *arg)
    1: {
    1:     uint32 *leakedroots = (uint32 *)arg;
    1:     JSGCRootHashEntry *rhe = (JSGCRootHashEntry *)hdr;
    1: 
    1:     (*leakedroots)++;
    1:     fprintf(stderr,
    1:             "JS engine warning: leaking GC root \'%s\' at %p\n",
    1:             rhe->name ? (char *)rhe->name : "", rhe->root);
    1: 
    1:     return JS_DHASH_NEXT;
    1: }
    1: 
    1: static void
    1: CheckLeakedRoots(JSRuntime *rt)
    1: {
    1:     uint32 leakedroots = 0;
    1: 
    1:     /* Warn (but don't assert) debug builds of any remaining roots. */
    1:     JS_DHashTableEnumerate(&rt->gcRootsHash, js_root_printer,
    1:                            &leakedroots);
    1:     if (leakedroots > 0) {
    1:         if (leakedroots == 1) {
    1:             fprintf(stderr,
11799: "JS engine warning: 1 GC root remains after destroying the JSRuntime at %p.\n"
    1: "                   This root may point to freed memory. Objects reachable\n"
12282: "                   through it have not been finalized.\n",
12282:                     (void *) rt);
    1:         } else {
    1:             fprintf(stderr,
11799: "JS engine warning: %lu GC roots remain after destroying the JSRuntime at %p.\n"
    1: "                   These roots may point to freed memory. Objects reachable\n"
    1: "                   through them have not been finalized.\n",
12282:                     (unsigned long) leakedroots, (void *) rt);
    1:         }
    1:     }
    1: }
    1: 
    1: typedef struct NamedRootDumpArgs {
    1:     void (*dump)(const char *name, void *rp, void *data);
    1:     void *data;
    1: } NamedRootDumpArgs;
    1: 
18907: static JSDHashOperator
    1: js_named_root_dumper(JSDHashTable *table, JSDHashEntryHdr *hdr, uint32 number,
    1:                      void *arg)
    1: {
    1:     NamedRootDumpArgs *args = (NamedRootDumpArgs *) arg;
    1:     JSGCRootHashEntry *rhe = (JSGCRootHashEntry *)hdr;
    1: 
    1:     if (rhe->name)
    1:         args->dump(rhe->name, rhe->root, args->data);
    1:     return JS_DHASH_NEXT;
    1: }
    1: 
18563: JS_BEGIN_EXTERN_C
    1: void
    1: js_DumpNamedRoots(JSRuntime *rt,
    1:                   void (*dump)(const char *name, void *rp, void *data),
    1:                   void *data)
    1: {
    1:     NamedRootDumpArgs args;
    1: 
    1:     args.dump = dump;
    1:     args.data = data;
    1:     JS_DHashTableEnumerate(&rt->gcRootsHash, js_named_root_dumper, &args);
    1: }
18563: JS_END_EXTERN_C
    1: 
    1: #endif /* DEBUG */
    1: 
    1: typedef struct GCRootMapArgs {
    1:     JSGCRootMapFun map;
    1:     void *data;
    1: } GCRootMapArgs;
    1: 
18907: static JSDHashOperator
    1: js_gcroot_mapper(JSDHashTable *table, JSDHashEntryHdr *hdr, uint32 number,
    1:                  void *arg)
    1: {
    1:     GCRootMapArgs *args = (GCRootMapArgs *) arg;
    1:     JSGCRootHashEntry *rhe = (JSGCRootHashEntry *)hdr;
    1:     intN mapflags;
 3164:     int op;
    1: 
    1:     mapflags = args->map(rhe->root, rhe->name, args->data);
    1: 
    1: #if JS_MAP_GCROOT_NEXT == JS_DHASH_NEXT &&                                     \
    1:     JS_MAP_GCROOT_STOP == JS_DHASH_STOP &&                                     \
    1:     JS_MAP_GCROOT_REMOVE == JS_DHASH_REMOVE
    1:     op = (JSDHashOperator)mapflags;
    1: #else
    1:     op = JS_DHASH_NEXT;
    1:     if (mapflags & JS_MAP_GCROOT_STOP)
    1:         op |= JS_DHASH_STOP;
    1:     if (mapflags & JS_MAP_GCROOT_REMOVE)
    1:         op |= JS_DHASH_REMOVE;
    1: #endif
    1: 
 3164:     return (JSDHashOperator) op;
    1: }
    1: 
    1: uint32
    1: js_MapGCRoots(JSRuntime *rt, JSGCRootMapFun map, void *data)
    1: {
    1:     GCRootMapArgs args;
    1:     uint32 rv;
    1: 
    1:     args.map = map;
    1:     args.data = data;
    1:     JS_LOCK_GC(rt);
    1:     rv = JS_DHashTableEnumerate(&rt->gcRootsHash, js_gcroot_mapper, &args);
    1:     JS_UNLOCK_GC(rt);
    1:     return rv;
    1: }
    1: 
    1: JSBool
    1: js_RegisterCloseableIterator(JSContext *cx, JSObject *obj)
    1: {
    1:     JSRuntime *rt;
    1:     JSBool ok;
    1: 
    1:     rt = cx->runtime;
    1:     JS_ASSERT(!rt->gcRunning);
    1: 
    1:     JS_LOCK_GC(rt);
34299:     ok = rt->gcIteratorTable.append(obj);
    1:     JS_UNLOCK_GC(rt);
    1:     return ok;
    1: }
    1: 
    1: static void
 3025: CloseNativeIterators(JSContext *cx)
    1: {
34299:     JSRuntime *rt = cx->runtime;
34299:     size_t length = rt->gcIteratorTable.length();
34299:     JSObject **array = rt->gcIteratorTable.begin();
34299: 
34299:     size_t newLength = 0;
34299:     for (size_t i = 0; i < length; ++i) {
34299:         JSObject *obj = array[i];
36410:         if (js_IsAboutToBeFinalized(obj))
 3025:             js_CloseNativeIterator(cx, obj);
    1:         else
34299:             array[newLength++] = obj;
    1:     }
34299:     rt->gcIteratorTable.resize(newLength);
    1: }
    1: 
32553: void
32553: JSRuntime::setGCTriggerFactor(uint32 factor)
32553: {
32553:     JS_ASSERT(factor >= 100);
32553: 
32553:     gcTriggerFactor = factor;
32553:     setGCLastBytes(gcLastBytes);
32553: }
32553: 
32553: void
32553: JSRuntime::setGCLastBytes(size_t lastBytes)
32553: {
32553:     gcLastBytes = lastBytes;
32553:     uint64 triggerBytes = uint64(lastBytes) * uint64(gcTriggerFactor / 100);
32553:     if (triggerBytes != size_t(triggerBytes))
32553:         triggerBytes = size_t(-1);
32553:     gcTriggerBytes = size_t(triggerBytes);
32553: }
32553: 
33952: void
33952: JSGCFreeLists::purge()
33952: {
33952:     /*
33952:      * Return the free list back to the arena so the GC finalization will not
33952:      * run the finalizers over unitialized bytes from free things.
33952:      */
33952:     for (JSGCThing **p = finalizables; p != JS_ARRAY_END(finalizables); ++p) {
34288:         JSGCThing *freeListHead = *p;
34288:         if (freeListHead) {
34288:             JSGCArenaInfo *a = THING_TO_ARENA(freeListHead);
36410:             JS_ASSERT(!a->freeList);
36410:             a->freeList = freeListHead;
34288:             *p = NULL;
34288:         }
33952:     }
33952:     doubles = NULL;
33952: }
33952: 
35075: void
35075: JSGCFreeLists::moveTo(JSGCFreeLists *another)
35075: {
35075:     *another = *this;
35075:     doubles = NULL;
35075:     memset(finalizables, 0, sizeof(finalizables));
35075:     JS_ASSERT(isEmpty());
35075: }
35075: 
33952: static inline bool
32553: IsGCThresholdReached(JSRuntime *rt)
32553: {
32553: #ifdef JS_GC_ZEAL
32553:     if (rt->gcZeal >= 1)
32553:         return true;
32553: #endif
32553: 
32553:     /*
32553:      * Since the initial value of the gcLastBytes parameter is not equal to
32553:      * zero (see the js_InitGC function) the return value is false when
32553:      * the gcBytes value is close to zero at the JS engine start.
32553:      */
34288:     return rt->isGCMallocLimitReached() || rt->gcBytes >= rt->gcTriggerBytes;
32553: }
32553: 
35075: static inline JSGCFreeLists *
35075: GetGCFreeLists(JSContext *cx)
35075: {
35075:     JSThreadData *td = JS_THREAD_DATA(cx);
35075:     if (!td->localRootStack)
35075:         return &td->gcFreeLists;
35075:     JS_ASSERT(td->gcFreeLists.isEmpty());
35075:     return &td->localRootStack->gcFreeLists;
35075: }
35075: 
33952: static JSGCThing *
33952: RefillFinalizableFreeList(JSContext *cx, unsigned thingKind)
    1: {
35075:     JS_ASSERT(!GetGCFreeLists(cx)->finalizables[thingKind]);
33952:     JSRuntime *rt = cx->runtime;
33952:     JS_LOCK_GC(rt);
33952:     JS_ASSERT(!rt->gcRunning);
33952:     if (rt->gcRunning) {
33952:         METER(rt->gcStats.finalfail++);
33952:         JS_UNLOCK_GC(rt);
33952:         return NULL;
33952:     }
33952: 
33952:     METER(JSGCArenaStats *astats = &cx->runtime->gcStats.arenaStats[thingKind]);
35105:     bool canGC = !JS_ON_TRACE(cx) && !JS_THREAD_DATA(cx)->waiveGCQuota;
33952:     bool doGC = canGC && IsGCThresholdReached(rt);
33952:     JSGCArenaList *arenaList = &rt->gcArenaList[thingKind];
33952:     JSGCArenaInfo *a;
    1:     for (;;) {
33952:         if (doGC) {
    1:             /*
    1:              * Keep rt->gcLock across the call into js_GC so we don't starve
    1:              * and lose to racing threads who deplete the heap just after
    1:              * js_GC has replenished it (or has synchronized with a racing
    1:              * GC that collected a bunch of garbage).  This unfair scheduling
    1:              * can happen on certain operating systems. For the gory details,
    1:              * see bug 162779 at https://bugzilla.mozilla.org/.
    1:              */
    1:             js_GC(cx, GC_LAST_DITCH);
12282:             METER(astats->retry++);
33952:             canGC = false;
33995: 
33995:             /*
33995:              * The JSGC_END callback can legitimately allocate new GC things
33995:              * and populate the free list. If that happens, just return that
33995:              * list head.
33995:              */
35075:             JSGCThing *freeList = GetGCFreeLists(cx)->finalizables[thingKind];
33995:             if (freeList) {
33995:                 JS_UNLOCK_GC(rt);
33995:                 return freeList;
33995:             }
    1:         }
    1: 
33952:         while ((a = arenaList->cursor) != NULL) {
33952:             arenaList->cursor = a->prev;
36410:             JSGCThing *freeList = a->freeList;
33952:             if (freeList) {
36410:                 a->freeList = NULL;
33952:                 JS_UNLOCK_GC(rt);
33952:                 return freeList;
33952:             }
33952:         }
33952: 
34329:         a = NewGCArena(cx);
33952:         if (a)
33952:             break;
33952:         if (!canGC) {
33952:             METER(astats->fail++);
33952:             JS_UNLOCK_GC(rt);
33952:             return NULL;
33952:         }
33952:         doGC = true;
33952:     }
33952: 
33952:     /*
33952:      * Do only minimal initialization of the arena inside the GC lock. We
33952:      * can do the rest outside the lock because no other threads will see
33952:      * the arena until the GC is run.
33952:      */
33952:     a->list = arenaList;
33952:     a->prev = arenaList->head;
36410:     a->prevUnmarkedPage = 0;
36410:     a->freeList = NULL;
36410:     a->unmarkedChildren = 0;
33952:     arenaList->head = a;
33952:     JS_UNLOCK_GC(rt);
33952: 
33952:     unsigned nthings = THINGS_PER_ARENA(arenaList->thingSize);
33952:     uint8 *flagsStart = THING_FLAGP(a, nthings - 1);
33952:     memset(flagsStart, 0, nthings);
33952: 
33952:     /* Turn all things in the arena into a free list. */
33952:     return MakeNewArenaFreeList(a, arenaList->thingSize, nthings);
33952: }
33952: 
35075: static inline void
35075: CheckGCFreeListLink(JSGCThing *thing)
35075: {
35075:     /*
35075:      * The GC things on the free lists come from one arena and the things on
35075:      * the free list are linked in ascending address order.
35075:      */
35075:     JS_ASSERT_IF(thing->link,
35075:                  THING_TO_ARENA(thing) == THING_TO_ARENA(thing->link));
35075:     JS_ASSERT_IF(thing->link, thing < thing->link);
35075: }
35075: 
33952: void *
35075: js_NewFinalizableGCThing(JSContext *cx, unsigned thingKind)
33952: {
33952:     JS_ASSERT(thingKind < FINALIZE_LIMIT);
33952: #ifdef JS_THREADSAFE
33952:     JS_ASSERT(cx->thread);
33952: #endif
33952: 
33952:     /* Updates of metering counters here may not be thread-safe. */
33952:     METER(cx->runtime->gcStats.arenaStats[thingKind].alloc++);
33952: 
33952:     JSGCThing **freeListp =
33952:         JS_THREAD_DATA(cx)->gcFreeLists.finalizables + thingKind;
33952:     JSGCThing *thing = *freeListp;
35075:     if (thing) {
35075:         JS_ASSERT(!JS_THREAD_DATA(cx)->localRootStack);
35075:         *freeListp = thing->link;
35075:         cx->weakRoots.finalizableNewborns[thingKind] = thing;
35075:         CheckGCFreeListLink(thing);
35075:         METER(astats->localalloc++);
35075:         return thing;
35075:     }
35075: 
35075:     /*
35075:      * To avoid for the local roots on each GC allocation when the local roots
35075:      * are not active we move the GC free lists from JSThreadData to lrs in
35075:      * JS_EnterLocalRootScope(). This way with inactive local roots we only
35075:      * check for non-null lrs only when we exhaust the free list.
35075:      */
35075:     JSLocalRootStack *lrs = JS_THREAD_DATA(cx)->localRootStack;
33952:     for (;;) {
35075:         if (lrs) {
35075:             freeListp = lrs->gcFreeLists.finalizables + thingKind;
35075:             thing = *freeListp;
33947:             if (thing) {
33952:                 *freeListp = thing->link;
33952:                 METER(astats->localalloc++);
    1:                 break;
    1:             }
35075:         }
    1: 
33952:         thing = RefillFinalizableFreeList(cx, thingKind);
33952:         if (thing) {
33995:             /*
33995:              * See comments in RefillFinalizableFreeList about a possibility
33995:              * of *freeListp == thing.
33995:              */
33995:             JS_ASSERT(!*freeListp || *freeListp == thing);
33952:             *freeListp = thing->link;
33952:             break;
33952:         }
33952: 
33952:         js_ReportOutOfMemory(cx);
33952:         return NULL;
    1:     }
33952: 
35075:     CheckGCFreeListLink(thing);
    1:     if (lrs) {
    1:         /*
    1:          * If we're in a local root scope, don't set newborn[type] at all, to
    1:          * avoid entraining garbage from it for an unbounded amount of time
    1:          * on this context.  A caller will leave the local root scope and pop
    1:          * this reference, allowing thing to be GC'd if it has no other refs.
    1:          * See JS_EnterLocalRootScope and related APIs.
    1:          */
    1:         if (js_PushLocalRoot(cx, lrs, (jsval) thing) < 0) {
33952:             JS_ASSERT(thing->link == *freeListp);
33952:             *freeListp = thing;
33952:             return NULL;
    1:         }
    1:     } else {
    1:         /*
    1:          * No local root scope, so we're stuck with the old, fragile model of
    1:          * depending on a pigeon-hole newborn per type per context.
    1:          */
33952:         cx->weakRoots.finalizableNewborns[thingKind] = thing;
    1:     }
    1: 
33952:     return thing;
    1: }
    1: 
33952: static JSGCThing *
33747: TurnUsedArenaIntoDoubleList(JSGCArenaInfo *a)
33747: {
33952:     JSGCThing *head;
33952:     JSGCThing **tailp = &head;
33952:     jsuword thing = ARENA_INFO_TO_START(a);
33747: 
33747:     /*
33747:      * When m below points the last bitmap's word in the arena, its high bits
33952:      * corresponds to non-existing cells and thingptr is outside the space
33747:      * allocated for doubles. ClearDoubleArenaFlags sets such bits to 1. Thus
33747:      * even for this last word its bit is unset iff the corresponding cell
33747:      * exists and free.
33747:      */
33747:     for (jsbitmap *m = DOUBLE_ARENA_BITMAP(a);
33747:          m != DOUBLE_ARENA_BITMAP_END(a);
33747:          ++m) {
33952:         JS_ASSERT(thing < reinterpret_cast<jsuword>(DOUBLE_ARENA_BITMAP(a)));
33952:         JS_ASSERT((thing - ARENA_INFO_TO_START(a)) %
33747:                   (JS_BITS_PER_WORD * sizeof(jsdouble)) == 0);
33747: 
33747:         jsbitmap bits = *m;
33747:         if (bits == jsbitmap(-1)) {
33952:             thing += JS_BITS_PER_WORD * sizeof(jsdouble);
33747:         } else {
33747:             /*
33747:              * We have some zero bits. Turn corresponding cells into a list
33747:              * unrolling the loop for better performance.
33747:              */
33747:             const unsigned unroll = 4;
33747:             const jsbitmap unrollMask = (jsbitmap(1) << unroll) - 1;
33747:             JS_STATIC_ASSERT((JS_BITS_PER_WORD & unrollMask) == 0);
33747: 
33747:             for (unsigned n = 0; n != JS_BITS_PER_WORD; n += unroll) {
33747:                 jsbitmap bitsChunk = bits & unrollMask;
33747:                 bits >>= unroll;
33747:                 if (bitsChunk == unrollMask) {
33952:                     thing += unroll * sizeof(jsdouble);
33747:                 } else {
33747: #define DO_BIT(bit)                                                           \
33747:                     if (!(bitsChunk & (jsbitmap(1) << (bit)))) {              \
33952:                         JS_ASSERT(thing - ARENA_INFO_TO_START(a) <=           \
33747:                                   (DOUBLES_PER_ARENA - 1) * sizeof(jsdouble));\
33952:                         JSGCThing *t = reinterpret_cast<JSGCThing *>(thing);  \
33952:                         *tailp = t;                                           \
33952:                         tailp = &t->link;                                     \
33747:                     }                                                         \
33952:                     thing += sizeof(jsdouble);
33747:                     DO_BIT(0);
33747:                     DO_BIT(1);
33747:                     DO_BIT(2);
33747:                     DO_BIT(3);
33747: #undef DO_BIT
33747:                 }
33747:             }
33747:         }
33747:     }
33747:     *tailp = NULL;
33747:     return head;
33747: }
33747: 
33952: static JSGCThing *
12282: RefillDoubleFreeList(JSContext *cx)
    1: {
35075:     JS_ASSERT(!GetGCFreeLists(cx)->doubles);
33747: 
33747:     JSRuntime *rt = cx->runtime;
33747:     JS_ASSERT(!rt->gcRunning);
33747: 
33747:     JS_LOCK_GC(rt);
33747: 
12282:     JSGCArenaInfo *a;
35105:     bool canGC = !JS_ON_TRACE(cx) && !JS_THREAD_DATA(cx)->waiveGCQuota;
33747:     bool doGC = canGC && IsGCThresholdReached(rt);
33747:     for (;;) {
33747:         if (doGC) {
33747:             js_GC(cx, GC_LAST_DITCH);
33747:             METER(rt->gcStats.doubleArenaStats.retry++);
33747:             canGC = false;
35075: 
35075:             /* See comments in RefillFinalizableFreeList. */
35075:             JSGCThing *freeList = GetGCFreeLists(cx)->doubles;
35075:             if (freeList) {
35075:                 JS_UNLOCK_GC(rt);
35075:                 return freeList;
35075:             }
33747:         }
33747: 
33747:         /*
33747:          * Loop until we find arena with some free doubles. We turn arenas
33747:          * into free lists outside the lock to minimize contention between
33747:          * threads.
33747:          */
33747:         while (!!(a = rt->gcDoubleArenaList.cursor)) {
33952:             JS_ASSERT(!a->hasMarkedDoubles);
33747:             rt->gcDoubleArenaList.cursor = a->prev;
33747:             JS_UNLOCK_GC(rt);
33952:             JSGCThing *list = TurnUsedArenaIntoDoubleList(a);
33747:             if (list)
33747:                 return list;
12282:             JS_LOCK_GC(rt);
12282:         }
34329:         a = NewGCArena(cx);
33747:         if (a)
33747:             break;
33747:         if (!canGC) {
12282:             METER(rt->gcStats.doubleArenaStats.fail++);
12282:             JS_UNLOCK_GC(rt);
12282:             return NULL;
    1:         }
33747:         doGC = true;
12282:     }
33747: 
12282:     a->list = NULL;
36410:     a->freeList = NULL;
33952:     a->hasMarkedDoubles = false;
33952:     a->prev = rt->gcDoubleArenaList.head;
33952:     rt->gcDoubleArenaList.head = a;
12282:     JS_UNLOCK_GC(rt);
33952:     return MakeNewArenaFreeList(a, sizeof(jsdouble), DOUBLES_PER_ARENA);
12282: }
12282: 
    1: JSBool
12850: js_NewDoubleInRootedValue(JSContext *cx, jsdouble d, jsval *vp)
    1: {
12282:     /* Updates of metering counters here are not thread-safe. */
33747:     METER(JSGCArenaStats *astats = &cx->runtime->gcStats.doubleArenaStats);
12282:     METER(astats->alloc++);
33747: 
35075:     JSGCThing **freeListp = &JS_THREAD_DATA(cx)->gcFreeLists.doubles;
35075:     JSGCThing *thing = *freeListp;
35075:     if (thing) {
35075:         METER(astats->localalloc++);
35075:         JS_ASSERT(!JS_THREAD_DATA(cx)->localRootStack);
35075:         CheckGCFreeListLink(thing);
35075:         *freeListp = thing->link;
35075: 
35075:         jsdouble *dp = reinterpret_cast<jsdouble *>(thing);
35075:         *dp = d;
35075:         *vp = DOUBLE_TO_JSVAL(dp);
35075:         return true;
35075:     }
35075: 
35075:     JSLocalRootStack *lrs = JS_THREAD_DATA(cx)->localRootStack;
35075:     for (;;) {
35075:         if (lrs) {
35075:             freeListp = &lrs->gcFreeLists.doubles;
35075:             thing = *freeListp;
35075:             if (thing) {
35075:                 METER(astats->localalloc++);
35075:                 break;
35075:             }
35075:         }
33952:         thing = RefillDoubleFreeList(cx);
35075:         if (thing) {
35075:             JS_ASSERT(!*freeListp || *freeListp == thing);
35075:             break;
35075:         }
35075: 
35075:         if (!JS_ON_TRACE(cx)) {
35075:             /* Trace code handle this on its own. */
35075:             js_ReportOutOfMemory(cx);
12282:             METER(astats->fail++);
35075:         }
33747:         return false;
    1:     }
35075: 
35075:     CheckGCFreeListLink(thing);
35075:     *freeListp = thing->link;
35075: 
33952:     jsdouble *dp = reinterpret_cast<jsdouble *>(thing);
33952:     *dp = d;
33952:     *vp = DOUBLE_TO_JSVAL(dp);
35075:     return !lrs || js_PushLocalRoot(cx, lrs, *vp) >= 0;
12850: }
12850: 
12850: jsdouble *
12850: js_NewWeaklyRootedDouble(JSContext *cx, jsdouble d)
12850: {
12850:     jsval v;
12850:     if (!js_NewDoubleInRootedValue(cx, d, &v))
12850:         return NULL;
12850: 
35075:     jsdouble *dp = JSVAL_TO_DOUBLE(v);
33581:     cx->weakRoots.newbornDouble = dp;
12282:     return dp;
12282: }
    1: 
    1: /*
12282:  * Shallow GC-things can be locked just by setting the GCF_LOCK bit, because
12282:  * they have no descendants to mark during the GC. Currently the optimization
12282:  * is only used for non-dependant strings.
    1:  */
33582: static uint8 *
33582: GetShallowGCThingFlag(void *thing)
33582: {
33582:     if (JSString::isStatic(thing))
33582:         return NULL;
33582:     JSGCArenaInfo *a = THING_TO_ARENA(thing);
33582:     if (!a->list || !IsFinalizableStringKind(a->list->thingKind))
33582:         return NULL;
33582: 
33582:     JS_ASSERT(sizeof(JSString) == a->list->thingSize);
33582:     JSString *str = (JSString *) thing;
33582:     if (str->isDependent())
33582:         return NULL;
33582: 
33582:     uint32 index = THING_TO_INDEX(thing, sizeof(JSString));
33582:     return THING_FLAGP(a, index);
33582: }
    1: 
    1: /* This is compatible with JSDHashEntryStub. */
    1: typedef struct JSGCLockHashEntry {
    1:     JSDHashEntryHdr hdr;
12282:     const void      *thing;
    1:     uint32          count;
    1: } JSGCLockHashEntry;
    1: 
    1: JSBool
    1: js_LockGCThingRT(JSRuntime *rt, void *thing)
    1: {
32734:     if (!thing)
12282:         return JS_TRUE;
12282: 
    1:     JS_LOCK_GC(rt);
    1: 
    1:     /*
    1:      * Avoid adding a rt->gcLocksHash entry for shallow things until someone
12282:      * nests a lock.
    1:      */
33582:     uint8 *flagp = GetShallowGCThingFlag(thing);
33582:     JSBool ok;
33582:     JSGCLockHashEntry *lhe;
33582:     if (flagp && !(*flagp & GCF_LOCK)) {
12282:         *flagp |= GCF_LOCK;
12282:         METER(rt->gcStats.lock++);
12282:         ok = JS_TRUE;
12282:         goto out;
12282:     }
12282: 
    1:     if (!rt->gcLocksHash) {
12282:         rt->gcLocksHash = JS_NewDHashTable(JS_DHashGetStubOps(), NULL,
    1:                                            sizeof(JSGCLockHashEntry),
    1:                                            GC_ROOTS_SIZE);
    1:         if (!rt->gcLocksHash) {
    1:             ok = JS_FALSE;
12282:             goto out;
    1:         }
    1:     }
    1: 
    1:     lhe = (JSGCLockHashEntry *)
    1:           JS_DHashTableOperate(rt->gcLocksHash, thing, JS_DHASH_ADD);
    1:     if (!lhe) {
    1:         ok = JS_FALSE;
12282:         goto out;
    1:     }
    1:     if (!lhe->thing) {
12282:         lhe->thing = thing;
12282:         lhe->count = 1;
    1:     } else {
    1:         JS_ASSERT(lhe->count >= 1);
    1:         lhe->count++;
    1:     }
12282: 
    1:     METER(rt->gcStats.lock++);
    1:     ok = JS_TRUE;
12282:   out:
    1:     JS_UNLOCK_GC(rt);
    1:     return ok;
    1: }
    1: 
36410: void
    1: js_UnlockGCThingRT(JSRuntime *rt, void *thing)
    1: {
32734:     if (!thing)
36410:         return;
    1: 
    1:     JS_LOCK_GC(rt);
33582: 
33582:     uint8 *flagp = GetShallowGCThingFlag(thing);
33582:     JSGCLockHashEntry *lhe;
33582:     if (flagp && !(*flagp & GCF_LOCK))
12282:         goto out;
    1:     if (!rt->gcLocksHash ||
    1:         (lhe = (JSGCLockHashEntry *)
    1:          JS_DHashTableOperate(rt->gcLocksHash, thing,
    1:                               JS_DHASH_LOOKUP),
    1:              JS_DHASH_ENTRY_IS_FREE(&lhe->hdr))) {
12282:         /* Shallow entry is not in the hash -> clear its lock bit. */
33582:         if (flagp)
12282:             *flagp &= ~GCF_LOCK;
12282:         else
12282:             goto out;
    1:     } else {
    1:         if (--lhe->count != 0)
    1:             goto out;
    1:         JS_DHashTableOperate(rt->gcLocksHash, thing, JS_DHASH_REMOVE);
    1:     }
    1: 
    1:     rt->gcPoke = JS_TRUE;
12282:     METER(rt->gcStats.unlock++);
    1:   out:
    1:     JS_UNLOCK_GC(rt);
    1: }
    1: 
  583: JS_PUBLIC_API(void)
  583: JS_TraceChildren(JSTracer *trc, void *thing, uint32 kind)
  583: {
  583:     switch (kind) {
32603:       case JSTRACE_OBJECT: {
    1:         /* If obj has no map, it must be a newborn. */
32603:         JSObject *obj = (JSObject *) thing;
    1:         if (!obj->map)
    1:             break;
  583:         obj->map->ops->trace(trc, obj);
32603:         break;
  583:       }
32603: 
32603:       case JSTRACE_STRING: {
32603:         JSString *str = (JSString *) thing;
29366:         if (str->isDependent())
29366:             JS_CALL_STRING_TRACER(trc, str->dependentBase(), "base");
  583:         break;
32603:       }
  583: 
  583: #if JS_HAS_XML_SUPPORT
  583:       case JSTRACE_XML:
  583:         js_TraceXML(trc, (JSXML *)thing);
  583:         break;
    1: #endif
    1:     }
    1: }
    1: 
    1: /*
36410:  * When the native stack is low, the GC does not call JS_TraceChildren to mark
36410:  * the reachable "children" of the thing. Rather the thing is put aside and
36410:  * JS_TraceChildren is called later with more space on the C stack.
36410:  *
36410:  * To implement such delayed marking of the children with minimal overhead for
36410:  * the normal case of sufficient native stack, the code adds two fields to
36410:  * JSGCArenaInfo. The first field, JSGCArenaInfo::prevUnmarkedPage, links all
36410:  * arenas with delayed things into a stack list with the pointer to stack top
36410:  * in JSRuntime::gcUnmarkedArenaStackTop. DelayMarkingChildren adds arenas to
36410:  * the stack as necessary while MarkDelayedChildren pops the arenas from the
36410:  * stack until it empties.
36410:  *
36410:  * The second field, JSGCArenaInfo::unmarkedChildren, is a bitmap that tells
36410:  * for which things the GC should call JS_TraceChildren later. The bitmap is
36410:  * a single word. As such it does not pinpoint the delayed things in the arena
36410:  * but rather tells the intervals containing ThingsPerUnmarkedBit(thingSize)
36410:  * things. Later the code in MarkDelayedChildren discovers such intervals
36410:  * and calls JS_TraceChildren on any marked thing in the interval. This
36410:  * implies that JS_TraceChildren can be called many times for a single thing
36410:  * if the thing shares the same interval with some delayed things. This should
36410:  * be fine as any GC graph marking/traversing hooks must allow repeated calls
36410:  * during the same GC cycle. In particular, xpcom cycle collector relies on
36410:  * this.
36410:  *
36410:  * Note that such repeated scanning may slow down the GC. In particular, it is
36410:  * possible to construct an object graph where the GC calls JS_TraceChildren
36410:  * ThingsPerUnmarkedBit(thingSize) for almost all things in the graph. We
36410:  * tolerate this as the max value for ThingsPerUnmarkedBit(thingSize) is 4.
36410:  * This is archived for JSObject on 32 bit system as it is exactly JSObject
36410:  * that has the smallest size among the GC things that can be delayed. On 32
36410:  * bit CPU we have less than 128 objects per 4K GC arena so each bit in
36410:  * unmarkedChildren covers 4 objects.
    1:  */
36410: inline unsigned
36410: ThingsPerUnmarkedBit(unsigned thingSize)
36410: {
36410:     return JS_HOWMANY(THINGS_PER_ARENA(thingSize), JS_BITS_PER_WORD);
36410: }
    1: 
    1: static void
36410: DelayMarkingChildren(JSRuntime *rt, uint8 *flagp)
    1: {
 5917:     JSGCArenaInfo *a;
36410:     uint32 unmarkedBitIndex;
    1:     jsuword bit;
    1: 
36410:     JS_ASSERT(*flagp & GCF_MARK);
36410: 
36410:     METER(rt->gcStats.unmarked++);
 5917:     a = FLAGP_TO_ARENA(flagp);
36410:     unmarkedBitIndex = FLAGP_TO_INDEX(flagp) /
36410:                        ThingsPerUnmarkedBit(a->list->thingSize);
36410:     JS_ASSERT(unmarkedBitIndex < JS_BITS_PER_WORD);
36410:     bit = (jsuword)1 << unmarkedBitIndex;
36410:     if (a->unmarkedChildren != 0) {
36410:         JS_ASSERT(rt->gcUnmarkedArenaStackTop);
36410:         if (a->unmarkedChildren & bit) {
36410:             /* bit already covers things with children to mark later. */
    1:             return;
    1:         }
36410:         a->unmarkedChildren |= bit;
    1:     } else {
    1:         /*
36410:          * The thing is the first thing with not yet marked children in the
 5917:          * whole arena, so push the arena on the stack of arenas with things
36410:          * to be marked later unless the arena has already been pushed. We
36410:          * detect that through checking prevUnmarkedPage as the field is 0
 5917:          * only for not yet pushed arenas. To ensure that
36410:          *   prevUnmarkedPage != 0
36410:          * even when the stack contains one element, we make prevUnmarkedPage
 5917:          * for the arena at the bottom to point to itself.
 5917:          *
36410:          * See comments in MarkDelayedChildren.
    1:          */
36410:         a->unmarkedChildren = bit;
36410:         if (a->prevUnmarkedPage == 0) {
36410:             if (!rt->gcUnmarkedArenaStackTop) {
 5917:                 /* Stack was empty, mark the arena as the bottom element. */
36410:                 a->prevUnmarkedPage = ARENA_INFO_TO_PAGE(a);
 5917:             } else {
36410:                 JS_ASSERT(rt->gcUnmarkedArenaStackTop->prevUnmarkedPage != 0);
36410:                 a->prevUnmarkedPage =
36410:                     ARENA_INFO_TO_PAGE(rt->gcUnmarkedArenaStackTop);
    1:             }
36410:             rt->gcUnmarkedArenaStackTop = a;
    1:         }
36410:         JS_ASSERT(rt->gcUnmarkedArenaStackTop);
    1:     }
36410: #ifdef DEBUG
36410:     rt->gcMarkLaterCount += ThingsPerUnmarkedBit(a->list->thingSize);
36410:     METER_UPDATE_MAX(rt->gcStats.maxunmarked, rt->gcMarkLaterCount);
36410: #endif
    1: }
    1: 
    1: static void
36410: MarkDelayedChildren(JSTracer *trc)
    1: {
    1:     JSRuntime *rt;
 5917:     JSGCArenaInfo *a, *aprev;
33582:     uint32 thingSize, traceKind;
36410:     uint32 thingsPerUnmarkedBit;
36410:     uint32 unmarkedBitIndex, thingIndex, indexLimit, endIndex;
    1:     JSGCThing *thing;
    1:     uint8 *flagp;
    1: 
  583:     rt = trc->context->runtime;
36410:     a = rt->gcUnmarkedArenaStackTop;
 5917:     if (!a) {
36410:         JS_ASSERT(rt->gcMarkLaterCount == 0);
    1:         return;
    1:     }
    1: 
    1:     for (;;) {
    1:         /*
 5917:          * The following assert verifies that the current arena belongs to the
36410:          * unmarked stack, since DelayMarkingChildren ensures that even for
36410:          * the stack's bottom, prevUnmarkedPage != 0 but rather points to
36410:          * itself.
    1:          */
36410:         JS_ASSERT(a->prevUnmarkedPage != 0);
36410:         JS_ASSERT(rt->gcUnmarkedArenaStackTop->prevUnmarkedPage != 0);
 5917:         thingSize = a->list->thingSize;
33582:         traceKind = GetFinalizableArenaTraceKind(a);
33952:         indexLimit = THINGS_PER_ARENA(thingSize);
36410:         thingsPerUnmarkedBit = ThingsPerUnmarkedBit(thingSize);
 5917: 
    1:         /*
36410:          * We cannot use do-while loop here as a->unmarkedChildren can be zero
 5917:          * before the loop as a leftover from the previous iterations. See
 5917:          * comments after the loop.
    1:          */
36410:         while (a->unmarkedChildren != 0) {
36410:             unmarkedBitIndex = JS_FLOOR_LOG2W(a->unmarkedChildren);
36410:             a->unmarkedChildren &= ~((jsuword)1 << unmarkedBitIndex);
36410: #ifdef DEBUG
36410:             JS_ASSERT(rt->gcMarkLaterCount >= thingsPerUnmarkedBit);
36410:             rt->gcMarkLaterCount -= thingsPerUnmarkedBit;
36410: #endif
36410:             thingIndex = unmarkedBitIndex * thingsPerUnmarkedBit;
36410:             endIndex = thingIndex + thingsPerUnmarkedBit;
    1: 
    1:             /*
 5917:              * endIndex can go beyond the last allocated thing as the real
 5917:              * limit can be "inside" the bit.
    1:              */
 5917:             if (endIndex > indexLimit)
 5917:                 endIndex = indexLimit;
 5917:             JS_ASSERT(thingIndex < indexLimit);
 5917:             do {
 5917:                 flagp = THING_FLAGP(a, thingIndex);
36410:                 if (*flagp & GCF_MARK) {
 5917:                     thing = FLAGP_TO_THING(flagp, thingSize);
33582:                     JS_TraceChildren(trc, thing, traceKind);
36410:                 }
 5917:             } while (++thingIndex != endIndex);
    1:         }
 5917: 
    1:         /*
 5917:          * We finished tracing of all things in the the arena but we can only
 5917:          * pop it from the stack if the arena is the stack's top.
    1:          *
 5917:          * When JS_TraceChildren from the above calls JS_CallTracer that in
36410:          * turn on low C stack calls DelayMarkingChildren and the latter
36410:          * pushes new arenas to the unmarked stack, we have to skip popping
 5917:          * of this arena until it becomes the top of the stack again.
    1:          */
36410:         if (a == rt->gcUnmarkedArenaStackTop) {
36410:             aprev = ARENA_PAGE_TO_INFO(a->prevUnmarkedPage);
36410:             a->prevUnmarkedPage = 0;
 5917:             if (a == aprev) {
    1:                 /*
36410:                  * prevUnmarkedPage points to itself and we reached the
 5917:                  * bottom of the stack.
    1:                  */
    1:                 break;
    1:             }
36410:             rt->gcUnmarkedArenaStackTop = a = aprev;
    1:         } else {
36410:             a = rt->gcUnmarkedArenaStackTop;
    1:         }
    1:     }
36410:     JS_ASSERT(rt->gcUnmarkedArenaStackTop);
36410:     JS_ASSERT(rt->gcUnmarkedArenaStackTop->prevUnmarkedPage == 0);
36410:     rt->gcUnmarkedArenaStackTop = NULL;
36410:     JS_ASSERT(rt->gcMarkLaterCount == 0);
    1: }
    1: 
  583: JS_PUBLIC_API(void)
  583: JS_CallTracer(JSTracer *trc, void *thing, uint32 kind)
    1: {
  583:     JSContext *cx;
  583:     JSRuntime *rt;
12282:     JSGCArenaInfo *a;
12282:     uintN index;
    1:     uint8 *flagp;
  583: 
  771:     JS_ASSERT(thing);
  583:     JS_ASSERT(JS_IS_VALID_TRACE_KIND(kind));
  583:     JS_ASSERT(trc->debugPrinter || trc->debugPrintArg);
  583: 
  583:     if (!IS_GC_MARKING_TRACER(trc)) {
  583:         trc->callback(trc, thing, kind);
  583:         goto out;
  583:     }
  583: 
  583:     cx = trc->context;
  583:     rt = cx->runtime;
  583:     JS_ASSERT(rt->gcMarkingTracer == trc);
  583:     JS_ASSERT(rt->gcLevel > 0);
  583: 
  583:     /*
 5917:      * Optimize for string and double as their size is known and their tracing
 5917:      * is not recursive.
  583:      */
 5917:     switch (kind) {
 5917:       case JSTRACE_DOUBLE:
12282:         a = THING_TO_ARENA(thing);
12282:         JS_ASSERT(!a->list);
33952:         if (!a->hasMarkedDoubles) {
12282:             ClearDoubleArenaFlags(a);
33952:             a->hasMarkedDoubles = true;
  583:         }
12282:         index = DOUBLE_THING_TO_INDEX(thing);
13029:         JS_SET_BIT(DOUBLE_ARENA_BITMAP(a), index);
 5917:         goto out;
 5917: 
 5917:       case JSTRACE_STRING:
 5917:         for (;;) {
32734:             if (JSString::isStatic(thing))
32674:                 goto out;
33582:             a = THING_TO_ARENA(thing);
33582:             flagp = THING_FLAGP(a, THING_TO_INDEX(thing, sizeof(JSString)));
33582:             JS_ASSERT(kind == GetFinalizableArenaTraceKind(a));
29366:             if (!((JSString *) thing)->isDependent()) {
 5917:                 *flagp |= GCF_MARK;
  583:                 goto out;
  583:             }
    1:             if (*flagp & GCF_MARK)
  583:                 goto out;
    1:             *flagp |= GCF_MARK;
29366:             thing = ((JSString *) thing)->dependentBase();
 5917:         }
 5917:         /* NOTREACHED */
 5917:     }
 5917: 
33582:     JS_ASSERT(kind == GetFinalizableArenaTraceKind(THING_TO_ARENA(thing)));
 8005:     flagp = GetGCThingFlags(thing);
    1:     if (*flagp & GCF_MARK)
  583:         goto out;
 5917: 
    1:     *flagp |= GCF_MARK;
    1:     if (!cx->insideGCMarkCallback) {
  583:         /*
  583:          * With JS_GC_ASSUME_LOW_C_STACK defined the mark phase of GC always
  583:          * uses the non-recursive code that otherwise would be called only on
  583:          * a low C stack condition.
  583:          */
  583: #ifdef JS_GC_ASSUME_LOW_C_STACK
  583: # define RECURSION_TOO_DEEP() JS_TRUE
  583: #else
  583:         int stackDummy;
  583: # define RECURSION_TOO_DEEP() (!JS_CHECK_STACK_SIZE(cx, stackDummy))
  583: #endif
  583:         if (RECURSION_TOO_DEEP())
36410:             DelayMarkingChildren(rt, flagp);
  583:         else
  583:             JS_TraceChildren(trc, thing, kind);
    1:     } else {
    1:         /*
    1:          * For API compatibility we allow for the callback to assume that
 5917:          * after it calls JS_MarkGCThing for the last time, the callback can
 5917:          * start to finalize its own objects that are only referenced by
 5917:          * unmarked GC things.
    1:          *
    1:          * Since we do not know which call from inside the callback is the
 5917:          * last, we ensure that children of all marked things are traced and
36410:          * call MarkDelayedChildren(trc) after tracing the thing.
    1:          *
36410:          * As MarkDelayedChildren unconditionally invokes JS_TraceChildren
36410:          * for the things with unmarked children, calling DelayMarkingChildren
 5917:          * is useless here. Hence we always trace thing's children even with a
 5917:          * low native stack.
    1:          */
    1:         cx->insideGCMarkCallback = JS_FALSE;
  583:         JS_TraceChildren(trc, thing, kind);
36410:         MarkDelayedChildren(trc);
    1:         cx->insideGCMarkCallback = JS_TRUE;
    1:     }
  583: 
  583:   out:
  583: #ifdef DEBUG
  583:     trc->debugPrinter = NULL;
  583:     trc->debugPrintArg = NULL;
  583: #endif
  583:     return;     /* to avoid out: right_curl when DEBUG is not defined */
    1: }
    1: 
  583: void
  771: js_CallValueTracerIfGCThing(JSTracer *trc, jsval v)
  583: {
  771:     void *thing;
  771:     uint32 kind;
  771: 
  771:     if (JSVAL_IS_DOUBLE(v) || JSVAL_IS_STRING(v)) {
  771:         thing = JSVAL_TO_TRACEABLE(v);
  771:         kind = JSVAL_TRACE_KIND(v);
36410:         JS_ASSERT(kind == js_GetGCThingTraceKind(thing));
  771:     } else if (JSVAL_IS_OBJECT(v) && v != JSVAL_NULL) {
  771:         /* v can be an arbitrary GC thing reinterpreted as an object. */
  771:         thing = JSVAL_TO_OBJECT(v);
 8005:         kind = js_GetGCThingTraceKind(thing);
  771:     } else {
  771:         return;
  583:     }
  771:     JS_CallTracer(trc, thing, kind);
  771: }
  583: 
18907: static JSDHashOperator
  583: gc_root_traversal(JSDHashTable *table, JSDHashEntryHdr *hdr, uint32 num,
  583:                   void *arg)
    1: {
    1:     JSGCRootHashEntry *rhe = (JSGCRootHashEntry *)hdr;
  583:     JSTracer *trc = (JSTracer *)arg;
    1:     jsval *rp = (jsval *)rhe->root;
    1:     jsval v = *rp;
    1: 
32694:     /* Ignore null reference, scalar values, and static strings. */
36410:     if (JSVAL_IS_TRACEABLE(v)) {
    1: #ifdef DEBUG
36410:         if (!JSString::isStatic(JSVAL_TO_GCTHING(v))) {
33582:             bool root_points_to_gcArenaList = false;
    1:             jsuword thing = (jsuword) JSVAL_TO_GCTHING(v);
33582:             JSRuntime *rt = trc->context->runtime;
33582:             for (unsigned i = 0; i != FINALIZE_LIMIT; i++) {
33582:                 JSGCArenaList *arenaList = &rt->gcArenaList[i];
33582:                 size_t thingSize = arenaList->thingSize;
33952:                 size_t limit = THINGS_PER_ARENA(thingSize) * thingSize;
33952:                 for (JSGCArenaInfo *a = arenaList->head; a; a = a->prev) {
 5917:                     if (thing - ARENA_INFO_TO_START(a) < limit) {
33582:                         root_points_to_gcArenaList = true;
    1:                         break;
    1:                     }
    1:                 }
    1:             }
12282:             if (!root_points_to_gcArenaList) {
36410:                 for (JSGCArenaInfo *a = rt->gcDoubleArenaList.head;
36410:                      a;
36410:                      a = a->prev) {
12282:                     if (thing - ARENA_INFO_TO_START(a) <
12282:                         DOUBLES_PER_ARENA * sizeof(jsdouble)) {
33582:                         root_points_to_gcArenaList = true;
12282:                         break;
12282:                     }
    1:                 }
    1:             }
    1:             if (!root_points_to_gcArenaList && rhe->name) {
    1:                 fprintf(stderr,
    1: "JS API usage error: the address passed to JS_AddNamedRoot currently holds an\n"
    1: "invalid jsval.  This is usually caused by a missing call to JS_RemoveRoot.\n"
    1: "The root's name is \"%s\".\n",
    1:                         rhe->name);
    1:             }
    1:             JS_ASSERT(root_points_to_gcArenaList);
36410:         }
    1: #endif
  583:         JS_SET_TRACING_NAME(trc, rhe->name ? rhe->name : "root");
  771:         js_CallValueTracerIfGCThing(trc, v);
  583:     }
  583: 
  583:     return JS_DHASH_NEXT;
  583: }
  583: 
18907: static JSDHashOperator
  583: gc_lock_traversal(JSDHashTable *table, JSDHashEntryHdr *hdr, uint32 num,
  583:                   void *arg)
  583: {
  583:     JSGCLockHashEntry *lhe = (JSGCLockHashEntry *)hdr;
  583:     void *thing = (void *)lhe->thing;
  583:     JSTracer *trc = (JSTracer *)arg;
  583:     uint32 traceKind;
  583: 
  583:     JS_ASSERT(lhe->count >= 1);
 8005:     traceKind = js_GetGCThingTraceKind(thing);
  583:     JS_CALL_TRACER(trc, thing, traceKind, "locked object");
    1:     return JS_DHASH_NEXT;
    1: }
    1: 
  583: #define TRACE_JSVALS(trc, len, vec, name)                                     \
    1:     JS_BEGIN_MACRO                                                            \
    1:     jsval _v, *_vp, *_end;                                                    \
    1:                                                                               \
    1:         for (_vp = vec, _end = _vp + len; _vp < _end; _vp++) {                \
    1:             _v = *_vp;                                                        \
  583:             if (JSVAL_IS_TRACEABLE(_v)) {                                     \
  583:                 JS_SET_TRACING_INDEX(trc, name, _vp - (vec));                 \
  583:                 JS_CallTracer(trc, JSVAL_TO_TRACEABLE(_v),                    \
  583:                               JSVAL_TRACE_KIND(_v));                          \
  583:             }                                                                 \
    1:         }                                                                     \
    1:     JS_END_MACRO
    1: 
    1: void
  583: js_TraceStackFrame(JSTracer *trc, JSStackFrame *fp)
    1: {
13702:     uintN nslots, minargs, skip;
 4127: 
    1:     if (fp->callobj)
  786:         JS_CALL_OBJECT_TRACER(trc, fp->callobj, "call");
    1:     if (fp->argsobj)
30248:         JS_CALL_OBJECT_TRACER(trc, JSVAL_TO_OBJECT(fp->argsobj), "arguments");
    1:     if (fp->varobj)
  786:         JS_CALL_OBJECT_TRACER(trc, fp->varobj, "variables");
    1:     if (fp->script) {
  583:         js_TraceScript(trc, fp->script);
21992: 
21992:         /* fp->slots is null for watch pseudo-frames, see js_watch_set. */
21992:         if (fp->slots) {
    1:             /*
    1:              * Don't mark what has not been pushed yet, or what has been
    1:              * popped already.
    1:              */
32797:             if (fp->regs && fp->regs->sp) {
16072:                 nslots = (uintN) (fp->regs->sp - fp->slots);
21992:                 JS_ASSERT(nslots >= fp->script->nfixed);
21992:             } else {
21992:                 nslots = fp->script->nfixed;
21992:             }
16072:             TRACE_JSVALS(trc, nslots, fp->slots, "slot");
    1:         }
16072:     } else {
16072:         JS_ASSERT(!fp->slots);
16072:         JS_ASSERT(!fp->regs);
    1:     }
    1: 
    1:     /* Allow for primitive this parameter due to JSFUN_THISP_* flags. */
32774:     JS_CALL_VALUE_TRACER(trc, fp->thisv, "this");
    1: 
    1:     if (fp->argv) {
34352:         JS_CALL_VALUE_TRACER(trc, fp->calleeValue(), "callee");
    1:         nslots = fp->argc;
 6040:         skip = 0;
    1:         if (fp->fun) {
13702:             minargs = FUN_MINARGS(fp->fun);
13702:             if (minargs > nslots)
13702:                 nslots = minargs;
13702:             if (!FUN_INTERPRETED(fp->fun)) {
13702:                 JS_ASSERT(!(fp->fun->flags & JSFUN_FAST_NATIVE));
    1:                 nslots += fp->fun->u.n.extra;
    1:             }
13702:             if (fp->fun->flags & JSFRAME_ROOTED_ARGV)
 6040:                 skip = 2 + fp->argc;
 4250:         }
 6040:         TRACE_JSVALS(trc, 2 + nslots - skip, fp->argv - 2 + skip, "operand");
    1:     }
17182: 
  583:     JS_CALL_VALUE_TRACER(trc, fp->rval, "rval");
  583:     if (fp->scopeChain)
  583:         JS_CALL_OBJECT_TRACER(trc, fp->scopeChain, "scope chain");
    1: }
    1: 
33952: void
33952: JSWeakRoots::mark(JSTracer *trc)
    1: {
33952: #ifdef DEBUG
33952:     const char * const newbornNames[] = {
33952:         "newborn_object",             /* FINALIZE_OBJECT */
33952:         "newborn_function",           /* FINALIZE_FUNCTION */
33581: #if JS_HAS_XML_SUPPORT
33952:         "newborn_xml",                /* FINALIZE_XML */
 8005: #endif
33952:         "newborn_string",             /* FINALIZE_STRING */
33952:         "newborn_external_string0",   /* FINALIZE_EXTERNAL_STRING0 */
33952:         "newborn_external_string1",   /* FINALIZE_EXTERNAL_STRING1 */
33952:         "newborn_external_string2",   /* FINALIZE_EXTERNAL_STRING2 */
33952:         "newborn_external_string3",   /* FINALIZE_EXTERNAL_STRING3 */
33952:         "newborn_external_string4",   /* FINALIZE_EXTERNAL_STRING4 */
33952:         "newborn_external_string5",   /* FINALIZE_EXTERNAL_STRING5 */
33952:         "newborn_external_string6",   /* FINALIZE_EXTERNAL_STRING6 */
33952:         "newborn_external_string7",   /* FINALIZE_EXTERNAL_STRING7 */
33952:     };
33952: #endif
33952:     for (size_t i = 0; i != JS_ARRAY_LENGTH(finalizableNewborns); ++i) {
33952:         void *newborn = finalizableNewborns[i];
33952:         if (newborn) {
33952:             JS_CALL_TRACER(trc, newborn, GetFinalizableTraceKind(i),
33952:                            newbornNames[i]);
 8005:         }
33952:     }
33952:     if (newbornDouble)
33952:         JS_CALL_DOUBLE_TRACER(trc, newbornDouble, "newborn_double");
33952:     JS_CALL_VALUE_TRACER(trc, lastAtom, "lastAtom");
  771:     JS_SET_TRACING_NAME(trc, "lastInternalResult");
33952:     js_CallValueTracerIfGCThing(trc, lastInternalResult);
    1: }
  583: 
24499: JS_REQUIRES_STACK JS_FRIEND_API(void)
  583: js_TraceContext(JSTracer *trc, JSContext *acx)
  583: {
 4521:     JSStackFrame *fp, *nextChain;
  583:     JSStackHeader *sh;
  583:     JSTempValueRooter *tvr;
  583: 
11145:     if (IS_GC_MARKING_TRACER(trc)) {
19196: 
19196: #define FREE_OLD_ARENAS(pool)                                                 \
19196:         JS_BEGIN_MACRO                                                        \
19196:             int64 _age;                                                       \
19196:             JSArena * _a = (pool).current;                                    \
19196:             if (_a == (pool).first.next &&                                    \
19196:                 _a->avail == _a->base + sizeof(int64)) {                      \
19196:                 _age = JS_Now() - *(int64 *) _a->base;                        \
19196:                 if (_age > (int64) acx->runtime->gcEmptyArenaPoolLifespan *   \
19196:                            1000)                                              \
19196:                     JS_FreeArenaPool(&(pool));                                \
19196:             }                                                                 \
19196:         JS_END_MACRO
19196: 
  583:         /*
19196:          * Release the stackPool's arenas if the stackPool has existed for
19196:          * longer than the limit specified by gcEmptyArenaPoolLifespan.
10985:          */
19196:         FREE_OLD_ARENAS(acx->stackPool);
19196: 
19196:         /*
19196:          * Release the regexpPool's arenas based on the same criterion as for
19196:          * the stackPool.
19196:          */
19196:         FREE_OLD_ARENAS(acx->regexpPool);
11145:     }
10985: 
  583:     /*
 4521:      * Iterate frame chain and dormant chains.
  583:      *
  583:      * (NB: see comment on this whole "dormant" thing in js_Execute.)
23224:      *
23224:      * Since js_GetTopStackFrame needs to dereference cx->thread to check for
23224:      * JIT frames, we check for non-null thread here and avoid null checks
23224:      * there. See bug 471197.
  583:      */
23224: #ifdef JS_THREADSAFE
23224:     if (acx->thread)
23224: #endif
23224:     {
22652:         fp = js_GetTopStackFrame(acx);
 4521:         nextChain = acx->dormantFrameChain;
 4521:         if (!fp)
 4521:             goto next_chain;
  583: 
 4521:         /* The top frame must not be dormant. */
 4521:         JS_ASSERT(!fp->dormantNext);
 4521:         for (;;) {
  583:             do {
  583:                 js_TraceStackFrame(trc, fp);
  583:             } while ((fp = fp->down) != NULL);
 4521: 
 4521:           next_chain:
 4521:             if (!nextChain)
 4521:                 break;
 4521:             fp = nextChain;
 4521:             nextChain = nextChain->dormantNext;
  583:         }
23224:     }
  583: 
  583:     /* Mark other roots-by-definition in acx. */
22795:     if (acx->globalObject && !JS_HAS_OPTION(acx, JSOPTION_UNROOTED_GLOBAL))
  583:         JS_CALL_OBJECT_TRACER(trc, acx->globalObject, "global object");
33952:     acx->weakRoots.mark(trc);
  583:     if (acx->throwing) {
  583:         JS_CALL_VALUE_TRACER(trc, acx->exception, "exception");
  583:     } else {
  583:         /* Avoid keeping GC-ed junk stored in JSContext.exception. */
  583:         acx->exception = JSVAL_NULL;
  583:     }
  583: 
  583:     for (sh = acx->stackHeaders; sh; sh = sh->down) {
  583:         METER(trc->context->runtime->gcStats.stackseg++);
  583:         METER(trc->context->runtime->gcStats.segslots += sh->nslots);
  583:         TRACE_JSVALS(trc, sh->nslots, JS_STACK_SEGMENT(sh), "stack");
  583:     }
  583: 
  583:     for (tvr = acx->tempValueRooters; tvr; tvr = tvr->down) {
  583:         switch (tvr->count) {
  583:           case JSTVU_SINGLE:
  583:             JS_SET_TRACING_NAME(trc, "tvr->u.value");
  771:             js_CallValueTracerIfGCThing(trc, tvr->u.value);
  583:             break;
  583:           case JSTVU_TRACE:
  583:             tvr->u.trace(trc, tvr);
  583:             break;
  583:           case JSTVU_SPROP:
30852:             tvr->u.sprop->trace(trc);
  583:             break;
  583:           case JSTVU_WEAK_ROOTS:
33952:             tvr->u.weakRoots->mark(trc);
  583:             break;
27012:           case JSTVU_COMPILER:
27012:             tvr->u.compiler->trace(trc);
 3235:             break;
 7359:           case JSTVU_SCRIPT:
 7359:             js_TraceScript(trc, tvr->u.script);
 7359:             break;
33128:           case JSTVU_ENUMERATOR:
33128:             static_cast<JSAutoEnumStateRooter *>(tvr)->mark(trc);
33128:             break;
  583:           default:
  583:             JS_ASSERT(tvr->count >= 0);
  583:             TRACE_JSVALS(trc, tvr->count, tvr->u.array, "tvr->u.array");
  583:         }
  583:     }
  583: 
  583:     if (acx->sharpObjectMap.depth > 0)
  583:         js_TraceSharpMap(trc, &acx->sharpObjectMap);
23094: 
23094:     js_TraceRegExpStatics(trc, acx);
28086: 
28086: #ifdef JS_TRACER
32726:     InterpState* state = acx->interpState;
32726:     while (state) {
32726:         if (state->nativeVp)
32726:             TRACE_JSVALS(trc, state->nativeVpLen, state->nativeVp, "nativeVp");
32726:         state = state->prev;
32726:     }
28086: #endif
  583: }
  583: 
24499: JS_REQUIRES_STACK void
  786: js_TraceRuntime(JSTracer *trc, JSBool allAtoms)
  583: {
  583:     JSRuntime *rt = trc->context->runtime;
  583:     JSContext *iter, *acx;
  583: 
  583:     JS_DHashTableEnumerate(&rt->gcRootsHash, gc_root_traversal, trc);
  583:     if (rt->gcLocksHash)
  583:         JS_DHashTableEnumerate(rt->gcLocksHash, gc_lock_traversal, trc);
 4529:     js_TraceAtomState(trc, allAtoms);
12282:     js_TraceRuntimeNumberState(trc);
35076:     js_MarkTraps(trc);
  583: 
  583:     iter = NULL;
  583:     while ((acx = js_ContextIterator(rt, JS_TRUE, &iter)) != NULL)
  583:         js_TraceContext(trc, acx);
  958: 
31843:     js_TraceThreads(rt, trc);
31843: 
  958:     if (rt->gcExtraRootsTraceOp)
  958:         rt->gcExtraRootsTraceOp(trc, rt->gcExtraRootsData);
17185: 
23471: #ifdef JS_TRACER
24384:     for (int i = 0; i < JSBUILTIN_LIMIT; i++) {
24384:         if (rt->builtinFunctions[i])
24384:             JS_CALL_OBJECT_TRACER(trc, rt->builtinFunctions[i], "builtin function");
24384:     }
23471: #endif
  583: }
  583: 
27546: void
27546: js_TriggerGC(JSContext *cx, JSBool gcLocked)
27546: {
27546:     JSRuntime *rt = cx->runtime;
27546: 
27546: #ifdef JS_THREADSAFE
27546:     JS_ASSERT(cx->requestDepth > 0);
27546: #endif
27546:     JS_ASSERT(!rt->gcRunning);
27546:     if (rt->gcIsNeeded)
27546:         return;
27546: 
27546:     js_TriggerAllOperationCallbacks(rt, gcLocked);
27546: }
27546: 
11041: static void
11041: ProcessSetSlotRequest(JSContext *cx, JSSetSlotRequest *ssr)
11041: {
28003:     JSObject *obj = ssr->obj;
28003:     JSObject *pobj = ssr->pobj;
28003:     uint32 slot = ssr->slot;
11041: 
11041:     while (pobj) {
12674:         pobj = js_GetWrappedObject(cx, pobj);
11041:         if (pobj == obj) {
28003:             ssr->cycle = true;
11041:             return;
11041:         }
11041:         pobj = JSVAL_TO_OBJECT(STOBJ_GET_SLOT(pobj, slot));
11041:     }
11041: 
11041:     pobj = ssr->pobj;
28003:     if (slot == JSSLOT_PROTO) {
32603:         obj->setProto(pobj);
28003:     } else {
28003:         JS_ASSERT(slot == JSSLOT_PARENT);
32603:         obj->setParent(pobj);
11041:     }
11041: }
11041: 
26569: void
26569: js_DestroyScriptsToGC(JSContext *cx, JSThreadData *data)
18285: {
26569:     JSScript **listp, *script;
26569: 
26569:     for (size_t i = 0; i != JS_ARRAY_LENGTH(data->scriptsToGC); ++i) {
26569:         listp = &data->scriptsToGC[i];
18285:         while ((script = *listp) != NULL) {
18285:             *listp = script->u.nextToGC;
18285:             script->u.nextToGC = NULL;
18285:             js_DestroyScript(cx, script);
18285:         }
18285:     }
26569: }
18285: 
33582: static inline void
33582: FinalizeGCThing(JSContext *cx, JSObject *obj, unsigned thingKind)
30274: {
33582:     JS_ASSERT(thingKind == FINALIZE_FUNCTION || thingKind == FINALIZE_OBJECT);
33582: 
30274:     /* Cope with stillborn objects that have no map. */
30274:     if (!obj->map)
30274:         return;
30274: 
30274:     if (JS_UNLIKELY(cx->debugHooks->objectHook != NULL)) {
30274:         cx->debugHooks->objectHook(cx, obj, JS_FALSE,
30274:                                    cx->debugHooks->objectHookData);
30274:     }
30274: 
30274:     /* Finalize obj first, in case it needs map and slots. */
30654:     JSClass *clasp = STOBJ_GET_CLASS(obj);
30654:     if (clasp->finalize)
30654:         clasp->finalize(cx, obj);
30274: 
30274: #ifdef INCLUDE_MOZILLA_DTRACE
30274:     if (JAVASCRIPT_OBJECT_FINALIZE_ENABLED())
30274:         jsdtrace_object_finalize(obj);
30274: #endif
30274: 
30274:     if (JS_LIKELY(OBJ_IS_NATIVE(obj)))
30274:         OBJ_SCOPE(obj)->drop(cx, obj);
30274:     js_FreeSlots(cx, obj);
30274: }
30274: 
33582: static inline void
33582: FinalizeGCThing(JSContext *cx, JSFunction *fun, unsigned thingKind)
33582: {
33582:     JS_ASSERT(thingKind == FINALIZE_FUNCTION);
34635:     ::FinalizeGCThing(cx, FUN_OBJECT(fun), thingKind);
33582: }
33582: 
33582: #if JS_HAS_XML_SUPPORT
33582: static inline void
33582: FinalizeGCThing(JSContext *cx, JSXML *xml, unsigned thingKind)
33582: {
33582:     js_FinalizeXML(cx, xml);
33582: }
33582: #endif
33582: 
33581: JS_STATIC_ASSERT(JS_EXTERNAL_STRING_LIMIT == 8);
33581: static JSStringFinalizeOp str_finalizers[JS_EXTERNAL_STRING_LIMIT] = {
30275:     NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL
30275: };
30275: 
30275: intN
30275: js_ChangeExternalStringFinalizer(JSStringFinalizeOp oldop,
30275:                                  JSStringFinalizeOp newop)
30275: {
33581:     for (uintN i = 0; i != JS_ARRAY_LENGTH(str_finalizers); i++) {
30275:         if (str_finalizers[i] == oldop) {
30275:             str_finalizers[i] = newop;
33581:             return intN(i);
30275:         }
30275:     }
30275:     return -1;
30275: }
30275: 
30275: /*
30275:  * cx is NULL when we are called from js_FinishAtomState to force the
30275:  * finalization of the permanently interned strings.
30275:  */
33582: static void
33582: FinalizeString(JSRuntime *rt, JSString *str, unsigned thingKind, JSContext *cx)
30275: {
30275:     jschar *chars;
30275:     JSBool valid;
30275:     JSStringFinalizeOp finalizer;
30275: 
30275:     JS_RUNTIME_UNMETER(rt, liveStrings);
32674:     JS_ASSERT(!JSString::isStatic(str));
33582:     JS_ASSERT(IsFinalizableStringKind(thingKind));
30275:     if (str->isDependent()) {
30275:         /* A dependent string can not be external and must be valid. */
33582:         JS_ASSERT(thingKind == FINALIZE_STRING);
30275:         JS_ASSERT(str->dependentBase());
30275:         JS_RUNTIME_UNMETER(rt, liveDependentStrings);
30275:         valid = JS_TRUE;
30275:     } else {
30275:         /* A stillborn string has null chars, so is not valid. */
30275:         chars = str->flatChars();
30275:         valid = (chars != NULL);
30275:         if (valid) {
33582:             if (thingKind == FINALIZE_STRING) {
30851:                 if (cx)
30851:                     cx->free(chars);
30851:                 else
30851:                     rt->free(chars);
30275:             } else {
33582:                 unsigned type = thingKind - FINALIZE_EXTERNAL_STRING0;
33582:                 JS_ASSERT(type < JS_ARRAY_LENGTH(str_finalizers));
30275:                 finalizer = str_finalizers[type];
30275:                 if (finalizer) {
30275:                     /*
30275:                      * Assume that the finalizer for the permanently interned
30275:                      * string knows how to deal with null context.
30275:                      */
30275:                     finalizer(cx, str);
30275:                 }
30275:             }
30275:         }
30275:     }
30275:     if (valid && str->isDeflated())
30275:         js_PurgeDeflatedStringCache(rt, str);
30275: }
30275: 
33582: static inline void
33582: FinalizeGCThing(JSContext *cx, JSString *str, unsigned thingKind)
33582: {
33582:     return FinalizeString(cx->runtime, str, thingKind, cx);
33582: }
33582: 
33582: void
33582: js_FinalizeStringRT(JSRuntime *rt, JSString *str)
33582: {
33582:     JS_ASSERT(!JSString::isStatic(str));
33582: 
33582:     unsigned thingKind = THING_TO_ARENA(str)->list->thingKind;
33582:     FinalizeString(rt, str, thingKind, NULL);
33582: }
33582: 
33582: template<typename T>
33582: static void
33582: FinalizeArenaList(JSContext *cx, unsigned thingKind,
33582:                   JSGCArenaInfo **emptyArenas)
33582: {
33582:     JSGCArenaList *arenaList = &cx->runtime->gcArenaList[thingKind];
33582:     JS_ASSERT(sizeof(T) == arenaList->thingSize);
33582: 
33952:     JSGCArenaInfo **ap = &arenaList->head;
33582:     JSGCArenaInfo *a = *ap;
33582:     if (!a)
33582:         return;
33582: 
33582: #ifdef JS_GCMETER
33582:     uint32 nlivearenas = 0, nkilledarenas = 0, nthings = 0;
33582: #endif
33582:     for (;;) {
33952:         JS_ASSERT(a->list == arenaList);
36410:         JS_ASSERT(a->prevUnmarkedPage == 0);
36410:         JS_ASSERT(a->unmarkedChildren == 0);
33952: 
33952:         JSGCThing *freeList = NULL;
33952:         JSGCThing **tailp = &freeList;
33582:         bool allClear = true;
33952:         uint8 *flagp = THING_FLAGP(a, 0);
33952:         JSGCThing *thing =
33952:             reinterpret_cast<JSGCThing *>(ARENA_INFO_TO_START(a));
33952:         JSGCThing *thingsEnd =
33952:             reinterpret_cast<JSGCThing *>(ARENA_INFO_TO_START(a) +
33952:                                           THINGS_PER_ARENA(sizeof(T)) *
33952:                                           sizeof(T));
36410:         JSGCThing* nextFree = a->freeList ? a->freeList : thingsEnd;
33952:         for (;; thing = NextThing(thing, sizeof(T)), --flagp) {
33952:              if (thing == nextFree) {
33952:                 if (thing == thingsEnd)
33952:                     break;
33952:                 JS_ASSERT(!*flagp);
33952:                 nextFree = nextFree->link;
33952:                 if (!nextFree) {
33952:                     nextFree = thingsEnd;
33952:                 } else {
33952:                     JS_ASSERT(thing < nextFree);
33952:                     JS_ASSERT(nextFree < thingsEnd);
33952:                 }
33952:             } else {
33952:                 JS_ASSERT(!(*flagp & ~(GCF_MARK | GCF_LOCK)));
33952:                 if (*flagp) {
33582:                     *flagp &= ~GCF_MARK;
33582:                     allClear = false;
33582:                     METER(nthings++);
33952:                     continue;
33952:                 }
33952: 
33952:                 /*
33952:                  * Call the finalizer with cleared flags so
33952:                  * js_IsAboutToBeFinalized will be false.
33952:                  */
33952:                 *flagp = 0;
34635:                 ::FinalizeGCThing(cx, reinterpret_cast<T *>(thing), thingKind);
33582: #ifdef DEBUG
33582:                 memset(thing, JS_FREE_PATTERN, sizeof(T));
33582: #endif
33582:             }
33952:             *tailp = thing;
33952:             tailp = &thing->link;
33582:         }
33952: 
33952: #ifdef DEBUG
33952:         /* Check that the free list is consistent. */
33952:         unsigned nfree = 0;
33952:         if (freeList) {
33952:             JS_ASSERT(tailp != &freeList);
33952:             JSGCThing *thing = freeList;
33952:             for (;;) {
33952:                 ++nfree;
33952:                 if (&thing->link == tailp)
33952:                     break;
33952:                 JS_ASSERT(thing < thing->link);
33952:                 thing = thing->link;
33952:             }
33952:         }
33952: #endif
33582:         if (allClear) {
33582:             /*
33582:              * Forget just assembled free list head for the arena and
33582:              * add the arena itself to the destroy list.
33582:              */
33952:             JS_ASSERT(nfree == THINGS_PER_ARENA(sizeof(T)));
33582:             *ap = a->prev;
33582:             a->prev = *emptyArenas;
33582:             *emptyArenas = a;
33582:             METER(nkilledarenas++);
33582:         } else {
33952:             JS_ASSERT(nfree < THINGS_PER_ARENA(sizeof(T)));
33952:             *tailp = NULL;
36410:             a->freeList = freeList;
33582:             ap = &a->prev;
33582:             METER(nlivearenas++);
33582:         }
33582:         if (!(a = *ap))
33582:             break;
33582:     }
33952:     arenaList->cursor = arenaList->head;
33582: 
33582:     METER(UpdateArenaStats(&rt->gcStats.arenaStats[thingKind],
33582:                            nlivearenas, nkilledarenas, nthings));
33582: }
33582: 
11041: /*
11041:  * The gckind flag bit GC_LOCK_HELD indicates a call from js_NewGCThing with
11041:  * rt->gcLock already held, so the lock should be kept on return.
    1:  */
    1: void
    1: js_GC(JSContext *cx, JSGCInvocationKind gckind)
    1: {
    1:     JSRuntime *rt;
    1:     JSBool keepAtoms;
11022:     JSGCCallback callback;
  583:     JSTracer trc;
33582:     JSGCArenaInfo *emptyArenas, *a, **ap;
    1: #ifdef JS_THREADSAFE
    1:     uint32 requestDebit;
    1: #endif
10954: #ifdef JS_GCMETER
12282:     uint32 nlivearenas, nkilledarenas, nthings;
10954: #endif
    1: 
19987:     JS_ASSERT_IF(gckind == GC_LAST_DITCH, !JS_ON_TRACE(cx));
    1:     rt = cx->runtime;
26569: 
    1: #ifdef JS_THREADSAFE
26569:     /*
26569:      * We allow js_GC calls outside a request but the context must be bound
26569:      * to the current thread.
26569:      */
26569:     JS_ASSERT(CURRENT_THREAD_IS_ME(cx->thread));
26569: 
    1:     /* Avoid deadlock. */
    1:     JS_ASSERT(!JS_IS_RUNTIME_LOCKED(rt));
    1: #endif
    1: 
11041:     if (gckind & GC_KEEP_ATOMS) {
11041:         /*
11041:          * The set slot request and last ditch GC kinds preserve all atoms and
11041:          * weak roots.
11041:          */
    1:         keepAtoms = JS_TRUE;
    1:     } else {
    1:         /* Keep atoms when a suspended compile is running on another context. */
    1:         keepAtoms = (rt->gcKeepAtoms != 0);
 6038:         JS_CLEAR_WEAK_ROOTS(&cx->weakRoots);
    1:     }
    1: 
    1:     /*
    1:      * Don't collect garbage if the runtime isn't up, and cx is not the last
    1:      * context in the runtime.  The last context must force a GC, and nothing
    1:      * should suppress that final collection or there may be shutdown leaks,
    1:      * or runtime bloat until the next context is created.
    1:      */
    1:     if (rt->state != JSRTS_UP && gckind != GC_LAST_CONTEXT)
    1:         return;
    1: 
11057:   restart_at_beginning:
    1:     /*
    1:      * Let the API user decide to defer a GC if it wants to (unless this
11022:      * is the last context).  Invoke the callback regardless. Sample the
11022:      * callback in case we are freely racing with a JS_SetGCCallback{,RT} on
11022:      * another thread.
    1:      */
11057:     if (gckind != GC_SET_SLOT_REQUEST && (callback = rt->gcCallback)) {
11022:         JSBool ok;
11022: 
11041:         if (gckind & GC_LOCK_HELD)
11022:             JS_UNLOCK_GC(rt);
11022:         ok = callback(cx, JSGC_BEGIN);
11041:         if (gckind & GC_LOCK_HELD)
11022:             JS_LOCK_GC(rt);
15499:         if (!ok && gckind != GC_LAST_CONTEXT) {
15499:             /*
15499:              * It's possible that we've looped back to this code from the 'goto
15499:              * restart_at_beginning' below in the GC_SET_SLOT_REQUEST code and
15499:              * that rt->gcLevel is now 0. Don't return without notifying!
15499:              */
15499:             if (rt->gcLevel == 0 && (gckind & GC_LOCK_HELD))
15499:                 JS_NOTIFY_GC_DONE(rt);
    1:             return;
    1:         }
15499:     }
    1: 
    1:     /* Lock out other GC allocator and collector invocations. */
11041:     if (!(gckind & GC_LOCK_HELD))
    1:         JS_LOCK_GC(rt);
    1: 
    1:     METER(rt->gcStats.poke++);
    1:     rt->gcPoke = JS_FALSE;
    1: 
    1: #ifdef JS_THREADSAFE
    1:     /*
27384:      * Check if the GC is already running on this or another thread and
27384:      * delegate the job to it.
    1:      */
    1:     if (rt->gcLevel > 0) {
27384:         JS_ASSERT(rt->gcThread);
27384: 
    1:         /* Bump gcLevel to restart the current GC, so it finds new garbage. */
    1:         rt->gcLevel++;
10954:         METER_UPDATE_MAX(rt->gcStats.maxlevel, rt->gcLevel);
    1: 
27384:         /*
27384:          * If the GC runs on another thread, temporarily suspend the current
27384:          * request and wait until the GC is done.
27384:          */
27384:         if (rt->gcThread != cx->thread) {
27384:             requestDebit = js_DiscountRequestsForGC(cx);
27384:             js_RecountRequestsAfterGC(rt, requestDebit);
27384:         }
11041:         if (!(gckind & GC_LOCK_HELD))
    1:             JS_UNLOCK_GC(rt);
    1:         return;
    1:     }
    1: 
    1:     /* No other thread is in GC, so indicate that we're now in GC. */
    1:     rt->gcLevel = 1;
    1:     rt->gcThread = cx->thread;
    1: 
25087:     /*
25087:      * Notify all operation callbacks, which will give them a chance to
25087:      * yield their current request. Contexts that are not currently
25087:      * executing will perform their callback at some later point,
25087:      * which then will be unnecessary, but harmless.
25087:      */
25087:     js_NudgeOtherContexts(cx);
25087: 
27384:     /*
27384:      * Discount all the requests on the current thread from contributing
27384:      * to rt->requestCount before we wait for all other requests to finish.
27384:      * JS_NOTIFY_REQUEST_DONE, which will wake us up, is only called on
27384:      * rt->requestCount transitions to 0.
27384:      */
27384:     requestDebit = js_CountThreadRequests(cx);
27384:     JS_ASSERT_IF(cx->requestDepth != 0, requestDebit >= 1);
27384:     rt->requestCount -= requestDebit;
    1:     while (rt->requestCount > 0)
    1:         JS_AWAIT_REQUEST_DONE(rt);
27384:     rt->requestCount += requestDebit;
    1: 
    1: #else  /* !JS_THREADSAFE */
    1: 
    1:     /* Bump gcLevel and return rather than nest; the outer gc will restart. */
    1:     rt->gcLevel++;
10954:     METER_UPDATE_MAX(rt->gcStats.maxlevel, rt->gcLevel);
    1:     if (rt->gcLevel > 1)
    1:         return;
    1: 
    1: #endif /* !JS_THREADSAFE */
    1: 
    1:     /*
    1:      * Set rt->gcRunning here within the GC lock, and after waiting for any
    1:      * active requests to end, so that new requests that try to JS_AddRoot,
    1:      * JS_RemoveRoot, or JS_RemoveRootRT block in JS_BeginRequest waiting for
    1:      * rt->gcLevel to drop to zero, while request-less calls to the *Root*
    1:      * APIs block in js_AddRoot or js_RemoveRoot (see above in this file),
    1:      * waiting for GC to finish.
    1:      */
    1:     rt->gcRunning = JS_TRUE;
11041: 
11041:     if (gckind == GC_SET_SLOT_REQUEST) {
11041:         JSSetSlotRequest *ssr;
11041: 
11041:         while ((ssr = rt->setSlotRequests) != NULL) {
11041:             rt->setSlotRequests = ssr->next;
11041:             JS_UNLOCK_GC(rt);
11041:             ssr->next = NULL;
11041:             ProcessSetSlotRequest(cx, ssr);
11041:             JS_LOCK_GC(rt);
11041:         }
11041: 
11041:         /*
11041:          * We assume here that killing links to parent and prototype objects
11041:          * does not create garbage (such objects typically are long-lived and
11041:          * widely shared, e.g. global objects, Function.prototype, etc.). We
11041:          * collect garbage only if a racing thread attempted GC and is waiting
11041:          * for us to finish (gcLevel > 1) or if someone already poked us.
11041:          */
27546:         if (rt->gcLevel == 1 && !rt->gcPoke && !rt->gcIsNeeded)
11041:             goto done_running;
11689: 
11623:         rt->gcLevel = 0;
11041:         rt->gcPoke = JS_FALSE;
11623:         rt->gcRunning = JS_FALSE;
11623: #ifdef JS_THREADSAFE
11623:         rt->gcThread = NULL;
11623: #endif
11057:         gckind = GC_LOCK_HELD;
11057:         goto restart_at_beginning;
11041:     }
11041: 
    1:     JS_UNLOCK_GC(rt);
    1: 
19575: #ifdef JS_TRACER
19575:     if (JS_ON_TRACE(cx))
19575:         goto out;
24528: #endif
24499:     VOUCH_HAVE_STACK();
19575: 
27546:     /* Clear gcIsNeeded now, when we are about to start a normal GC cycle. */
27546:     rt->gcIsNeeded = JS_FALSE;
27546: 
32553:     /* Reset malloc counter. */
34288:     rt->resetGCMallocBytes();
32553: 
10217: #ifdef JS_DUMP_SCOPE_METERS
    1:   { extern void js_DumpScopeMeters(JSRuntime *rt);
    1:     js_DumpScopeMeters(rt);
    1:   }
    1: #endif
    1: 
18092: #ifdef JS_TRACER
26569:     js_PurgeJITOracle();
18092: #endif
    1: 
    1:   restart:
    1:     rt->gcNumber++;
36410:     JS_ASSERT(!rt->gcUnmarkedArenaStackTop);
36410:     JS_ASSERT(rt->gcMarkLaterCount == 0);
    1: 
28312:     /*
28312:      * Reset the property cache's type id generator so we can compress ids.
28312:      * Same for the protoHazardShape proxy-shape standing in for all object
28312:      * prototypes having readonly or setter properties.
28312:      */
34360:     if (rt->shapeGen & SHAPE_OVERFLOW_BIT
34360: #ifdef JS_GC_ZEAL
34360:         || rt->gcZeal >= 1
34360: #endif
34360:         ) {
30733:         rt->gcRegenShapes = true;
30845:         rt->gcRegenShapesScopeFlag ^= JSScope::SHAPE_REGEN;
11377:         rt->shapeGen = 0;
28312:         rt->protoHazardShape = 0;
30733:     }
30733: 
30733:     js_PurgeThreads(cx);
33534: #ifdef JS_TRACER
33534:     if (gckind == GC_LAST_CONTEXT) {
33534:         /* Clear builtin functions, which are recreated on demand. */
33534:         memset(rt->builtinFunctions, 0, sizeof rt->builtinFunctions);
33534:     }
33534: #endif
    1: 
    1:     /*
    1:      * Mark phase.
    1:      */
  583:     JS_TRACER_INIT(&trc, cx, NULL);
  583:     rt->gcMarkingTracer = &trc;
  583:     JS_ASSERT(IS_GC_MARKING_TRACER(&trc));
12282: 
33747: #ifdef DEBUG
33952:     for (a = rt->gcDoubleArenaList.head; a; a = a->prev)
33952:         JS_ASSERT(!a->hasMarkedDoubles);
33747: #endif
12282: 
  786:     js_TraceRuntime(&trc, keepAtoms);
    1:     js_MarkScriptFilenames(rt, keepAtoms);
  583: 
    1:     /*
  583:      * Mark children of things that caused too deep recursion during the above
  583:      * tracing.
    1:      */
36410:     MarkDelayedChildren(&trc);
    1: 
    1:     JS_ASSERT(!cx->insideGCMarkCallback);
    1:     if (rt->gcCallback) {
    1:         cx->insideGCMarkCallback = JS_TRUE;
    1:         (void) rt->gcCallback(cx, JSGC_MARK_END);
    1:         JS_ASSERT(cx->insideGCMarkCallback);
    1:         cx->insideGCMarkCallback = JS_FALSE;
    1:     }
36410:     JS_ASSERT(rt->gcMarkLaterCount == 0);
    1: 
  583:     rt->gcMarkingTracer = NULL;
  583: 
30851: #ifdef JS_THREADSAFE
32553:     cx->createDeallocatorTask();
30851: #endif
30851: 
    1:     /*
    1:      * Sweep phase.
    1:      *
    1:      * Finalize as we sweep, outside of rt->gcLock but with rt->gcRunning set
    1:      * so that any attempt to allocate a GC-thing from a finalizer will fail,
    1:      * rather than nest badly and leave the unmarked newborn to be swept.
    1:      *
 4529:      * We first sweep atom state so we can use js_IsAboutToBeFinalized on
 4529:      * JSString or jsdouble held in a hashtable to check if the hashtable
 4529:      * entry can be freed. Note that even after the entry is freed, JSObject
 4529:      * finalizers can continue to access the corresponding jsdouble* and
 4529:      * JSString* assuming that they are unique. This works since the
 4529:      * atomization API must not be called during GC.
 4529:      */
 4529:     js_SweepAtomState(cx);
 4529: 
 5816:     /* Finalize iterator states before the objects they iterate over. */
 5816:     CloseNativeIterators(cx);
 5816: 
 5816:     /* Finalize watch points associated with unreachable objects. */
 5816:     js_SweepWatchPoints(cx);
 5816: 
10217: #ifdef DEBUG
10217:     /* Save the pre-sweep count of scope-mapped properties. */
10217:     rt->liveScopePropsPreSweep = rt->liveScopeProps;
10217: #endif
10217: 
 4529:     /*
33582:      * We finalize JSObject instances before JSString, double and other GC
33582:      * things to ensure that object's finalizer can access them even if they
33582:      * will be freed.
33582:      *
33582:      * To minimize the number of checks per each to be freed object and
33582:      * function we use separated list finalizers when a debug hook is
33582:      * installed.
    1:      */
32823:     emptyArenas = NULL;
33582:     FinalizeArenaList<JSObject>(cx, FINALIZE_OBJECT, &emptyArenas);
33582:     FinalizeArenaList<JSFunction>(cx, FINALIZE_FUNCTION, &emptyArenas);
 6117: #if JS_HAS_XML_SUPPORT
33582:     FinalizeArenaList<JSXML>(cx, FINALIZE_XML, &emptyArenas);
 6117: #endif
33582:     FinalizeArenaList<JSString>(cx, FINALIZE_STRING, &emptyArenas);
33582:     for (unsigned i = FINALIZE_EXTERNAL_STRING0;
33582:          i <= FINALIZE_EXTERNAL_STRING_LAST;
33582:          ++i) {
33582:         FinalizeArenaList<JSString>(cx, i, &emptyArenas);
    1:     }
    1: 
33952:     ap = &rt->gcDoubleArenaList.head;
12282:     METER((nlivearenas = 0, nkilledarenas = 0, nthings = 0));
12282:     while ((a = *ap) != NULL) {
33952:         if (!a->hasMarkedDoubles) {
12282:             /* No marked double values in the arena. */
12282:             *ap = a->prev;
32823:             a->prev = emptyArenas;
32823:             emptyArenas = a;
12282:             METER(nkilledarenas++);
12282:         } else {
33952:             a->hasMarkedDoubles = false;
12282:             ap = &a->prev;
12282: #ifdef JS_GCMETER
33747:             for (size_t i = 0; i != DOUBLES_PER_ARENA; ++i) {
36419:                 if (IsMarkedDouble(a, i))
12282:                     METER(nthings++);
12282:             }
12282:             METER(nlivearenas++);
12282: #endif
12282:         }
12282:     }
12282:     METER(UpdateArenaStats(&rt->gcStats.doubleArenaStats,
12282:                            nlivearenas, nkilledarenas, nthings));
33952:     rt->gcDoubleArenaList.cursor = rt->gcDoubleArenaList.head;
12282: 
    1:     /*
    1:      * Sweep the runtime's property tree after finalizing objects, in case any
 4529:      * had watchpoints referencing tree nodes.
    1:      */
    1:     js_SweepScopeProperties(cx);
    1: 
    1:     /*
    1:      * Sweep script filenames after sweeping functions in the generic loop
    1:      * above. In this way when a scripted function's finalizer destroys the
    1:      * script and calls rt->destroyScriptHook, the hook can still access the
    1:      * script's filename. See bug 323267.
    1:      */
    1:     js_SweepScriptFilenames(rt);
    1: 
32823:     /*
32823:      * Destroy arenas after we finished the sweeping so finalizers can safely
32823:      * use js_IsAboutToBeFinalized().
32823:      */
32823:     DestroyGCArenas(rt, emptyArenas);
32823: 
30851: #ifdef JS_THREADSAFE
30851:     cx->submitDeallocatorTask();
30851: #endif
30851: 
    1:     if (rt->gcCallback)
    1:         (void) rt->gcCallback(cx, JSGC_FINALIZE_END);
    1: #ifdef DEBUG_srcnotesize
    1:   { extern void DumpSrcNoteSizeHist();
    1:     DumpSrcNoteSizeHist();
    1:     printf("GC HEAP SIZE %lu\n", (unsigned long)rt->gcBytes);
    1:   }
    1: #endif
    1: 
10217: #ifdef JS_SCOPE_DEPTH_METER
10217:   { static FILE *fp;
10217:     if (!fp)
10217:         fp = fopen("/tmp/scopedepth.stats", "w");
10217: 
10217:     if (fp) {
10217:         JS_DumpBasicStats(&rt->protoLookupDepthStats, "proto-lookup depth", fp);
10217:         JS_DumpBasicStats(&rt->scopeSearchDepthStats, "scope-search depth", fp);
10217:         JS_DumpBasicStats(&rt->hostenvScopeDepthStats, "hostenv scope depth", fp);
10217:         JS_DumpBasicStats(&rt->lexicalScopeDepthStats, "lexical scope depth", fp);
10217: 
10217:         putc('\n', fp);
10217:         fflush(fp);
10217:     }
10217:   }
10217: #endif /* JS_SCOPE_DEPTH_METER */
10217: 
17182: #ifdef JS_DUMP_LOOP_STATS
17182:   { static FILE *lsfp;
17182:     if (!lsfp)
17182:         lsfp = fopen("/tmp/loopstats", "w");
17182:     if (lsfp) {
17182:         JS_DumpBasicStats(&rt->loopStats, "loops", lsfp);
17182:         fflush(lsfp);
17182:     }
17182:   }
17182: #endif /* JS_DUMP_LOOP_STATS */
17182: 
19575: #ifdef JS_TRACER
19575: out:
19575: #endif
    1:     JS_LOCK_GC(rt);
    1: 
    1:     /*
    1:      * We want to restart GC if js_GC was called recursively or if any of the
    1:      * finalizers called js_RemoveRoot or js_UnlockGCThingRT.
    1:      */
24499:     if (!JS_ON_TRACE(cx) && (rt->gcLevel > 1 || rt->gcPoke)) {
24499:         VOUCH_HAVE_STACK();
    1:         rt->gcLevel = 1;
    1:         rt->gcPoke = JS_FALSE;
    1:         JS_UNLOCK_GC(rt);
    1:         goto restart;
    1:     }
11041: 
32553:     rt->setGCLastBytes(rt->gcBytes);
11041:   done_running:
    1:     rt->gcLevel = 0;
30733:     rt->gcRunning = rt->gcRegenShapes = false;
    1: 
    1: #ifdef JS_THREADSAFE
    1:     rt->gcThread = NULL;
    1:     JS_NOTIFY_GC_DONE(rt);
    1: 
    1:     /*
11041:      * Unlock unless we have GC_LOCK_HELD which requires locked GC on return.
    1:      */
11041:     if (!(gckind & GC_LOCK_HELD))
    1:         JS_UNLOCK_GC(rt);
    1: #endif
    1: 
11022:     /*
11022:      * Execute JSGC_END callback outside the lock. Again, sample the callback
11022:      * pointer in case it changes, since we are outside of the GC vs. requests
11022:      * interlock mechanism here.
11022:      */
11057:     if (gckind != GC_SET_SLOT_REQUEST && (callback = rt->gcCallback)) {
    1:         JSWeakRoots savedWeakRoots;
    1:         JSTempValueRooter tvr;
    1: 
11041:         if (gckind & GC_KEEP_ATOMS) {
    1:             /*
    1:              * We allow JSGC_END implementation to force a full GC or allocate
11041:              * new GC things. Thus we must protect the weak roots from garbage
11041:              * collection and overwrites.
    1:              */
    1:             savedWeakRoots = cx->weakRoots;
    1:             JS_PUSH_TEMP_ROOT_WEAK_COPY(cx, &savedWeakRoots, &tvr);
    1:             JS_KEEP_ATOMS(rt);
    1:             JS_UNLOCK_GC(rt);
    1:         }
    1: 
11022:         (void) callback(cx, JSGC_END);
    1: 
11041:         if (gckind & GC_KEEP_ATOMS) {
    1:             JS_LOCK_GC(rt);
    1:             JS_UNKEEP_ATOMS(rt);
    1:             JS_POP_TEMP_ROOT(cx, &tvr);
    1:         } else if (gckind == GC_LAST_CONTEXT && rt->gcPoke) {
    1:             /*
    1:              * On shutdown iterate until JSGC_END callback stops creating
    1:              * garbage.
    1:              */
11057:             goto restart_at_beginning;
    1:         }
    1:     }
    1: }
