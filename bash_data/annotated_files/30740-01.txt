30266: /* -*- Mode: C++; c-basic-offset: 4; indent-tabs-mode: nil; tab-width: 4 -*- */
30266: /* vi: set ts=4 sw=4 expandtab: (add to ~/.vimrc: set modeline modelines=5) */
17275: /* ***** BEGIN LICENSE BLOCK *****
17275:  * Version: MPL 1.1/GPL 2.0/LGPL 2.1
17275:  *
17275:  * The contents of this file are subject to the Mozilla Public License Version
17275:  * 1.1 (the "License"); you may not use this file except in compliance with
17275:  * the License. You may obtain a copy of the License at
17275:  * http://www.mozilla.org/MPL/
17275:  *
17275:  * Software distributed under the License is distributed on an "AS IS" basis,
17275:  * WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
17275:  * for the specific language governing rights and limitations under the
17275:  * License.
17275:  *
17275:  * The Original Code is [Open Source Virtual Machine].
17275:  *
17275:  * The Initial Developer of the Original Code is
17275:  * Adobe System Incorporated.
17275:  * Portions created by the Initial Developer are Copyright (C) 2004-2007
17275:  * the Initial Developer. All Rights Reserved.
17275:  *
17275:  * Contributor(s):
17275:  *   Adobe AS3 Team
17275:  *
17275:  * Alternatively, the contents of this file may be used under the terms of
17275:  * either the GNU General Public License Version 2 or later (the "GPL"), or
17275:  * the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
17275:  * in which case the provisions of the GPL or the LGPL are applicable instead
17275:  * of those above. If you wish to allow use of your version of this file only
17275:  * under the terms of either the GPL or the LGPL, and not to allow others to
17275:  * use your version of this file under the terms of the MPL, indicate your
17275:  * decision by deleting the provisions above and replace them with the notice
17275:  * and other provisions required by the GPL or the LGPL. If you do not delete
17275:  * the provisions above, a recipient may use your version of this file under
17275:  * the terms of any one of the MPL, the GPL or the LGPL.
17275:  *
17275:  * ***** END LICENSE BLOCK ***** */
17275: 
17275: #include "nanojit.h"
17275: 
20893: #ifdef FEATURE_NANOJIT
20893: 
17668: #ifdef AVMPLUS_PORTING_API
17668: #include "portapi_nanojit.h"
17668: #endif
17668: 
20893: #if defined(AVMPLUS_UNIX) && defined(AVMPLUS_ARM)
17516: #include <asm/unistd.h>
30460: extern "C" void __clear_cache(void *BEG, void *END);
17516: #endif
17516: 
25109: #ifdef AVMPLUS_SPARC
25109: extern  "C"    void sync_instruction_memory(caddr_t v, u_int len);
25109: #endif
25109: 
17275: namespace nanojit
17275: {
26545:     int UseSoftfloat = 0;
17275: 
17275: #ifdef NJ_VERBOSE
17275:     class VerboseBlockReader: public LirFilter
17275:     {
17275:         Assembler *assm;
17275:         LirNameMap *names;
22662:         InsList block;
20893:         bool flushnext;
17275:     public:
17275:         VerboseBlockReader(LirFilter *in, Assembler *a, LirNameMap *n)
20893:             : LirFilter(in), assm(a), names(n), block(a->_gc), flushnext(false)
20893:         {}
17275: 
17275:         void flush() {
20893:             flushnext = false;
20893:             if (!block.isEmpty()) {
20893:                 for (int j=0,n=block.size(); j < n; j++) {
20893:                     LIns *i = block[j];
22648:                     assm->outputf("    %s", names->formatIns(i));
20893:                 }
17275:                 block.clear();
17275:             }
20893:         }
20893: 
20893:         void flush_add(LInsp i) {
20893:             flush();
20893:             block.add(i);
20893:         }
17275: 
17275:         LInsp read() {
17275:             LInsp i = in->read();
30252:             if (i->isop(LIR_start)) {
17275:                 flush();
17275:                 return i;
17275:             }
17275:             if (i->isGuard()) {
20893:                 flush_add(i);
17275:                 if (i->oprnd1())
17275:                     block.add(i->oprnd1());
17275:             }
28545:             else if (i->isRet() || i->isBranch()) {
20893:                 flush_add(i);
20893:             }
17687:             else {
20893:                 if (flushnext)
20893:                     flush();
22706:                 block.add(i);//flush_add(i);
20893:                 if (i->isop(LIR_label))
20893:                     flushnext = true;
17275:             }
17275:             return i;
17275:         }
17275:     };
29883: 
29883:     /* A listing filter for LIR, going through backwards.  It merely
29883:        passes its input to its output, but notes it down too.  When
29883:        destructed, prints out what went through.  Is intended to be
29883:        used to print arbitrary intermediate transformation stages of
29883:        LIR. */
29883:     class ReverseLister : public LirFilter
29883:     {
29883:         avmplus::GC* _gc;
29883:         LirNameMap*  _names;
29883:         const char*  _title;
29883:         StringList*  _strs;
29883:         LogControl*  _logc;
29883:     public:
29883:         ReverseLister(LirFilter* in, avmplus::GC* gc,
29883:                       LirNameMap* names, LogControl* logc, const char* title)
29883:             : LirFilter(in)
29883:         {
29883:             _gc    = gc;
29883:             _names = names;
29883:             _title = title;
29883:             _strs  = new StringList(gc);
29883:             _logc  = logc;
29883:         }
29883: 
29883:         ~ReverseLister()
29883:         {
29883:             _logc->printf("\n");
29883:             _logc->printf("=== BEGIN %s ===\n", _title);
29883:             int i, j;
29883:             const char* prefix = "  ";
29883:             for (j = 0, i = _strs->size()-1; i >= 0; i--, j++) {
29883:                 char* str = _strs->get(i);
29883:                 _logc->printf("%s%02d: %s\n", prefix, j, str);
29883:                 _gc->Free(str);
29883:             }
29883:             delete _strs;
29883:             _logc->printf("=== END %s ===\n", _title);
29883:             _logc->printf("\n");
29883:         }
29883: 
29883:         LInsp read()
29883:         {
29883:             LInsp i = in->read();
29883:             const char* str = _names->formatIns(i);
29883:             char* cpy = (char*)_gc->Alloc(strlen(str) + 1,  0/*AllocFlags*/);
29883:             strcpy(cpy, str);
29883:             _strs->add(cpy);
29883:             return i;
29883:         }
29883:     };
17275: #endif
17275: 
17275:     /**
17275:      * Need the following:
17275:      *
17275:      *    - merging paths ( build a graph? ), possibly use external rep to drive codegen
17275:      */
29883:     Assembler::Assembler(Fragmento* frago, LogControl* logc)
20893:         : hasLoop(0)
20893:         , _frago(frago)
17275:         , _gc(frago->core()->gc)
22667:         , config(frago->core()->config)
17275:     {
17275:         AvmCore *core = frago->core();
17275:         nInit(core);
29883:         verbose_only( _logc = logc; )
29883:         verbose_only( _outputCache = 0; )
29883:         verbose_only( outlineEOL[0] = '\0'; )
17275: 
17275:         internalReset();
17275:         pageReset();
17275:     }
17275: 
17275:     void Assembler::arReset()
17275:     {
17275:         _activation.highwatermark = 0;
17275:         _activation.lowwatermark = 0;
17275:         _activation.tos = 0;
17275: 
17275:         for(uint32_t i=0; i<NJ_MAX_STACK_ENTRY; i++)
17275:             _activation.entry[i] = 0;
17275:     }
17275: 
17275:      void Assembler::registerResetAll()
17275:     {
17275:         nRegisterResetAll(_allocator);
17275: 
17275:         // keep a tally of the registers to check that our allocator works correctly
17275:         debug_only(_allocator.count = _allocator.countFree(); )
17275:         debug_only(_allocator.checkCount(); )
17275:         debug_only(_fpuStkDepth = 0; )
17275:     }
17275: 
17275:     Register Assembler::registerAlloc(RegisterMask allow)
17275:     {
17275:         RegAlloc &regs = _allocator;
17275: //        RegisterMask prefer = livePastCall(_ins) ? saved : scratch;
17275:         RegisterMask prefer = SavedRegs & allow;
17275:         RegisterMask free = regs.free & allow;
17275: 
17275:         RegisterMask set = prefer;
17275:         if (set == 0) set = allow;
17275: 
17275:         if (free)
17275:         {
17275:             // at least one is free
17275:             set &= free;
17275: 
17275:             // ok we have at least 1 free register so let's try to pick
17275:             // the best one given the profile of the instruction
17275:             if (!set)
17275:             {
17275:                 // desired register class is not free so pick first of any class
17275:                 set = free;
17275:             }
17275:             NanoAssert((set & allow) != 0);
17275:             Register r = nRegisterAllocFromSet(set);
17275:             regs.used |= rmask(r);
17275:             return r;
17275:         }
17275:         counter_increment(steals);
17275: 
17275:         // nothing free, steal one
17275:         // LSRA says pick the one with the furthest use
20893:         LIns* vic = findVictim(regs, allow);
18776:         NanoAssert(vic != NULL);
18776: 
17275:         Reservation* resv = getresv(vic);
28548:         NanoAssert(resv);
17275: 
17275:         // restore vic
17275:         Register r = resv->reg;
17275:         regs.removeActive(r);
17275:         resv->reg = UnknownReg;
17275: 
17275:         asm_restore(vic, resv, r);
17275:         return r;
17275:     }
17275: 
20893:     /**
20893:      * these instructions don't have to be saved & reloaded to spill,
20893:      * they can just be recalculated w/out any inputs.
20893:      */
20893:     bool Assembler::canRemat(LIns *i) {
30437:         return i->isconst() || i->isconstq() || i->isop(LIR_ialloc);
20893:     }
20893: 
17275:     void Assembler::internalReset()
17275:     {
17275:         // readies for a brand spanking new code generation pass.
17275:         registerResetAll();
17275:         arReset();
17275:     }
17275: 
17275:     NIns* Assembler::pageAlloc(bool exitPage)
17275:     {
17275:         Page*& list = (exitPage) ? _nativeExitPages : _nativePages;
17275:         Page* page = _frago->pageAlloc();
17275:         if (page)
17275:         {
17275:             page->next = list;
17275:             list = page;
22655:             nMarkExecute(page, PAGE_READ|PAGE_WRITE|PAGE_EXEC);
20893:             _stats.pages++;
17275:         }
17275:         else
17275:         {
22662:             // return a location that is 'safe' to write to while we are out of mem
17275:             setError(OutOMem);
22662:             return _startingIns;
17275:         }
17275:         return &page->code[sizeof(page->code)/sizeof(NIns)]; // just past the end
17275:     }
17275: 
17275:     void Assembler::pageReset()
17275:     {
17275:         pagesFree(_nativePages);
17275:         pagesFree(_nativeExitPages);
17275: 
17275:         _nIns = 0;
17275:         _nExitIns = 0;
22662:         _startingIns = 0;
20893:         _stats.pages = 0;
17275: 
17275:         nativePageReset();
17275:     }
17275: 
17275:     void Assembler::pagesFree(Page*& page)
17275:     {
17275:         while(page)
17275:         {
17275:             Page *next = page->next;  // pull next ptr prior to free
17275:             _frago->pageFree(page);
17275:             page = next;
17275:         }
17275:     }
17275: 
20893:     #define bytesFromTop(x)        ( (size_t)(x) - (size_t)pageTop(x) )
20893:     #define bytesToBottom(x)    ( (size_t)pageBottom(x) - (size_t)(x) )
20893:     #define bytesBetween(x,y)    ( (size_t)(x) - (size_t)(y) )
20893: 
20893:     int32_t Assembler::codeBytes()
20893:     {
20893:         // start and end on same page?
20893:         size_t exit = 0;
20893:         int32_t pages = _stats.pages;
20893:         if (_nExitIns-1 == _stats.codeExitStart)
20893:             ;
20893:         else if (samepage(_nExitIns,_stats.codeExitStart))
20893:             exit = bytesBetween(_stats.codeExitStart, _nExitIns);
20893:         else
20893:         {
20893:             pages--;
20893:             exit = ((intptr_t)_stats.codeExitStart & (NJ_PAGE_SIZE-1)) ? bytesFromTop(_stats.codeExitStart)+1 : 0;
20893:             exit += bytesToBottom(_nExitIns)+1;
20893:         }
20893: 
20893:         size_t main = 0;
20893:         if (_nIns-1 == _stats.codeStart)
20893:             ;
20893:         else if (samepage(_nIns,_stats.codeStart))
20893:             main = bytesBetween(_stats.codeStart, _nIns);
20893:         else
20893:         {
20893:             pages--;
20893:             main = ((intptr_t)_stats.codeStart & (NJ_PAGE_SIZE-1)) ? bytesFromTop(_stats.codeStart)+1 : 0;
20893:             main += bytesToBottom(_nIns)+1;
20893:         }
28810:         //nj_dprintf("size %d, exit is %d, main is %d, page count %d, sizeof %d\n", (int)((pages) * NJ_PAGE_SIZE + main + exit),(int)exit, (int)main, (int)_stats.pages, (int)sizeof(Page));
20893:         return (pages) * NJ_PAGE_SIZE + main + exit;
20893:     }
20893: 
20893:     #undef bytesFromTop
20893:     #undef bytesToBottom
20893:     #undef byteBetween
20893: 
17275:     Page* Assembler::handoverPages(bool exitPages)
17275:     {
17275:         Page*& list = (exitPages) ? _nativeExitPages : _nativePages;
17275:         NIns*& ins =  (exitPages) ? _nExitIns : _nIns;
17275:         Page* start = list;
17275:         list = 0;
17275:         ins = 0;
17275:         return start;
17275:     }
17275: 
17275:     #ifdef _DEBUG
17275:     bool Assembler::onPage(NIns* where, bool exitPages)
17275:     {
17275:         Page* page = (exitPages) ? _nativeExitPages : _nativePages;
17275:         bool on = false;
17275:         while(page)
17275:         {
17275:             if (samepage(where-1,page))
17275:                 on = true;
17275:             page = page->next;
17275:         }
17275:         return on;
17275:     }
17275: 
17275:     void Assembler::pageValidate()
17275:     {
28550:         NanoAssert(!error());
28550:         // _nIns and _nExitIns need to be at least on one of these pages
17275:         NanoAssertMsg( onPage(_nIns)&& onPage(_nExitIns,true), "Native instruction pointer overstep paging bounds; check overrideProtect for last instruction");
17275:     }
17275:     #endif
17275: 
17275:     #ifdef _DEBUG
17275: 
17275:     void Assembler::resourceConsistencyCheck()
17275:     {
28550:         NanoAssert(!error());
17275: 
17275: #ifdef NANOJIT_IA32
25469:         NanoAssert((_allocator.active[FST0] && _fpuStkDepth == -1) ||
25469:             (!_allocator.active[FST0] && _fpuStkDepth == 0));
17275: #endif
17275: 
20893:         AR &ar = _activation;
17275:         // check AR entries
20893:         NanoAssert(ar.highwatermark < NJ_MAX_STACK_ENTRY);
17275:         LIns* ins = 0;
17275:         RegAlloc* regs = &_allocator;
20893:         for(uint32_t i = ar.lowwatermark; i < ar.tos; i++)
17275:         {
20893:             ins = ar.entry[i];
17275:             if ( !ins )
17275:                 continue;
17275:             Reservation *r = getresv(ins);
20893:             NanoAssert(r != 0);
20893:             if (r->arIndex) {
30437:                 if (ins->isop(LIR_ialloc)) {
20893:                     int j=i+1;
20893:                     for (int n = i + (ins->size()>>2); j < n; j++) {
20893:                         NanoAssert(ar.entry[j]==ins);
20893:                     }
20893:                     NanoAssert(r->arIndex == (uint32_t)j-1);
20893:                     i = j-1;
20893:                 }
20893:                 else if (ins->isQuad()) {
20893:                     NanoAssert(ar.entry[i - stack_direction(1)]==ins);
20893:                     i += 1; // skip high word
20893:                 }
20893:                 else {
20893:                     NanoAssertMsg(r->arIndex == i, "Stack record index mismatch");
20893:                 }
20893:             }
17275:             NanoAssertMsg( r->reg==UnknownReg || regs->isConsistent(r->reg,ins), "Register record mismatch");
17275:         }
17275: 
20893:         registerConsistencyCheck();
17275:     }
17275: 
20893:     void Assembler::registerConsistencyCheck()
17275:     {
17275:         // check registers
17275:         RegAlloc *regs = &_allocator;
17275:         uint32_t managed = regs->managed;
17275:         Register r = FirstReg;
17275:         while(managed)
17275:         {
17275:             if (managed&1)
17275:             {
17275:                 if (regs->isFree(r))
17275:                 {
17275:                     NanoAssert(regs->getActive(r)==0);
17275:                 }
17275:                 else
17275:                 {
17275:                     LIns* ins = regs->getActive(r);
17275:                     // @todo we should be able to check across RegAlloc's somehow (to include savedGP...)
17275:                     Reservation *v = getresv(ins);
20893:                     NanoAssert(v != 0);
17275:                     NanoAssertMsg( regs->getActive(v->reg)==ins, "Register record mismatch");
17275:                 }
17275:             }
17275: 
17275:             // next register in bitfield
17275:             r = nextreg(r);
17275:             managed >>= 1;
17275:         }
17275:     }
17275:     #endif /* _DEBUG */
17275: 
17275:     void Assembler::findRegFor2(RegisterMask allow, LIns* ia, Reservation* &resva, LIns* ib, Reservation* &resvb)
17275:     {
17275:         if (ia == ib)
17275:         {
17275:             findRegFor(ia, allow);
17275:             resva = resvb = getresv(ia);
17275:         }
17275:         else
17275:         {
17275:             Register rb = UnknownReg;
17275:             resvb = getresv(ib);
20893:             if (resvb && (rb = resvb->reg) != UnknownReg) {
20893:                 if (allow & rmask(rb)) {
20893:                     // ib already assigned to an allowable reg, keep that one
17275:                     allow &= ~rmask(rb);
20893:                 } else {
20893:                     // ib assigned to unusable reg, pick a different one below.
20893:                     rb = UnknownReg;
20893:                 }
20893:             }
17275:             Register ra = findRegFor(ia, allow);
17275:             resva = getresv(ia);
17275:             NanoAssert(error() || (resva != 0 && ra != UnknownReg));
17275:             if (rb == UnknownReg)
17275:             {
17275:                 allow &= ~rmask(ra);
17275:                 findRegFor(ib, allow);
17275:                 resvb = getresv(ib);
17275:             }
17275:         }
17275:     }
17275: 
17275:     Register Assembler::findSpecificRegFor(LIns* i, Register w)
17275:     {
17275:         return findRegFor(i, rmask(w));
17275:     }
17275: 
20919:     Register Assembler::getBaseReg(LIns *i, int &d, RegisterMask allow)
20919:     {
30437:         if (i->isop(LIR_ialloc)) {
20919:             d += findMemFor(i);
20919:             return FP;
20919:         } else {
20919:             return findRegFor(i, allow);
20919:         }
20919:     }
20919: 
17275:     Register Assembler::findRegFor(LIns* i, RegisterMask allow)
17275:     {
30437:         if (i->isop(LIR_ialloc)) {
20893:             // never allocate a reg for this w/out stack space too
20893:             findMemFor(i);
20893:         }
20893: 
17275:         Reservation* resv = getresv(i);
17275:         Register r;
17275: 
18776:         // if we have an existing reservation and it has a non-unknown
18776:         // register allocated, and that register is in our allowed mask,
18776:         // return it.
17275:         if (resv && (r=resv->reg) != UnknownReg && (rmask(r) & allow)) {
20893:             _allocator.useActive(r);
17275:             return r;
17275:         }
17275: 
18776:         // figure out what registers are preferred for this instruction
17275:         RegisterMask prefer = hint(i, allow);
18776: 
18776:         // if we didn't have a reservation, allocate one now
29867:         if (!resv) {
29867:             (resv = i->resv())->init();
29867:         }
17275: 
20893:         r = resv->reg;
20919: 
20919: #ifdef AVMPLUS_IA32
20893:         if (r != UnknownReg &&
25469:             (((rmask(r)&XmmRegs) && !(allow&XmmRegs)) ||
25469:                  ((rmask(r)&x87Regs) && !(allow&x87Regs))))
17275:         {
20893:             // x87 <-> xmm copy required
20893:             //_nvprof("fpu-evict",1);
20893:             evict(r);
20893:             r = UnknownReg;
20893:         }
20919: #endif
20893: 
20893:         if (r == UnknownReg)
20893:         {
17275:             r = resv->reg = registerAlloc(prefer);
17275:             _allocator.addActive(r, i);
17275:             return r;
17275:         }
17275:         else
17275:         {
18776:             // the already-allocated register isn't in the allowed mask;
18776:             // we need to grab a new one and then copy over the old
18776:             // contents to the new.
17275:             resv->reg = UnknownReg;
17275:             _allocator.retire(r);
17275:             Register s = resv->reg = registerAlloc(prefer);
17275:             _allocator.addActive(s, i);
18220:             if ((rmask(r) & GpRegs) && (rmask(s) & GpRegs)) {
26537: #ifdef NANOJIT_ARM
26537:                 MOV(r, s);
26537: #else
17275:                 MR(r, s);
26537: #endif
17275:             }
20893:             else {
17378:                 asm_nongp_copy(r, s);
17275:             }
17275:             return s;
17275:         }
17275:     }
17275: 
17275:     int Assembler::findMemFor(LIns *i)
17275:     {
17275:         Reservation* resv = getresv(i);
17275:         if (!resv)
29867:             (resv = i->resv())->init();
20893:         if (!resv->arIndex) {
17275:             resv->arIndex = arReserve(i);
20893:             NanoAssert(resv->arIndex <= _activation.highwatermark);
20893:         }
17275:         return disp(resv);
17275:     }
17275: 
17275:     Register Assembler::prepResultReg(LIns *i, RegisterMask allow)
17275:     {
17275:         Reservation* resv = getresv(i);
17275:         const bool pop = !resv || resv->reg == UnknownReg;
17275:         Register rr = findRegFor(i, allow);
17275:         freeRsrcOf(i, pop);
17275:         return rr;
17275:     }
17275: 
20919:     void Assembler::asm_spilli(LInsp i, Reservation *resv, bool pop)
20919:     {
20919:         int d = disp(resv);
20919:         Register rr = resv->reg;
30437:         bool quad = i->opcode() == LIR_iparam || i->isQuad();
29883:         verbose_only( if (d && (_logc->lcbits & LC_RegAlloc)) {
29883:                          outputForEOL("  <= spill %s",
29883:                                       _thisfrag->lirbuf->names->formatRef(i)); } )
20919:         asm_spill(rr, d, pop, quad);
20919:     }
20919: 
29861:     // NOTE: Because this function frees slots on the stack, it is not safe to
29861:     // follow a call to this with a call to anything which might spill a
29861:     // register, as the stack can be corrupted. Refer to bug 495239 for a more
29861:     // detailed description.
17275:     void Assembler::freeRsrcOf(LIns *i, bool pop)
17275:     {
17275:         Reservation* resv = getresv(i);
17275:         int index = resv->arIndex;
17275:         Register rr = resv->reg;
17275: 
17275:         if (rr != UnknownReg)
17275:         {
20893:             asm_spilli(i, resv, pop);
17275:             _allocator.retire(rr);    // free any register associated with entry
17275:         }
30644:         if (index) {
30644:             NanoAssert(_activation.entry[index] == i);
17275:             arFree(index);            // free any stack stack space associated with entry
30644:         }
29867:         i->resv()->clear();
17275:     }
17275: 
17275:     void Assembler::evict(Register r)
17275:     {
17275:         registerAlloc(rmask(r));
17275:         _allocator.addFree(r);
17275:     }
17275: 
17275:     void Assembler::patch(GuardRecord *lr)
17275:     {
30740:         if (!lr->jmp) // the guard might have been eliminated as redundant
30740:             return;
20931:         Fragment *frag = lr->exit->target;
20893:         NanoAssert(frag->fragEntry != 0);
22669:         NIns* was = nPatchBranch((NIns*)lr->jmp, frag->fragEntry);
27540:         NanoAssert(frag->fragEntry != was);
17308:         verbose_only(verbose_outputf("patching jump at %p to target %p (was %p)\n",
22669:             lr->jmp, frag->fragEntry, was);)
21489:         (void)was;
17275:     }
17275: 
20931:     void Assembler::patch(SideExit *exit)
17275:     {
20931:         GuardRecord *rec = exit->guards;
20931:         AvmAssert(rec);
20931:         while (rec) {
20931:             patch(rec);
20946:             rec = rec->next;
20931:         }
17275:     }
17275: 
25099: #ifdef NANOJIT_IA32
25099:     void Assembler::patch(SideExit* exit, SwitchInfo* si)
25099:     {
25099:         for (GuardRecord* lr = exit->guards; lr; lr = lr->next) {
25099:             Fragment *frag = lr->exit->target;
25099:             NanoAssert(frag->fragEntry != 0);
25099:             si->table[si->index] = frag->fragEntry;
25099:         }
25099:     }
25099: #endif
25099: 
17308:     NIns* Assembler::asm_exit(LInsp guard)
17275:     {
20931:         SideExit *exit = guard->record()->exit;
17275:         NIns* at = 0;
17275:         if (!_branchStateMap->get(exit))
17275:         {
17308:             at = asm_leave_trace(guard);
17275:         }
17275:         else
17275:         {
17275:             RegAlloc* captured = _branchStateMap->get(exit);
20893:             intersectRegisterState(*captured);
17308:             verbose_only(
29883:                 verbose_outputf("## merging trunk with %s",
17308:                     _frago->labels->format(exit->target));
29883:                 verbose_outputf("%010lx:", (unsigned long)_nIns);
17308:             )
17275:             at = exit->target->fragEntry;
20893:             NanoAssert(at != 0);
17308:             _branchStateMap->remove(exit);
17275:         }
17275:         return at;
17275:     }
17275: 
17308:     NIns* Assembler::asm_leave_trace(LInsp guard)
17275:     {
17275:         verbose_only( int32_t nativeSave = _stats.native );
29883:         verbose_only( verbose_outputf("----------------------------------- ## END exit block %p", guard);)
17275: 
17275:         RegAlloc capture = _allocator;
17275: 
17275:         // this point is unreachable.  so free all the registers.
17275:         // if an instruction has a stack entry we will leave it alone,
20893:         // otherwise we free it entirely.  intersectRegisterState will restore.
17275:         releaseRegisters();
17275: 
17275:         swapptrs();
17275:         _inExit = true;
17275: 
17275:         //verbose_only( verbose_outputf("         LIR_xend swapptrs, _nIns is now %08X(%08X), _nExitIns is now %08X(%08X)",_nIns, *_nIns,_nExitIns,*_nExitIns) );
17275:         debug_only( _sv_fpuStkDepth = _fpuStkDepth; _fpuStkDepth = 0; )
17275: 
17516:         nFragExit(guard);
17516: 
21477:         // restore the callee-saved register and parameters
21477:         assignSavedRegs();
21477:         assignParamRegs();
17275: 
20893:         intersectRegisterState(capture);
17275: 
17275:         // this can be useful for breaking whenever an exit is taken
17275:         //INT3();
17275:         //NOP();
17275: 
17275:         // we are done producing the exit logic for the guard so demark where our exit block code begins
22669:         NIns* jmpTarget = _nIns;     // target in exit path for our mainline conditional jump
17275: 
17275:         // swap back pointers, effectively storing the last location used in the exit path
17275:         swapptrs();
17275:         _inExit = false;
17275: 
17275:         //verbose_only( verbose_outputf("         LIR_xt/xf swapptrs, _nIns is now %08X(%08X), _nExitIns is now %08X(%08X)",_nIns, *_nIns,_nExitIns,*_nExitIns) );
29883:         verbose_only( verbose_outputf("%010lx:", (unsigned long)jmpTarget);)
29883:         verbose_only( verbose_outputf("----------------------------------- ## BEGIN exit block (LIR_xt|LIR_xf)") );
17275: 
17275: #ifdef NANOJIT_IA32
21683:         NanoAssertMsgf(_fpuStkDepth == _sv_fpuStkDepth, "LIR_xtf, _fpuStkDepth=%d, expect %d",_fpuStkDepth, _sv_fpuStkDepth);
17275:         debug_only( _fpuStkDepth = _sv_fpuStkDepth; _sv_fpuStkDepth = 9999; )
17275: #endif
17275: 
17275:         verbose_only(_stats.exitnative += (_stats.native-nativeSave));
17275: 
22669:         return jmpTarget;
17275:     }
17275: 
18099:     void Assembler::beginAssembly(Fragment *frag, RegAllocMap* branchStateMap)
17275:     {
24863:         internalReset();
24863: 
20893:         _thisfrag = frag;
17275:         _activation.lowwatermark = 1;
17275:         _activation.tos = _activation.lowwatermark;
17275:         _activation.highwatermark = _activation.tos;
17275: 
17275:         counter_reset(native);
17275:         counter_reset(exitnative);
17275:         counter_reset(steals);
17275:         counter_reset(spills);
17275:         counter_reset(remats);
17275: 
17275:         setError(None);
17275: 
17275:         // native code gen buffer setup
17275:         nativePageSetup();
17275: 
22662:         // When outOMem, nIns is set to startingIns and we overwrite the region until the error is handled
22662:         underrunProtect(LARGEST_UNDERRUN_PROT);  // the largest value passed to underrunProtect()
28184:         recordStartingInstructionPointer();
22662: 
17668:     #ifdef AVMPLUS_PORTING_API
17668:         _endJit2Addr = _nExitIns;
17668:     #endif
17668: 
17275:         // make sure we got memory at least one page
17308:         if (error()) return;
17275: 
20893: #ifdef PERFM
20893:         _stats.pages = 0;
20893:         _stats.codeStart = _nIns-1;
20893:         _stats.codeExitStart = _nExitIns-1;
28810:         //nj_dprintf("pageReset %d start %x exit start %x\n", _stats.pages, (int)_stats.codeStart, (int)_stats.codeExitStart);
20893: #endif /* PERFM */
20893: 
20893:         _epilogue = genEpilogue();
17275:         _branchStateMap = branchStateMap;
17275: 
22648:         verbose_only( outputAddr=true; )
22648:         verbose_only( asm_output("[epilogue]"); )
17275:     }
17275: 
17308:     void Assembler::assemble(Fragment* frag,  NInsList& loopJumps)
17275:     {
17308:         if (error()) return;
17275:         AvmCore *core = _frago->core();
17275:         _thisfrag = frag;
17275: 
29883:         // Used for debug printing, if needed
29883:         verbose_only(
30342:         ReverseLister *pp_init = NULL;
30342:         ReverseLister *pp_after_sf1 = NULL;
30342:         ReverseLister *pp_after_sf2 = NULL;
29883:         )
29883: 
17516:         // set up backwards pipeline: assembler -> StackFilter -> LirReader
29883:         avmplus::GC *gc = core->gc;
17275:         LirReader bufreader(frag->lastIns);
29883: 
29883:         // Used to construct the pipeline
29883:         LirFilter* prev = &bufreader;
29883: 
29883:         // The LIR passes through these filters as listed in this
29883:         // function, viz, top to bottom.
29883: 
29883:         // INITIAL PRINTING
29883:         verbose_only( if (_logc->lcbits & LC_ReadLIR) {
29883:         pp_init = new ReverseLister(prev, gc, frag->lirbuf->names, _logc,
29883:                                     "Initial LIR");
29883:         prev = pp_init;
29883:         })
29883: 
29883:         // STOREFILTER for sp
29883:         StackFilter storefilter1(prev, gc, frag->lirbuf, frag->lirbuf->sp);
29883:         prev = &storefilter1;
29883: 
29883:         verbose_only( if (_logc->lcbits & LC_AfterSF_SP) {
29883:         pp_after_sf1 = new ReverseLister(prev, gc, frag->lirbuf->names, _logc,
29883:                                          "After Storefilter(sp)");
29883:         prev = pp_after_sf1;
29883:         })
29883: 
29883:         // STOREFILTER for rp
29883:         StackFilter storefilter2(prev, gc, frag->lirbuf, frag->lirbuf->rp);
29883:         prev = &storefilter2;
29883: 
29883:         verbose_only( if (_logc->lcbits & LC_AfterSF_RP) {
29883:         pp_after_sf2 = new ReverseLister(prev, gc, frag->lirbuf->names, _logc,
30277:                                          "After StoreFilter(rp) (final LIR)");
29883:         prev = pp_after_sf2;
29883:         })
29883: 
29883:         // end of pipeline
17275:         verbose_only(
29883:         VerboseBlockReader vbr(prev, this, frag->lirbuf->names);
29883:         if (_logc->lcbits & LC_Assembly)
29883:             prev = &vbr;
17275:         )
17275: 
17275:         verbose_only(_thisfrag->compileNbr++; )
17275:         verbose_only(_frago->_stats.compiles++; )
17308:         verbose_only(_frago->_stats.totalCompiles++; )
17275:         _inExit = false;
30253: 
30253:         LabelStateMap labels(_gc);
30253:         NInsMap patches(_gc);
30253:         gen(prev, loopJumps, labels, patches);
21490:         frag->loopEntry = _nIns;
20931:         //frag->outbound = core->config.tree_opt? _latestGuard : 0;
28810:         //nj_dprintf("assemble frag %X entry %X\n", (int)frag, (int)frag->fragEntry);
20893: 
20893:         if (!error()) {
20893:             // patch all branches
30253:             while (!patches.isEmpty())
20893:             {
30253:                 NIns* where = patches.lastKey();
30253:                 LInsp targ = patches.removeLast();
30253:                 LabelState *label = labels.get(targ);
20893:                 NIns* ntarg = label->addr;
20893:                 if (ntarg) {
20893:                     nPatchBranch(where,ntarg);
20893:                 }
20893:                 else {
27017:                     setError(UnknownBranch);
20893:                     break;
20893:                 }
20893:             }
20893:         }
22662:         else {
28184:             // In case of failure, reset _nIns ready for the next assembly run.
28184:             resetInstructionPointer();
22662:         }
29883: 
29883:         // If we were accumulating debug info in the various ReverseListers,
29883:         // destruct them now.  Their destructors cause them to emit whatever
29883:         // contents they have accumulated.
29883:         verbose_only(
29883:         if (pp_init)       delete pp_init;
29883:         if (pp_after_sf1)  delete pp_after_sf1;
29883:         if (pp_after_sf2)  delete pp_after_sf2;
29883:         )
17275:     }
17275: 
17308:     void Assembler::endAssembly(Fragment* frag, NInsList& loopJumps)
17275:     {
24486:         // don't try to patch code if we are in an error state since we might have partially
24486:         // overwritten the code cache already
24486:         if (error())
24486:             return;
24486: 
20946:         NIns* SOT = 0;
20946:         if (frag->isRoot()) {
21490:             SOT = frag->loopEntry;
29883:             verbose_only( verbose_outputf("%010lx:", (unsigned long)_nIns); )
20946:         } else {
20946:             SOT = frag->root->fragEntry;
20946:         }
20946:         AvmAssert(SOT);
17275:         while(!loopJumps.isEmpty())
17275:         {
17275:             NIns* loopJump = (NIns*)loopJumps.removeLast();
29883:             verbose_only( verbose_outputf("## patching branch at %010lx to %010lx",
29883:                                           loopJump, SOT); )
20946:             nPatchBranch(loopJump, SOT);
17275:         }
17275: 
21490:         NIns* fragEntry = 0;
21490: 
17308:         if (!error())
17308:         {
21490:             fragEntry = genPrologue();
22648:             verbose_only( outputAddr=true; )
22648:             verbose_only( asm_output("[prologue]"); )
17308:         }
17275: 
17275:         // something bad happened?
17275:         if (!error())
17275:         {
17275:             // check for resource leaks
17275:             debug_only(
17275:                 for(uint32_t i=_activation.lowwatermark;i<_activation.highwatermark; i++) {
21683:                     NanoAssertMsgf(_activation.entry[i] == 0, "frame entry %d wasn't freed",-4*i);
17275:                 }
17275:             )
17275: 
21490:             frag->fragEntry = fragEntry;
17308:             NIns* code = _nIns;
20893: #ifdef PERFM
20893:             _nvprof("code", codeBytes());  // requires that all pages are released between begin/endAssembly()otherwise we double count
20893: #endif
17308:             // let the fragment manage the pages if we're using trees and there are branches
17308:             Page* manage = (_frago->core()->config.tree_opt) ? handoverPages() : 0;
17308:             frag->setCode(code, manage); // root of tree should manage all pages
28810:             //nj_dprintf("endAssembly frag %X entry %X\n", (int)frag, (int)frag->fragEntry);
17275:         }
22662:         else
22662:         {
28184:             // In case of failure, reset _nIns ready for the next assembly run.
28184:             resetInstructionPointer();
22662:         }
17275: 
21683:         NanoAssertMsgf(error() || _fpuStkDepth == 0,"_fpuStkDepth %d",_fpuStkDepth);
17275: 
17275:         internalReset();  // clear the reservation tables and regalloc
21483:         NanoAssert( !_branchStateMap || _branchStateMap->isEmpty());
17275:         _branchStateMap = 0;
17275: 
25902:         // Tell Valgrind that new code has been generated, and it must flush
25902:         // any translations it has for the memory range generated into.
25902:         VALGRIND_DISCARD_TRANSLATIONS(pageTop(_nIns-1),     NJ_PAGE_SIZE);
25902:         VALGRIND_DISCARD_TRANSLATIONS(pageTop(_nExitIns-1), NJ_PAGE_SIZE);
25902: 
18773: #ifdef AVMPLUS_ARM
17275:         // If we've modified the code, we need to flush so we don't end up trying
17275:         // to execute junk
18773: # if defined(UNDER_CE)
17275:         FlushInstructionCache(GetCurrentProcess(), NULL, NULL);
25485: # elif defined(AVMPLUS_UNIX)
25485:         for (int i = 0; i < 2; i++) {
25485:             Page *p = (i == 0) ? _nativePages : _nativeExitPages;
25485: 
25485:             Page *first = p;
25485:             while (p) {
25485:                 if (!p->next || p->next != p+1) {
25485:                     __clear_cache((char*)first, (char*)(p+1));
25485:                     first = p->next;
25485:                 }
25485:                 p = p->next;
25485:             }
25485:         }
25485: # endif
25485: #endif
25485: 
25485: #ifdef AVMPLUS_SPARC
25109:         // Clear Instruction Cache
25109:         for (int i = 0; i < 2; i++) {
25109:             Page *p = (i == 0) ? _nativePages : _nativeExitPages;
25109: 
25109:             Page *first = p;
25109:             while (p) {
25109:                 if (!p->next || p->next != p+1) {
25109:                     sync_instruction_memory((char *)first, NJ_PAGE_SIZE);
25109:                     first = p->next;
25109:                 }
25109:                 p = p->next;
25109:             }
25109:         }
18773: #endif
18773: 
17668: # ifdef AVMPLUS_PORTING_API
22662:         NanoJIT_PortAPI_FlushInstructionCache(_nIns, _startingIns);
17668:         NanoJIT_PortAPI_FlushInstructionCache(_nExitIns, _endJit2Addr);
17668: # endif
17275:     }
17275: 
17275:     void Assembler::copyRegisters(RegAlloc* copyTo)
17275:     {
17275:         *copyTo = _allocator;
17275:     }
17275: 
17275:     void Assembler::releaseRegisters()
17275:     {
17275:         for (Register r = FirstReg; r <= LastReg; r = nextreg(r))
17275:         {
17275:             LIns *i = _allocator.getActive(r);
17275:             if (i)
17275:             {
17275:                 // clear reg allocation, preserve stack allocation.
17275:                 Reservation* resv = getresv(i);
17275:                 NanoAssert(resv != 0);
17275:                 _allocator.retire(r);
17275:                 if (r == resv->reg)
17275:                     resv->reg = UnknownReg;
17275: 
17275:                 if (!resv->arIndex && resv->reg == UnknownReg)
17275:                 {
29867:                     i->resv()->clear();
17275:                 }
17275:             }
17275:         }
17275:     }
17275: 
20893: #ifdef PERFM
20893: #define countlir_live() _nvprof("lir-live",1)
20893: #define countlir_ret() _nvprof("lir-ret",1)
20893: #define countlir_alloc() _nvprof("lir-alloc",1)
20893: #define countlir_var() _nvprof("lir-var",1)
20893: #define countlir_use() _nvprof("lir-use",1)
20893: #define countlir_def() _nvprof("lir-def",1)
20893: #define countlir_imm() _nvprof("lir-imm",1)
20893: #define countlir_param() _nvprof("lir-param",1)
20893: #define countlir_cmov() _nvprof("lir-cmov",1)
20893: #define countlir_ld() _nvprof("lir-ld",1)
20893: #define countlir_ldq() _nvprof("lir-ldq",1)
20893: #define countlir_alu() _nvprof("lir-alu",1)
20893: #define countlir_qjoin() _nvprof("lir-qjoin",1)
20893: #define countlir_qlo() _nvprof("lir-qlo",1)
20893: #define countlir_qhi() _nvprof("lir-qhi",1)
20893: #define countlir_fpu() _nvprof("lir-fpu",1)
20893: #define countlir_st() _nvprof("lir-st",1)
20893: #define countlir_stq() _nvprof("lir-stq",1)
20893: #define countlir_jmp() _nvprof("lir-jmp",1)
20893: #define countlir_jcc() _nvprof("lir-jcc",1)
20893: #define countlir_label() _nvprof("lir-label",1)
20893: #define countlir_xcc() _nvprof("lir-xcc",1)
20893: #define countlir_x() _nvprof("lir-x",1)
20893: #define countlir_loop() _nvprof("lir-loop",1)
20893: #define countlir_call() _nvprof("lir-call",1)
20893: #else
20893: #define countlir_live()
20893: #define countlir_ret()
20893: #define countlir_alloc()
20893: #define countlir_var()
20893: #define countlir_use()
20893: #define countlir_def()
20893: #define countlir_imm()
20893: #define countlir_param()
20893: #define countlir_cmov()
20893: #define countlir_ld()
20893: #define countlir_ldq()
20893: #define countlir_alu()
20893: #define countlir_qjoin()
20893: #define countlir_qlo()
20893: #define countlir_qhi()
20893: #define countlir_fpu()
20893: #define countlir_st()
20893: #define countlir_stq()
20893: #define countlir_jmp()
20893: #define countlir_jcc()
20893: #define countlir_label()
20893: #define countlir_xcc()
20893: #define countlir_x()
20893: #define countlir_loop()
20893: #define countlir_call()
20893: #endif
20893: 
30253:     void Assembler::gen(LirFilter* reader,  NInsList& loopJumps, LabelStateMap& labels,
30253:                         NInsMap& patches)
17275:     {
25099:         // trace must end with LIR_x, LIR_loop, LIR_ret, or LIR_xtbl
25085:         NanoAssert(reader->pos()->isop(LIR_x) ||
25085:                    reader->pos()->isop(LIR_loop) ||
25099:                    reader->pos()->isop(LIR_ret) ||
25099:                    reader->pos()->isop(LIR_xtbl));
17275: 
30253:         InsList pending_lives(_gc);
30253: 
30277:         for (LInsp ins = reader->read(); !ins->isop(LIR_start) && !error();
30277:                                          ins = reader->read())
17275:         {
30277:             /* What's going on here: we're visiting all the LIR nodes
30277:                in the buffer, working strictly backwards in
30277:                buffer-order, and generating machine instructions for
30277:                them as we go.
30277: 
30277:                But we're not visiting all of them, only the ones that
30277:                made it through the filter pipeline that we're reading
30277:                from.  For each visited node, we first determine
30277:                whether it's actually necessary, and if not skip it.
30277:                Otherwise we fall into the big switch, which calls a
30277:                target-specific routine to generate the required
30277:                instructions.
30277:    
30277:                For each node, we need to decide whether we need to
30277:                generate any code.  This is a rather subtle part of the
30277:                generation algorithm.
30277:  
30277:                There are two categories:
30277:  
30277:                "statement" nodes -- ones with side effects.  Anything
30277:                that could change control flow or the state of memory.
30277:                These we must absolutely retain.  That accounts for the
30277:                first part of the following disjunction for 'required'.
30277:  
30277:                The rest are "value" nodes, which compute a value based
30277:                only on the operands to the node (and, in the case of
30277:                loads, the state of memory).  It's safe to omit these
30277:                if the value(s) computed are not used later.  Since
30277:                we're visiting nodes in reverse order, if some
30277:                previously visited (viz, later in the buffer ordering)
30277:                node uses the value computed by this node, then this
30277:                node will already have a register assigned to hold that
30277:                value.  Hence we can consult the reservation to detect
30277:                whether the value is in fact used.  That's the second
30277:                part of the disjunction.
30277:             */
30277:             bool required = ins->isStmt() || ins->resv()->used;
30277:             if (!required)
30277:                 continue;
30277:  
17275:             LOpcode op = ins->opcode();
17275:             switch(op)
17275:             {
17275:                 default:
21683:                     NanoAssertMsgf(false, "unsupported LIR instruction: %d (~0x40: %d)", op, op&~LIR64);
17275:                     break;
17275: 
20893:                 case LIR_live: {
20893:                     countlir_live();
20893:                     pending_lives.add(ins->oprnd1());
20893:                     break;
20893:                 }
20893: 
20893:                 case LIR_ret:  {
20893:                     countlir_ret();
22706:                     if (_nIns != _epilogue) {
22706:                         JMP(_epilogue);
22706:                     }
22706:                     assignSavedRegs();
22706: #ifdef NANOJIT_ARM
22706:                     // the epilogue moves R2 to R0; we may want to do this
22706:                     // after assignSavedRegs
22706:                     findSpecificRegFor(ins->oprnd1(), R2);
22706: #else
22706:                     findSpecificRegFor(ins->oprnd1(), retRegs[0]);
22706: #endif
22706:                     break;
22706:                 }
22706: 
22706:                 case LIR_fret: {
22706:                     countlir_ret();
22706:                     if (_nIns != _epilogue) {
22706:                         JMP(_epilogue);
22706:                     }
22706:                     assignSavedRegs();
22706: #ifdef NANOJIT_IA32
22706:                     findSpecificRegFor(ins->oprnd1(), FST0);
22706: #else
22706:                     NanoAssert(false);
22706: #endif
22706:                     fpu_pop();
20893:                     break;
20893:                 }
20893: 
20893:                 // allocate some stack space.  the value of this instruction
20893:                 // is the address of the stack space.
30437:                 case LIR_ialloc: {
20893:                     countlir_alloc();
20893:                     Reservation *resv = getresv(ins);
20893:                     NanoAssert(resv->arIndex != 0);
20893:                     Register r = resv->reg;
20893:                     if (r != UnknownReg) {
20893:                         _allocator.retire(r);
20893:                         resv->reg = UnknownReg;
20893:                         asm_restore(ins, resv, r);
20893:                     }
20893:                     freeRsrcOf(ins, 0);
20893:                     break;
20893:                 }
17275:                 case LIR_int:
17275:                 {
20893:                     countlir_imm();
20921:                     asm_int(ins);
17275:                     break;
17275:                 }
17275:                 case LIR_quad:
17275:                 {
20893:                     countlir_imm();
17378:                     asm_quad(ins);
17275:                     break;
17275:                 }
29869: #if !defined NANOJIT_64BIT
17275:                 case LIR_callh:
17275:                 {
17275:                     // return result of quad-call in register
17275:                     prepResultReg(ins, rmask(retRegs[1]));
17275:                     // if hi half was used, we must use the call to ensure it happens
22661:                     findSpecificRegFor(ins->oprnd1(), retRegs[0]);
17275:                     break;
17275:                 }
29869: #endif
30437:                 case LIR_iparam:
17275:                 {
20893:                     countlir_param();
20921:                     asm_param(ins);
17275:                     break;
17275:                 }
17275:                 case LIR_qlo:
17275:                 {
20893:                     countlir_qlo();
20921:                     asm_qlo(ins);
17275:                     break;
17275:                 }
17275:                 case LIR_qhi:
17275:                 {
20893:                     countlir_qhi();
20921:                     asm_qhi(ins);
17275:                     break;
17275:                 }
18254:                 case LIR_qcmov:
17275:                 case LIR_cmov:
17275:                 {
20893:                     countlir_cmov();
20921:                     asm_cmov(ins);
17275:                     break;
17275:                 }
17275:                 case LIR_ld:
17275:                 case LIR_ldc:
17275:                 case LIR_ldcb:
21469:                 case LIR_ldcs:
17275:                 {
20893:                     countlir_ld();
20921:                     asm_ld(ins);
17275:                     break;
17275:                 }
17275:                 case LIR_ldq:
20893:                 case LIR_ldqc:
17275:                 {
20893:                     countlir_ldq();
17275:                     asm_load64(ins);
17275:                     break;
17275:                 }
17275:                 case LIR_neg:
17275:                 case LIR_not:
17275:                 {
20893:                     countlir_alu();
20921:                     asm_neg_not(ins);
17275:                     break;
17275:                 }
17275:                 case LIR_qjoin:
17275:                 {
20893:                     countlir_qjoin();
17275:                     asm_qjoin(ins);
17275:                     break;
17275:                 }
17275: 
18220: #if defined NANOJIT_64BIT
18220:                 case LIR_qiadd:
18220:                 case LIR_qiand:
18220:                 case LIR_qilsh:
18645:                 case LIR_qior:
18220:                 {
18220:                     asm_qbinop(ins);
18220:                     break;
18220:                 }
18220: #endif
18220: 
17275:                 case LIR_add:
30437:                 case LIR_iaddp:
17275:                 case LIR_sub:
17275:                 case LIR_mul:
17275:                 case LIR_and:
17275:                 case LIR_or:
17275:                 case LIR_xor:
17275:                 case LIR_lsh:
17275:                 case LIR_rsh:
17275:                 case LIR_ush:
29354:                 case LIR_div:
29354:                 case LIR_mod:
17275:                 {
20893:                     countlir_alu();
20921:                     asm_arith(ins);
17275:                     break;
17275:                 }
17275:                 case LIR_fneg:
17275:                 {
20893:                     countlir_fpu();
17378:                     asm_fneg(ins);
17275:                     break;
17275:                 }
17275:                 case LIR_fadd:
17275:                 case LIR_fsub:
17275:                 case LIR_fmul:
17275:                 case LIR_fdiv:
17275:                 {
20893:                     countlir_fpu();
17378:                     asm_fop(ins);
17275:                     break;
17275:                 }
17275:                 case LIR_i2f:
17275:                 {
20893:                     countlir_fpu();
17378:                     asm_i2f(ins);
17275:                     break;
17275:                 }
17275:                 case LIR_u2f:
17275:                 {
20893:                     countlir_fpu();
17378:                     asm_u2f(ins);
17275:                     break;
17275:                 }
17275:                 case LIR_sti:
17275:                 {
20893:                     countlir_st();
30238:                     asm_store32(ins->oprnd1(), ins->disp(), ins->oprnd2());
17308:                     break;
17308:                 }
17308:                 case LIR_stqi:
17308:                 {
20893:                     countlir_stq();
17275:                     LIns* value = ins->oprnd1();
17275:                     LIns* base = ins->oprnd2();
30238:                     int dr = ins->disp();
20893:                     if (value->isop(LIR_qjoin))
20893:                     {
17308:                         // this is correct for little-endian only
17308:                         asm_store32(value->oprnd1(), dr, base);
17308:                         asm_store32(value->oprnd2(), dr+4, base);
17308:                     }
20893:                     else
20893:                     {
17275:                         asm_store64(value, dr, base);
17308:                     }
17275:                     break;
17275:                 }
20893: 
20893:                 case LIR_j:
20893:                 {
20893:                     countlir_jmp();
20893:                     LInsp to = ins->getTarget();
30253:                     LabelState *label = labels.get(to);
20893:                     // the jump is always taken so whatever register state we
20893:                     // have from downstream code, is irrelevant to code before
20893:                     // this jump.  so clear it out.  we will pick up register
20893:                     // state from the jump target, if we have seen that label.
20893:                     releaseRegisters();
20893:                     if (label && label->addr) {
20893:                         // forward jump - pick up register state from target.
20893:                         unionRegisterState(label->regs);
20893:                         JMP(label->addr);
20893:                     }
20893:                     else {
20893:                         // backwards jump
20893:                         hasLoop = true;
30253:                         handleLoopCarriedExprs(pending_lives);
20893:                         if (!label) {
20893:                             // save empty register state at loop header
30253:                             labels.add(to, 0, _allocator);
20893:                         }
20893:                         else {
20893:                             intersectRegisterState(label->regs);
20893:                         }
20893:                         JMP(0);
30253:                         patches.put(_nIns, to);
20893:                     }
20893:                     break;
20893:                 }
20893: 
20893:                 case LIR_jt:
20893:                 case LIR_jf:
20893:                 {
20893:                     countlir_jcc();
20893:                     LInsp to = ins->getTarget();
20893:                     LIns* cond = ins->oprnd1();
30253:                     LabelState *label = labels.get(to);
20893:                     if (label && label->addr) {
20893:                         // forward jump to known label.  need to merge with label's register state.
20893:                         unionRegisterState(label->regs);
30730:                         asm_branch(op == LIR_jf, cond, label->addr);
20893:                     }
20893:                     else {
20893:                         // back edge.
20893:                         hasLoop = true;
30253:                         handleLoopCarriedExprs(pending_lives);
20893:                         if (!label) {
20893:                             // evict all registers, most conservative approach.
20893:                             evictRegs(~_allocator.free);
30253:                             labels.add(to, 0, _allocator);
20893:                         }
20893:                         else {
20893:                             // evict all registers, most conservative approach.
20893:                             intersectRegisterState(label->regs);
20893:                         }
30730:                         NIns *branch = asm_branch(op == LIR_jf, cond, 0);
30253:                         patches.put(branch,to);
20893:                     }
20893:                     break;
20893:                 }
20893:                 case LIR_label:
20893:                 {
20893:                     countlir_label();
30253:                     LabelState *label = labels.get(ins);
20893:                     if (!label) {
20893:                         // label seen first, normal target of forward jump, save addr & allocator
30253:                         labels.add(ins, _nIns, _allocator);
20893:                     }
20893:                     else {
20893:                         // we're at the top of a loop
20893:                         hasLoop = true;
20893:                         NanoAssert(label->addr == 0 && label->regs.isValid());
20893:                         //evictRegs(~_allocator.free);
20893:                         intersectRegisterState(label->regs);
20893:                         label->addr = _nIns;
20893:                     }
29883:                     verbose_only( if (_logc->lcbits & LC_Assembly) { outputAddr=true; asm_output("[%s]", _thisfrag->lirbuf->names->formatRef(ins)); } )
20893:                     break;
20893:                 }
24289:                 case LIR_xbarrier: {
24289:                     break;
24289:                 }
25099: #ifdef NANOJIT_IA32
25099:                 case LIR_xtbl: {
25099:                     NIns* exit = asm_exit(ins); // does intersectRegisterState()
25099:                     asm_switch(ins, exit);
25099:                     break;
25099:                 }
25099: #else
25099:                  case LIR_xtbl:
25099:                     NanoAssertMsg(0, "Not supported for this architecture");
25099:                     break;
25099: #endif
17275:                 case LIR_xt:
17275:                 case LIR_xf:
17275:                 {
20893:                     countlir_xcc();
17275:                     // we only support cmp with guard right now, also assume it is 'close' and only emit the branch
20893:                     NIns* exit = asm_exit(ins); // does intersectRegisterState()
17275:                     LIns* cond = ins->oprnd1();
30730:                     asm_branch(op == LIR_xf, cond, exit);
17275:                     break;
17275:                 }
17275:                 case LIR_x:
17275:                 {
20893:                     countlir_x();
29883:                     verbose_only( if (_logc->lcbits & LC_Assembly)
29883:                                       asm_output("FIXME-whats-this?\n"); )
17275:                     // generate the side exit branch on the main trace.
17308:                     NIns *exit = asm_exit(ins);
17275:                     JMP( exit );
17275:                     break;
17275:                 }
17275:                 case LIR_loop:
17275:                 {
20893:                     countlir_loop();
20921:                     asm_loop(ins, loopJumps);
21477:                     assignSavedRegs();
21477:                     assignParamRegs();
17275:                     break;
17275:                 }
20921: 
17308:                 case LIR_feq:
17308:                 case LIR_fle:
17308:                 case LIR_flt:
17308:                 case LIR_fgt:
17308:                 case LIR_fge:
17308:                 {
20893:                     countlir_fpu();
20921:                     asm_fcond(ins);
17308:                     break;
17308:                 }
17275:                 case LIR_eq:
17368:                 case LIR_ov:
17275:                 case LIR_le:
17275:                 case LIR_lt:
17275:                 case LIR_gt:
17275:                 case LIR_ge:
17275:                 case LIR_ult:
17275:                 case LIR_ule:
17275:                 case LIR_ugt:
17275:                 case LIR_uge:
17275:                 {
20893:                     countlir_alu();
20921:                     asm_cond(ins);
17275:                     break;
17275:                 }
17275: 
17275:                 case LIR_fcall:
29869: #if defined NANOJIT_64BIT
29869:                 case LIR_callh:
29869: #endif
17275:                 case LIR_call:
17275:                 {
20893:                     countlir_call();
17275:                     Register rr = UnknownReg;
20893:                     if ((op&LIR64))
17275:                     {
20893:                         // fcall or fcalli
20893:                         Reservation* rR = getresv(ins);
17378:                         rr = asm_prep_fcall(rR, ins);
17275:                     }
17275:                     else
17275:                     {
17275:                         rr = retRegs[0];
17275:                         prepResultReg(ins, rmask(rr));
17275:                     }
17275: 
17275:                     // do this after we've handled the call result, so we dont
17275:                     // force the call result to be spilled unnecessarily.
20893: 
20893:                     evictScratchRegs();
17275: 
17687:                     asm_call(ins);
17275:                 }
17275:             }
17275: 
21483:             if (error())
21483:                 return;
21483: 
17275:             // check that all is well (don't check in exit paths since its more complicated)
17275:             debug_only( pageValidate(); )
17275:             debug_only( resourceConsistencyCheck();  )
17275:         }
17275:     }
17275: 
25099:     /*
25099:      * Write a jump table for the given SwitchInfo and store the table
25099:      * address in the SwitchInfo. Every entry will initially point to
25099:      * target.
25099:      */
25099:     void Assembler::emitJumpTable(SwitchInfo* si, NIns* target)
25099:     {
25099:         underrunProtect(si->count * sizeof(NIns*) + 20);
30030:         _nIns = reinterpret_cast<NIns*>(uintptr_t(_nIns) & ~(sizeof(NIns*) - 1));
25099:         for (uint32_t i = 0; i < si->count; ++i) {
30461:             _nIns = (NIns*) (((uint8*) _nIns) - sizeof(NIns*));
30461:             *(NIns**) _nIns = target;
25099:         }
25099:         si->table = (NIns**) _nIns;
25099:     }
25099: 
21477:     void Assembler::assignSavedRegs()
17275:     {
20893:         // restore saved regs
20893:         releaseRegisters();
20893:         LirBuffer *b = _thisfrag->lirbuf;
20893:         for (int i=0, n = NumSavedRegs; i < n; i++) {
21477:             LIns *p = b->savedRegs[i];
20893:             if (p)
30026:                 findSpecificRegFor(p, savedRegs[p->paramArg()]);
20893:         }
20893:     }
18776: 
21477:     void Assembler::reserveSavedRegs()
20893:     {
20893:         LirBuffer *b = _thisfrag->lirbuf;
20893:         for (int i=0, n = NumSavedRegs; i < n; i++) {
21477:             LIns *p = b->savedRegs[i];
20893:             if (p)
20893:                 findMemFor(p);
20893:         }
20893:     }
18776: 
21477:     // restore parameter registers
21477:     void Assembler::assignParamRegs()
21477:     {
21477:         LInsp state = _thisfrag->lirbuf->state;
21477:         if (state)
30026:             findSpecificRegFor(state, argRegs[state->paramArg()]);
21477:         LInsp param1 = _thisfrag->lirbuf->param1;
21477:         if (param1)
30026:             findSpecificRegFor(param1, argRegs[param1->paramArg()]);
21477:     }
21477: 
30253:     void Assembler::handleLoopCarriedExprs(InsList& pending_lives)
20893:     {
20893:         // ensure that exprs spanning the loop are marked live at the end of the loop
21477:         reserveSavedRegs();
20893:         for (int i=0, n=pending_lives.size(); i < n; i++) {
20893:             findMemFor(pending_lives[i]);
20893:         }
30451:         /*
30451:          * TODO: I'm not positive, but I think the following line needs to be
30451:          * added, otherwise the pending_lives will build up and never get
30451:          * cleared.
30451:          */
30451:         pending_lives.clear();
20893:     }
20893: 
20893:     void Assembler::arFree(uint32_t idx)
20893:     {
20893:         AR &ar = _activation;
20893:         LIns *i = ar.entry[idx];
20893:         do {
20893:             ar.entry[idx] = 0;
20893:             idx--;
20893:         } while (ar.entry[idx] == i);
17275:     }
17275: 
17275: #ifdef NJ_VERBOSE
17275:     void Assembler::printActivationState()
17275:     {
17275:         bool verbose_activation = false;
17275:         if (!verbose_activation)
17275:             return;
17275: 
17275: #ifdef NANOJIT_ARM
20921:         // @todo Why is there here?!?  This routine should be indep. of platform
17275:         verbose_only(
29883:             if (_logc->lcbits & LC_Assembly) {
17275:                 char* s = &outline[0];
17275:                 memset(s, ' ', 51);  s[51] = '\0';
17275:                 s += strlen(s);
17275:                 sprintf(s, " SP ");
17275:                 s += strlen(s);
17275:                 for(uint32_t i=_activation.lowwatermark; i<_activation.tos;i++) {
17275:                     LInsp ins = _activation.entry[i];
17275:                     if (ins && ins !=_activation.entry[i+1]) {
17275:                         sprintf(s, "%d(%s) ", 4*i, _thisfrag->lirbuf->names->formatRef(ins));
17275:                         s += strlen(s);
17275:                     }
17275:                 }
17275:                 output(&outline[0]);
17275:             }
17275:         )
17275: #else
17275:         verbose_only(
17275:             char* s = &outline[0];
29883:             if (_logc->lcbits & LC_Assembly) {
17275:                 memset(s, ' ', 51);  s[51] = '\0';
17275:                 s += strlen(s);
17275:                 sprintf(s, " ebp ");
17275:                 s += strlen(s);
17275: 
17275:                 for(uint32_t i=_activation.lowwatermark; i<_activation.tos;i++) {
17275:                     LInsp ins = _activation.entry[i];
24107:                     if (ins) {
17275:                         sprintf(s, "%d(%s) ", -4*i,_thisfrag->lirbuf->names->formatRef(ins));
17275:                         s += strlen(s);
17275:                     }
17275:                 }
17275:                 output(&outline[0]);
17275:             }
17275:         )
17275: #endif
17275:     }
17275: #endif
17275: 
20893:     bool canfit(int32_t size, int32_t loc, AR &ar) {
20893:         for (int i=0; i < size; i++) {
20893:             if (ar.entry[loc+stack_direction(i)])
20893:                 return false;
20893:         }
20893:         return true;
20893:     }
20893: 
17275:     uint32_t Assembler::arReserve(LIns* l)
17275:     {
17275:         //verbose_only(printActivationState());
30437:         int32_t size = l->isop(LIR_ialloc) ? (l->size()>>2) : l->isQuad() ? 2 : sizeof(intptr_t)>>2;
20893:         AR &ar = _activation;
20893:         const int32_t tos = ar.tos;
20893:         int32_t start = ar.lowwatermark;
17275:         int32_t i = 0;
17275:         NanoAssert(start>0);
20893: 
20893:         if (size == 1) {
20893:             // easy most common case -- find a hole, or make the frame bigger
20893:             for (i=start; i < NJ_MAX_STACK_ENTRY; i++) {
20893:                 if (ar.entry[i] == 0) {
20893:                     // found a hole
20893:                     ar.entry[i] = l;
20893:                     break;
20893:                 }
20893:             }
20893:         }
20893:         else if (size == 2) {
20893:             if ( (start&1)==1 ) start++;  // even 8 boundary
20893:             for (i=start; i < NJ_MAX_STACK_ENTRY; i+=2) {
20893:                 if ( (ar.entry[i+stack_direction(1)] == 0) && (i==tos || (ar.entry[i] == 0)) ) {
20893:                     // found 2 adjacent aligned slots
29861:                     NanoAssert(ar.entry[i] == 0);
29861:                     NanoAssert(ar.entry[i+stack_direction(1)] == 0);
20893:                     ar.entry[i] = l;
20893:                     ar.entry[i+stack_direction(1)] = l;
20893:                     break;
20893:                 }
20893:             }
20893:         }
20893:         else {
20893:             // alloc larger block on 8byte boundary.
20893:             if (start < size) start = size;
20893:             if ((start&1)==1) start++;
20893:             for (i=start; i < NJ_MAX_STACK_ENTRY; i+=2) {
20893:                 if (canfit(size, i, ar)) {
20893:                     // place the entry in the table and mark the instruction with it
20893:                     for (int32_t j=0; j < size; j++) {
29861:                         NanoAssert(ar.entry[i+stack_direction(j)] == 0);
29861:                         ar.entry[i+stack_direction(j)] = l;
20893:                     }
20893:                     break;
20893:                 }
20893:             }
20893:         }
20893:         if (i >= (int32_t)ar.tos) {
20893:             ar.tos = ar.highwatermark = i+1;
20893:         }
20893:         if (tos+size >= NJ_MAX_STACK_ENTRY) {
17275:             setError(StackFull);
17275:         }
17275:         return i;
17275:     }
17275: 
20893:     /**
20893:      * move regs around so the SavedRegs contains the highest priority regs.
20893:      */
20893:     void Assembler::evictScratchRegs()
20893:     {
20893:         // find the top GpRegs that are candidates to put in SavedRegs
20893: 
20893:         // tosave is a binary heap stored in an array.  the root is tosave[0],
20893:         // left child is at i+1, right child is at i+2.
20893: 
20893:         Register tosave[LastReg-FirstReg+1];
20893:         int len=0;
20893:         RegAlloc *regs = &_allocator;
20893:         for (Register r = FirstReg; r <= LastReg; r = nextreg(r)) {
20893:             if (rmask(r) & GpRegs) {
20893:                 LIns *i = regs->getActive(r);
20893:                 if (i) {
20893:                     if (canRemat(i)) {
20893:                         evict(r);
20893:                     }
20893:                     else {
20893:                         int32_t pri = regs->getPriority(r);
20893:                         // add to heap by adding to end and bubbling up
20893:                         int j = len++;
20893:                         while (j > 0 && pri > regs->getPriority(tosave[j/2])) {
20893:                             tosave[j] = tosave[j/2];
20893:                             j /= 2;
20893:                         }
20893:                         NanoAssert(size_t(j) < sizeof(tosave)/sizeof(tosave[0]));
20893:                         tosave[j] = r;
20893:                     }
20893:                 }
20893:             }
20893:         }
20893: 
20893:         // now primap has the live exprs in priority order.
20893:         // allocate each of the top priority exprs to a SavedReg
20893: 
20893:         RegisterMask allow = SavedRegs;
20893:         while (allow && len > 0) {
20893:             // get the highest priority var
20893:             Register hi = tosave[0];
22659:             if (!(rmask(hi) & SavedRegs)) {
20893:                 LIns *i = regs->getActive(hi);
20893:                 Register r = findRegFor(i, allow);
20893:                 allow &= ~rmask(r);
22659:             }
22659:             else {
22659:                 // hi is already in a saved reg, leave it alone.
22659:                 allow &= ~rmask(hi);
22659:             }
20893: 
20893:             // remove from heap by replacing root with end element and bubbling down.
20893:             if (allow && --len > 0) {
20893:                 Register last = tosave[len];
20893:                 int j = 0;
20893:                 while (j+1 < len) {
20893:                     int child = j+1;
20893:                     if (j+2 < len && regs->getPriority(tosave[j+2]) > regs->getPriority(tosave[j+1]))
20893:                         child++;
20893:                     if (regs->getPriority(last) > regs->getPriority(tosave[child]))
20893:                         break;
20893:                     tosave[j] = tosave[child];
20893:                     j = child;
20893:                 }
20893:                 tosave[j] = last;
20893:             }
20893:         }
20893: 
20893:         // now evict everything else.
20893:         evictRegs(~SavedRegs);
20893:     }
20893: 
20893:     void Assembler::evictRegs(RegisterMask regs)
17275:     {
17275:         // generate code to restore callee saved registers
17275:         // @todo speed this up
20893:         for (Register r = FirstReg; r <= LastReg; r = nextreg(r)) {
20893:             if ((rmask(r) & regs) && _allocator.getActive(r)) {
17275:                 evict(r);
17275:             }
17275:         }
17275:     }
17275: 
17275:     /**
17275:      * Merge the current state of the registers with a previously stored version
20893:      * current == saved    skip
20893:      * current & saved     evict current, keep saved
20893:      * current & !saved    evict current  (unionRegisterState would keep)
20893:      * !current & saved    keep saved
17275:      */
20893:     void Assembler::intersectRegisterState(RegAlloc& saved)
17275:     {
17275:         // evictions and pops first
17275:         RegisterMask skip = 0;
22648:         verbose_only(bool shouldMention=false; )
17275:         for (Register r=FirstReg; r <= LastReg; r = nextreg(r))
17275:         {
17275:             LIns * curins = _allocator.getActive(r);
17275:             LIns * savedins = saved.getActive(r);
17275:             if (curins == savedins)
17275:             {
22648:                 //verbose_only( if (curins) verbose_outputf("                                              skip %s", regNames[r]); )
17275:                 skip |= rmask(r);
17275:             }
17275:             else
17275:             {
20893:                 if (curins) {
20893:                     //_nvprof("intersect-evict",1);
22648:                     verbose_only( shouldMention=true; )
17275:                     evict(r);
20893:                 }
17275: 
17275:                 #ifdef NANOJIT_IA32
22648:                 if (savedins && (rmask(r) & x87Regs)) {
22648:                     verbose_only( shouldMention=true; )
17275:                     FSTP(r);
22648:                 }
17275:                 #endif
17275:             }
17275:         }
20893:         assignSaved(saved, skip);
29883:         verbose_only(
29883:             if (shouldMention)
29883:                 verbose_outputf("## merging registers (intersect) "
29883:                                 "with existing edge");
29883:         )
20893:     }
17275: 
20893:     /**
20893:      * Merge the current state of the registers with a previously stored version.
20893:      *
20893:      * current == saved    skip
20893:      * current & saved     evict current, keep saved
20893:      * current & !saved    keep current (intersectRegisterState would evict)
20893:      * !current & saved    keep saved
20893:      */
20893:     void Assembler::unionRegisterState(RegAlloc& saved)
20893:     {
20893:         // evictions and pops first
22648:         verbose_only(bool shouldMention=false; )
20893:         RegisterMask skip = 0;
20893:         for (Register r=FirstReg; r <= LastReg; r = nextreg(r))
20893:         {
20893:             LIns * curins = _allocator.getActive(r);
20893:             LIns * savedins = saved.getActive(r);
20893:             if (curins == savedins)
20893:             {
22648:                 //verbose_only( if (curins) verbose_outputf("                                              skip %s", regNames[r]); )
20893:                 skip |= rmask(r);
20893:             }
20893:             else
20893:             {
20893:                 if (curins && savedins) {
20893:                     //_nvprof("union-evict",1);
22648:                     verbose_only( shouldMention=true; )
20893:                     evict(r);
20893:                 }
20893: 
20893:                 #ifdef NANOJIT_IA32
20893:                 if (rmask(r) & x87Regs) {
20893:                     if (savedins) {
20893:                         FSTP(r);
20893:                     }
20893:                     else {
20893:                         // saved state did not have fpu reg allocated,
20893:                         // so we must evict here to keep x87 stack balanced.
20893:                         evict(r);
20893:                     }
22648:                     verbose_only( shouldMention=true; )
20893:                 }
20893:                 #endif
20893:             }
20893:         }
20893:         assignSaved(saved, skip);
22648:         verbose_only( if (shouldMention) verbose_outputf("                                              merging registers (union) with existing edge");  )
20893:     }
20893: 
20893:     void Assembler::assignSaved(RegAlloc &saved, RegisterMask skip)
20893:     {
17275:         // now reassign mainline registers
17275:         for (Register r=FirstReg; r <= LastReg; r = nextreg(r))
17275:         {
17275:             LIns *i = saved.getActive(r);
17275:             if (i && !(skip&rmask(r)))
17275:                 findSpecificRegFor(i, r);
17275:         }
17275:         debug_only(saved.used = 0);  // marker that we are no longer in exit path
17275:     }
17275: 
28549:     // scan table for instruction with the lowest priority, meaning it is used
28549:     // furthest in the future.
28549:     LIns* Assembler::findVictim(RegAlloc &regs, RegisterMask allow)
28549:     {
28549:         NanoAssert(allow != 0);
28549:         LIns *i, *a=0;
28549:         int allow_pri = 0x7fffffff;
28549:         for (Register r=FirstReg; r <= LastReg; r = nextreg(r))
28549:         {
28549:             if ((allow & rmask(r)) && (i = regs.getActive(r)) != 0)
28549:             {
28549:                 int pri = canRemat(i) ? 0 : regs.getPriority(r);
28549:                 if (!a || pri < allow_pri) {
28549:                     a = i;
28549:                     allow_pri = pri;
28549:                 }
28549:             }
28549:         }
28549:         NanoAssert(a != 0);
28549:         return a;
28549:     }
28549: 
17275:     #ifdef NJ_VERBOSE
29861:         // "outline" must be able to hold the output line in addition to the
29861:         // outlineEOL buffer, which is concatenated onto outline just before it
29861:         // is printed.
17275:         char Assembler::outline[8192];
22648:         char Assembler::outlineEOL[512];
22648: 
22648:         void Assembler::outputForEOL(const char* format, ...)
22648:         {
22648:             va_list args;
22648:             va_start(args, format);
22648:             outlineEOL[0] = '\0';
22648:             vsprintf(outlineEOL, format, args);
22648:         }
17275: 
17275:         void Assembler::outputf(const char* format, ...)
17275:         {
17275:             va_list     args;
17275:             va_start(args, format);
17275:             outline[0] = '\0';
29861: 
29861:             // Format the output string and remember the number of characters
29861:             // that were written.
29861:             uint32_t outline_len = vsprintf(outline, format, args);
29861: 
29861:             // Add the EOL string to the output, ensuring that we leave enough
29861:             // space for the terminating NULL character, then reset it so it
29861:             // doesn't repeat on the next outputf.
29861:             strncat(outline, outlineEOL, sizeof(outline)-(outline_len+1));
29861:             outlineEOL[0] = '\0';
29861: 
17275:             output(outline);
17275:         }
17275: 
17275:         void Assembler::output(const char* s)
17275:         {
17275:             if (_outputCache)
17275:             {
17275:                 char* str = (char*)_gc->Alloc(strlen(s)+1);
17275:                 strcpy(str, s);
17275:                 _outputCache->add(str);
17275:             }
17275:             else
17275:             {
29883:                 _logc->printf("%s\n", s);
17275:             }
17275:         }
17275: 
17275:         void Assembler::output_asm(const char* s)
17275:         {
29883:             if (!(_logc->lcbits & LC_Assembly))
17275:                 return;
29861: 
29861:             // Add the EOL string to the output, ensuring that we leave enough
29861:             // space for the terminating NULL character, then reset it so it
29861:             // doesn't repeat on the next outputf.
29861:             strncat(outline, outlineEOL, sizeof(outline)-(strlen(outline)+1));
29861:             outlineEOL[0] = '\0';
29861: 
17275:             output(s);
17275:         }
17275: 
17275:         char* Assembler::outputAlign(char *s, int col)
17275:         {
17275:             int len = strlen(s);
17275:             int add = ((col-len)>0) ? col-len : 1;
17275:             memset(&s[len], ' ', add);
17275:             s[col] = '\0';
17275:             return &s[col];
17275:         }
17275:     #endif // verbose
17275: 
17275:     #endif /* FEATURE_NANOJIT */
17275: 
17275: #if defined(FEATURE_NANOJIT) || defined(NJ_VERBOSE)
17275:     uint32_t CallInfo::_count_args(uint32_t mask) const
17275:     {
17275:         uint32_t argc = 0;
17275:         uint32_t argt = _argtypes;
20893:         for (uint32_t i = 0; i < MAXARGS; ++i) {
17275:             argt >>= 2;
22661:             if (!argt)
22661:                 break;
17275:             argc += (argt & mask) != 0;
17275:         }
17275:         return argc;
17275:     }
17687: 
17687:     uint32_t CallInfo::get_sizes(ArgSize* sizes) const
17687:     {
17687:         uint32_t argt = _argtypes;
17687:         uint32_t argc = 0;
20893:         for (uint32_t i = 0; i < MAXARGS; i++) {
17687:             argt >>= 2;
17687:             ArgSize a = ArgSize(argt&3);
17687:             if (a != ARGSIZE_NONE) {
17687:                 sizes[argc++] = a;
22661:             } else {
22661:                 break;
17275:             }
17687:         }
17687:         return argc;
17687:     }
20893: 
20893:     void LabelStateMap::add(LIns *label, NIns *addr, RegAlloc &regs) {
22647:         LabelState *st = NJ_NEW(gc, LabelState)(addr, regs);
20893:         labels.put(label, st);
17687:     }
20893: 
21503:     LabelStateMap::~LabelStateMap() {
21503:         LabelState *st;
21503: 
21503:         while (!labels.isEmpty()) {
21503:             st = labels.removeLast();
21503:             delete st;
21503:         }
21503:     }
21503: 
20893:     LabelState* LabelStateMap::get(LIns *label) {
20893:         return labels.get(label);
20893:     }
20893: }
20893: #endif // FEATURE_NANOJIT
