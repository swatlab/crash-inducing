29366: /* -*- Mode: C++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*-
29900:  * vim: set ts=4 sw=4 et tw=99 ft=cpp:
17186:  *
17186:  * ***** BEGIN LICENSE BLOCK *****
17186:  * Version: MPL 1.1/GPL 2.0/LGPL 2.1
17186:  *
17186:  * The contents of this file are subject to the Mozilla Public License Version
17186:  * 1.1 (the "License"); you may not use this file except in compliance with
17186:  * the License. You may obtain a copy of the License at
17186:  * http://www.mozilla.org/MPL/
17186:  *
17186:  * Software distributed under the License is distributed on an "AS IS" basis,
17186:  * WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
17186:  * for the specific language governing rights and limitations under the
17186:  * License.
17186:  *
17186:  * The Original Code is Mozilla SpiderMonkey JavaScript 1.9 code, released
17186:  * May 28, 2008.
17186:  *
17186:  * The Initial Developer of the Original Code is
17339:  *   Brendan Eich <brendan@mozilla.org>
17186:  *
17186:  * Contributor(s):
17339:  *   Andreas Gal <gal@mozilla.com>
17671:  *   Mike Shaver <shaver@mozilla.org>
17671:  *   David Anderson <danderson@mozilla.com>
17186:  *
17186:  * Alternatively, the contents of this file may be used under the terms of
17186:  * either of the GNU General Public License Version 2 or later (the "GPL"),
17186:  * or the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
17186:  * in which case the provisions of the GPL or the LGPL are applicable instead
17186:  * of those above. If you wish to allow use of your version of this file only
17186:  * under the terms of either the GPL or the LGPL, and not to allow others to
17186:  * use your version of this file under the terms of the MPL, indicate your
17186:  * decision by deleting the provisions above and replace them with the notice
17186:  * and other provisions required by the GPL or the LGPL. If you do not delete
17186:  * the provisions above, a recipient may use your version of this file under
17186:  * the terms of any one of the MPL, the GPL or the LGPL.
17186:  *
17186:  * ***** END LICENSE BLOCK ***** */
17186: 
17196: #ifndef jstracer_h___
17196: #define jstracer_h___
17186: 
19171: #ifdef JS_TRACER
18091: 
32777: #include "jstypes.h"
32777: #include "jsbuiltins.h"
21521: #include "jscntxt.h"
33560: #include "jsdhash.h"
32777: #include "jsinterp.h"
17186: #include "jslock.h"
17393: #include "jsnum.h"
33930: #include "jsvector.h"
17434: 
37741: namespace js {
37741: 
21459: #if defined(DEBUG) && !defined(JS_JIT_SPEW)
21459: #define JS_JIT_SPEW
21459: #endif
21459: 
17886: template <typename T>
31920: class Queue {
17886:     T* _data;
17886:     unsigned _len;
17886:     unsigned _max;
31920:     nanojit::Allocator* alloc;
17891: 
31913: public:
17891:     void ensure(unsigned size) {
36398:         if (_max > size)
36398:             return;
35361:         if (!_max)
36398:             _max = 8;
36398:         _max = JS_MAX(_max * 2, size);
31920:         if (alloc) {
31920:             T* tmp = new (*alloc) T[_max];
31920:             memcpy(tmp, _data, _len * sizeof(T));
31920:             _data = tmp;
31920:         } else {
17891:             _data = (T*)realloc(_data, _max * sizeof(T));
31920:         }
24246: #if defined(DEBUG)
24246:         memset(&_data[_len], 0xcd, _max - _len);
24246: #endif
17891:     }
31913: 
32616:     Queue(nanojit::Allocator* alloc)
31920:         : alloc(alloc)
31920:     {
35361:         this->_max =
17886:         this->_len = 0;
35361:         this->_data = NULL;
17886:     }
17886: 
17886:     ~Queue() {
35361:         if (!alloc)
17886:             free(_data);
17886:     }
17886: 
17973:     bool contains(T a) {
23709:         for (unsigned n = 0; n < _len; ++n) {
17973:             if (_data[n] == a)
17973:                 return true;
23709:         }
17973:         return false;
17973:     }
17973: 
17886:     void add(T a) {
18621:         ensure(_len + 1);
17886:         JS_ASSERT(_len <= _max);
17891:         _data[_len++] = a;
17886:     }
17891: 
18621:     void add(T* chunk, unsigned size) {
18621:         ensure(_len + size);
18621:         JS_ASSERT(_len <= _max);
18621:         memcpy(&_data[_len], chunk, size * sizeof(T));
18621:         _len += size;
18621:     }
18621: 
18051:     void addUnique(T a) {
18051:         if (!contains(a))
18051:             add(a);
18051:     }
18051: 
17891:     void setLength(unsigned len) {
17891:         ensure(len + 1);
17891:         _len = len;
17886:     }
17886: 
17886:     void clear() {
17886:         _len = 0;
17886:     }
17886: 
31473:     T & get(unsigned i) {
31473:         JS_ASSERT(i < length());
31473:         return _data[i];
31473:     }
31473: 
22609:     const T & get(unsigned i) const {
29900:         JS_ASSERT(i < length());
22609:         return _data[i];
22609:     }
22609: 
31473:     T & operator [](unsigned i) {
31473:         return get(i);
31473:     }
31473: 
29900:     const T & operator [](unsigned i) const {
29900:         return get(i);
29900:     }
29900: 
17886:     unsigned length() const {
17886:         return _len;
17886:     }
17886: 
17886:     T* data() const {
17886:         return _data;
17886:     }
36401: 
36401:     int offsetOf(T slot) {
36401:         T* p = _data;
36401:         unsigned n = 0;
36401:         for (n = 0; n < _len; ++n)
36401:             if (*p++ == slot)
36401:                 return n;
36401:         return -1;
36401:     }
36401: 
17886: };
17886: 
17434: /*
17487:  * Tracker is used to keep track of values being manipulated by the interpreter
33550:  * during trace recording.  It maps opaque, 4-byte aligned address to LIns pointers.
33550:  * pointers. To do this efficiently, we observe that the addresses of jsvals
33550:  * living in the interpreter tend to be aggregated close to each other -
33550:  * usually on the same page (where a tracker page doesn't have to be the same
33550:  * size as the OS page size, but it's typically similar).  The Tracker
33550:  * consists of a linked-list of structures representing a memory page, which
33550:  * are created on-demand as memory locations are used.
33550:  *
33550:  * For every address, first we split it into two parts: upper bits which
33550:  * represent the "base", and lower bits which represent an offset against the
33550:  * base.  For the offset, we then right-shift it by two because the bottom two
33550:  * bits of a 4-byte aligned address are always zero.  The mapping then
33550:  * becomes:
33550:  *
33550:  *   page = page in pagelist such that Base(address) == page->base,
33550:  *   page->map[Offset(address)]
17293:  */
17487: class Tracker {
33550:     #define TRACKER_PAGE_SZB        4096
33550:     #define TRACKER_PAGE_ENTRIES    (TRACKER_PAGE_SZB >> 2)    // each slot is 4 bytes
33550:     #define TRACKER_PAGE_MASK       jsuword(TRACKER_PAGE_SZB - 1)
33550: 
33177:     struct TrackerPage {
33177:         struct TrackerPage* next;
17424:         jsuword             base;
33550:         nanojit::LIns*      map[TRACKER_PAGE_ENTRIES];
17293:     };
33177:     struct TrackerPage* pagelist;
17293: 
33177:     jsuword             getTrackerPageBase(const void* v) const;
33550:     jsuword             getTrackerPageOffset(const void* v) const;
33177:     struct TrackerPage* findTrackerPage(const void* v) const;
33177:     struct TrackerPage* addTrackerPage(const void* v);
17293: public:
17293:     Tracker();
17293:     ~Tracker();
17293: 
17773:     bool            has(const void* v) const;
17596:     nanojit::LIns*  get(const void* v) const;
17596:     void            set(const void* v, nanojit::LIns* ins);
17293:     void            clear();
17293: };
17293: 
35044: class VMFragment : public nanojit::Fragment {
35044: public:
35044:     VMFragment(const void* _ip verbose_only(, uint32_t profFragID))
35044:       : Fragment(_ip verbose_only(, profFragID))
35044:     {}
35044: 
35044:     /*
35044:      * If this is anchored off a TreeFragment, this points to that tree fragment.
35044:      * Otherwise, it is |this|.
35044:      */
35044:     TreeFragment* root;
35083: 
35083:     TreeFragment* toTreeFragment();
35044: };
35044: 
33951: #if defined(JS_JIT_SPEW) || defined(NJ_NO_VARIADIC_MACROS)
29883: 
29883: enum LC_TMBits {
30860:     /*
30860:      * Output control bits for all non-Nanojit code.  Only use bits 16 and
30860:      * above, since Nanojit uses 0 .. 15 itself.
30860:      */
29883:     LC_TMMinimal  = 1<<16,
29883:     LC_TMTracer   = 1<<17,
29883:     LC_TMRecorder = 1<<18,
32636:     LC_TMAbort    = 1<<19,
32636:     LC_TMStats    = 1<<20,
32636:     LC_TMRegexp   = 1<<21,
32636:     LC_TMTreeVis  = 1<<22
29883: };
29883: 
30342: #endif
30342: 
33951: #ifdef NJ_NO_VARIADIC_MACROS
30342: 
30342: #define debug_only_stmt(action)            /* */
30342: static void debug_only_printf(int mask, const char *fmt, ...) {}
30342: #define debug_only_print0(mask, str)       /* */
30342: 
30342: #elif defined(JS_JIT_SPEW)
30342: 
29883: // Top level logging controller object.
37741: extern nanojit::LogControl LogController;
29883: 
32784: // Top level profiling hook, needed to harvest profile info from Fragments
32784: // whose logical lifetime is about to finish
37741: extern void FragProfiling_FragFinalizer(nanojit::Fragment* f, TraceMonitor*);
32784: 
29883: #define debug_only_stmt(stmt) \
29883:     stmt
30860: 
29883: #define debug_only_printf(mask, fmt, ...)                                      \
30860:     JS_BEGIN_MACRO                                                             \
37741:         if ((LogController.lcbits & (mask)) > 0) {                             \
37741:             LogController.printf(fmt, __VA_ARGS__);                            \
30860:             fflush(stdout);                                                    \
30860:         }                                                                      \
30860:     JS_END_MACRO
30860: 
29883: #define debug_only_print0(mask, str)                                           \
30860:     JS_BEGIN_MACRO                                                             \
37741:         if ((LogController.lcbits & (mask)) > 0) {                             \
37741:             LogController.printf("%s", str);                                   \
30860:             fflush(stdout);                                                    \
30860:         }                                                                      \
30860:     JS_END_MACRO
29883: 
23450: #else
29883: 
29883: #define debug_only_stmt(action)            /* */
29883: #define debug_only_printf(mask, fmt, ...)  /* */
29883: #define debug_only_print0(mask, str)       /* */
29883: 
23450: #endif
23450: 
17981: /*
24290:  * The oracle keeps track of hit counts for program counter locations, as
24290:  * well as slots that should not be demoted to int because we know them to
24290:  * overflow or they result in type-unstable traces. We are using simple
24290:  * hash tables.  Collisions lead to loss of optimization (demotable slots
24290:  * are not demoted, etc.) but have no correctness implications.
17981:  */
17981: #define ORACLE_SIZE 4096
17981: 
17981: class Oracle {
22613:     avmplus::BitSet _stackDontDemote;
22613:     avmplus::BitSet _globalDontDemote;
29354:     avmplus::BitSet _pcDontDemote;
17981: public:
24290:     Oracle();
24290: 
23456:     JS_REQUIRES_STACK void markGlobalSlotUndemotable(JSContext* cx, unsigned slot);
23456:     JS_REQUIRES_STACK bool isGlobalSlotUndemotable(JSContext* cx, unsigned slot) const;
23456:     JS_REQUIRES_STACK void markStackSlotUndemotable(JSContext* cx, unsigned slot);
33564:     JS_REQUIRES_STACK void markStackSlotUndemotable(JSContext* cx, unsigned slot, const void* pc);
23456:     JS_REQUIRES_STACK bool isStackSlotUndemotable(JSContext* cx, unsigned slot) const;
33564:     JS_REQUIRES_STACK bool isStackSlotUndemotable(JSContext* cx, unsigned slot, const void* pc) const;
29354:     void markInstructionUndemotable(jsbytecode* pc);
29354:     bool isInstructionUndemotable(jsbytecode* pc) const;
29354: 
24290:     void clearDemotability();
24290:     void clear() {
24290:         clearDemotability();
24290:     }
17981: };
17981: 
30445: #if defined(_MSC_VER) && _MSC_VER >= 1400 || (defined(__GNUC__) && __GNUC__ >= 4)
29896: #define USE_TRACE_TYPE_ENUM
29896: #endif
29896: 
29896: /*
29896:  * The types of values calculated during tracing, used to specialize operations
29896:  * to the types of those values.  These loosely correspond to the values of the
29896:  * JSVAL_* language types, but we add a few further divisions to enable further
29896:  * optimization at execution time.  Do not rely on this loose correspondence for
29896:  * correctness without adding static assertions!
29896:  *
29896:  * The ifdefs enforce that this enum occupies only one byte of memory, where
29896:  * possible.  If it doesn't, type maps will occupy more space but should
29896:  * otherwise work correctly.  A static assertion in jstracer.cpp verifies that
29896:  * this requirement is correctly enforced by these compilers.
29896:  */
37741: enum TraceType_
30342: #if defined(_MSC_VER) && _MSC_VER >= 1400
29896: : int8_t
29896: #endif
29896: {
29896:     TT_OBJECT         = 0, /* pointer to JSObject whose class is not js_FunctionClass */
29896:     TT_INT32          = 1, /* 32-bit signed integer */
29896:     TT_DOUBLE         = 2, /* pointer to jsdouble */
29896:     TT_JSVAL          = 3, /* arbitrary jsval */
29896:     TT_STRING         = 4, /* pointer to JSString */
29896:     TT_NULL           = 5, /* null */
29896:     TT_PSEUDOBOOLEAN  = 6, /* true, false, or undefined (0, 1, or 2) */
36401:     TT_FUNCTION       = 7, /* pointer to JSObject whose class is js_FunctionClass */
36401:     TT_IGNORE         = 8
29896: }
30440: #if defined(__GNUC__) && defined(USE_TRACE_TYPE_ENUM)
29896: __attribute__((packed))
29896: #endif
29896: ;
29896: 
29896: #ifdef USE_TRACE_TYPE_ENUM
37741: typedef TraceType_ TraceType;
29896: #else
37741: typedef int8_t TraceType;
29896: #endif
29896: 
30647: /*
30647:  * This indicates an invalid type or error. Note that it should not be used in typemaps,
30647:  * because it is the wrong size. It can only be used as a uint32, for example as the
30647:  * return value from a function that returns a type as a uint32.
30647:  */
30647: const uint32 TT_INVALID = uint32(-1);
29896: 
17992: typedef Queue<uint16> SlotList;
17992: 
37741: class TypeMap : public Queue<TraceType> {
17981: public:
37741:     TypeMap(nanojit::Allocator* alloc) : Queue<TraceType>(alloc) {}
36401:     void set(unsigned stackSlots, unsigned ngslots,
37741:              const TraceType* stackTypeMap, const TraceType* globalTypeMap);
29880:     JS_REQUIRES_STACK void captureTypes(JSContext* cx, JSObject* globalObj, SlotList& slots, unsigned callDepth);
29880:     JS_REQUIRES_STACK void captureMissingGlobalTypes(JSContext* cx, JSObject* globalObj, SlotList& slots,
24246:                                                      unsigned stackSlots);
18239:     bool matches(TypeMap& other) const;
37741:     void fromRaw(TraceType* other, unsigned numSlots);
17981: };
17981: 
29893: #define JS_TM_EXITCODES(_)    \
29893:     /*                                                                          \
29893:      * An exit at a possible branch-point in the trace at which to attach a     \
29893:      * future secondary trace. Therefore the recorder must generate different   \
29893:      * code to handle the other outcome of the branch condition from the        \
29893:      * primary trace's outcome.                                                 \
29893:      */                                                                         \
29893:     _(BRANCH)                                                                   \
29893:     /*                                                                          \
29893:      * Exit at a tableswitch via a numbered case.                               \
29893:      */                                                                         \
29893:     _(CASE)                                                                     \
29893:     /*                                                                          \
29893:      * Exit at a tableswitch via the default case.                              \
29893:      */                                                                         \
29893:     _(DEFAULT)                                                                  \
29893:     _(LOOP)                                                                     \
29893:     _(NESTED)                                                                   \
29893:     /*                                                                          \
29893:      * An exit from a trace because a condition relied upon at recording time   \
29893:      * no longer holds, where the alternate path of execution is so rare or     \
29893:      * difficult to address in native code that it is not traced at all, e.g.   \
29893:      * negative array index accesses, which differ from positive indexes in     \
29893:      * that they require a string-based property lookup rather than a simple    \
29893:      * memory access.                                                           \
29893:      */                                                                         \
29893:     _(MISMATCH)                                                                 \
29893:     /*                                                                          \
29893:      * A specialization of MISMATCH_EXIT to handle allocation failures.         \
29893:      */                                                                         \
29893:     _(OOM)                                                                      \
29893:     _(OVERFLOW)                                                                 \
29893:     _(UNSTABLE_LOOP)                                                            \
29893:     _(TIMEOUT)                                                                  \
29893:     _(DEEP_BAIL)                                                                \
33564:     _(STATUS)                                                                   \
33564:     /* Exit is almost recursive and wants a peer at recursive_pc */             \
33564:     _(RECURSIVE_UNLINKED)                                                       \
33564:     /* Exit is recursive, and there are no more frames */                       \
33564:     _(RECURSIVE_LOOP)                                                           \
33564:     /* Exit is recursive, but type-mismatched guarding on a down frame */       \
33564:     _(RECURSIVE_MISMATCH)                                                       \
33564:     /* Exit is recursive, and the JIT wants to try slurping interp frames */    \
33564:     _(RECURSIVE_EMPTY_RP)                                                       \
33564:     /* Slurping interp frames in up-recursion failed */                         \
33564:     _(RECURSIVE_SLURP_FAIL)                                                     \
33564:     /* Tried to slurp an interp frame, but the pc or argc mismatched */         \
33564:     _(RECURSIVE_SLURP_MISMATCH)
29893: 
21521: enum ExitType {
29893:     #define MAKE_EXIT_CODE(x) x##_EXIT,
29893:     JS_TM_EXITCODES(MAKE_EXIT_CODE)
29893:     #undef MAKE_EXIT_CODE
29895:     TOTAL_EXIT_TYPES
21521: };
21521: 
33564: struct FrameInfo;
33564: 
21521: struct VMSideExit : public nanojit::SideExit
21521: {
22925:     JSObject* block;
25111:     jsbytecode* pc;
25111:     jsbytecode* imacpc;
21521:     intptr_t sp_adj;
21521:     intptr_t rp_adj;
21521:     int32_t calldepth;
21521:     uint32 numGlobalSlots;
21521:     uint32 numStackSlots;
21521:     uint32 numStackSlotsBelowCurrentFrame;
21521:     ExitType exitType;
30034:     uintN lookupFlags;
33564:     void* recursive_pc;
33564:     FrameInfo* recursive_down;
33564:     unsigned hitcount;
33564:     unsigned slurpFailSlot;
37741:     TraceType slurpType;
28086: 
28086:     /*
28086:      * Ordinarily 0.  If a slow native function is atop the stack, the 1 bit is
28086:      * set if constructing and the other bits are a pointer to the funobj.
28086:      */
28446:     uintptr_t nativeCalleeWord;
28086: 
28086:     JSObject * nativeCallee() {
28086:         return (JSObject *) (nativeCalleeWord & ~1);
28086:     }
28086: 
28086:     bool constructing() {
28086:         return bool(nativeCalleeWord & 1);
28086:     }
28086: 
28086:     void setNativeCallee(JSObject *callee, bool constructing) {
28446:         nativeCalleeWord = uintptr_t(callee) | (constructing ? 1 : 0);
28086:     }
31495: 
37741:     inline TraceType* stackTypeMap() {
37741:         return (TraceType*)(this + 1);
31495:     }
31495: 
37741:     inline TraceType& stackType(unsigned i) {
33564:         JS_ASSERT(i < numStackSlots);
33564:         return stackTypeMap()[i];
33564:     }
33564: 
37741:     inline TraceType* globalTypeMap() {
37741:         return (TraceType*)(this + 1) + this->numStackSlots;
31495:     }
31495: 
37741:     inline TraceType* fullTypeMap() {
31495:         return stackTypeMap();
31495:     }
31495: 
35044:     inline VMFragment* fromFrag() {
35044:         return (VMFragment*)from;
35044:     }
35044: 
35044:     inline TreeFragment* root() {
35044:         return fromFrag()->root;
31495:     }
21521: };
21521: 
31918: class VMAllocator : public nanojit::Allocator
31475: {
31475: 
31475: public:
31475:     VMAllocator() : mOutOfMemory(false), mSize(0)
31475:     {}
31475: 
31475:     size_t size() {
31475:         return mSize;
31475:     }
31475: 
31475:     bool outOfMemory() {
31475:         return mOutOfMemory;
31475:     }
31475: 
33545:     struct Mark
33545:     {
33545:         VMAllocator& vma;
33545:         bool committed;
33545:         nanojit::Allocator::Chunk* saved_chunk;
33545:         char* saved_top;
33545:         char* saved_limit;
33545:         size_t saved_size;
33545: 
33545:         Mark(VMAllocator& vma) :
33545:             vma(vma),
33545:             committed(false),
33545:             saved_chunk(vma.current_chunk),
33545:             saved_top(vma.current_top),
33545:             saved_limit(vma.current_limit),
33545:             saved_size(vma.mSize)
33545:         {}
33545: 
33545:         ~Mark()
33545:         {
33545:             if (!committed)
33545:                 vma.rewind(*this);
33545:         }
33545: 
33545:         void commit() { committed = true; }
33545:     };
33545: 
33545:     void rewind(const Mark& m) {
33545:         while (current_chunk != m.saved_chunk) {
33545:             Chunk *prev = current_chunk->prev;
33545:             freeChunk(current_chunk);
33545:             current_chunk = prev;
33545:         }
33545:         current_top = m.saved_top;
33545:         current_limit = m.saved_limit;
33545:         mSize = m.saved_size;
33545:         memset(current_top, 0, current_limit - current_top);
33545:     }
33545: 
31475:     bool mOutOfMemory;
31475:     size_t mSize;
31920: 
31475:     /*
31475:      * FIXME: Area the LIR spills into if we encounter an OOM mid-way
31475:      * through compilation; we must check mOutOfMemory before we run out
31475:      * of mReserve, otherwise we're in undefined territory. This area
31475:      * used to be one page, now 16 to be "safer". This is a temporary
31475:      * and quite unsatisfactory approach to handling OOM in Nanojit.
31475:      */
31475:     uintptr_t mReserve[0x10000];
31475: };
31475: 
31918: struct REHashKey {
31918:     size_t re_length;
31918:     uint16 re_flags;
31918:     const jschar* re_chars;
31918: 
31918:     REHashKey(size_t re_length, uint16 re_flags, const jschar *re_chars)
31918:         : re_length(re_length)
31918:         , re_flags(re_flags)
31918:         , re_chars(re_chars)
31918:     {}
31918: 
31918:     bool operator==(const REHashKey& other) const
31918:     {
31918:         return ((this->re_length == other.re_length) &&
31918:                 (this->re_flags == other.re_flags) &&
31918:                 !memcmp(this->re_chars, other.re_chars,
31918:                         this->re_length * sizeof(jschar)));
31918:     }
31918: };
31918: 
31918: struct REHashFn {
31918:     static size_t hash(const REHashKey& k) {
31918:         return
31918:             k.re_length +
31918:             k.re_flags +
31918:             nanojit::murmurhash(k.re_chars, k.re_length * sizeof(jschar));
31918:     }
31918: };
31918: 
24612: struct FrameInfo {
24612:     JSObject*       block;      // caller block chain head
25111:     jsbytecode*     pc;         // caller fp->regs->pc
25111:     jsbytecode*     imacpc;     // caller fp->imacpc
31924:     uint32          spdist;     // distance from fp->slots to fp->regs->sp at JSOP_CALL
28949: 
28949:     /*
28949:      * Bit  15 (0x8000) is a flag that is set if constructing (called through new).
28949:      * Bits 0-14 are the actual argument count. This may be less than fun->nargs.
31924:      * NB: This is argc for the callee, not the caller.
28949:      */
31924:     uint32          argc;
28887: 
28840:     /*
31924:      * Number of stack slots in the caller, not counting slots pushed when
31924:      * invoking the callee. That is, slots after JSOP_CALL completes but
31924:      * without the return value. This is also equal to the number of slots
31924:      * between fp->down->argv[-2] (calleR fp->callee) and fp->argv[-2]
31924:      * (calleE fp->callee).
28840:      */
31924:     uint32          callerHeight;
31924: 
31924:     /* argc of the caller */
31924:     uint32          callerArgc;
28949: 
28949:     // Safer accessors for argc.
31924:     enum { CONSTRUCTING_FLAG = 0x10000 };
28949:     void   set_argc(uint16 argc, bool constructing) {
31924:         this->argc = uint32(argc) | (constructing ? CONSTRUCTING_FLAG: 0);
28949:     }
33166:     uint16 get_argc() const { return uint16(argc & ~CONSTRUCTING_FLAG); }
31924:     bool   is_constructing() const { return (argc & CONSTRUCTING_FLAG) != 0; }
28993: 
28993:     // The typemap just before the callee is called.
37741:     TraceType* get_typemap() { return (TraceType*) (this+1); }
37741:     const TraceType* get_typemap() const { return (TraceType*) (this+1); }
21521: };
21521: 
21433: struct UnstableExit
21433: {
35044:     VMFragment* fragment;
21521:     VMSideExit* exit;
21433:     UnstableExit* next;
21433: };
21433: 
34351: enum RecordReason
33564: {
34351:     Record_Branch,
34351:     Record_EnterFrame,
34351:     Record_LeaveFrame
33564: };
33564: 
33564: enum RecursionStatus
33564: {
33564:     Recursion_None,             /* No recursion has been compiled yet. */
33564:     Recursion_Disallowed,       /* This tree cannot be recursive. */
33564:     Recursion_Unwinds,          /* Tree is up-recursive only. */
33564:     Recursion_Detected          /* Tree has down recursion and maybe up recursion. */
33564: };
33564: 
36361: struct LinkableFragment : public VMFragment
36361: {
36361:     LinkableFragment(const void* _ip, nanojit::Allocator* alloc
36361:                      verbose_only(, uint32_t profFragID))
36361:       : VMFragment(_ip verbose_only(, profFragID)), typeMap(alloc), nStackTypes(0)
36361:     { }
36361: 
36361:     uint32                  branchCount;
24246:     TypeMap                 typeMap;
24491:     unsigned                nStackTypes;
24491:     SlotList*               globalSlots;
36361: };
36361: 
36361: /*
36361:  * argc is cx->fp->argc at the trace loop header, i.e., the number of arguments
36361:  * pushed for the innermost JS frame. This is required as part of the fragment
36361:  * key because the fragment will write those arguments back to the interpreter
36361:  * stack when it exits, using its typemap, which implicitly incorporates a
36361:  * given value of argc. Without this feature, a fragment could be called as an
36361:  * inner tree with two different values of argc, and entry type checking or
36361:  * exit frame synthesis could crash.
36361:  */
36361: struct TreeFragment : public LinkableFragment
36361: {
36361:     TreeFragment(const void* _ip, nanojit::Allocator* alloc, JSObject* _globalObj,
36361:                  uint32 _globalShape, uint32 _argc verbose_only(, uint32_t profFragID)):
36361:         LinkableFragment(_ip, alloc verbose_only(, profFragID)),
36361:         first(NULL),
36361:         next(NULL),
36361:         peer(NULL),
36361:         globalObj(_globalObj),
36361:         globalShape(_globalShape),
36361:         argc(_argc),
36361:         dependentTrees(alloc),
36361:         linkedTrees(alloc),
36361:         sideExits(alloc),
36361:         gcthings(alloc),
36361:         sprops(alloc)
36361:     { }
36361: 
36361:     TreeFragment* first;
36361:     TreeFragment* next;
36361:     TreeFragment* peer;
36361:     JSObject* globalObj;
36361:     uint32 globalShape;
36361:     uint32 argc;
25491:     /* Dependent trees must be trashed if this tree dies, and updated on missing global types */
35044:     Queue<TreeFragment*>    dependentTrees;
25491:     /* Linked trees must be updated on missing global types, but are not dependent */
35044:     Queue<TreeFragment*>    linkedTrees;
25627: #ifdef DEBUG
25627:     const char*             treeFileName;
25627:     uintN                   treeLineNumber;
25627:     uintN                   treePCOffset;
25627: #endif
36361:     JSScript*               script;
33564:     RecursionStatus         recursion;
36361:     UnstableExit*           unstableExits;
36361:     Queue<VMSideExit*>      sideExits;
36361:     ptrdiff_t               nativeStackBase;
36361:     unsigned                maxCallDepth;
36361:     /* All embedded GC things are registered here so the GC can scan them. */
36361:     Queue<jsval>            gcthings;
36361:     Queue<JSScopeProperty*> sprops;
36361:     unsigned                maxNativeStackSlots;
24246: 
24491:     inline unsigned nGlobalTypes() {
24491:         return typeMap.length() - nStackTypes;
24246:     }
37741:     inline TraceType* globalTypeMap() {
24491:         return typeMap.data() + nStackTypes;
24246:     }
37741:     inline TraceType* stackTypeMap() {
24246:         return typeMap.data();
24246:     }
31495: 
36361:     JS_REQUIRES_STACK void initialize(JSContext* cx, SlotList *globalSlots);
31495:     UnstableExit* removeUnstableExit(VMSideExit* exit);
17413: };
17413: 
36361: inline TreeFragment*
36361: VMFragment::toTreeFragment()
36361: {
36361:     JS_ASSERT(root == this);
36361:     return static_cast<TreeFragment*>(this);
36361: }
36361: 
37741: typedef enum BuiltinStatus {
37741:     BUILTIN_BAILED = 1,
37741:     BUILTIN_ERROR = 2
37741: } BuiltinStatus;
27166: 
32709: // Arguments objects created on trace have a private value that points to an
32709: // instance of this struct. The struct includes a typemap that is allocated
32709: // as part of the object.
37741: struct ArgsPrivateNative {
32709:     double      *argv;
32709: 
37741:     static ArgsPrivateNative *create(VMAllocator &alloc, unsigned argc)
32709:     {
37741:         return (ArgsPrivateNative*) new (alloc) char[sizeof(ArgsPrivateNative) + argc];
32709:     }
32709: 
37741:     TraceType *typemap()
32709:     {
37741:         return (TraceType*) (this+1);
32709:     }
32709: };
32709: 
27166: static JS_INLINE void
37741: SetBuiltinError(JSContext *cx)
27166: {
37741:     cx->interpState->builtinStatus |= BUILTIN_ERROR;
27166: }
27166: 
33542: #ifdef DEBUG_RECORDING_STATUS_NOT_BOOL
33542: /* #define DEBUG_RECORDING_STATUS_NOT_BOOL to detect misuses of RecordingStatus */
33542: struct RecordingStatus {
27933:     int code;
33542:     bool operator==(RecordingStatus &s) { return this->code == s.code; };
33542:     bool operator!=(RecordingStatus &s) { return this->code != s.code; };
23111: };
33542: enum RecordingStatusCodes {
33542:     RECORD_ERROR_code     = 0,
33542:     RECORD_STOP_code      = 1,
33542: 
33542:     RECORD_CONTINUE_code  = 3,
33542:     RECORD_IMACRO_code    = 4
27933: };
33542: RecordingStatus RECORD_CONTINUE = { RECORD_CONTINUE_code };
33542: RecordingStatus RECORD_STOP     = { RECORD_STOP_code };
33542: RecordingStatus RECORD_IMACRO   = { RECORD_IMACRO_code };
33542: RecordingStatus RECORD_ERROR    = { RECORD_ERROR_code };
33542: 
33542: struct AbortableRecordingStatus {
33542:     int code;
33542:     bool operator==(AbortableRecordingStatus &s) { return this->code == s.code; };
33542:     bool operator!=(AbortableRecordingStatus &s) { return this->code != s.code; };
33542: };
33542: enum AbortableRecordingStatusCodes {
33542:     ARECORD_ERROR_code     = 0,
33542:     ARECORD_STOP_code      = 1,
33542:     ARECORD_ABORTED_code   = 2,
33542:     ARECORD_CONTINUE_code  = 3,
35083:     ARECORD_IMACRO_code    = 4,
35083:     ARECORD_COMPLETED_code = 5
33542: };
33542: AbortableRecordingStatus ARECORD_ERROR    = { ARECORD_ERROR_code };
33542: AbortableRecordingStatus ARECORD_STOP     = { ARECORD_STOP_code };
33542: AbortableRecordingStatus ARECORD_CONTINUE = { ARECORD_CONTINUE_code };
33542: AbortableRecordingStatus ARECORD_IMACRO   = { ARECORD_IMACRO_code };
33542: AbortableRecordingStatus ARECORD_ABORTED =  { ARECORD_ABORTED_code };
35083: AbortableRecordingStatus ARECORD_COMPLETED =  { ARECORD_COMPLETED_code };
33542: 
33542: static inline AbortableRecordingStatus
33542: InjectStatus(RecordingStatus rs)
33542: {
33542:     AbortableRecordingStatus ars = { rs.code };
33542:     return ars;
33542: }
33542: static inline AbortableRecordingStatus
33542: InjectStatus(AbortableRecordingStatus ars)
33542: {
33542:     return ars;
33542: }
33542: 
33542: static inline bool
33542: StatusAbortsRecording(AbortableRecordingStatus ars)
33542: {
35083:     return ars == ARECORD_ERROR || ars == ARECORD_STOP;
33542: }
27933: #else
33542: 
33542: /*
33542:  * Normally, during recording, when the recorder cannot continue, it returns
33542:  * ARECORD_STOP to indicate that recording should be aborted by the top-level
33542:  * recording function. However, if the recorder reenters the interpreter (e.g.,
33542:  * when executing an inner loop), there will be an immediate abort. This
33542:  * condition must be carefully detected and propagated out of all nested
33542:  * recorder calls lest the now-invalid TraceRecorder object be accessed
33542:  * accidentally. This condition is indicated by the ARECORD_ABORTED value.
33542:  *
33542:  * The AbortableRecordingStatus enumeration represents the general set of
33542:  * possible results of calling a recorder function. Functions that cannot
33542:  * possibly return ARECORD_ABORTED may statically guarantee this to the caller
33542:  * using the RecordingStatus enumeration. Ideally, C++ would allow subtyping
33542:  * of enumerations, but it doesn't. To simulate subtype conversion manually,
33542:  * code should call InjectStatus to inject a value of the restricted set into a
33542:  * value of the general set.
33542:  */
33542: 
33542: enum RecordingStatus {
33542:     RECORD_ERROR      = 0,  // Error; propagate to interpreter.
33542:     RECORD_STOP       = 1,  // Recording should be aborted at the top-level
33542:                             // call to the recorder.
33542:                             // (value reserved for ARECORD_ABORTED)
33542:     RECORD_CONTINUE   = 3,  // Continue recording.
33542:     RECORD_IMACRO     = 4   // Entered imacro; continue recording.
27933:                             // Only JSOP_IS_IMACOP opcodes may return this.
27933: };
33542: 
33542: enum AbortableRecordingStatus {
33542:     ARECORD_ERROR     = 0,
33542:     ARECORD_STOP      = 1,
33542:     ARECORD_ABORTED   = 2,  // Recording has already been aborted; the recorder
33542:                             // has been deleted.
33542:     ARECORD_CONTINUE  = 3,
35083:     ARECORD_IMACRO    = 4,
35083:     ARECORD_COMPLETED = 5   // Recording of the current trace recorder completed
33542: };
33542: 
33542: static JS_ALWAYS_INLINE AbortableRecordingStatus
33542: InjectStatus(RecordingStatus rs)
33542: {
33542:     return static_cast<AbortableRecordingStatus>(rs);
33542: }
33542: 
33542: static JS_ALWAYS_INLINE AbortableRecordingStatus
33542: InjectStatus(AbortableRecordingStatus ars)
33542: {
33542:     return ars;
33542: }
33542: 
35083: /*
35083:  * Return whether the recording status requires the current recording session
35083:  * to be deleted. ABORTED and COMPLETED indicate the recording session is
35083:  * already deleted, so they return 'false'.
35083:  */
33542: static JS_ALWAYS_INLINE bool
33542: StatusAbortsRecording(AbortableRecordingStatus ars)
33542: {
35083:     return ars <= ARECORD_STOP;
33542: }
27933: #endif
27933: 
31473: class SlotMap;
33564: class SlurpInfo;
27933: 
31473: /* Results of trying to compare two typemaps together */
31473: enum TypeConsensus
31473: {
31473:     TypeConsensus_Okay,         /* Two typemaps are compatible */
31473:     TypeConsensus_Undemotes,    /* Not compatible now, but would be with pending undemotes. */
31473:     TypeConsensus_Bad           /* Typemaps are not compatible */
31473: };
23111: 
38568: typedef HashMap<nanojit::LIns*, JSObject*> GuardedShapeTable;
38568: 
35083: #ifdef DEBUG
37741: # define AbortRecording(cx, reason) AbortRecordingImpl(cx, reason)
35083: #else
37741: # define AbortRecording(cx, reason) AbortRecordingImpl(cx)
35083: #endif
35083: 
34351: class TraceRecorder
34351: {
34351:     /*************************************************************** Recording session constants */
34351: 
34351:     /* The context in which recording started. */
34351:     JSContext* const                cx;
34351: 
34351:     /* Cached value of JS_TRACE_MONITOR(cx). */
37741:     TraceMonitor* const             traceMonitor;
34351: 
34351:     /* The Fragment being recorded by this recording session. */
35044:     VMFragment* const               fragment;
34351: 
36361:     /* The root fragment representing the tree. */
36361:     TreeFragment* const             tree;
34351: 
34351:     /* The reason we started recording. */
34351:     RecordReason const              recordReason;
34351: 
34351:     /* The global object from the start of recording until now. */
34351:     JSObject* const                 globalObj;
34351: 
34351:     /* If non-null, the (pc of the) outer loop aborted to start recording this loop. */
34351:     jsbytecode* const               outer;
34351: 
34351:     /* If |outer|, the argc to use when looking up |outer| in the fragments table. */
34351:     uint32 const                    outerArgc;
34351: 
34351:     /* The current frame's lexical block when recording started. */
34351:     JSObject* const                 lexicalBlock;
34351: 
34351:     /* If non-null, the side exit from which we are growing. */
34351:     VMSideExit* const               anchor;
34351: 
34351:     /* The LIR-generation pipeline used to build |fragment|. */
34351:     nanojit::LirWriter* const       lir;
34351: 
34351:     /* Instructions yielding the corresponding trace-const members of InterpState. */
34351:     nanojit::LIns* const            cx_ins;
34351:     nanojit::LIns* const            eos_ins;
34351:     nanojit::LIns* const            eor_ins;
34351:     nanojit::LIns* const            loopLabel;
34351: 
36401:     /* Lazy slot import state. */
36401:     unsigned                        importStackSlots;
36401:     unsigned                        importGlobalSlots;
36401:     TypeMap                         importTypeMap;
36401: 
34351:     /*
34351:      * The LirBuffer used to supply memory to our LirWriter pipeline. Also contains the most recent
34351:      * instruction for {sp, rp, state}. Also contains names for debug JIT spew. Should be split.
34351:      */
34351:     nanojit::LirBuffer* const       lirbuf;
34351: 
34351:     /*
34351:      * Remembers traceAlloc state before recording started; automatically rewinds when mark is
34351:      * destroyed on a failed compilation.
34351:      */
33545:     VMAllocator::Mark               mark;
34351: 
35085:     /* Remembers the number of sideExits in treeInfo before recording started. */
35085:     const unsigned                  numSideExitsBefore;
35085: 
34351:     /*********************************************************** Recording session mutable state */
34351: 
34351:     /* Maps interpreter stack values to the instruction generating that value. */
17596:     Tracker                         tracker;
34351: 
34351:     /* Maps interpreter stack values to the instruction writing back to the native stack. */
17815:     Tracker                         nativeFrameTracker;
34351: 
34351:     /* The start of the global object's dslots we assume for the trackers. */
34351:     jsval*                          global_dslots;
34351: 
34351:     /* The number of interpreted calls entered (and not yet left) since recording began. */
17789:     unsigned                        callDepth;
34351: 
34351:     /* The current atom table, mirroring the interpreter loop's variable of the same name. */
17611:     JSAtom**                        atoms;
34351: 
34351:     /* FIXME: Dead, but soon to be used for something or other. */
34351:     Queue<jsbytecode*>              cfgMerges;
34351: 
34351:     /* Indicates whether the current tree should be trashed when the recording session ends. */
34351:     bool                            trashSelf;
34351: 
34351:     /* A list of trees to trash at the end of the recording session. */
35044:     Queue<TreeFragment*>            whichTreesToTrash;
34351: 
38568:     /* The set of objects whose shapes already have been guarded. */
38568:     GuardedShapeTable               guardedShapeTable;
38568: 
34351:     /***************************************** Temporal state hoisted into the recording session */
34351: 
34351:     /* Carry the return value from a STOP/RETURN to the subsequent record_LeaveFrame. */
17818:     nanojit::LIns*                  rval_ins;
34351: 
34351:     /* Carry the return value from a native call to the record_NativeCallComplete. */
28086:     nanojit::LIns*                  native_rval_ins;
34351: 
34351:     /* Carry the return value of js_NewInstance to record_NativeCallComplete. */
28086:     nanojit::LIns*                  newobj_ins;
34351: 
34351:     /* Carry the JSSpecializedNative used to generate a call to record_NativeCallComplete. */
34351:     JSSpecializedNative*            pendingSpecializedNative;
34351: 
34351:     /* Carry whether this is a jsval on the native stack from finishGetProp to monitorRecording. */
34351:     jsval*                          pendingUnboxSlot;
34351: 
34351:     /* Carry a guard condition to the beginning of the next monitorRecording. */
34351:     nanojit::LIns*                  pendingGuardCondition;
34351: 
34351:     /* Carry whether we have an always-exit from emitIf to checkTraceEnd. */
34351:     bool                            pendingLoop;
34351: 
34351:     /* Temporary JSSpecializedNative used to describe non-specialized fast natives. */
32669:     JSSpecializedNative             generatedSpecializedNative;
34351: 
37741:     /* Temporary TraceType array used to construct temporary typemaps. */
37741:     js::Vector<TraceType, 256>      tempTypeMap;
17323: 
34351:     /************************************************************* 10 bajillion member functions */
34351: 
35099:     nanojit::LIns* insImmVal(jsval val);
31843:     nanojit::LIns* insImmObj(JSObject* obj);
31843:     nanojit::LIns* insImmFun(JSFunction* fun);
31843:     nanojit::LIns* insImmStr(JSString* str);
31843:     nanojit::LIns* insImmSprop(JSScopeProperty* sprop);
32746:     nanojit::LIns* p2i(nanojit::LIns* ins);
31843: 
34351:     /*
34351:      * Examines current interpreter state to record information suitable for returning to the
34351:      * interpreter through a side exit of the given type.
34351:      */
34351:     JS_REQUIRES_STACK VMSideExit* snapshot(ExitType exitType);
34351: 
34351:     /*
34351:      * Creates a separate but identical copy of the given side exit, allowing the guards associated
34351:      * with each to be entirely separate even after subsequent patching.
34351:      */
34351:     JS_REQUIRES_STACK VMSideExit* copy(VMSideExit* exit);
34351: 
34351:     /*
34351:      * Creates an instruction whose payload is a GuardRecord for the given exit.  The instruction
34351:      * is suitable for use as the final argument of a single call to LirBuffer::insGuard; do not
34351:      * reuse the returned value.
34351:      */
34351:     JS_REQUIRES_STACK nanojit::GuardRecord* createGuardRecord(VMSideExit* exit);
34351: 
17815:     bool isGlobal(jsval* p) const;
36401:     ptrdiff_t nativeGlobalSlot(jsval *p) const;
18193:     ptrdiff_t nativeGlobalOffset(jsval* p) const;
22652:     JS_REQUIRES_STACK ptrdiff_t nativeStackOffset(jsval* p) const;
36401:     JS_REQUIRES_STACK ptrdiff_t nativeStackSlot(jsval* p) const;
35083:     JS_REQUIRES_STACK ptrdiff_t nativespOffset(jsval* p) const;
37741:     JS_REQUIRES_STACK void import(nanojit::LIns* base, ptrdiff_t offset, jsval* p, TraceType t,
18045:                                   const char *prefix, uintN index, JSStackFrame *fp);
36361:     JS_REQUIRES_STACK void import(TreeFragment* tree, nanojit::LIns* sp, unsigned stackSlots,
37741:                                   unsigned callDepth, unsigned ngslots, TraceType* typeMap);
17815:     void trackNativeStackUse(unsigned slots);
17381: 
25938:     JS_REQUIRES_STACK bool isValidSlot(JSScope* scope, JSScopeProperty* sprop);
22652:     JS_REQUIRES_STACK bool lazilyImportGlobalSlot(unsigned slot);
36401:     JS_REQUIRES_STACK void importGlobalSlot(unsigned slot);
17891: 
26972:     JS_REQUIRES_STACK void guard(bool expected, nanojit::LIns* cond, ExitType exitType);
27540:     JS_REQUIRES_STACK void guard(bool expected, nanojit::LIns* cond, VMSideExit* exit);
38603:     JS_REQUIRES_STACK nanojit::LIns* guard_xov(nanojit::LOpcode op, nanojit::LIns* d0,
38603:                                                nanojit::LIns* d1, VMSideExit* exit);
33591:     JS_REQUIRES_STACK nanojit::LIns* slurpInt32Slot(nanojit::LIns* val_ins, jsval* vp,
33591:                                                     VMSideExit* exit);
33591:     JS_REQUIRES_STACK nanojit::LIns* slurpDoubleSlot(nanojit::LIns* val_ins, jsval* vp,
33591:                                                      VMSideExit* exit);
33591:     JS_REQUIRES_STACK nanojit::LIns* slurpStringSlot(nanojit::LIns* val_ins, jsval* vp,
33591:                                                      VMSideExit* exit);
33591:     JS_REQUIRES_STACK nanojit::LIns* slurpObjectSlot(nanojit::LIns* val_ins, jsval* vp,
33591:                                                      VMSideExit* exit);
33591:     JS_REQUIRES_STACK nanojit::LIns* slurpFunctionSlot(nanojit::LIns* val_ins, jsval* vp,
33591:                                                        VMSideExit* exit);
33591:     JS_REQUIRES_STACK nanojit::LIns* slurpNullSlot(nanojit::LIns* val_ins, jsval* vp,
33591:                                                    VMSideExit* exit);
33591:     JS_REQUIRES_STACK nanojit::LIns* slurpBoolSlot(nanojit::LIns* val_ins, jsval* vp,
33591:                                                    VMSideExit* exit);
33591:     JS_REQUIRES_STACK nanojit::LIns* slurpSlot(nanojit::LIns* val_ins, jsval* vp,
33591:                                                VMSideExit* exit);
33591:     JS_REQUIRES_STACK void slurpSlot(nanojit::LIns* val_ins, jsval* vp, SlurpInfo* info);
33564:     JS_REQUIRES_STACK AbortableRecordingStatus slurpDownFrames(jsbytecode* return_pc);
34351:     JS_REQUIRES_STACK AbortableRecordingStatus upRecursion();
34351:     JS_REQUIRES_STACK AbortableRecordingStatus downRecursion();
23111: 
17721:     nanojit::LIns* addName(nanojit::LIns* ins, const char* name);
17346: 
32700:     nanojit::LIns* writeBack(nanojit::LIns* i, nanojit::LIns* base, ptrdiff_t offset,
32700:                              bool demote);
32700:     JS_REQUIRES_STACK void set(jsval* p, nanojit::LIns* l, bool initializing = false,
32700:                                bool demote = true);
38532:     nanojit::LIns* getFromTracker(jsval* p);
24381:     JS_REQUIRES_STACK nanojit::LIns* get(jsval* p);
36662:     JS_REQUIRES_STACK nanojit::LIns* attemptImport(jsval* p);
31444:     JS_REQUIRES_STACK nanojit::LIns* addr(jsval* p);
31444: 
24381:     JS_REQUIRES_STACK bool known(jsval* p);
24381:     JS_REQUIRES_STACK void checkForGlobalObjectReallocation();
17320: 
31473:     JS_REQUIRES_STACK TypeConsensus selfTypeStability(SlotMap& smap);
33564:     JS_REQUIRES_STACK TypeConsensus peerTypeStability(SlotMap& smap, const void* ip,
35044:                                                       TreeFragment** peer);
17410: 
22652:     JS_REQUIRES_STACK jsval& argval(unsigned n) const;
22652:     JS_REQUIRES_STACK jsval& varval(unsigned n) const;
22652:     JS_REQUIRES_STACK jsval& stackval(int n) const;
17412: 
31075:     struct NameResult {
31075:         // |tracked| is true iff the result of the name lookup is a variable that
31075:         // is already in the tracker. The rest of the fields are set only if
31075:         // |tracked| is false.
31075:         bool             tracked;
32593:         jsval            v;              // current property value
31075:         JSObject         *obj;           // Call object where name was found
32593:         nanojit::LIns    *obj_ins;       // LIR value for obj
31075:         JSScopeProperty  *sprop;         // sprop name was resolved to
31075:     };
31075: 
37694:     JS_REQUIRES_STACK nanojit::LIns* scopeChain();
37694:     JS_REQUIRES_STACK nanojit::LIns* entryScopeChain() const;
30248:     JS_REQUIRES_STACK JSStackFrame* frameIfInRange(JSObject* obj, unsigned* depthp = NULL) const;
33542:     JS_REQUIRES_STACK RecordingStatus traverseScopeChain(JSObject *obj, nanojit::LIns *obj_ins, JSObject *obj2, nanojit::LIns *&obj2_ins);
33542:     JS_REQUIRES_STACK AbortableRecordingStatus scopeChainProp(JSObject* obj, jsval*& vp, nanojit::LIns*& ins, NameResult& nr);
33542:     JS_REQUIRES_STACK RecordingStatus callProp(JSObject* obj, JSProperty* sprop, jsid id, jsval*& vp, nanojit::LIns*& ins, NameResult& nr);
18286: 
22652:     JS_REQUIRES_STACK nanojit::LIns* arg(unsigned n);
22652:     JS_REQUIRES_STACK void arg(unsigned n, nanojit::LIns* i);
22652:     JS_REQUIRES_STACK nanojit::LIns* var(unsigned n);
22652:     JS_REQUIRES_STACK void var(unsigned n, nanojit::LIns* i);
28923:     JS_REQUIRES_STACK nanojit::LIns* upvar(JSScript* script, JSUpvarArray* uva, uintN index, jsval& v);
30248:     nanojit::LIns* stackLoad(nanojit::LIns* addr, uint8 type);
22652:     JS_REQUIRES_STACK nanojit::LIns* stack(int n);
22652:     JS_REQUIRES_STACK void stack(int n, nanojit::LIns* i);
17412: 
23456:     JS_REQUIRES_STACK nanojit::LIns* alu(nanojit::LOpcode op, jsdouble v0, jsdouble v1,
21799:                                          nanojit::LIns* s0, nanojit::LIns* s1);
17469:     nanojit::LIns* f2i(nanojit::LIns* f);
36402:     nanojit::LIns* f2u(nanojit::LIns* f);
22652:     JS_REQUIRES_STACK nanojit::LIns* makeNumberInt32(nanojit::LIns* f);
23456:     JS_REQUIRES_STACK nanojit::LIns* stringify(jsval& v);
21685: 
36437:     JS_REQUIRES_STACK nanojit::LIns* newArguments(nanojit::LIns* callee_ins);
32709: 
33542:     JS_REQUIRES_STACK RecordingStatus call_imacro(jsbytecode* imacro);
17469: 
33542:     JS_REQUIRES_STACK AbortableRecordingStatus ifop();
33542:     JS_REQUIRES_STACK RecordingStatus switchop();
25099: #ifdef NANOJIT_IA32
33542:     JS_REQUIRES_STACK AbortableRecordingStatus tableswitch();
25099: #endif
33542:     JS_REQUIRES_STACK RecordingStatus inc(jsval& v, jsint incr, bool pre = true);
33542:     JS_REQUIRES_STACK RecordingStatus inc(jsval v, nanojit::LIns*& v_ins, jsint incr,
27933:                                             bool pre = true);
33542:     JS_REQUIRES_STACK RecordingStatus incHelper(jsval v, nanojit::LIns* v_ins,
31480:                                                   nanojit::LIns*& v_after, jsint incr);
33542:     JS_REQUIRES_STACK AbortableRecordingStatus incProp(jsint incr, bool pre = true);
33542:     JS_REQUIRES_STACK RecordingStatus incElem(jsint incr, bool pre = true);
33542:     JS_REQUIRES_STACK AbortableRecordingStatus incName(jsint incr, bool pre = true);
18687: 
23093:     JS_REQUIRES_STACK void strictEquality(bool equal, bool cmpCase);
33542:     JS_REQUIRES_STACK AbortableRecordingStatus equality(bool negate, bool tryBranchAfterCond);
33542:     JS_REQUIRES_STACK AbortableRecordingStatus equalityHelper(jsval l, jsval r,
23223:                                                                 nanojit::LIns* l_ins, nanojit::LIns* r_ins,
23223:                                                                 bool negate, bool tryBranchAfterCond,
23223:                                                                 jsval& rval);
33542:     JS_REQUIRES_STACK AbortableRecordingStatus relational(nanojit::LOpcode op, bool tryBranchAfterCond);
17467: 
33542:     JS_REQUIRES_STACK RecordingStatus unary(nanojit::LOpcode op);
33542:     JS_REQUIRES_STACK RecordingStatus binary(nanojit::LOpcode op);
17467: 
33560:     JS_REQUIRES_STACK RecordingStatus guardShape(nanojit::LIns* obj_ins, JSObject* obj,
33560:                                                  uint32 shape, const char* name,
32777:                                                  nanojit::LIns* map_ins, VMSideExit* exit);
17417: 
33560: #if defined DEBUG_notme && defined XP_UNIX
33560:     void dumpGuardedShapes(const char* prefix);
33560: #endif
33560: 
33560:     void forgetGuardedShapes();
33560: 
30244:     inline nanojit::LIns* map(nanojit::LIns *obj_ins);
22652:     JS_REQUIRES_STACK bool map_is_native(JSObjectMap* map, nanojit::LIns* map_ins,
22652:                                          nanojit::LIns*& ops_ins, size_t op_offset = 0);
33542:     JS_REQUIRES_STACK AbortableRecordingStatus test_property_cache(JSObject* obj, nanojit::LIns* obj_ins,
22652:                                                                      JSObject*& obj2, jsuword& pcval);
33542:     JS_REQUIRES_STACK RecordingStatus guardNativePropertyOp(JSObject* aobj,
30847:                                                               nanojit::LIns* map_ins);
33542:     JS_REQUIRES_STACK RecordingStatus guardPropertyCacheHit(nanojit::LIns* obj_ins,
30847:                                                               nanojit::LIns* map_ins,
30847:                                                               JSObject* aobj,
30847:                                                               JSObject* obj2,
30847:                                                               JSPropCacheEntry* entry,
30847:                                                               jsuword& pcval);
30847: 
28554:     void stobj_set_fslot(nanojit::LIns *obj_ins, unsigned slot,
31849:                          nanojit::LIns* v_ins);
28554:     void stobj_set_dslot(nanojit::LIns *obj_ins, unsigned slot, nanojit::LIns*& dslots_ins,
31849:                          nanojit::LIns* v_ins);
22626:     void stobj_set_slot(nanojit::LIns* obj_ins, unsigned slot, nanojit::LIns*& dslots_ins,
22626:                         nanojit::LIns* v_ins);
22626: 
37776:     nanojit::LIns* stobj_get_const_fslot(nanojit::LIns* obj_ins, unsigned slot);
17899:     nanojit::LIns* stobj_get_fslot(nanojit::LIns* obj_ins, unsigned slot);
27012:     nanojit::LIns* stobj_get_dslot(nanojit::LIns* obj_ins, unsigned index,
27012:                                    nanojit::LIns*& dslots_ins);
17459:     nanojit::LIns* stobj_get_slot(nanojit::LIns* obj_ins, unsigned slot,
17459:                                   nanojit::LIns*& dslots_ins);
31061: 
32684:     nanojit::LIns* stobj_get_private(nanojit::LIns* obj_ins) {
32684:         return stobj_get_fslot(obj_ins, JSSLOT_PRIVATE);
30248:     }
31061: 
32603:     nanojit::LIns* stobj_get_proto(nanojit::LIns* obj_ins) {
32603:         return stobj_get_fslot(obj_ins, JSSLOT_PROTO);
32603:     }
32603: 
31061:     nanojit::LIns* stobj_get_parent(nanojit::LIns* obj_ins) {
31061:         return stobj_get_fslot(obj_ins, JSSLOT_PARENT);
31061:     }
31061: 
33542:     JS_REQUIRES_STACK AbortableRecordingStatus name(jsval*& vp, nanojit::LIns*& ins, NameResult& nr);
37685:     JS_REQUIRES_STACK AbortableRecordingStatus prop(JSObject* obj, nanojit::LIns* obj_ins,
37685:                                                     uint32 *slotp, nanojit::LIns** v_insp,
37685:                                                     jsval* outp);
37685:     JS_REQUIRES_STACK AbortableRecordingStatus propTail(JSObject* obj, nanojit::LIns* obj_ins,
37685:                                                         JSObject* obj2, jsuword pcval,
37685:                                                         uint32 *slotp, nanojit::LIns** v_insp,
37685:                                                         jsval* outp);
33542:     JS_REQUIRES_STACK RecordingStatus denseArrayElement(jsval& oval, jsval& idx, jsval*& vp,
28411:                                                         nanojit::LIns*& v_ins,
28411:                                                         nanojit::LIns*& addr_ins);
37754:     JS_REQUIRES_STACK AbortableRecordingStatus typedArrayElement(jsval& oval, jsval& idx, jsval*& vp,
37754:                                                                  nanojit::LIns*& v_ins,
37754:                                                                  nanojit::LIns*& addr_ins);
33542:     JS_REQUIRES_STACK AbortableRecordingStatus getProp(JSObject* obj, nanojit::LIns* obj_ins);
33542:     JS_REQUIRES_STACK AbortableRecordingStatus getProp(jsval& v);
33542:     JS_REQUIRES_STACK RecordingStatus getThis(nanojit::LIns*& this_ins);
17758: 
32761:     JS_REQUIRES_STACK VMSideExit* enterDeepBailCall();
31444:     JS_REQUIRES_STACK void leaveDeepBailCall();
31444: 
33542:     JS_REQUIRES_STACK RecordingStatus primitiveToStringInPlace(jsval* vp);
31444:     JS_REQUIRES_STACK void finishGetProp(nanojit::LIns* obj_ins, nanojit::LIns* vp_ins,
31444:                                          nanojit::LIns* ok_ins, jsval* outp);
33542:     JS_REQUIRES_STACK RecordingStatus getPropertyByName(nanojit::LIns* obj_ins, jsval* idvalp,
31444:                                                           jsval* outp);
33542:     JS_REQUIRES_STACK RecordingStatus getPropertyByIndex(nanojit::LIns* obj_ins,
31444:                                                            nanojit::LIns* index_ins, jsval* outp);
33542:     JS_REQUIRES_STACK RecordingStatus getPropertyById(nanojit::LIns* obj_ins, jsval* outp);
33542:     JS_REQUIRES_STACK RecordingStatus getPropertyWithNativeGetter(nanojit::LIns* obj_ins,
32558:                                                                     JSScopeProperty* sprop,
32558:                                                                     jsval* outp);
31444: 
33542:     JS_REQUIRES_STACK RecordingStatus nativeSet(JSObject* obj, nanojit::LIns* obj_ins,
30847:                                                   JSScopeProperty* sprop,
30847:                                                   jsval v, nanojit::LIns* v_ins);
33542:     JS_REQUIRES_STACK RecordingStatus setProp(jsval &l, JSPropCacheEntry* entry,
30847:                                                 JSScopeProperty* sprop,
30847:                                                 jsval &v, nanojit::LIns*& v_ins);
33542:     JS_REQUIRES_STACK RecordingStatus setCallProp(JSObject *callobj, nanojit::LIns *callobj_ins,
31075:                                                     JSScopeProperty *sprop, nanojit::LIns *v_ins,
31075:                                                     jsval v);
33542:     JS_REQUIRES_STACK RecordingStatus initOrSetPropertyByName(nanojit::LIns* obj_ins,
31829:                                                                 jsval* idvalp, jsval* rvalp,
31829:                                                                 bool init);
33542:     JS_REQUIRES_STACK RecordingStatus initOrSetPropertyByIndex(nanojit::LIns* obj_ins,
31829:                                                                  nanojit::LIns* index_ins,
31829:                                                                  jsval* rvalp, bool init);
35466:     JS_REQUIRES_STACK AbortableRecordingStatus setElem(int lval_spindex, int idx_spindex,
35466:                                                        int v_spindex);
30847: 
31902:     JS_REQUIRES_STACK nanojit::LIns* box_jsval(jsval v, nanojit::LIns* v_ins);
31902:     JS_REQUIRES_STACK nanojit::LIns* unbox_jsval(jsval v, nanojit::LIns* v_ins, VMSideExit* exit);
22652:     JS_REQUIRES_STACK bool guardClass(JSObject* obj, nanojit::LIns* obj_ins, JSClass* clasp,
38497:                                       VMSideExit* exit, nanojit::LOpcode loadOp = nanojit::LIR_ldp);
38497:     bool guardConstClass(JSObject* obj, nanojit::LIns* obj_ins, JSClass* clasp, VMSideExit* exit) {
38497:         return guardClass(obj, obj_ins, clasp, exit, nanojit::LIR_ldcp);
38497:     }
22652:     JS_REQUIRES_STACK bool guardDenseArray(JSObject* obj, nanojit::LIns* obj_ins,
21685:                                            ExitType exitType = MISMATCH_EXIT);
32777:     JS_REQUIRES_STACK bool guardDenseArray(JSObject* obj, nanojit::LIns* obj_ins,
32777:                                            VMSideExit* exit);
29513:     JS_REQUIRES_STACK bool guardHasPrototype(JSObject* obj, nanojit::LIns* obj_ins,
29513:                                              JSObject** pobj, nanojit::LIns** pobj_ins,
29513:                                              VMSideExit* exit);
33542:     JS_REQUIRES_STACK RecordingStatus guardPrototypeHasNoIndexedProperties(JSObject* obj,
27933:                                                                              nanojit::LIns* obj_ins,
21521:                                                                              ExitType exitType);
33542:     JS_REQUIRES_STACK RecordingStatus guardNotGlobalObject(JSObject* obj,
27933:                                                              nanojit::LIns* obj_ins);
37009:     JS_REQUIRES_STACK JSStackFrame* entryFrame() const;
37009:     JS_REQUIRES_STACK void clearEntryFrameSlotsFromTracker(Tracker& which);
37009:     JS_REQUIRES_STACK void clearCurrentFrameSlotsFromTracker(Tracker& which);
37009:     JS_REQUIRES_STACK void clearFrameSlotsFromTracker(Tracker& which, JSStackFrame* fp, unsigned nslots);
37694:     JS_REQUIRES_STACK void putActivationObjects();
33542:     JS_REQUIRES_STACK RecordingStatus guardCallee(jsval& callee);
31460:     JS_REQUIRES_STACK JSStackFrame      *guardArguments(JSObject *obj, nanojit::LIns* obj_ins,
31460:                                                         unsigned *depthp);
37214:     JS_REQUIRES_STACK nanojit::LIns* guardArgsLengthNotAssigned(nanojit::LIns* argsobj_ins);
33542:     JS_REQUIRES_STACK RecordingStatus getClassPrototype(JSObject* ctor,
27933:                                                           nanojit::LIns*& proto_ins);
33542:     JS_REQUIRES_STACK RecordingStatus getClassPrototype(JSProtoKey key,
27933:                                                           nanojit::LIns*& proto_ins);
33542:     JS_REQUIRES_STACK RecordingStatus newArray(JSObject* ctor, uint32 argc, jsval* argv,
28086:                                                  jsval* rval);
33542:     JS_REQUIRES_STACK RecordingStatus newString(JSObject* ctor, uint32 argc, jsval* argv,
28086:                                                   jsval* rval);
33542:     JS_REQUIRES_STACK RecordingStatus interpretedFunctionCall(jsval& fval, JSFunction* fun,
27933:                                                                 uintN argc, bool constructing);
30847:     JS_REQUIRES_STACK void propagateFailureToBuiltinStatus(nanojit::LIns *ok_ins,
30847:                                                            nanojit::LIns *&status_ins);
33542:     JS_REQUIRES_STACK RecordingStatus emitNativeCall(JSSpecializedNative* sn, uintN argc,
32678:                                                        nanojit::LIns* args[], bool rooted);
30847:     JS_REQUIRES_STACK void emitNativePropertyOp(JSScope* scope,
30847:                                                 JSScopeProperty* sprop,
30847:                                                 nanojit::LIns* obj_ins,
30847:                                                 bool setflag,
30847:                                                 nanojit::LIns* boxed_ins);
33542:     JS_REQUIRES_STACK RecordingStatus callSpecializedNative(JSNativeTraceInfo* trcinfo, uintN argc,
22652:                                                               bool constructing);
33542:     JS_REQUIRES_STACK RecordingStatus callNative(uintN argc, JSOp mode);
33542:     JS_REQUIRES_STACK RecordingStatus functionCall(uintN argc, JSOp mode);
17921: 
22652:     JS_REQUIRES_STACK void trackCfgMerges(jsbytecode* pc);
26557:     JS_REQUIRES_STACK void emitIf(jsbytecode* pc, bool cond, nanojit::LIns* x);
22652:     JS_REQUIRES_STACK void fuseIf(jsbytecode* pc, bool cond, nanojit::LIns* x);
33542:     JS_REQUIRES_STACK AbortableRecordingStatus checkTraceEnd(jsbytecode* pc);
19068: 
21685:     bool hasMethod(JSObject* obj, jsid id);
24299:     JS_REQUIRES_STACK bool hasIteratorMethod(JSObject* obj);
21685: 
27014:     JS_REQUIRES_STACK jsatomid getFullIndex(ptrdiff_t pcoff = 0);
27012: 
37741:     JS_REQUIRES_STACK TraceType determineSlotType(jsval* vp);
27540: 
35083:     JS_REQUIRES_STACK AbortableRecordingStatus compile();
33564:     JS_REQUIRES_STACK AbortableRecordingStatus closeLoop();
33564:     JS_REQUIRES_STACK AbortableRecordingStatus closeLoop(VMSideExit* exit);
33564:     JS_REQUIRES_STACK AbortableRecordingStatus closeLoop(SlotMap& slotMap, VMSideExit* exit);
33542:     JS_REQUIRES_STACK AbortableRecordingStatus endLoop();
33542:     JS_REQUIRES_STACK AbortableRecordingStatus endLoop(VMSideExit* exit);
35044:     JS_REQUIRES_STACK void joinEdgesToEntry(TreeFragment* peer_root);
35044:     JS_REQUIRES_STACK void adjustCallerTypes(TreeFragment* f);
35044:     JS_REQUIRES_STACK void prepareTreeCall(TreeFragment* inner, nanojit::LIns*& inner_sp_ins);
35044:     JS_REQUIRES_STACK void emitTreeCall(TreeFragment* inner, VMSideExit* exit, nanojit::LIns* inner_sp_ins);
37741:     JS_REQUIRES_STACK void determineGlobalTypes(TraceType* typeMap);
34351:     JS_REQUIRES_STACK VMSideExit* downSnapshot(FrameInfo* downFrame);
35044:     JS_REQUIRES_STACK TreeFragment* findNestedCompatiblePeer(TreeFragment* f);
35044:     JS_REQUIRES_STACK AbortableRecordingStatus attemptTreeCall(TreeFragment* inner,
33564:                                                                uintN& inlineCallCount);
17409: 
34351:     static JS_REQUIRES_STACK bool recordLoopEdge(JSContext* cx, TraceRecorder* r,
34351:                                                  uintN& inlineCallCount);
33564: 
34351:     /* Allocators associated with this recording session. */
34351:     VMAllocator& tempAlloc() const { return *traceMonitor->tempAlloc; }
34351:     VMAllocator& traceAlloc() const { return *traceMonitor->traceAlloc; }
34351:     VMAllocator& dataAlloc() const { return *traceMonitor->dataAlloc; }
34351: 
34351:     /* Member declarations for each opcode, to be called before interpreting the opcode. */
34351: #define OPDEF(op,val,name,token,length,nuses,ndefs,prec,format)               \
34351:     JS_REQUIRES_STACK AbortableRecordingStatus record_##op();
34351: # include "jsopcode.tbl"
34351: #undef OPDEF
34351: 
35083:     inline void* operator new(size_t size) { return calloc(1, size); }
35083:     inline void operator delete(void *p) { free(p); }
35083: 
35083:     JS_REQUIRES_STACK
36361:     TraceRecorder(JSContext* cx, VMSideExit*, VMFragment*,
37741:                   unsigned stackSlots, unsigned ngslots, TraceType* typeMap,
35083:                   VMSideExit* expectedInnerExit, jsbytecode* outerTree,
35083:                   uint32 outerArgc, RecordReason reason);
35083: 
35083:     /* The destructor should only be called through finish*, not directly. */
35083:     ~TraceRecorder();
35083:     JS_REQUIRES_STACK AbortableRecordingStatus finishSuccessfully();
35083:     JS_REQUIRES_STACK AbortableRecordingStatus finishAbort(const char* reason);
35083: 
34351:     friend class ImportBoxedStackSlotVisitor;
34351:     friend class ImportUnboxedStackSlotVisitor;
34351:     friend class ImportGlobalSlotVisitor;
34351:     friend class AdjustCallerGlobalTypesVisitor;
34351:     friend class AdjustCallerStackTypesVisitor;
34351:     friend class TypeCompatibilityVisitor;
36439:     friend class ImportFrameSlotsVisitor;
34351:     friend class SlotMap;
34351:     friend class DefaultSlotMap;
34351:     friend class DetermineTypesVisitor;
34351:     friend class RecursiveSlotMap;
34351:     friend class UpRecursiveSlotMap;
37741:     friend bool MonitorLoopEdge(JSContext*, uintN&, RecordReason);
37741:     friend void AbortRecording(JSContext*, const char*);
34351: 
34351: public:
35083:     static bool JS_REQUIRES_STACK
36361:     startRecorder(JSContext*, VMSideExit*, VMFragment*,
37741:                   unsigned stackSlots, unsigned ngslots, TraceType* typeMap,
34351:                   VMSideExit* expectedInnerExit, jsbytecode* outerTree,
34351:                   uint32 outerArgc, RecordReason reason);
34351: 
34351:     /* Accessors. */
35044:     VMFragment*         getFragment() const { return fragment; }
36361:     TreeFragment*       getTree() const { return tree; }
35083:     bool                outOfMemory() const { return traceMonitor->outOfMemory(); }
34351: 
34351:     /* Entry points / callbacks from the interpreter. */
35083:     JS_REQUIRES_STACK AbortableRecordingStatus monitorRecording(JSOp op);
33564:     JS_REQUIRES_STACK AbortableRecordingStatus record_EnterFrame(uintN& inlineCallCount);
33542:     JS_REQUIRES_STACK AbortableRecordingStatus record_LeaveFrame();
33542:     JS_REQUIRES_STACK AbortableRecordingStatus record_SetPropHit(JSPropCacheEntry* entry,
27933:                                                                   JSScopeProperty* sprop);
33542:     JS_REQUIRES_STACK AbortableRecordingStatus record_DefLocalFunSetSlot(uint32 slot, JSObject* obj);
33542:     JS_REQUIRES_STACK AbortableRecordingStatus record_NativeCallComplete();
33560:     void forgetGuardedShapesForObject(JSObject* obj);
18706: 
33157: #ifdef DEBUG
34351:     /* Debug printing functionality to emit printf() on trace. */
33272:     JS_REQUIRES_STACK void tprint(const char *format, int count, nanojit::LIns *insa[]);
33272:     JS_REQUIRES_STACK void tprint(const char *format);
33272:     JS_REQUIRES_STACK void tprint(const char *format, nanojit::LIns *ins);
33272:     JS_REQUIRES_STACK void tprint(const char *format, nanojit::LIns *ins1,
33272:                                   nanojit::LIns *ins2);
33272:     JS_REQUIRES_STACK void tprint(const char *format, nanojit::LIns *ins1,
33272:                                   nanojit::LIns *ins2, nanojit::LIns *ins3);
33272:     JS_REQUIRES_STACK void tprint(const char *format, nanojit::LIns *ins1,
33272:                                   nanojit::LIns *ins2, nanojit::LIns *ins3,
33157:                                   nanojit::LIns *ins4);
33272:     JS_REQUIRES_STACK void tprint(const char *format, nanojit::LIns *ins1,
33272:                                   nanojit::LIns *ins2, nanojit::LIns *ins3,
33157:                                   nanojit::LIns *ins4, nanojit::LIns *ins5);
33272:     JS_REQUIRES_STACK void tprint(const char *format, nanojit::LIns *ins1,
33272:                                   nanojit::LIns *ins2, nanojit::LIns *ins3,
33272:                                   nanojit::LIns *ins4, nanojit::LIns *ins5,
33272:                                   nanojit::LIns *ins6);
33157: #endif
34351: };
33157: 
35331: #define TRACING_ENABLED(cx)       ((cx)->jitEnabled)
19093: #define TRACE_RECORDER(cx)        (JS_TRACE_MONITOR(cx).recorder)
19093: #define SET_TRACE_RECORDER(cx,tr) (JS_TRACE_MONITOR(cx).recorder = (tr))
17186: 
27012: #define JSOP_IN_RANGE(op,lo,hi)   (uintN((op) - (lo)) <= uintN((hi) - (lo)))
27012: #define JSOP_IS_BINARY(op)        JSOP_IN_RANGE(op, JSOP_BITOR, JSOP_MOD)
27012: #define JSOP_IS_UNARY(op)         JSOP_IN_RANGE(op, JSOP_NEG, JSOP_POS)
27012: #define JSOP_IS_EQUALITY(op)      JSOP_IN_RANGE(op, JSOP_EQ, JSOP_NE)
21685: 
23111: #define TRACE_ARGS_(x,args)                                                   \
23110:     JS_BEGIN_MACRO                                                            \
33171:         if (TraceRecorder* tr_ = TRACE_RECORDER(cx)) {                        \
33542:             AbortableRecordingStatus status = tr_->record_##x args;           \
33542:             if (StatusAbortsRecording(status)) {                              \
37741:                 AbortRecording(cx, #x);                                       \
33542:                 if (status == ARECORD_ERROR)                                  \
27933:                     goto error;                                               \
27933:             }                                                                 \
33542:             JS_ASSERT(status != ARECORD_IMACRO);                              \
27933:         }                                                                     \
23110:     JS_END_MACRO
23110: 
23111: #define TRACE_ARGS(x,args)      TRACE_ARGS_(x, args)
19582: #define TRACE_0(x)              TRACE_ARGS(x, ())
19093: #define TRACE_1(x,a)            TRACE_ARGS(x, (a))
19093: #define TRACE_2(x,a,b)          TRACE_ARGS(x, (a, b))
19093: 
22652: extern JS_REQUIRES_STACK bool
37741: MonitorLoopEdge(JSContext* cx, uintN& inlineCallCount, RecordReason reason);
18683: 
22652: extern JS_REQUIRES_STACK void
37741: AbortRecording(JSContext* cx, const char* reason);
17350: 
17442: extern void
37741: InitJIT(TraceMonitor *tm);
17442: 
17726: extern void
37741: FinishJIT(TraceMonitor *tm);
17726: 
17976: extern void
37741: PurgeScriptFragments(JSContext* cx, JSScript* script);
24879: 
26826: extern bool
37741: OverfullJITCache(TraceMonitor* tm);
32767: 
32767: extern void
37741: FlushJITCache(JSContext* cx);
26826: 
24879: extern void
37741: PurgeJITOracle();
18277: 
24384: extern JSObject *
37741: GetBuiltinFunction(JSContext *cx, uintN index);
24384: 
27884: extern void
37741: SetMaxCodeCacheBytes(JSContext* cx, uint32 bytes);
27884: 
32709: extern bool
37741: NativeToValue(JSContext* cx, jsval& v, TraceType type, double* slot);
32709: 
39893: extern bool
39893: InCustomIterNextTryRegion(jsbytecode *pc);
39893: 
29368: #ifdef MOZ_TRACEVIS
29368: 
29368: extern JS_FRIEND_API(bool)
38585: StartTraceVis(const char* filename);
29368: 
29368: extern JS_FRIEND_API(JSBool)
38585: StartTraceVisNative(JSContext *cx, JSObject *obj, uintN argc, jsval *argv, jsval *rval);
29368: 
29368: extern JS_FRIEND_API(bool)
38585: StopTraceVis();
29368: 
29368: extern JS_FRIEND_API(JSBool)
38585: StopTraceVisNative(JSContext *cx, JSObject *obj, uintN argc, jsval *argv, jsval *rval);
29368: 
29368: /* Must contain no more than 16 items. */
29368: enum TraceVisState {
32748:     // Special: means we returned from current activity to last
29368:     S_EXITLAST,
32748:     // Activities
29368:     S_INTERP,
29368:     S_MONITOR,
29368:     S_RECORD,
29368:     S_COMPILE,
29368:     S_EXECUTE,
32748:     S_NATIVE,
32748:     // Events: these all have (bit 3) == 1.
32748:     S_RESET = 8
29368: };
29368: 
29368: /* Reason for an exit to the interpreter. */
29368: enum TraceVisExitReason {
29368:     R_NONE,
29368:     R_ABORT,
37741:     /* Reasons in MonitorLoopEdge */
29368:     R_INNER_SIDE_EXIT,
29368:     R_DOUBLES,
29368:     R_CALLBACK_PENDING,
29368:     R_OOM_GETANCHOR,
29368:     R_BACKED_OFF,
29368:     R_COLD,
29368:     R_FAIL_RECORD_TREE,
29368:     R_MAX_PEERS,
29368:     R_FAIL_EXECUTE_TREE,
29368:     R_FAIL_STABILIZE,
29368:     R_FAIL_EXTEND_FLUSH,
29368:     R_FAIL_EXTEND_MAX_BRANCHES,
29368:     R_FAIL_EXTEND_START,
29368:     R_FAIL_EXTEND_COLD,
29368:     R_NO_EXTEND_OUTER,
29368:     R_MISMATCH_EXIT,
29368:     R_OOM_EXIT,
29368:     R_TIMEOUT_EXIT,
29368:     R_DEEP_BAIL_EXIT,
29368:     R_STATUS_EXIT,
29368:     R_OTHER_EXIT
29368: };
29368: 
32748: enum TraceVisFlushReason {
32748:     FR_DEEP_BAIL,
32748:     FR_OOM,
32748:     FR_GLOBAL_SHAPE_MISMATCH,
32748:     FR_GLOBALS_FULL
32748: };
32748: 
31063: const unsigned long long MS64_MASK = 0xfull << 60;
31063: const unsigned long long MR64_MASK = 0x1full << 55;
29368: const unsigned long long MT64_MASK = ~(MS64_MASK | MR64_MASK);
29368: 
29368: extern FILE* traceVisLogFile;
31063: extern JSHashTable *traceVisScriptTable;
31063: 
31063: extern JS_FRIEND_API(void)
37741: StoreTraceVisState(JSContext *cx, TraceVisState s, TraceVisExitReason r);
29368: 
29368: static inline void
37741: LogTraceVisState(JSContext *cx, TraceVisState s, TraceVisExitReason r)
29368: {
29368:     if (traceVisLogFile) {
29368:         unsigned long long sllu = s;
29368:         unsigned long long rllu = r;
29368:         unsigned long long d = (sllu << 60) | (rllu << 55) | (rdtsc() & MT64_MASK);
29368:         fwrite(&d, sizeof(d), 1, traceVisLogFile);
29368:     }
31063:     if (traceVisScriptTable) {
37741:         StoreTraceVisState(cx, s, r);
31063:     }
29368: }
29368: 
32748: /*
37741:  * Although this runs the same code as LogTraceVisState, it is a separate
32748:  * function because the meaning of the log entry is different. Also, the entry
32748:  * formats may diverge someday.
32748:  */
32748: static inline void
37741: LogTraceVisEvent(JSContext *cx, TraceVisState s, TraceVisFlushReason r)
32748: {
37741:     LogTraceVisState(cx, s, (TraceVisExitReason) r);
32748: }
32748: 
29368: static inline void
37741: EnterTraceVisState(JSContext *cx, TraceVisState s, TraceVisExitReason r)
29368: {
37741:     LogTraceVisState(cx, s, r);
29368: }
29368: 
29368: static inline void
37741: ExitTraceVisState(JSContext *cx, TraceVisExitReason r)
29368: {
37741:     LogTraceVisState(cx, S_EXITLAST, r);
29368: }
29368: 
29368: struct TraceVisStateObj {
29368:     TraceVisExitReason r;
31063:     JSContext *mCx;
29368: 
31063:     inline TraceVisStateObj(JSContext *cx, TraceVisState s) : r(R_NONE)
29368:     {
37741:         EnterTraceVisState(cx, s, R_NONE);
31063:         mCx = cx;
29368:     }
29368:     inline ~TraceVisStateObj()
29368:     {
37741:         ExitTraceVisState(mCx, r);
29368:     }
29368: };
29368: 
29368: #endif /* MOZ_TRACEVIS */
29368: 
37741: }      /* namespace js */
32581: 
19171: #else  /* !JS_TRACER */
19171: 
19599: #define TRACE_0(x)              ((void)0)
19171: #define TRACE_1(x,a)            ((void)0)
19171: #define TRACE_2(x,a,b)          ((void)0)
19171: 
19171: #endif /* !JS_TRACER */
18091: 
17196: #endif /* jstracer_h___ */
