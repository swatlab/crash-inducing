50491: /*
50491:  * Copyright (C) 2008 Apple Inc. All rights reserved.
50491:  *
50491:  * Redistribution and use in source and binary forms, with or without
50491:  * modification, are permitted provided that the following conditions
50491:  * are met:
50491:  * 1. Redistributions of source code must retain the above copyright
50491:  *    notice, this list of conditions and the following disclaimer.
50491:  * 2. Redistributions in binary form must reproduce the above copyright
50491:  *    notice, this list of conditions and the following disclaimer in the
50491:  *    documentation and/or other materials provided with the distribution.
50491:  *
50491:  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS'' AND ANY
50491:  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
50491:  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
50491:  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
50491:  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
50491:  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
50491:  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
50491:  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
50491:  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
50491:  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
50491:  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. 
50491:  */
50491: 
50491: #ifndef ExecutableAllocator_h
50491: #define ExecutableAllocator_h
50491: 
50491: #include <stddef.h> // for ptrdiff_t
50491: #include <limits>
80202: 
80202: #include "jsalloc.h"
80202: #include "jsapi.h"
80202: #include "jsprvtd.h"
80202: 
50646: #include "assembler/wtf/Assertions.h"
80202: #include "js/HashTable.h"
80202: #include "js/Vector.h"
50491: 
68931: #if WTF_CPU_SPARC
68931: #ifdef linux  // bugzilla 502369
68931: static void sync_instruction_memory(caddr_t v, u_int len)
68931: {
68931:     caddr_t end = v + len;
68931:     caddr_t p = v;
68931:     while (p < end) {
68931:         asm("flush %0" : : "r" (p));
68931:         p += 32;
68931:     }
68931: }
68931: #else
68931: extern  "C" void sync_instruction_memory(caddr_t v, u_int len);
68931: #endif
68931: #endif
68931: 
71317: #if WTF_OS_IOS
50491: #include <libkern/OSCacheControl.h>
50491: #include <sys/mman.h>
50491: #endif
50491: 
71317: #if WTF_OS_SYMBIAN
50491: #include <e32std.h>
50491: #endif
50491: 
71317: #if WTF_CPU_MIPS && WTF_OS_LINUX
50491: #include <sys/cachectl.h>
50491: #endif
50491: 
50491: #if ENABLE_ASSEMBLER_WX_EXCLUSIVE
50491: #define PROTECTION_FLAGS_RW (PROT_READ | PROT_WRITE)
50491: #define PROTECTION_FLAGS_RX (PROT_READ | PROT_EXEC)
50491: #define INITIAL_PROTECTION_FLAGS PROTECTION_FLAGS_RX
50491: #else
50491: #define INITIAL_PROTECTION_FLAGS (PROT_READ | PROT_WRITE | PROT_EXEC)
50491: #endif
50491: 
50491: #if ENABLE_ASSEMBLER
50491: 
58064: //#define DEBUG_STRESS_JSC_ALLOCATOR
58064: 
50491: namespace JSC {
50491: 
69289:   class ExecutableAllocator;
69289: 
77559:   enum CodeKind { METHOD_CODE, REGEXP_CODE };
77559: 
64265:   // These are reference-counted. A new one starts with a count of 1. 
50491:   class ExecutablePool {
64559: 
64559:     JS_DECLARE_ALLOCATION_FRIENDS_FOR_PRIVATE_CONSTRUCTOR;
64243:     friend class ExecutableAllocator;
50491: private:
50491:     struct Allocation {
50491:         char* pages;
50491:         size_t size;
71317: #if WTF_OS_SYMBIAN
50491:         RChunk* chunk;
50491: #endif
50491:     };
64243: 
69289:     ExecutableAllocator* m_allocator;
64243:     char* m_freePtr;
64243:     char* m_end;
64243:     Allocation m_allocation;
50491: 
50491:     // Reference count for automatic reclamation.
60168:     unsigned m_refCount;
50491:  
77559:     // Number of bytes currently used for Method and Regexp JIT code.
77559:     size_t m_mjitCodeMethod;
77559:     size_t m_mjitCodeRegexp;
77559: 
50491: public:
64243:     // Flag for downstream use, whether to try to release references to this pool.
64243:     bool m_destroy;
64243: 
64243:     // GC number in which the m_destroy flag was most recently set. Used downstream to
64243:     // remember whether m_destroy was computed for the currently active GC.
64243:     size_t m_gcNumber;
64243: 
64269:     void release(bool willDestroy = false)
64269:     { 
64269:         JS_ASSERT(m_refCount != 0);
70260:         // XXX: disabled, see bug 654820.
70260:         //JS_ASSERT_IF(willDestroy, m_refCount == 1);
64269:         if (--m_refCount == 0) {
64559:             js::UnwantedForeground::delete_(this);
64269:         }
64269:     }
64269: 
64269: private:
50491:     // It should be impossible for us to roll over, because only small
50491:     // pools have multiple holders, and they have one holder per chunk
50491:     // of generated code, and they only hold 16KB or so of code.
60168:     void addRef()
60168:     {
60168:         JS_ASSERT(m_refCount);
60168:         ++m_refCount;
60168:     }
60168: 
69289:     ExecutablePool(ExecutableAllocator* allocator, Allocation a)
69289:       : m_allocator(allocator), m_freePtr(a.pages), m_end(m_freePtr + a.size), m_allocation(a),
77559:         m_refCount(1), m_mjitCodeMethod(0), m_mjitCodeRegexp(0), m_destroy(false), m_gcNumber(0)
64265:     { }
64265: 
69289:     ~ExecutablePool();
50491: 
77559:     void* alloc(size_t n, CodeKind kind)
50491:     {
64243:         JS_ASSERT(n <= available());
50491:         void *result = m_freePtr;
50491:         m_freePtr += n;
77559: 
77559:         if ( kind == REGEXP_CODE )
77559:             m_mjitCodeRegexp += n;
77559:         else
77559:             m_mjitCodeMethod += n;
77559: 
50491:         return result;
50491:     }
50491:     
64243:     size_t available() const { 
64243:         JS_ASSERT(m_end >= m_freePtr);
64243:         return m_end - m_freePtr;
64243:     }
50491: };
50491: 
90106: enum AllocationBehavior
90106: {
90106:     AllocationCanRandomize,
90106:     AllocationDeterministic
90106: };
90106: 
50491: class ExecutableAllocator {
80222:     typedef void (*DestroyCallback)(void* addr, size_t size);
69289:     enum ProtectionSetting { Writable, Executable };
80222:     DestroyCallback destroyCallback;
50491: 
90106:     void initSeed();
90106: 
50491: public:
90106:     explicit ExecutableAllocator(AllocationBehavior allocBehavior)
90106:       : destroyCallback(NULL),
90106:         allocBehavior(allocBehavior)
50491:     {
64268:         if (!pageSize) {
64267:             pageSize = determinePageSize();
64268:             /*
64268:              * On Windows, VirtualAlloc effectively allocates in 64K chunks.
64268:              * (Technically, it allocates in page chunks, but the starting
64268:              * address is always a multiple of 64K, so each allocation uses up
64268:              * 64K of address space.)  So a size less than that would be
64268:              * pointless.  But it turns out that 64KB is a reasonable size for
64268:              * all platforms.  (This assumes 4KB pages.)
64268:              */
64268:             largeAllocSize = pageSize * 16;
64268:         }
64268: 
90106: #if WTF_OS_WINDOWS
90106:         initSeed();
90106: #endif
90106: 
69289:         JS_ASSERT(m_smallPools.empty());
50491:     }
50491: 
58751:     ~ExecutableAllocator()
58751:     {
69289:         for (size_t i = 0; i < m_smallPools.length(); i++)
69289:             m_smallPools[i]->release(/* willDestroy = */true);
70257:         // XXX: temporarily disabled because it fails;  see bug 654820.
70257:         //JS_ASSERT(m_pools.empty());     // if this asserts we have a pool leak
58751:     }
50491: 
64243:     // alloc() returns a pointer to some memory, and also (by reference) a
64243:     // pointer to reference-counted pool. The caller owns a reference to the
64243:     // pool; i.e. alloc() increments the count before returning the object.
77559:     void* alloc(size_t n, ExecutablePool** poolp, CodeKind type)
64243:     {
64243:         // Round 'n' up to a multiple of word size; if all allocations are of
64243:         // word sized quantities, then all subsequent allocations will be
64243:         // aligned.
64243:         n = roundUpAllocationSize(n, sizeof(void*));
64243:         if (n == OVERSIZE_ALLOCATION) {
64243:             *poolp = NULL;
64243:             return NULL;
64243:         }
50491: 
64243:         *poolp = poolForSize(n);
64243:         if (!*poolp)
64243:             return NULL;
64243: 
64243:         // This alloc is infallible because poolForSize() just obtained
64243:         // (found, or created if necessary) a pool that had enough space.
77559:         void *result = (*poolp)->alloc(n, type);
64243:         JS_ASSERT(result);
64243:         return result;
64243:     }
64243: 
69289:     void releasePoolPages(ExecutablePool *pool) {
69289:         JS_ASSERT(pool->m_allocation.pages);
80222:         if (destroyCallback)
80222:             destroyCallback(pool->m_allocation.pages, pool->m_allocation.size);
69289:         systemRelease(pool->m_allocation);
99965:         JS_ASSERT(m_pools.initialized());
69289:         m_pools.remove(m_pools.lookup(pool));   // this asserts if |pool| is not in m_pools
69289:     }
69289: 
83122:     void sizeOfCode(size_t *method, size_t *regexp, size_t *unused) const;
69289: 
80222:     void setDestroyCallback(DestroyCallback destroyCallback) {
80222:         this->destroyCallback = destroyCallback;
80222:     }
80222: 
90106:     void setRandomize(bool enabled) {
90106:         allocBehavior = enabled ? AllocationCanRandomize : AllocationDeterministic;
90106:     }
90106: 
64243: private:
64268:     static size_t pageSize;
64268:     static size_t largeAllocSize;
90106: #if WTF_OS_WINDOWS
90106:     static int64_t rngSeed;
90106: #endif
64268: 
64266:     static const size_t OVERSIZE_ALLOCATION = size_t(-1);
64266: 
64266:     static size_t roundUpAllocationSize(size_t request, size_t granularity)
64266:     {
64266:         // Something included via windows.h defines a macro with this name,
64266:         // which causes the function below to fail to compile.
64266:         #ifdef _MSC_VER
64266:         # undef max
64266:         #endif
64266: 
64266:         if ((std::numeric_limits<size_t>::max() - granularity) <= request)
64266:             return OVERSIZE_ALLOCATION;
64266:         
64266:         // Round up to next page boundary
64266:         size_t size = request + (granularity - 1);
64266:         size = size & ~(granularity - 1);
64266:         JS_ASSERT(size >= request);
64266:         return size;
64266:     }
64266: 
69289:     // On OOM, this will return an Allocation where pages is NULL.
90106:     ExecutablePool::Allocation systemAlloc(size_t n);
69289:     static void systemRelease(const ExecutablePool::Allocation& alloc);
90106:     void *computeRandomAllocationAddress();
69289: 
64265:     ExecutablePool* createPool(size_t n)
64265:     {
64268:         size_t allocSize = roundUpAllocationSize(n, pageSize);
64265:         if (allocSize == OVERSIZE_ALLOCATION)
64265:             return NULL;
69289: 
69289:         if (!m_pools.initialized() && !m_pools.init())
69289:             return NULL;
69289: 
64265: #ifdef DEBUG_STRESS_JSC_ALLOCATOR
69289:         ExecutablePool::Allocation a = systemAlloc(size_t(4294967291));
64265: #else
69289:         ExecutablePool::Allocation a = systemAlloc(allocSize);
64265: #endif
64265:         if (!a.pages)
64265:             return NULL;
64265: 
69289:         ExecutablePool *pool = js::OffTheBooks::new_<ExecutablePool>(this, a);
69289:         if (!pool) {
69289:             systemRelease(a);
69289:             return NULL;
69289:         }
69289:         m_pools.put(pool);
69289:         return pool;
64265:     }
64265: 
71317: public:
50491:     ExecutablePool* poolForSize(size_t n)
50491:     {
58064: #ifndef DEBUG_STRESS_JSC_ALLOCATOR
58751:         // Try to fit in an existing small allocator.  Use the pool with the
58751:         // least available space that is big enough (best-fit).  This is the
58751:         // best strategy because (a) it maximizes the chance of the next
58751:         // allocation fitting in a small pool, and (b) it minimizes the
58751:         // potential waste when a small pool is next abandoned.
58751:         ExecutablePool *minPool = NULL;
69289:         for (size_t i = 0; i < m_smallPools.length(); i++) {
69289:             ExecutablePool *pool = m_smallPools[i];
58751:             if (n <= pool->available() && (!minPool || pool->available() < minPool->available()))
58751:                 minPool = pool;
58751:         }
58751:         if (minPool) {
58751:             minPool->addRef();
58751:             return minPool;
50491:         }
58064: #endif
50491: 
50491:         // If the request is large, we just provide a unshared allocator
64268:         if (n > largeAllocSize)
64265:             return createPool(n);
50491: 
50491:         // Create a new allocator
64268:         ExecutablePool* pool = createPool(largeAllocSize);
57437:         if (!pool)
57437:             return NULL;
50491:   	    // At this point, local |pool| is the owner.
50491: 
69289:         if (m_smallPools.length() < maxSmallPools) {
58751:             // We haven't hit the maximum number of live pools;  add the new pool.
69289:             m_smallPools.append(pool);
50491:             pool->addRef();
58751:         } else {
58751:             // Find the pool with the least space.
58751:             int iMin = 0;
69289:             for (size_t i = 1; i < m_smallPools.length(); i++)
69289:                 if (m_smallPools[i]->available() <
69289:                     m_smallPools[iMin]->available())
58751:                 {
58751:                     iMin = i;
58751:                 }
58751: 
58751:             // If the new allocator will result in more free space than the small
58751:             // pool with the least space, then we will use it instead
69289:             ExecutablePool *minPool = m_smallPools[iMin];
58751:             if ((pool->available() - n) > minPool->available()) {
58751:                 minPool->release();
69289:                 m_smallPools[iMin] = pool;
58751:                 pool->addRef();
58751:             }
50491:         }
50491: 
50491:    	    // Pass ownership to the caller.
50491:         return pool;
50491:     }
50491: 
50491: #if ENABLE_ASSEMBLER_WX_EXCLUSIVE
50491:     static void makeWritable(void* start, size_t size)
50491:     {
50491:         reprotectRegion(start, size, Writable);
50491:     }
50491: 
50491:     static void makeExecutable(void* start, size_t size)
50491:     {
50491:         reprotectRegion(start, size, Executable);
50491:     }
50491: #else
50491:     static void makeWritable(void*, size_t) {}
50491:     static void makeExecutable(void*, size_t) {}
50491: #endif
50491: 
50491: 
50491: #if WTF_CPU_X86 || WTF_CPU_X86_64
50491:     static void cacheFlush(void*, size_t)
50491:     {
50491:     }
50491: #elif WTF_CPU_MIPS
50491:     static void cacheFlush(void* code, size_t size)
50491:     {
50491: #if WTF_COMPILER_GCC && (GCC_VERSION >= 40300)
50491: #if WTF_MIPS_ISA_REV(2) && (GCC_VERSION < 40403)
50491:         int lineSize;
50491:         asm("rdhwr %0, $1" : "=r" (lineSize));
50491:         //
50491:         // Modify "start" and "end" to avoid GCC 4.3.0-4.4.2 bug in
50491:         // mips_expand_synci_loop that may execute synci one more time.
77559:         // "start" points to the first byte of the cache line.
50491:         // "end" points to the last byte of the line before the last cache line.
50491:         // Because size is always a multiple of 4, this is safe to set
50491:         // "end" to the last byte.
50491:         //
50491:         intptr_t start = reinterpret_cast<intptr_t>(code) & (-lineSize);
50491:         intptr_t end = ((reinterpret_cast<intptr_t>(code) + size - 1) & (-lineSize)) - 1;
50491:         __builtin___clear_cache(reinterpret_cast<char*>(start), reinterpret_cast<char*>(end));
50491: #else
50491:         intptr_t end = reinterpret_cast<intptr_t>(code) + size;
50491:         __builtin___clear_cache(reinterpret_cast<char*>(code), reinterpret_cast<char*>(end));
50491: #endif
50491: #else
50491:         _flush_cache(reinterpret_cast<char*>(code), size, BCACHE);
50491: #endif
50491:     }
77113: #elif WTF_CPU_ARM && WTF_OS_IOS
50491:     static void cacheFlush(void* code, size_t size)
50491:     {
50491:         sys_dcache_flush(code, size);
50491:         sys_icache_invalidate(code, size);
50491:     }
71317: #elif WTF_CPU_ARM_THUMB2 && WTF_IOS
50491:     static void cacheFlush(void* code, size_t size)
50491:     {
50491:         asm volatile (
50491:             "push    {r7}\n"
50491:             "mov     r0, %0\n"
50491:             "mov     r1, %1\n"
50491:             "movw    r7, #0x2\n"
50491:             "movt    r7, #0xf\n"
50491:             "movs    r2, #0x0\n"
50491:             "svc     0x0\n"
50491:             "pop     {r7}\n"
50491:             :
50491:             : "r" (code), "r" (reinterpret_cast<char*>(code) + size)
50491:             : "r0", "r1", "r2");
50491:     }
71317: #elif WTF_OS_SYMBIAN
50491:     static void cacheFlush(void* code, size_t size)
50491:     {
50491:         User::IMB_Range(code, static_cast<char*>(code) + size);
50491:     }
71317: #elif WTF_CPU_ARM_TRADITIONAL && WTF_OS_LINUX && WTF_COMPILER_RVCT
50491:     static __asm void cacheFlush(void* code, size_t size);
71317: #elif WTF_CPU_ARM_TRADITIONAL && (WTF_OS_LINUX || WTF_OS_ANDROID) && WTF_COMPILER_GCC
50491:     static void cacheFlush(void* code, size_t size)
50491:     {
50491:         asm volatile (
50491:             "push    {r7}\n"
50491:             "mov     r0, %0\n"
50491:             "mov     r1, %1\n"
50491:             "mov     r7, #0xf0000\n"
50491:             "add     r7, r7, #0x2\n"
50491:             "mov     r2, #0x0\n"
50491:             "svc     0x0\n"
50491:             "pop     {r7}\n"
50491:             :
50491:             : "r" (code), "r" (reinterpret_cast<char*>(code) + size)
50491:             : "r0", "r1", "r2");
50491:     }
68931: #elif WTF_CPU_SPARC
68931:     static void cacheFlush(void* code, size_t size)
68931:     {
68931:         sync_instruction_memory((caddr_t)code, size);
68931:     }
50491: #endif
50491: 
50491: private:
50491: 
50491: #if ENABLE_ASSEMBLER_WX_EXCLUSIVE
69289:     static void reprotectRegion(void*, size_t, ProtectionSetting);
50491: #endif
50491: 
69289:     // These are strong references;  they keep pools alive.
58751:     static const size_t maxSmallPools = 4;
58751:     typedef js::Vector<ExecutablePool *, maxSmallPools, js::SystemAllocPolicy> SmallExecPoolVector;
69289:     SmallExecPoolVector m_smallPools;
69289: 
69289:     // All live pools are recorded here, just for stats purposes.  These are
69289:     // weak references;  they don't keep pools alive.  When a pool is destroyed
69289:     // its reference is removed from m_pools.
69289:     typedef js::HashSet<ExecutablePool *, js::DefaultHasher<ExecutablePool *>, js::SystemAllocPolicy>
69289:             ExecPoolHashSet;
69289:     ExecPoolHashSet m_pools;    // All pools, just for stats purposes.
90106:     AllocationBehavior allocBehavior;
69289: 
64267:     static size_t determinePageSize();
50491: };
50491: 
50491: }
50491: 
50491: #endif // ENABLE(ASSEMBLER)
50491: 
50491: #endif // !defined(ExecutableAllocator)
