52826: /* -*- Mode: C++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*-
52826:  * vim: set ts=4 sw=4 et tw=99:
52826:  *
52826:  * ***** BEGIN LICENSE BLOCK *****
52826:  * Version: MPL 1.1/GPL 2.0/LGPL 2.1
52826:  *
52826:  * The contents of this file are subject to the Mozilla Public License Version
52826:  * 1.1 (the "License"); you may not use this file except in compliance with
52826:  * the License. You may obtain a copy of the License at
52826:  * http://www.mozilla.org/MPL/
52826:  *
52826:  * Software distributed under the License is distributed on an "AS IS" basis,
52826:  * WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
52826:  * for the specific language governing rights and limitations under the
52826:  * License.
52826:  *
52826:  * The Original Code is Mozilla SpiderMonkey JavaScript 1.9 code, released
52826:  * May 28, 2008.
52826:  *
52826:  * The Initial Developer of the Original Code is
52826:  *   Brendan Eich <brendan@mozilla.org>
52826:  *
52826:  * Contributor(s):
52826:  *   David Anderson <danderson@mozilla.com>
52826:  *   David Mandelin <dmandelin@mozilla.com>
52826:  *
52826:  * Alternatively, the contents of this file may be used under the terms of
52826:  * either of the GNU General Public License Version 2 or later (the "GPL"),
52826:  * or the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
52826:  * in which case the provisions of the GPL or the LGPL are applicable instead
52826:  * of those above. If you wish to allow use of your version of this file only
52826:  * under the terms of either the GPL or the LGPL, and not to allow others to
52826:  * use your version of this file under the terms of the MPL, indicate your
52826:  * decision by deleting the provisions above and replace them with the notice
52826:  * and other provisions required by the GPL or the LGPL. If you do not delete
52826:  * the provisions above, a recipient may use your version of this file under
52826:  * the terms of any one of the MPL, the GPL or the LGPL.
52826:  *
52826:  * ***** END LICENSE BLOCK ***** */
52826: #include "jsscope.h"
52826: #include "jsnum.h"
52826: #include "MonoIC.h"
52826: #include "StubCalls.h"
53590: #include "StubCalls-inl.h"
53301: #include "assembler/assembler/LinkBuffer.h"
53301: #include "assembler/assembler/MacroAssembler.h"
53590: #include "assembler/assembler/CodeLocation.h"
60591: #include "methodjit/CodeGenIncludes.h"
53590: #include "methodjit/Compiler.h"
60591: #include "methodjit/ICRepatcher.h"
62385: #include "methodjit/PolyIC.h"
53590: #include "InlineFrameAssembler.h"
53263: #include "jsobj.h"
53840: 
53840: #include "jsinterpinlines.h"
53263: #include "jsobjinlines.h"
52826: #include "jsscopeinlines.h"
53590: #include "jsscriptinlines.h"
52826: 
52826: using namespace js;
52826: using namespace js::mjit;
52826: using namespace js::mjit::ic;
52826: 
53590: typedef JSC::MacroAssembler::RegisterID RegisterID;
53590: typedef JSC::MacroAssembler::Address Address;
53590: typedef JSC::MacroAssembler::Jump Jump;
53590: typedef JSC::MacroAssembler::Imm32 Imm32;
53590: typedef JSC::MacroAssembler::ImmPtr ImmPtr;
53590: typedef JSC::MacroAssembler::Call Call;
62385: typedef JSC::MacroAssembler::Label Label;
62385: typedef JSC::MacroAssembler::DataLabel32 DataLabel32;
53590: 
53119: #if defined JS_MONOIC
53119: 
52826: static void
62386: PatchGetFallback(VMFrame &f, ic::GetGlobalNameIC *ic)
52826: {
58063:     Repatcher repatch(f.jit());
52826:     JSC::FunctionPtr fptr(JS_FUNC_TO_DATA_PTR(void *, stubs::GetGlobalName));
62386:     repatch.relink(ic->slowPathCall, fptr);
52826: }
52826: 
52826: void JS_FASTCALL
62386: ic::GetGlobalName(VMFrame &f, ic::GetGlobalNameIC *ic)
52826: {
53840:     JSObject *obj = f.fp()->scopeChain().getGlobal();
53840:     JSAtom *atom = f.fp()->script()->getAtom(GET_INDEX(f.regs.pc));
52826:     jsid id = ATOM_TO_JSID(atom);
52826: 
53531:     const Shape *shape = obj->nativeLookup(id);
53531:     if (!shape ||
53531:         !shape->hasDefaultGetterOrIsMethod() ||
53531:         !shape->hasSlot())
52826:     {
53531:         if (shape)
55503:             PatchGetFallback(f, ic);
52826:         stubs::GetGlobalName(f);
52826:         return;
52826:     }
53531:     uint32 slot = shape->slot;
52826: 
52826:     /* Patch shape guard. */
58063:     Repatcher repatcher(f.jit());
62386:     repatcher.repatch(ic->fastPathStart.dataLabel32AtOffset(ic->shapeOffset), obj->shape());
52826: 
52826:     /* Patch loads. */
62386:     JSC::CodeLocationLabel label = ic->fastPathStart.labelAtOffset(ic->loadStoreOffset);
62386:     repatcher.patchAddressOffsetForValueLoad(label, slot * sizeof(Value));
52826: 
52826:     /* Do load anyway... this time. */
52826:     stubs::GetGlobalName(f);
52826: }
52826: 
57784: template <JSBool strict>
52831: static void JS_FASTCALL
62386: DisabledSetGlobal(VMFrame &f, ic::SetGlobalNameIC *ic)
52831: {
54169:     JSScript *script = f.fp()->script();
54169:     JSAtom *atom = script->getAtom(GET_INDEX(f.regs.pc));
57784:     stubs::SetGlobalName<strict>(f, atom);
52831: }
52831: 
62386: template void JS_FASTCALL DisabledSetGlobal<true>(VMFrame &f, ic::SetGlobalNameIC *ic);
62386: template void JS_FASTCALL DisabledSetGlobal<false>(VMFrame &f, ic::SetGlobalNameIC *ic);
57784: 
57784: template <JSBool strict>
57784: static void JS_FASTCALL
62386: DisabledSetGlobalNoCache(VMFrame &f, ic::SetGlobalNameIC *ic)
57784: {
57784:     JSScript *script = f.fp()->script();
57784:     JSAtom *atom = script->getAtom(GET_INDEX(f.regs.pc));
57784:     stubs::SetGlobalNameNoCache<strict>(f, atom);
57784: }
57784: 
62386: template void JS_FASTCALL DisabledSetGlobalNoCache<true>(VMFrame &f, ic::SetGlobalNameIC *ic);
62386: template void JS_FASTCALL DisabledSetGlobalNoCache<false>(VMFrame &f, ic::SetGlobalNameIC *ic);
57784: 
52831: static void
62386: PatchSetFallback(VMFrame &f, ic::SetGlobalNameIC *ic)
52831: {
57784:     JSScript *script = f.fp()->script();
57784: 
58063:     Repatcher repatch(f.jit());
62386:     VoidStubSetGlobal stub = ic->usePropertyCache
57784:                              ? STRICT_VARIANT(DisabledSetGlobal)
57784:                              : STRICT_VARIANT(DisabledSetGlobalNoCache);
57784:     JSC::FunctionPtr fptr(JS_FUNC_TO_DATA_PTR(void *, stub));
62386:     repatch.relink(ic->slowPathCall, fptr);
62386: }
62386: 
62386: void
62386: SetGlobalNameIC::patchExtraShapeGuard(Repatcher &repatcher, int32 shape)
62386: {
62409:     JS_ASSERT(hasExtraStub);
62386: 
62386:     JSC::CodeLocationLabel label(JSC::MacroAssemblerCodePtr(extraStub.start()));
62386:     repatcher.repatch(label.dataLabel32AtOffset(extraShapeGuard), shape);
62386: }
62386: 
62386: void
62386: SetGlobalNameIC::patchInlineShapeGuard(Repatcher &repatcher, int32 shape)
62386: {
62386:     JSC::CodeLocationDataLabel32 label = fastPathStart.dataLabel32AtOffset(shapeOffset);
62386:     repatcher.repatch(label, shape);
52831: }
52831: 
62385: static LookupStatus
62386: UpdateSetGlobalNameStub(VMFrame &f, ic::SetGlobalNameIC *ic, JSObject *obj, const Shape *shape)
62385: {
62385:     Repatcher repatcher(ic->extraStub);
62385: 
62386:     ic->patchExtraShapeGuard(repatcher, obj->shape());
62386: 
62385:     JSC::CodeLocationLabel label(JSC::MacroAssemblerCodePtr(ic->extraStub.start()));
62385:     label = label.labelAtOffset(ic->extraStoreOffset);
62385:     repatcher.patchAddressOffsetForValueStore(label, shape->slot * sizeof(Value),
62385:                                               ic->vr.isTypeKnown());
62385: 
62385:     return Lookup_Cacheable;
62385: }
62385: 
62385: static LookupStatus
62386: AttachSetGlobalNameStub(VMFrame &f, ic::SetGlobalNameIC *ic, JSObject *obj, const Shape *shape)
62385: {
62385:     Assembler masm;
62385: 
62385:     Label start = masm.label();
62385: 
62385:     DataLabel32 shapeLabel;
62385:     Jump guard = masm.branch32WithPatch(Assembler::NotEqual, ic->shapeReg, Imm32(obj->shape()),
62385:                                         shapeLabel);
62385: 
62385:     /* A constant object needs rematerialization. */
62385:     if (ic->objConst)
62385:         masm.move(ImmPtr(obj), ic->objReg);
62385: 
62385:     JS_ASSERT(obj->branded());
62385: 
62385:     /*
62385:      * Load obj->slots. If ic->objConst, then this clobbers objReg, because
62385:      * ic->objReg == ic->shapeReg.
62385:      */
62385:     masm.loadPtr(Address(ic->objReg, offsetof(JSObject, slots)), ic->shapeReg);
62385: 
62385:     /* Test if overwriting a function-tagged slot. */
62385:     Address slot(ic->shapeReg, sizeof(Value) * shape->slot);
62385:     Jump isNotObject = masm.testObject(Assembler::NotEqual, slot);
62385: 
62385:     /* Now, test if the object is a function object. */
62385:     masm.loadPayload(slot, ic->shapeReg);
62385:     Jump isFun = masm.testFunction(Assembler::Equal, ic->shapeReg);
62385: 
62385:     /* Restore shapeReg to obj->slots, since we clobbered it. */
62385:     if (ic->objConst)
62385:         masm.move(ImmPtr(obj), ic->objReg);
62385:     masm.loadPtr(Address(ic->objReg, offsetof(JSObject, slots)), ic->shapeReg);
62385: 
62385:     /* If the object test fails, shapeReg is still obj->slots. */
62385:     isNotObject.linkTo(masm.label(), &masm);
62385:     DataLabel32 store = masm.storeValueWithAddressOffsetPatch(ic->vr, slot);
62385: 
62385:     Jump done = masm.jump();
62385: 
62385:     JITScript *jit = f.jit();
62385:     LinkerHelper linker(masm);
62385:     JSC::ExecutablePool *ep = linker.init(f.cx);
62385:     if (!ep)
62385:         return Lookup_Error;
62385:     if (!jit->execPools.append(ep)) {
62385:         ep->release();
62385:         js_ReportOutOfMemory(f.cx);
62385:         return Lookup_Error;
62385:     }
62385: 
62385:     if (!linker.verifyRange(jit)) {
62385:         ep->release();
62385:         return Lookup_Uncacheable;
62385:     }
62385: 
62385:     linker.link(done, ic->fastPathStart.labelAtOffset(ic->fastRejoinOffset));
62385:     linker.link(guard, ic->slowPathStart);
62385:     linker.link(isFun, ic->slowPathStart);
62385: 
62385:     JSC::CodeLocationLabel cs = linker.finalize();
62385:     JaegerSpew(JSpew_PICs, "generated setgname stub at %p\n", cs.executableAddress());
62385: 
62385:     Repatcher repatcher(f.jit());
62385:     repatcher.relink(ic->fastPathStart.jumpAtOffset(ic->inlineShapeJump), cs);
62385: 
62385:     int offset = linker.locationOf(shapeLabel) - linker.locationOf(start);
62385:     ic->extraShapeGuard = offset;
62385:     JS_ASSERT(ic->extraShapeGuard == offset);
62385: 
62385:     ic->extraStub = JSC::JITCode(cs.executableAddress(), linker.size());
62385:     offset = linker.locationOf(store) - linker.locationOf(start);
62385:     ic->extraStoreOffset = offset;
62385:     JS_ASSERT(ic->extraStoreOffset == offset);
62385: 
62409:     ic->hasExtraStub = true;
62409: 
62385:     return Lookup_Cacheable;
62385: }
62385: 
62385: static LookupStatus
62386: UpdateSetGlobalName(VMFrame &f, ic::SetGlobalNameIC *ic, JSObject *obj, const Shape *shape)
62385: {
62385:     /* Give globals a chance to appear. */
62385:     if (!shape)
62385:         return Lookup_Uncacheable;
62385: 
62385:     if (shape->isMethod() ||
62385:         !shape->hasDefaultSetter() ||
62385:         !shape->writable() ||
62385:         !shape->hasSlot())
62385:     {
62385:         /* Disable the IC for weird shape attributes. */
62385:         PatchSetFallback(f, ic);
62385:         return Lookup_Uncacheable;
62385:     }
62385: 
62385:     /* Branded sets must guard that they don't overwrite method-valued properties. */
62385:     if (obj->branded()) {
62385:         /*
62385:          * If this slot has a function valued property, the tail of this opcode
62385:          * could change the shape. Even if it doesn't, the IC is probably
62385:          * pointless, because it will always hit the function-test path and
62385:          * bail out. In these cases, don't bother building or updating the IC.
62385:          */
62385:         const Value &v = obj->getSlot(shape->slot);
62385:         if (v.isObject() && v.toObject().isFunction()) {
62385:             /*
62385:              * If we're going to rebrand, the object may unbrand, allowing this
62385:              * IC to come back to life. In that case, we don't disable the IC.
62385:              */
62385:             if (!ChangesMethodValue(v, f.regs.sp[-1]))
62385:                 PatchSetFallback(f, ic);
62385:             return Lookup_Uncacheable;
62385:         }
62385: 
62409:         if (ic->hasExtraStub)
62385:             return UpdateSetGlobalNameStub(f, ic, obj, shape);
62385: 
62385:         return AttachSetGlobalNameStub(f, ic, obj, shape);
62385:     }
62385: 
62385:     /* Object is not branded, so we can use the inline path. */
62385:     Repatcher repatcher(f.jit());
62386:     ic->patchInlineShapeGuard(repatcher, obj->shape());
62386: 
62386:     JSC::CodeLocationLabel label = ic->fastPathStart.labelAtOffset(ic->loadStoreOffset);
62386:     repatcher.patchAddressOffsetForValueStore(label, shape->slot * sizeof(Value),
62385:                                               ic->vr.isTypeKnown());
62385: 
62385:     return Lookup_Cacheable;
62385: }
62385: 
52831: void JS_FASTCALL
62386: ic::SetGlobalName(VMFrame &f, ic::SetGlobalNameIC *ic)
52831: {
53840:     JSObject *obj = f.fp()->scopeChain().getGlobal();
57784:     JSScript *script = f.fp()->script();
57784:     JSAtom *atom = script->getAtom(GET_INDEX(f.regs.pc));
62385:     const Shape *shape = obj->nativeLookup(ATOM_TO_JSID(atom));
52831: 
62386:     LookupStatus status = UpdateSetGlobalName(f, ic, obj, shape);
62385:     if (status == Lookup_Error)
62385:         THROW();
53116: 
62385:     if (ic->usePropertyCache)
57784:         STRICT_VARIANT(stubs::SetGlobalName)(f, atom);
57784:     else
57784:         STRICT_VARIANT(stubs::SetGlobalNameNoCache)(f, atom);
52831: }
52831: 
56192: class EqualityICLinker : public LinkerHelper
56192: {
56192:     VMFrame &f;
56192: 
56192:   public:
58064:     EqualityICLinker(Assembler &masm, VMFrame &f)
58064:         : LinkerHelper(masm), f(f)
56192:     { }
56192: 
58064:     bool init(JSContext *cx) {
58064:         JSC::ExecutablePool *pool = LinkerHelper::init(cx);
56192:         if (!pool)
56192:             return false;
56192:         JSScript *script = f.fp()->script();
56192:         JITScript *jit = script->getJIT(f.fp()->isConstructing());
56192:         if (!jit->execPools.append(pool)) {
56192:             pool->release();
56192:             js_ReportOutOfMemory(cx);
56192:             return false;
56192:         }
56192:         return true;
56192:     }
56192: };
56192: 
56192: /* Rough over-estimate of how much memory we need to unprotect. */
56192: static const uint32 INLINE_PATH_LENGTH = 64;
56192: 
56192: class EqualityCompiler : public BaseCompiler
56192: {
56192:     VMFrame &f;
56192:     EqualityICInfo &ic;
56192: 
56192:     Vector<Jump, 4, SystemAllocPolicy> jumpList;
56192:     Jump trueJump;
56192:     Jump falseJump;
56192:     
56192:   public:
56192:     EqualityCompiler(VMFrame &f, EqualityICInfo &ic)
56192:         : BaseCompiler(f.cx), f(f), ic(ic), jumpList(SystemAllocPolicy())
56192:     {
56192:     }
56192: 
56192:     void linkToStub(Jump j)
56192:     {
56192:         jumpList.append(j);
56192:     }
56192: 
56192:     void linkTrue(Jump j)
56192:     {
56192:         trueJump = j;
56192:     }
56192: 
56192:     void linkFalse(Jump j)
56192:     {
56192:         falseJump = j;
56192:     }
56192:     
56192:     void generateStringPath(Assembler &masm)
56192:     {
56575:         const ValueRemat &lvr = ic.lvr;
56575:         const ValueRemat &rvr = ic.rvr;
56192: 
56575:         if (!lvr.isConstant() && !lvr.isType(JSVAL_TYPE_STRING)) {
56192:             Jump lhsFail = masm.testString(Assembler::NotEqual, lvr.typeReg());
56192:             linkToStub(lhsFail);
56192:         }
56192:         
56575:         if (!rvr.isConstant() && !rvr.isType(JSVAL_TYPE_STRING)) {
56192:             Jump rhsFail = masm.testString(Assembler::NotEqual, rvr.typeReg());
56192:             linkToStub(rhsFail);
56192:         }
56192: 
56192:         RegisterID tmp = ic.tempReg;
56192:         
56192:         /* Test if lhs/rhs are atomized. */
56192:         Imm32 atomizedFlags(JSString::FLAT | JSString::ATOMIZED);
56192:         
59888:         masm.load32(Address(lvr.dataReg(), JSString::offsetOfLengthAndFlags()), tmp);
56192:         masm.and32(Imm32(JSString::TYPE_FLAGS_MASK), tmp);
56192:         Jump lhsNotAtomized = masm.branch32(Assembler::NotEqual, tmp, atomizedFlags);
56192:         linkToStub(lhsNotAtomized);
56192: 
56575:         if (!rvr.isConstant()) {
59888:             masm.load32(Address(rvr.dataReg(), JSString::offsetOfLengthAndFlags()), tmp);
56192:             masm.and32(Imm32(JSString::TYPE_FLAGS_MASK), tmp);
56192:             Jump rhsNotAtomized = masm.branch32(Assembler::NotEqual, tmp, atomizedFlags);
56192:             linkToStub(rhsNotAtomized);
56192:         }
56192: 
56575:         if (rvr.isConstant()) {
56575:             JSString *str = rvr.value().toString();
56192:             JS_ASSERT(str->isAtomized());
56192:             Jump test = masm.branchPtr(ic.cond, lvr.dataReg(), ImmPtr(str));
56192:             linkTrue(test);
56192:         } else {
56192:             Jump test = masm.branchPtr(ic.cond, lvr.dataReg(), rvr.dataReg());
56192:             linkTrue(test);
56192:         }
56192: 
56192:         Jump fallthrough = masm.jump();
56192:         linkFalse(fallthrough);
56192:     }
56192: 
56192:     void generateObjectPath(Assembler &masm)
56192:     {
56192:         ValueRemat &lvr = ic.lvr;
56192:         ValueRemat &rvr = ic.rvr;
56192:         
56575:         if (!lvr.isConstant() && !lvr.isType(JSVAL_TYPE_OBJECT)) {
56192:             Jump lhsFail = masm.testObject(Assembler::NotEqual, lvr.typeReg());
56192:             linkToStub(lhsFail);
56192:         }
56192:         
56575:         if (!rvr.isConstant() && !rvr.isType(JSVAL_TYPE_OBJECT)) {
56192:             Jump rhsFail = masm.testObject(Assembler::NotEqual, rvr.typeReg());
56192:             linkToStub(rhsFail);
56192:         }
56192: 
56192:         Jump lhsHasEq = masm.branchTest32(Assembler::NonZero,
56192:                                           Address(lvr.dataReg(),
56192:                                                   offsetof(JSObject, flags)),
56192:                                           Imm32(JSObject::HAS_EQUALITY));
56192:         linkToStub(lhsHasEq);
56192: 
56575:         if (rvr.isConstant()) {
56575:             JSObject *obj = &rvr.value().toObject();
56192:             Jump test = masm.branchPtr(ic.cond, lvr.dataReg(), ImmPtr(obj));
56192:             linkTrue(test);
56192:         } else {
56192:             Jump test = masm.branchPtr(ic.cond, lvr.dataReg(), rvr.dataReg());
56192:             linkTrue(test);
56192:         }
56192: 
56192:         Jump fallthrough = masm.jump();
56192:         linkFalse(fallthrough);
56192:     }
56192: 
56192:     bool linkForIC(Assembler &masm)
56192:     {
58064:         EqualityICLinker buffer(masm, f);
58064:         if (!buffer.init(cx))
56192:             return false;
56192: 
58064:         Repatcher repatcher(f.jit());
58064: 
58064:         /* Overwrite the call to the IC with a call to the stub. */
58064:         JSC::FunctionPtr fptr(JS_FUNC_TO_DATA_PTR(void *, ic.stub));
58064:         repatcher.relink(ic.stubCall, fptr);
58064: 
58064:         // Silently fail, the IC is disabled now.
58064:         if (!buffer.verifyRange(f.jit()))
58064:             return true;
58064: 
56192:         /* Set the targets of all type test failures to go to the stub. */
56192:         for (size_t i = 0; i < jumpList.length(); i++)
56192:             buffer.link(jumpList[i], ic.stubEntry);
56192:         jumpList.clear();
56192: 
56192:         /* Set the targets for the the success and failure of the actual equality test. */
56192:         buffer.link(trueJump, ic.target);
56192:         buffer.link(falseJump, ic.fallThrough);
56192: 
58064:         CodeLocationLabel cs = buffer.finalize();
58063: 
56192:         /* Jump to the newly generated code instead of to the IC. */
58063:         repatcher.relink(ic.jumpToStub, cs);
56192: 
56192:         return true;
56192:     }
56192: 
56192:     bool update()
56192:     {
56192:         if (!ic.generated) {
56192:             Assembler masm;
56192:             Value rval = f.regs.sp[-1];
56192:             Value lval = f.regs.sp[-2];
56192:             
56192:             if (rval.isObject() && lval.isObject()) {
56192:                 generateObjectPath(masm);
56192:                 ic.generated = true;
56192:             } else if (rval.isString() && lval.isString()) {
56192:                 generateStringPath(masm);
56192:                 ic.generated = true;
56192:             } else {
56192:                 return true;
56192:             }
56192: 
56192:             return linkForIC(masm);
56192:         }
56192: 
56192:         return true;
56192:     }
56192: };
56192: 
56192: JSBool JS_FASTCALL
56192: ic::Equality(VMFrame &f, ic::EqualityICInfo *ic)
56192: {
56192:     EqualityCompiler cc(f, *ic);
56192:     if (!cc.update())
56192:         THROWV(JS_FALSE);
56192: 
56192:     return ic->stub(f);
56192: }
56192: 
53590: static void * JS_FASTCALL
55503: SlowCallFromIC(VMFrame &f, ic::CallICInfo *ic)
53590: {
57717:     stubs::SlowCall(f, ic->frameSize.getArgc(f));
53590:     return NULL;
53301: }
53301: 
53590: static void * JS_FASTCALL
55503: SlowNewFromIC(VMFrame &f, ic::CallICInfo *ic)
53301: {
57717:     stubs::SlowNew(f, ic->frameSize.staticArgc());
53590:     return NULL;
53523: }
53522: 
53590: /*
53590:  * Calls have an inline path and an out-of-line path. The inline path is used
53590:  * in the fastest case: the method has JIT'd code, and |argc == nargs|.
53590:  * 
53590:  * The inline path and OOL path are separated by a guard on the identity of
53590:  * the callee object. This guard starts as NULL and always fails on the first
53590:  * hit. On the OOL path, the callee is verified to be both a function and a
53590:  * scripted function. If these conditions hold, |ic::Call| is invoked.
53590:  *
53590:  * |ic::Call| first ensures that the callee has JIT code. If it doesn't, the
53590:  * call to |ic::Call| is patched to a slow path. If it does have JIT'd code,
53590:  * the following cases can occur:
53590:  *
53590:  *   1) args != nargs: The call to |ic::Call| is patched with a dynamically
53590:  *      generated stub. This stub inlines a path that looks like:
53590:  *      ----
53590:  *      push frame
53590:  *      if (callee is not compiled) {
53590:  *          Compile(callee);
53590:  *      }
53590:  *      call callee->arityLabel
53590:  *
53590:  *      The arity label is a special entry point for correcting frames for
53590:  *      arity mismatches.
53590:  *
53590:  *   2) args == nargs, and the inline call site was not patched yet.
53590:  *      The guard dividing the two paths is patched to guard on the given
53590:  *      function object identity, and the proceeding call is patched to
53590:  *      directly call the JIT code.
53590:  *
53590:  *   3) args == nargs, and the inline call site was patched already.
53590:  *      A small stub is created which extends the original guard to also
53590:  *      guard on the JSFunction lying underneath the function object.
53590:  *
53590:  * If the OOL path does not have a scripted function, but does have a
53590:  * scripted native, then a small stub is generated which inlines the native
53590:  * invocation.
53590:  */
55463: class CallCompiler : public BaseCompiler
53590: {
53590:     VMFrame &f;
53590:     CallICInfo &ic;
53590:     bool callingNew;
53306: 
53590:   public:
53590:     CallCompiler(VMFrame &f, CallICInfo &ic, bool callingNew)
57717:       : BaseCompiler(f.cx), f(f), ic(ic), callingNew(callingNew)
53590:     {
53590:     }
53301: 
58064:     JSC::ExecutablePool *poolForSize(LinkerHelper &linker, CallICInfo::PoolIndex index)
53590:     {
58064:         JSC::ExecutablePool *ep = linker.init(f.cx);
55463:         if (!ep)
53590:             return NULL;
53590:         JS_ASSERT(!ic.pools[index]);
53590:         ic.pools[index] = ep;
53590:         return ep;
53590:     }
53301: 
58064:     void disable(JITScript *jit)
58064:     {
58064:         JSC::CodeLocationCall oolCall = ic.slowPathStart.callAtOffset(ic.oolCallOffset);
58064:         Repatcher repatch(jit);
58064:         JSC::FunctionPtr fptr = callingNew
58064:                                 ? JSC::FunctionPtr(JS_FUNC_TO_DATA_PTR(void *, SlowNewFromIC))
58064:                                 : JSC::FunctionPtr(JS_FUNC_TO_DATA_PTR(void *, SlowCallFromIC));
58064:         repatch.relink(oolCall, fptr);
58064:     }
58064: 
58063:     bool generateFullCallStub(JITScript *from, JSScript *script, uint32 flags)
53590:     {
53590:         /*
53590:          * Create a stub that works with arity mismatches. Like the fast-path,
53590:          * this allocates a frame on the caller side, but also performs extra
53590:          * checks for compilability. Perhaps this should be a separate, shared
53590:          * trampoline, but for now we generate it dynamically.
53590:          */
53590:         Assembler masm;
54163:         InlineFrameAssembler inlFrame(masm, ic, flags);
53590:         RegisterID t0 = inlFrame.tempRegs.takeAnyReg();
53301: 
53590:         /* Generate the inline frame creation. */
54832:         inlFrame.assemble(ic.funGuard.labelAtOffset(ic.joinPointOffset).executableAddress());
53301: 
53590:         /* funPtrReg is still valid. Check if a compilation is needed. */
53590:         Address scriptAddr(ic.funPtrReg, offsetof(JSFunction, u) +
53590:                            offsetof(JSFunction::U::Scripted, script));
53590:         masm.loadPtr(scriptAddr, t0);
53301: 
53590:         /*
53590:          * Test if script->nmap is NULL - same as checking ncode, but faster
53590:          * here since ncode has two failure modes and we need to load out of
53590:          * nmap anyway.
53590:          */
55503:         size_t offset = callingNew
55503:                         ? offsetof(JSScript, jitArityCheckCtor)
55503:                         : offsetof(JSScript, jitArityCheckNormal);
55503:         masm.loadPtr(Address(t0, offset), t0);
55503:         Jump hasCode = masm.branchPtr(Assembler::Above, t0, ImmPtr(JS_UNJITTABLE_SCRIPT));
53523: 
53590:         /* Try and compile. On success we get back the nmap pointer. */
53590:         masm.storePtr(JSFrameReg, FrameAddress(offsetof(VMFrame, regs.fp)));
57717:         void *compilePtr = JS_FUNC_TO_DATA_PTR(void *, stubs::CompileFunction);
57717:         if (ic.frameSize.isStatic()) {
57717:             masm.move(Imm32(ic.frameSize.staticArgc()), Registers::ArgReg1);
57787:             masm.fallibleVMCall(compilePtr, script->code, ic.frameSize.staticLocalSlots());
57717:         } else {
57717:             masm.load32(FrameAddress(offsetof(VMFrame, u.call.dynamicArgc)), Registers::ArgReg1);
57766:             masm.fallibleVMCall(compilePtr, script->code, -1);
57717:         }
54832:         masm.loadPtr(FrameAddress(offsetof(VMFrame, regs.fp)), JSFrameReg);
53590: 
53590:         Jump notCompiled = masm.branchTestPtr(Assembler::Zero, Registers::ReturnReg,
53590:                                               Registers::ReturnReg);
53590: 
54832:         masm.jump(Registers::ReturnReg);
53590: 
53590:         hasCode.linkTo(masm.label(), &masm);
53590: 
53590:         /* Get nmap[ARITY], set argc, call. */
57717:         if (ic.frameSize.isStatic())
57717:             masm.move(Imm32(ic.frameSize.staticArgc()), JSParamReg_Argc);
57717:         else
57717:             masm.load32(FrameAddress(offsetof(VMFrame, u.call.dynamicArgc)), JSParamReg_Argc);
54832:         masm.jump(t0);
53590: 
58064:         LinkerHelper linker(masm);
58064:         JSC::ExecutablePool *ep = poolForSize(linker, CallICInfo::Pool_ScriptStub);
53590:         if (!ep)
53590:             return false;
53590: 
58064:         if (!linker.verifyRange(from)) {
58064:             disable(from);
58064:             return true;
58064:         }
58064: 
58064:         linker.link(notCompiled, ic.slowPathStart.labelAtOffset(ic.slowJoinOffset));
58064:         JSC::CodeLocationLabel cs = linker.finalize();
53590: 
53590:         JaegerSpew(JSpew_PICs, "generated CALL stub %p (%d bytes)\n", cs.executableAddress(),
53590:                    masm.size());
53590: 
58064:         Repatcher repatch(from);
53590:         JSC::CodeLocationJump oolJump = ic.slowPathStart.jumpAtOffset(ic.oolJumpOffset);
53590:         repatch.relink(oolJump, cs);
53590: 
53590:         return true;
53590:     }
53590: 
58064:     bool patchInlinePath(JITScript *from, JSScript *script, JSObject *obj)
53590:     {
57717:         JS_ASSERT(ic.frameSize.isStatic());
58064:         JITScript *jit = script->getJIT(callingNew);
57717: 
53590:         /* Very fast path. */
58063:         Repatcher repatch(from);
53590: 
58064:         if (!repatch.canRelink(ic.funGuard.jumpAtOffset(ic.hotJumpOffset),
58064:                                JSC::CodeLocationLabel(jit->fastEntry))) {
58064:             return false;
58064:         }
58064: 
53590:         ic.fastGuardedObject = obj;
53590: 
53590:         repatch.repatch(ic.funGuard, obj);
54832:         repatch.relink(ic.funGuard.jumpAtOffset(ic.hotJumpOffset),
55503:                        JSC::CodeLocationLabel(jit->fastEntry));
53590: 
58063:         JaegerSpew(JSpew_PICs, "patched CALL path %p (obj: %p)\n",
58063:                    ic.funGuard.executableAddress(), ic.fastGuardedObject);
58064: 
58064:         return true;
53590:     }
53590: 
58063:     bool generateStubForClosures(JITScript *from, JSObject *obj)
53590:     {
57717:         JS_ASSERT(ic.frameSize.isStatic());
57717: 
53590:         /* Slightly less fast path - guard on fun->getFunctionPrivate() instead. */
53590:         Assembler masm;
53590: 
53590:         Registers tempRegs;
53590:         tempRegs.takeReg(ic.funObjReg);
53590: 
53590:         RegisterID t0 = tempRegs.takeAnyReg();
53590: 
53590:         /* Guard that it's actually a function object. */
56575:         Jump claspGuard = masm.testObjClass(Assembler::NotEqual, ic.funObjReg, &js_FunctionClass);
53590: 
53590:         /* Guard that it's the same function. */
53590:         JSFunction *fun = obj->getFunctionPrivate();
60777:         masm.loadObjPrivate(ic.funObjReg, t0);
53590:         Jump funGuard = masm.branchPtr(Assembler::NotEqual, t0, ImmPtr(fun));
53590:         Jump done = masm.jump();
53590: 
58064:         LinkerHelper linker(masm);
58064:         JSC::ExecutablePool *ep = poolForSize(linker, CallICInfo::Pool_ClosureStub);
53590:         if (!ep)
53590:             return false;
53590: 
58064:         ic.hasJsFunCheck = true;
58064: 
58064:         if (!linker.verifyRange(from)) {
58064:             disable(from);
58064:             return true;
58064:         }
58064: 
58064:         linker.link(claspGuard, ic.slowPathStart);
58064:         linker.link(funGuard, ic.slowPathStart);
58064:         linker.link(done, ic.funGuard.labelAtOffset(ic.hotPathOffset));
58064:         JSC::CodeLocationLabel cs = linker.finalize();
53590: 
53590:         JaegerSpew(JSpew_PICs, "generated CALL closure stub %p (%d bytes)\n",
53590:                    cs.executableAddress(), masm.size());
53590: 
58063:         Repatcher repatch(from);
53590:         repatch.relink(ic.funJump, cs);
53590: 
53590:         return true;
53590:     }
53590: 
53590:     bool generateNativeStub()
53590:     {
58063:         JITScript *jit = f.jit();
58063: 
57718:         /* Snapshot the frameDepth before SplatApplyArgs modifies it. */
57718:         uintN initialFrameDepth = f.regs.sp - f.regs.fp->slots();
57718: 
57717:         /*
57717:          * SplatApplyArgs has not been called, so we call it here before
57717:          * potentially touching f.u.call.dynamicArgc.
57717:          */
57717:         Value *vp;
57717:         if (ic.frameSize.isStatic()) {
57787:             JS_ASSERT(f.regs.sp - f.regs.fp->slots() == (int)ic.frameSize.staticLocalSlots());
57717:             vp = f.regs.sp - (2 + ic.frameSize.staticArgc());
57717:         } else {
57717:             JS_ASSERT(*f.regs.pc == JSOP_FUNAPPLY && GET_ARGC(f.regs.pc) == 2);
57717:             if (!ic::SplatApplyArgs(f))       /* updates regs.sp */
57717:                 THROWV(true);
57718:             vp = f.regs.sp - (2 + f.u.call.dynamicArgc);
57717:         }
53590: 
53590:         JSObject *obj;
53590:         if (!IsFunctionObject(*vp, &obj))
53590:             return false;
53590: 
53590:         JSFunction *fun = obj->getFunctionPrivate();
53590:         if ((!callingNew && !fun->isNative()) || (callingNew && !fun->isConstructor()))
53590:             return false;
53590: 
53590:         if (callingNew)
53590:             vp[1].setMagicWithObjectOrNullPayload(NULL);
53590: 
57717:         if (!CallJSNative(cx, fun->u.n.native, ic.frameSize.getArgc(f), vp))
53590:             THROWV(true);
53590: 
53590:         /* Right now, take slow-path for IC misses or multiple stubs. */
53590:         if (ic.fastGuardedNative || ic.hasJsFunCheck)
53590:             return true;
53590: 
53590:         /* Native MIC needs to warm up first. */
53590:         if (!ic.hit) {
53590:             ic.hit = true;
53590:             return true;
53590:         }
53590: 
53590:         /* Generate fast-path for calling this native. */
53590:         Assembler masm;
53590: 
53590:         /* Guard on the function object identity, for now. */
53590:         Jump funGuard = masm.branchPtr(Assembler::NotEqual, ic.funObjReg, ImmPtr(obj));
53590: 
57717:         /* N.B. After this call, the frame will have a dynamic frame size. */
57717:         if (ic.frameSize.isDynamic()) {
57766:             masm.fallibleVMCall(JS_FUNC_TO_DATA_PTR(void *, ic::SplatApplyArgs),
57718:                                 f.regs.pc, initialFrameDepth);
57717:         }
57717: 
53590:         Registers tempRegs;
53590: #ifndef JS_CPU_X86
53590:         tempRegs.takeReg(Registers::ArgReg0);
53590:         tempRegs.takeReg(Registers::ArgReg1);
53590:         tempRegs.takeReg(Registers::ArgReg2);
53590: #endif
53590:         RegisterID t0 = tempRegs.takeAnyReg();
53590: 
53590:         /* Store pc. */
53590:         masm.storePtr(ImmPtr(cx->regs->pc),
54832:                        FrameAddress(offsetof(VMFrame, regs.pc)));
53301: 
57717:         /* Store sp (if not already set by ic::SplatApplyArgs). */
57717:         if (ic.frameSize.isStatic()) {
57718:             uint32 spOffset = sizeof(JSStackFrame) + initialFrameDepth * sizeof(Value);
53590:             masm.addPtr(Imm32(spOffset), JSFrameReg, t0);
54832:             masm.storePtr(t0, FrameAddress(offsetof(VMFrame, regs.sp)));
57717:         }
54832: 
54832:         /* Store fp. */
54832:         masm.storePtr(JSFrameReg, FrameAddress(offsetof(VMFrame, regs.fp)));
53301: 
57766:         /* Grab cx. */
53590: #ifdef JS_CPU_X86
53590:         RegisterID cxReg = tempRegs.takeAnyReg();
53590: #else
53590:         RegisterID cxReg = Registers::ArgReg0;
53590: #endif
53590:         masm.loadPtr(FrameAddress(offsetof(VMFrame, cx)), cxReg);
53301: 
53590:         /* Compute vp. */
53590: #ifdef JS_CPU_X86
53590:         RegisterID vpReg = t0;
53590: #else
53590:         RegisterID vpReg = Registers::ArgReg2;
53590: #endif
57717:         MaybeRegisterID argcReg;
57717:         if (ic.frameSize.isStatic()) {
57717:             uint32 vpOffset = sizeof(JSStackFrame) + (vp - f.regs.fp->slots()) * sizeof(Value);
57717:             masm.addPtr(Imm32(vpOffset), JSFrameReg, vpReg);
57717:         } else {
57717:             argcReg = tempRegs.takeAnyReg();
57717:             masm.load32(FrameAddress(offsetof(VMFrame, u.call.dynamicArgc)), argcReg.reg());
57717:             masm.loadPtr(FrameAddress(offsetof(VMFrame, regs.sp)), vpReg);
53590: 
57717:             /* vpOff = (argc + 2) * sizeof(Value) */
57717:             RegisterID vpOff = tempRegs.takeAnyReg();
57717:             masm.move(argcReg.reg(), vpOff);
57717:             masm.add32(Imm32(2), vpOff);  /* callee, this */
57717:             JS_STATIC_ASSERT(sizeof(Value) == 8);
57717:             masm.lshift32(Imm32(3), vpOff);
57717:             masm.subPtr(vpOff, vpReg);
57717: 
57717:             tempRegs.putReg(vpOff);
57717:         }
53590: 
53590:         /* Mark vp[1] as magic for |new|. */
53590:         if (callingNew) {
53590:             Value v;
53590:             v.setMagicWithObjectOrNullPayload(NULL);
53590:             masm.storeValue(v, Address(vpReg, sizeof(Value)));
53523:         }
53306: 
57766:         masm.setupABICall(Registers::NormalCall, 3);
57766:         masm.storeArg(2, vpReg);
57717:         if (ic.frameSize.isStatic())
57766:             masm.storeArg(1, Imm32(ic.frameSize.staticArgc()));
57717:         else
57766:             masm.storeArg(1, argcReg.reg());
57766:         masm.storeArg(0, cxReg);
60589:         masm.callWithABI(JS_FUNC_TO_DATA_PTR(void *, fun->u.n.native), false);
53590: 
53590:         Jump hasException = masm.branchTest32(Assembler::Zero, Registers::ReturnReg,
53590:                                               Registers::ReturnReg);
53590:         
53301: 
53590:         Jump done = masm.jump();
53301: 
53590:         /* Move JaegerThrowpoline into register for very far jump on x64. */
53590:         hasException.linkTo(masm.label(), &masm);
57766:         masm.throwInJIT();
53326: 
58064:         LinkerHelper linker(masm);
58064:         JSC::ExecutablePool *ep = poolForSize(linker, CallICInfo::Pool_NativeStub);
53590:         if (!ep)
53590:             THROWV(true);
53301: 
58064:         ic.fastGuardedNative = obj;
53590: 
58064:         if (!linker.verifyRange(jit)) {
58064:             disable(jit);
58064:             return true;
58064:         }
58064: 
58064:         linker.link(done, ic.slowPathStart.labelAtOffset(ic.slowJoinOffset));
58064:         linker.link(funGuard, ic.slowPathStart);
58064:         JSC::CodeLocationLabel cs = linker.finalize();
53590: 
53590:         JaegerSpew(JSpew_PICs, "generated native CALL stub %p (%d bytes)\n",
53590:                    cs.executableAddress(), masm.size());
53590: 
58063:         Repatcher repatch(jit);
53590:         repatch.relink(ic.funJump, cs);
53590: 
53590:         return true;
53301:     }
53301: 
53590:     void *update()
53590:     {
58063:         JITScript *jit = f.jit();
58063: 
54163:         stubs::UncachedCallResult ucr;
53590:         if (callingNew)
57717:             stubs::UncachedNewHelper(f, ic.frameSize.staticArgc(), &ucr);
53590:         else
57717:             stubs::UncachedCallHelper(f, ic.frameSize.getArgc(f), &ucr);
53590: 
54163:         // If the function cannot be jitted (generally unjittable or empty script),
54163:         // patch this site to go to a slow path always.
54163:         if (!ucr.codeAddr) {
62574:             if (ucr.unjittable)
58064:                 disable(jit);
53590:             return NULL;
53590:         }
53590:             
54163:         JSFunction *fun = ucr.fun;
54163:         JS_ASSERT(fun);
54163:         JSScript *script = fun->script();
54163:         JS_ASSERT(script);
54163:         JSObject *callee = ucr.callee;
54163:         JS_ASSERT(callee);
54163: 
53590:         uint32 flags = callingNew ? JSFRAME_CONSTRUCTING : 0;
53590: 
53590:         if (!ic.hit) {
54163:             ic.hit = true;
54163:             return ucr.codeAddr;
54163:         }
54163: 
57717:         if (!ic.frameSize.isStatic() || ic.frameSize.staticArgc() != fun->nargs) {
58063:             if (!generateFullCallStub(jit, script, flags))
53590:                 THROWV(NULL);
53590:         } else {
58064:             if (!ic.fastGuardedObject && patchInlinePath(jit, script, callee)) {
58064:                 // Nothing, done.
58064:             } else if (ic.fastGuardedObject &&
58064:                        !ic.hasJsFunCheck &&
53590:                        !ic.fastGuardedNative &&
53590:                        ic.fastGuardedObject->getFunctionPrivate() == fun) {
53590:                 /*
53590:                  * Note: Multiple "function guard" stubs are not yet
53590:                  * supported, thus the fastGuardedNative check.
53590:                  */
58063:                 if (!generateStubForClosures(jit, callee))
53590:                     THROWV(NULL);
53590:             } else {
58063:                 if (!generateFullCallStub(jit, script, flags))
53590:                     THROWV(NULL);
53590:             }
53590:         }
53590: 
54163:         return ucr.codeAddr;
53590:     }
53590: };
53590: 
53590: void * JS_FASTCALL
55503: ic::Call(VMFrame &f, CallICInfo *ic)
53590: {
55503:     CallCompiler cc(f, *ic, false);
53590:     return cc.update();
53590: }
53590: 
53590: void * JS_FASTCALL
55503: ic::New(VMFrame &f, CallICInfo *ic)
53590: {
55503:     CallCompiler cc(f, *ic, true);
53590:     return cc.update();
53590: }
53590: 
53590: void JS_FASTCALL
55503: ic::NativeCall(VMFrame &f, CallICInfo *ic)
53590: {
55503:     CallCompiler cc(f, *ic, false);
53590:     if (!cc.generateNativeStub())
57717:         stubs::SlowCall(f, ic->frameSize.getArgc(f));
53590: }
53590: 
53590: void JS_FASTCALL
55503: ic::NativeNew(VMFrame &f, CallICInfo *ic)
53590: {
55503:     CallCompiler cc(f, *ic, true);
53590:     if (!cc.generateNativeStub())
57717:         stubs::SlowNew(f, ic->frameSize.staticArgc());
57717: }
57717: 
57717: static inline bool
57717: BumpStack(VMFrame &f, uintN inc)
57717: {
57717:     static const unsigned MANY_ARGS = 1024;
57717:     static const unsigned MIN_SPACE = 500;
57717: 
57717:     /* If we are not passing many args, treat this as a normal call. */
57717:     if (inc < MANY_ARGS) {
57717:         if (f.regs.sp + inc < f.stackLimit)
57717:             return true;
57717:         StackSpace &stack = f.cx->stack();
57777:         if (!stack.bumpCommitAndLimit(f.entryfp, f.regs.sp, inc, &f.stackLimit)) {
57717:             js_ReportOverRecursed(f.cx);
57717:             return false;
57717:         }
57717:         return true;
57717:     }
57717: 
57717:     /*
57717:      * The purpose of f.stackLimit is to catch over-recursion based on
57717:      * assumptions about the average frame size. 'apply' with a large number of
57717:      * arguments breaks these assumptions and can result in premature "out of
57717:      * script quota" errors. Normally, apply will go through js::Invoke, which
57717:      * effectively starts a fresh stackLimit. Here, we bump f.stackLimit,
57717:      * if necessary, to allow for this 'apply' call, and a reasonable number of
57717:      * subsequent calls, to succeed without hitting the stackLimit. In theory,
57717:      * this a recursive chain containing apply to circumvent the stackLimit.
57717:      * However, since each apply call must consume at least MANY_ARGS slots,
57717:      * this sequence will quickly reach the end of the stack and OOM.
57717:      */
57717: 
57717:     uintN incWithSpace = inc + MIN_SPACE;
57717:     Value *bumpedWithSpace = f.regs.sp + incWithSpace;
57717:     if (bumpedWithSpace < f.stackLimit)
57717:         return true;
57717: 
57717:     StackSpace &stack = f.cx->stack();
57777:     if (stack.bumpCommitAndLimit(f.entryfp, f.regs.sp, incWithSpace, &f.stackLimit))
57717:         return true;
57717: 
57717:     if (!stack.ensureSpace(f.cx, f.regs.sp, incWithSpace))
57717:         return false;
57717:     f.stackLimit = bumpedWithSpace;
57717:     return true;
57717: }
57717: 
57717: /*
57717:  * SplatApplyArgs is only called for expressions of the form |f.apply(x, y)|.
57717:  * Additionally, the callee has already been checked to be the native apply.
57717:  * All successful paths through SplatApplyArgs must set f.u.call.dynamicArgc
57717:  * and f.regs.sp.
57717:  */
57717: JSBool JS_FASTCALL
57717: ic::SplatApplyArgs(VMFrame &f)
57717: {
57717:     JSContext *cx = f.cx;
57718:     JS_ASSERT(GET_ARGC(f.regs.pc) == 2);
57718: 
57718:     /*
57718:      * The lazyArgsObj flag indicates an optimized call |f.apply(x, arguments)|
57718:      * where the args obj has not been created or pushed on the stack. Thus,
57718:      * if lazyArgsObj is set, the stack for |f.apply(x, arguments)| is:
57718:      *
57718:      *  | Function.prototype.apply | f | x |
57718:      *
57718:      * Otherwise, if !lazyArgsObj, the stack is a normal 2-argument apply:
57718:      *
57718:      *  | Function.prototype.apply | f | x | arguments |
57718:      */
57718:     if (f.u.call.lazyArgsObj) {
57718:         Value *vp = f.regs.sp - 3;
57718:         JS_ASSERT(JS_CALLEE(cx, vp).toObject().getFunctionPrivate()->u.n.native == js_fun_apply);
57718: 
57718:         JSStackFrame *fp = f.regs.fp;
57718:         if (!fp->hasOverriddenArgs() &&
57718:             (!fp->hasArgsObj() ||
57744:              (fp->hasArgsObj() && !fp->argsObj().isArgsLengthOverridden() &&
57744:               !js_PrototypeHasIndexedProperties(cx, &fp->argsObj())))) {
57718: 
57718:             uintN n = fp->numActualArgs();
57718:             if (!BumpStack(f, n))
57718:                 THROWV(false);
57718:             f.regs.sp += n;
57718: 
57718:             Value *argv = JS_ARGV(cx, vp + 1 /* vp[1]'s argv */);
57718:             if (fp->hasArgsObj())
57718:                 fp->forEachCanonicalActualArg(CopyNonHoleArgsTo(&fp->argsObj(), argv));
57718:             else
57718:                 fp->forEachCanonicalActualArg(CopyTo(argv));
57718: 
57718:             f.u.call.dynamicArgc = n;
57718:             return true;
57718:         }
57718: 
57718:         /*
57718:          * Can't optimize; push the arguments object so that the stack matches
57718:          * the !lazyArgsObj stack state described above.
57718:          */
57718:         f.regs.sp++;
57718:         if (!js_GetArgsValue(cx, fp, &vp[3]))
57718:             THROWV(false);
57718:     }
57718: 
57717:     Value *vp = f.regs.sp - 4;
57717:     JS_ASSERT(JS_CALLEE(cx, vp).toObject().getFunctionPrivate()->u.n.native == js_fun_apply);
57717: 
57717:     /*
57717:      * This stub should mimic the steps taken by js_fun_apply. Step 1 and part
57717:      * of Step 2 have already been taken care of by calling jit code.
57717:      */
57717: 
57717:     /* Step 2 (part 2). */
57717:     if (vp[3].isNullOrUndefined()) {
57717:         f.regs.sp--;
57717:         f.u.call.dynamicArgc = 0;
57717:         return true;
57717:     }
57717: 
57717:     /* Step 3. */
57717:     if (!vp[3].isObject()) {
57717:         JS_ReportErrorNumber(cx, js_GetErrorMessage, NULL, JSMSG_BAD_APPLY_ARGS, js_apply_str);
57717:         THROWV(false);
57717:     }
57717: 
57717:     /* Steps 4-5. */
57717:     JSObject *aobj = &vp[3].toObject();
57717:     jsuint length;
57717:     if (!js_GetLengthProperty(cx, aobj, &length))
57717:         THROWV(false);
57717: 
57717:     JS_ASSERT(!JS_ON_TRACE(cx));
57717: 
57717:     /* Step 6. */
57717:     uintN n = uintN(JS_MIN(length, JS_ARGS_LENGTH_MAX));
57717: 
57717:     intN delta = n - 1;
57717:     if (delta > 0 && !BumpStack(f, delta))
57717:         THROWV(false);
57717:     f.regs.sp += delta;
57717: 
57717:     /* Steps 7-8. */
57717:     if (!GetElements(cx, aobj, n, f.regs.sp - n))
57717:         THROWV(false);
57717: 
57717:     f.u.call.dynamicArgc = n;
57717:     return true;
53590: }
53301: 
53405: void
55503: JITScript::purgeMICs()
53405: {
62386:     if (!nGetGlobalNames || !nSetGlobalNames)
58063:         return;
58063: 
58063:     Repatcher repatch(this);
58063: 
62386:     ic::GetGlobalNameIC *getGlobalNames_ = getGlobalNames();
62386:     for (uint32 i = 0; i < nGetGlobalNames; i++) {
62386:         ic::GetGlobalNameIC &ic = getGlobalNames_[i];
62386:         JSC::CodeLocationDataLabel32 label = ic.fastPathStart.dataLabel32AtOffset(ic.shapeOffset);
62386:         repatch.repatch(label, int(JSObjectMap::INVALID_SHAPE));
62386:     }
53405: 
62386:     ic::SetGlobalNameIC *setGlobalNames_ = setGlobalNames();
62386:     for (uint32 i = 0; i < nSetGlobalNames; i++) {
62386:         ic::SetGlobalNameIC &ic = setGlobalNames_[i];
62386:         ic.patchInlineShapeGuard(repatch, int32(JSObjectMap::INVALID_SHAPE));
62386: 
62409:         if (ic.hasExtraStub) {
62386:             Repatcher repatcher(ic.extraStub);
62386:             ic.patchExtraShapeGuard(repatcher, int32(JSObjectMap::INVALID_SHAPE));
53405:         }
53405:     }
53405: }
53405: 
53590: void
55503: ic::PurgeMICs(JSContext *cx, JSScript *script)
53590: {
55503:     /* MICs are purged during GC to handle changing shapes. */
55503:     JS_ASSERT(cx->runtime->gcRegenShapes);
55503: 
55503:     if (script->jitNormal)
55503:         script->jitNormal->purgeMICs();
55503:     if (script->jitCtor)
55503:         script->jitCtor->purgeMICs();
55503: }
55503: 
55503: void
58693: JITScript::nukeScriptDependentICs()
58693: {
58693:     if (!nCallICs)
58693:         return;
58693: 
58693:     Repatcher repatcher(this);
58693: 
62075:     ic::CallICInfo *callICs_ = callICs();
58693:     for (uint32 i = 0; i < nCallICs; i++) {
62075:         ic::CallICInfo &ic = callICs_[i];
58693:         if (!ic.fastGuardedObject)
58693:             continue;
58693:         repatcher.repatch(ic.funGuard, NULL);
58693:         repatcher.relink(ic.funJump, ic.slowPathStart);
58693:         ic.releasePool(CallICInfo::Pool_ClosureStub);
58693:         ic.fastGuardedObject = NULL;
58693:         ic.hasJsFunCheck = false;
58693:     }
58693: }
58693: 
58693: void
60258: JITScript::sweepCallICs(JSContext *cx, bool purgeAll)
55503: {
59895:     Repatcher repatcher(this);
58063: 
59895:     /*
59895:      * If purgeAll is set, purge stubs in the script except those covered by PurgePICs
59895:      * (which is always called during GC). We want to remove references which can keep
59895:      * alive pools that we are trying to destroy (see JSCompartment::sweep).
59895:      */
58063: 
62075:     ic::CallICInfo *callICs_ = callICs();
55503:     for (uint32 i = 0; i < nCallICs; i++) {
62075:         ic::CallICInfo &ic = callICs_[i];
53590: 
53590:         /*
53590:          * If the object is unreachable, we're guaranteed not to be currently
53590:          * executing a stub generated by a guard on that object. This lets us
53590:          * precisely GC call ICs while keeping the identity guard safe.
53590:          */
59895:         bool fastFunDead = ic.fastGuardedObject &&
60258:             (purgeAll || IsAboutToBeFinalized(cx, ic.fastGuardedObject));
59895:         bool nativeDead = ic.fastGuardedNative &&
60258:             (purgeAll || IsAboutToBeFinalized(cx, ic.fastGuardedNative));
53590: 
62850:         /*
62850:          * There are three conditions where we need to relink:
62850:          * (1) purgeAll is true.
62850:          * (2) The native is dead, since it always has a stub.
62850:          * (3) The fastFun is dead *and* there is a closure stub.
62850:          *
62850:          * Note although both objects can be non-NULL, there can only be one
62850:          * of [closure, native] stub per call IC.
62850:          */
62850:         if (purgeAll || nativeDead || (fastFunDead && ic.hasJsFunCheck)) {
62850:             repatcher.relink(ic.funJump, ic.slowPathStart);
62850:             ic.hit = false;
62850:         }
62850: 
53590:         if (fastFunDead) {
58063:             repatcher.repatch(ic.funGuard, NULL);
53590:             ic.releasePool(CallICInfo::Pool_ClosureStub);
53590:             ic.hasJsFunCheck = false;
53590:             ic.fastGuardedObject = NULL;
53590:         }
53590: 
53590:         if (nativeDead) {
53590:             ic.releasePool(CallICInfo::Pool_NativeStub);
53590:             ic.fastGuardedNative = NULL;
53590:         }
53590: 
59895:         if (purgeAll) {
59895:             ic.releasePool(CallICInfo::Pool_ScriptStub);
59895:             JSC::CodeLocationJump oolJump = ic.slowPathStart.jumpAtOffset(ic.oolJumpOffset);
59895:             JSC::CodeLocationLabel icCall = ic.slowPathStart.labelAtOffset(ic.icCallOffset);
59895:             repatcher.relink(oolJump, icCall);
59895:         }
62792:     }
53590: 
59895:     if (purgeAll) {
59895:         /* Purge ICs generating stubs into execPools. */
59895:         uint32 released = 0;
59895: 
62075:         ic::EqualityICInfo *equalityICs_ = equalityICs();
59895:         for (uint32 i = 0; i < nEqualityICs; i++) {
62075:             ic::EqualityICInfo &ic = equalityICs_[i];
59895:             if (!ic.generated)
59895:                 continue;
59895: 
59895:             JSC::FunctionPtr fptr(JS_FUNC_TO_DATA_PTR(void *, ic::Equality));
59895:             repatcher.relink(ic.stubCall, fptr);
59895:             repatcher.relink(ic.jumpToStub, ic.stubEntry);
59895: 
59895:             ic.generated = false;
59895:             released++;
59895:         }
59895: 
62386:         ic::SetGlobalNameIC *setGlobalNames_ = setGlobalNames();
62386:         for (uint32 i = 0; i < nSetGlobalNames; i ++) {
62386:             ic::SetGlobalNameIC &ic = setGlobalNames_[i];
62409:             if (!ic.hasExtraStub)
62385:                 continue;
62385:             repatcher.relink(ic.fastPathStart.jumpAtOffset(ic.inlineShapeJump), ic.slowPathStart);
62409:             ic.hasExtraStub = false;
62385:             released++;
62385:         }
62385: 
59895:         JS_ASSERT(released == execPools.length());
59895:         for (uint32 i = 0; i < released; i++)
59895:             execPools[i]->release();
59895:         execPools.clear();
53590:     }
53590: }
53590: 
55503: void
60258: ic::SweepCallICs(JSContext *cx, JSScript *script, bool purgeAll)
55503: {
55503:     if (script->jitNormal)
60258:         script->jitNormal->sweepCallICs(cx, purgeAll);
55503:     if (script->jitCtor)
60258:         script->jitCtor->sweepCallICs(cx, purgeAll);
55503: }
55503: 
53119: #endif /* JS_MONOIC */
53405: 
