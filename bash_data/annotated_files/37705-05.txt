30266: /* -*- Mode: C++; c-basic-offset: 4; indent-tabs-mode: nil; tab-width: 4 -*- */
30266: /* vi: set ts=4 sw=4 expandtab: (add to ~/.vimrc: set modeline modelines=5) */
25109: /* ***** BEGIN LICENSE BLOCK *****
25109:  * Version: MPL 1.1/GPL 2.0/LGPL 2.1
25109:  *
25109:  * The contents of this file are subject to the Mozilla Public License Version
25109:  * 1.1 (the "License"); you may not use this file except in compliance with
25109:  * the License. You may obtain a copy of the License at
25109:  * http://www.mozilla.org/MPL/
25109:  *
25109:  * Software distributed under the License is distributed on an "AS IS" basis,
25109:  * WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
25109:  * for the specific language governing rights and limitations under the
25109:  * License.
25109:  *
25109:  * The Original Code is [Open Source Virtual Machine].
25109:  *
25109:  * The Initial Developer of the Original Code is
25109:  * Adobe System Incorporated.
25109:  * Portions created by the Initial Developer are Copyright (C) 2004-2007
25109:  * the Initial Developer. All Rights Reserved.
25109:  *
25109:  * Contributor(s):
25109:  *   Adobe AS3 Team
25109:  *   leon.sha@sun.com
25109:  *
25109:  * Alternatively, the contents of this file may be used under the terms of
25109:  * either the GNU General Public License Version 2 or later (the "GPL"), or
25109:  * the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
25109:  * in which case the provisions of the GPL or the LGPL are applicable instead
25109:  * of those above. If you wish to allow use of your version of this file only
25109:  * under the terms of either the GPL or the LGPL, and not to allow others to
25109:  * use your version of this file under the terms of the MPL, indicate your
25109:  * decision by deleting the provisions above and replace them with the notice
25109:  * and other provisions required by the GPL or the LGPL. If you do not delete
25109:  * the provisions above, a recipient may use your version of this file under
25109:  * the terms of any one of the MPL, the GPL or the LGPL.
25109:  *
25109:  * ***** END LICENSE BLOCK ***** */
25109: 
25109: #include <sys/mman.h>
25109: #include <errno.h>
25109: #include "nanojit.h"
25109: 
25109: namespace nanojit
25109: {
25109: #ifdef FEATURE_NANOJIT
25109: 
25109: #ifdef NJ_VERBOSE
25109:     const char *regNames[] = {
25109:         "%g0", "%g1", "%g2", "%g3", "%g4", "%g5", "%g6", "%g7",
25109:         "%o0", "%o1", "%o2", "%o3", "%o4", "%o5", "%sp", "%o7",
25109:         "%l0", "%l1", "%l2", "%l3", "%l4", "%l5", "%l6", "%l7",
25109:         "%i0", "%i1", "%i2", "%i3", "%i4", "%i5", "%fp", "%i7",
25109:         "%f0", "%f1", "%f2", "%f3", "%f4", "%f5", "%f6", "%f7",
25109:         "%f8", "%f9", "%f10", "%f11", "%f12", "%f13", "%f14", "%f15",
25109:         "%f16", "%f17", "%f18", "%f19", "%f20", "%f21", "%f22", "%f23",
25109:         "%f24", "%f25", "%f26", "%f27", "%f28", "%f29", "%f30", "%f31"
25109:     };
25109: #endif
25109: 
25109:     const Register Assembler::argRegs[] = { I0, I1, I2, I3, I4, I5 };
25109:     const Register Assembler::retRegs[] = { O0 };
35366:     const Register Assembler::savedRegs[] = { L1 };
25109: 
25109:     static const int kLinkageAreaSize = 68;
25109:     static const int kcalleeAreaSize = 80; // The max size.
25109: 
33125: #define BIT_ROUND_UP(v,q)      ( (((uintptr_t)v)+(q)-1) & ~((q)-1) )
32556: #define TODO(x) do{ verbose_only(outputf(#x);) NanoAssertMsgf(false, "%s", #x); } while(0)
25109: 
25109:     void Assembler::nInit(AvmCore* core)
25109:     {
25109:         has_cmov = true;
25109:     }
25109: 
32634:     void Assembler::nBeginAssembly() {
32634:     }
32634: 
25109:     NIns* Assembler::genPrologue()
25109:     {
25109:         /**
25109:          * Prologue
25109:          */
30551:         underrunProtect(16);
36664:         uint32_t stackNeeded = STACK_GRANULARITY * _activation.stackSlotsNeeded();
25109:         uint32_t frameSize = stackNeeded + kcalleeAreaSize + kLinkageAreaSize;
25109:         frameSize = BIT_ROUND_UP(frameSize, 8);
25109: 
30551:         if (frameSize <= 4096)
30551:             SUBI(FP, frameSize, SP);
30551:         else {
30551:             SUB(FP, G1, SP);
30553:             ORI(G1, frameSize & 0x3FF, G1);
30553:             SETHI(frameSize, G1);
30551:         }
30551: 
33125:         verbose_only(
33125:         if (_logc->lcbits & LC_Assembly) {
33125:             outputf("        %p:",_nIns);
35364:             outputf("        patch entry:");
33125:         })
25109:         NIns *patchEntry = _nIns;
30551: 
30551:         // The frame size in SAVE is faked. We will still re-caculate SP later.
30551:         // We can use 0 here but it is not good for debuggers.
30551:         SAVEI(SP, -148, SP);
25109: 
25109:         // align the entry point
25109:         asm_align_code();
25109: 
25109:         return patchEntry;
25109:     }
25109: 
25109:     void Assembler::asm_align_code() {
25109:         while(uintptr_t(_nIns) & 15) {
25109:             NOP();
25109:         }
25109:     }
25109: 
25109:     void Assembler::nFragExit(LInsp guard)
25109:     {
25109:         SideExit* exit = guard->record()->exit;
25109:         Fragment *frag = exit->target;
25109:         GuardRecord *lr;
25109:         if (frag && frag->fragEntry)
25109:             {
25109:                 JMP(frag->fragEntry);
25109:                 lr = 0;
25109:             }
25109:         else
25109:             {
32634:                 // Target doesn't exit yet. Emit jump to epilog, and set up to patch later.
32634:                 if (!_epilogue)
32634:                     _epilogue = genEpilogue();
25109:                 lr = guard->record();
25109:                 JMP_long((intptr_t)_epilogue);
25109:                 lr->jmp = _nIns;
25109:             }
25109: 
25109:         // return value is GuardRecord*
25109:         SET32(int(lr), O0);
25109:     }
25109: 
25109:     NIns *Assembler::genEpilogue()
25109:     {
25109:         underrunProtect(12);
25109:         RESTORE(G0, G0, G0); //restore
25109:         JMPLI(I7, 8, G0); //ret
25109:         ORI(O0, 0, I0);
25109:         return  _nIns;
25109:     }
25109: 
25109:     void Assembler::asm_call(LInsp ins)
25109:     {
36374:         Register retReg = ( ins->isop(LIR_fcall) ? F0 : retRegs[0] );
36374:         prepResultReg(ins, rmask(retReg));
36374: 
36374:         // Do this after we've handled the call result, so we don't
36374:         // force the call result to be spilled unnecessarily.
36374: 
36374:         evictScratchRegs();
36374: 
25109:         const CallInfo* call = ins->callInfo();
25109: 
25109:         underrunProtect(8);
25109:         NOP();
25109: 
31051:         ArgSize sizes[MAXARGS];
25109:         uint32_t argc = call->get_sizes(sizes);
25109: 
33149:         NanoAssert(ins->isop(LIR_pcall) || ins->isop(LIR_fcall));
30298:         verbose_only(if (_logc->lcbits & LC_Assembly)
25109:                      outputf("        %p:", _nIns);
25109:                      )
33176:         bool indirect = call->isIndirect();
33176:         if (!indirect) {
25109:             CALL(call);
33176:         }
33176:         else {
33176:             argc--;
33176:             Register r = findSpecificRegFor(ins->arg(argc), I0);
33176:             JMPL(G0, I0, 15);
33176:         }
25109: 
25109:         uint32_t GPRIndex = O0;
25109:         uint32_t offset = kLinkageAreaSize; // start of parameters stack postion.
25109: 
25109:         for(int i=0; i<argc; i++)
25109:             {
25109:                 uint32_t j = argc-i-1;
25109:                 ArgSize sz = sizes[j];
25109:                 if (sz == ARGSIZE_F) {
25109:                     Register r = findRegFor(ins->arg(j), FpRegs);
25109:                     GPRIndex += 2;
25109:                     offset += 8;
25109: 
25109:                     underrunProtect(48);
25109:                     // We might be calling a varargs function.
25109:                     // So, make sure the GPR's are also loaded with
25109:                     // the value, or the stack contains it.
25109:                     if (GPRIndex-2 <= O5) {
25109:                         LDSW32(SP, offset-8, (Register)(GPRIndex-2));
25109:                     }
25109:                     if (GPRIndex-1 <= O5) {
25109:                         LDSW32(SP, offset-4, (Register)(GPRIndex-1));
25109:                     }
25109:                     STDF32(r, offset-8, SP);
25109:                 } else {
25109:                     if (GPRIndex > O5) {
25109:                         underrunProtect(12);
25109:                         Register r = findRegFor(ins->arg(j), GpRegs);
25109:                         STW32(r, offset, SP);
25109:                     } else {
25109:                         Register r = findSpecificRegFor(ins->arg(j), (Register)GPRIndex);
25109:                     }
25109:                     GPRIndex++;
25109:                     offset += 4;
25109:                 }
25109:             }
25109:     }
25109: 
32584:     Register Assembler::nRegisterAllocFromSet(RegisterMask set)
25109:     {
25109:         // need to implement faster way
25109:         int i=0;
25109:         while (!(set & rmask((Register)i)))
25109:             i ++;
25109:         _allocator.free &= ~rmask((Register)i);
25109:         return (Register) i;
25109:     }
25109: 
25109:     void Assembler::nRegisterResetAll(RegAlloc& a)
25109:     {
25109:         a.clear();
25109:         a.free = GpRegs | FpRegs;
25109:         debug_only( a.managed = a.free; )
25109:     }
25109: 
32583:     void Assembler::nPatchBranch(NIns* branch, NIns* location)
25109:     {
25109:         *(uint32_t*)&branch[0] &= 0xFFC00000;
25109:         *(uint32_t*)&branch[0] |= ((intptr_t)location >> 10) & 0x3FFFFF;
25109:         *(uint32_t*)&branch[1] &= 0xFFFFFC00;
25109:         *(uint32_t*)&branch[1] |= (intptr_t)location & 0x3FF;
25109:     }
25109: 
25109:     RegisterMask Assembler::hint(LIns* i, RegisterMask allow)
25109:     {
25109:         return allow;
25109:     }
25109: 
25109:     void Assembler::asm_qjoin(LIns *ins)
25109:     {
25109:         underrunProtect(40);
25109:         int d = findMemFor(ins);
25109:         AvmAssert(d);
25109:         LIns* lo = ins->oprnd1();
25109:         LIns* hi = ins->oprnd2();
25109: 
32754:         Register rr = ins->getReg();
32754:         if (isKnownReg(rr) && (rmask(rr) & FpRegs))
36666:             evict(ins);
25109: 
25109:         if (hi->isconst()) {
30048:             STW32(L2, d+4, FP);
30048:             SET32(hi->imm32(), L2);
25109:         } else {
25109:             Register rh = findRegFor(hi, GpRegs);
25109:             STW32(rh, d+4, FP);
25109:         }
25109: 
25109:         if (lo->isconst()) {
30048:             STW32(L2, d, FP);
30048:             SET32(lo->imm32(), L2);
25109:         } else {
25109:             // okay if r gets recycled.
25109:             Register rl = findRegFor(lo, GpRegs);
25109:             STW32(rl, d, FP);
25109:         }
25109: 
25109:         freeRsrcOf(ins, false);    // if we had a reg in use, emit a ST to flush it to mem
25109:     }
25109: 
25109: 
35365:     void Assembler::asm_restore(LInsp i, Register r)
25109:     {
25109:         underrunProtect(24);
32582:         if (i->isop(LIR_alloc)) {
30048:             ADD(FP, L2, r);
32754:             int32_t d = disp(i);
32754:             SET32(d, L2);
25109:         }
25109:         else if (i->isconst()) {
32754:             if (!i->getArIndex()) {
32754:                 i->markAsClear();
25109:             }
28219:             int v = i->imm32();
25109:             SET32(v, r);
25109:         } else {
25109:             int d = findMemFor(i);
25109:             if (rmask(r) & FpRegs) {
25109:                 LDDF32(FP, d, r);
25109:             } else {
25109:                 LDSW32(FP, d, r);
25109:             }
25109:         }
25109:     }
25109: 
36372:     void Assembler::asm_store32(LOpcode op, LIns *value, int dr, LIns *base)
25109:     {
36372:         switch (op) {
36372:             case LIR_sti:
36372:                 // handled by mainline code below for now
36372:                 break;
36372:             case LIR_stb:
36372:             case LIR_sts:
36372:                 NanoAssertMsg(0, "NJ_EXPANDED_LOADSTORE_SUPPORTED not yet supported for this architecture");
36372:                 return;
36372:             default:
36372:                 NanoAssertMsg(0, "asm_store32 should never receive this LIR opcode");
36372:                 return;
36372:         }
36372: 
25109:         underrunProtect(20);
25109:         if (value->isconst())
25109:             {
36552:                 Register rb = getBaseReg(base, dr, GpRegs);
28219:                 int c = value->imm32();
30048:                 STW32(L2, dr, rb);
30048:                 SET32(c, L2);
25109:             }
25109:         else
25109:             {
25109:                 // make sure what is in a register
25109:                 Register ra, rb;
37705:                 if (base->isconst()) {
25109:                     // absolute address
28219:                     dr += base->imm32();
25109:                     ra = findRegFor(value, GpRegs);
25109:                     rb = G0;
25109:                 } else {
37705:                     getBaseReg2(GpRegs, value, ra, GpRegs, base, rb, dr);
25109:                 }
25109:                 STW32(ra, dr, rb);
25109:             }
25109:     }
25109: 
25109:     void Assembler::asm_spill(Register rr, int d, bool pop, bool quad)
25109:     {
25109:         underrunProtect(24);
25109:         (void)quad;
25109:         if (d) {
25109:             if (rmask(rr) & FpRegs) {
25109:                 STDF32(rr, d, FP);
25109:             } else {
25109:                 STW32(rr, d, FP);
25109:             }
25109:         }
25109:     }
25109: 
25109:     void Assembler::asm_load64(LInsp ins)
25109:     {
37020:         NanoAssert(!ins->isop(LIR_ldq) && !ins->isop(LIR_ldqc));
37020: 
36372:         switch (ins->opcode()) {
37020:             case LIR_ldf:
37020:             case LIR_ldfc:
36372:                 // handled by mainline code below for now
36372:                 break;
36372:             case LIR_ld32f:
36372:             case LIR_ldc32f:
36372:                 NanoAssertMsg(0, "NJ_EXPANDED_LOADSTORE_SUPPORTED not yet supported for this architecture");
36372:                 return;
36372:             default:
36372:                 NanoAssertMsg(0, "asm_load64 should never receive this LIR opcode");
36372:                 return;
36372:         }
36372: 
25109:         underrunProtect(72);
25109:         LIns* base = ins->oprnd1();
30238:         int db = ins->disp();
32754:         Register rr = ins->getReg();
25109: 
32754:         int dr = disp(ins);
25109:         Register rb;
32582:         if (base->isop(LIR_alloc)) {
25109:             rb = FP;
25109:             db += findMemFor(base);
25109:         } else {
25109:             rb = findRegFor(base, GpRegs);
25109:         }
32754:         ins->setReg(UnknownReg);
25109: 
25109:         // don't use an fpu reg to simply load & store the value.
25109:         if (dr)
25109:             asm_mmq(FP, dr, rb, db);
25109: 
25109:         freeRsrcOf(ins, false);
25109: 
25109:         if (rr != UnknownReg)
25109:             {
25109:                 NanoAssert(rmask(rr)&FpRegs);
25109:                 _allocator.retire(rr);
25109:                 LDDF32(rb, db, rr);
25109:             }
25109:     }
25109: 
36372:     void Assembler::asm_store64(LOpcode op, LInsp value, int dr, LInsp base)
25109:     {
37020:         NanoAssert(op != LIR_stqi);
37020: 
36372:         switch (op) {
37020:             case LIR_stfi:
36372:                 // handled by mainline code below for now
36372:                 break;
36372:             case LIR_st32f:
36372:                 NanoAssertMsg(0, "NJ_EXPANDED_LOADSTORE_SUPPORTED not yet supported for this architecture");
36372:                 return;
36372:             default:
36372:                 NanoAssertMsg(0, "asm_store64 should never receive this LIR opcode");
36372:                 return;
36372:         }
36372: 
25109:         underrunProtect(48);
25109:         if (value->isconstq())
25109:             {
25109:                 // if a constant 64-bit value just store it now rather than
25109:                 // generating a pointless store/load/store sequence
25109:                 Register rb = findRegFor(base, GpRegs);
30048:                 STW32(L2, dr+4, rb);
30048:                 SET32(value->imm64_0(), L2);
30048:                 STW32(L2, dr, rb);
30048:                 SET32(value->imm64_1(), L2);
25109:                 return;
25109:             }
25109: 
37020:         if (value->isop(LIR_ldf) || value->isop(LIR_ldfc) || value->isop(LIR_qjoin))
25109:             {
25109:                 // value is 64bit struct or int64_t, or maybe a double.
25109:                 // it may be live in an FPU reg.  Either way, don't
25109:                 // put it in an FPU reg just to load & store it.
25109: 
25109:                 // a) if we know it's not a double, this is right.
25109:                 // b) if we guarded that its a double, this store could be on
25109:                 // the side exit, copying a non-double.
25109:                 // c) maybe its a double just being stored.  oh well.
25109: 
25109:                 int da = findMemFor(value);
25109:                 Register rb;
32582:                 if (base->isop(LIR_alloc)) {
25109:                     rb = FP;
25109:                     dr += findMemFor(base);
25109:                 } else {
25109:                     rb = findRegFor(base, GpRegs);
25109:                 }
25109:                 asm_mmq(rb, dr, FP, da);
25109:                 return;
25109:             }
25109: 
37020:         NanoAssert(!value->isop(LIR_ldq) || !value->isop(LIR_ldqc));
25109:         Register rb;
32582:         if (base->isop(LIR_alloc)) {
25109:             rb = FP;
25109:             dr += findMemFor(base);
25109:         } else {
25109:             rb = findRegFor(base, GpRegs);
25109:         }
25109: 
25109:         // if value already in a reg, use that, otherwise
25109:         // try to get it into XMM regs before FPU regs.
32754:         Register rv = ( value->isUnusedOrHasUnknownReg()
32754:                       ? findRegFor(value, FpRegs)
32754:                       : value->getReg() );
25109: 
25109:         STDF32(rv, dr, rb);
25109:     }
25109: 
25109:     /**
25109:      * copy 64 bits: (rd+dd) <- (rs+ds)
25109:      */
25109:     void Assembler::asm_mmq(Register rd, int dd, Register rs, int ds)
25109:     {
25109:         // value is either a 64bit struct or maybe a float
25109:         // that isn't live in an FPU reg.  Either way, don't
25109:         // put it in an FPU reg just to load & store it.
35316:         Register t = registerAllocTmp(GpRegs & ~(rmask(rd)|rmask(rs)));
25109:         STW32(t, dd+4, rd);
25109:         LDSW32(rs, ds+4, t);
25109:         STW32(t, dd, rd);
25109:         LDSW32(rs, ds, t);
25109:     }
25109: 
30730:     NIns* Assembler::asm_branch(bool branchOnFalse, LInsp cond, NIns* targ)
25109:     {
25109:         NIns* at = 0;
25109:         LOpcode condop = cond->opcode();
25109:         NanoAssert(cond->isCond());
25109:         if (condop >= LIR_feq && condop <= LIR_fge)
25109:             {
35314:                 return asm_fbranch(branchOnFalse, cond, targ);
25109:             }
25109: 
25109:         underrunProtect(32);
25109:         intptr_t tt = ((intptr_t)targ - (intptr_t)_nIns + 8) >> 2;
25109:         // !targ means that it needs patch.
25109:         if( !(isIMM22((int32_t)tt)) || !targ ) {
25109:             JMP_long_nocheck((intptr_t)targ);
25109:             at = _nIns;
25109:             NOP();
25109:             BA(0, 5);
25109:             tt = 4;
25109:         }
25109:         NOP();
25109: 
25109: 
25109:         // produce the branch
25109:         if (branchOnFalse)
25109:             {
25109:                 if (condop == LIR_eq)
25109:                     BNE(0, tt);
25109:                 else if (condop == LIR_ov)
25109:                     BVC(0, tt);
25109:                 else if (condop == LIR_lt)
25109:                     BGE(0, tt);
25109:                 else if (condop == LIR_le)
25109:                     BG(0, tt);
25109:                 else if (condop == LIR_gt)
25109:                     BLE(0, tt);
25109:                 else if (condop == LIR_ge)
25109:                     BL(0, tt);
25109:                 else if (condop == LIR_ult)
25109:                     BCC(0, tt);
25109:                 else if (condop == LIR_ule)
25109:                     BGU(0, tt);
25109:                 else if (condop == LIR_ugt)
25109:                     BLEU(0, tt);
25109:                 else //if (condop == LIR_uge)
25109:                     BCS(0, tt);
25109:             }
25109:         else // op == LIR_xt
25109:             {
25109:                 if (condop == LIR_eq)
25109:                     BE(0, tt);
25109:                 else if (condop == LIR_ov)
25109:                     BVS(0, tt);
25109:                 else if (condop == LIR_lt)
25109:                     BL(0, tt);
25109:                 else if (condop == LIR_le)
25109:                     BLE(0, tt);
25109:                 else if (condop == LIR_gt)
25109:                     BG(0, tt);
25109:                 else if (condop == LIR_ge)
25109:                     BGE(0, tt);
25109:                 else if (condop == LIR_ult)
25109:                     BCS(0, tt);
25109:                 else if (condop == LIR_ule)
25109:                     BLEU(0, tt);
25109:                 else if (condop == LIR_ugt)
25109:                     BGU(0, tt);
25109:                 else //if (condop == LIR_uge)
25109:                     BCC(0, tt);
25109:             }
25109:         asm_cmp(cond);
25109:         return at;
25109:     }
25109: 
25109:     void Assembler::asm_cmp(LIns *cond)
25109:     {
25109:         underrunProtect(12);
25109:         LOpcode condop = cond->opcode();
25109: 
30029:         // LIR_ov recycles the flags set by arithmetic ops
33125:         if (condop == LIR_ov)
25109:             return;
25109: 
25109:         LInsp lhs = cond->oprnd1();
25109:         LInsp rhs = cond->oprnd2();
25109: 
25109:         NanoAssert((!lhs->isQuad() && !rhs->isQuad()) || (lhs->isQuad() && rhs->isQuad()));
25109: 
25109:         NanoAssert(!lhs->isQuad() && !rhs->isQuad());
25109: 
25109:         // ready to issue the compare
25109:         if (rhs->isconst())
25109:             {
28219:                 int c = rhs->imm32();
36549:                 Register r = findRegFor(lhs, GpRegs);
25109:                 if (c == 0 && cond->isop(LIR_eq)) {
25109:                     ANDCC(r, r, G0);
25109:                 }
36549:                 else {
30048:                     SUBCC(r, L2, G0);
30048:                     SET32(c, L2);
25109:                 }
25109:             }
25109:         else
25109:             {
32754:                 Register ra, rb;
37705:                 findRegFor2(GpRegs, lhs, ra, GpRegs, rhs, rb);
25109:                 SUBCC(ra, rb, G0);
25109:             }
25109:     }
25109: 
25109:     void Assembler::asm_fcond(LInsp ins)
25109:     {
25109:         // only want certain regs
25109:         Register r = prepResultReg(ins, AllowableFlagRegs);
35314:         underrunProtect(8);
35319:         LOpcode condop = ins->opcode();
35314:         NanoAssert(condop >= LIR_feq && condop <= LIR_fge);
35314:         if (condop == LIR_feq)
35314:             MOVFEI(1, 0, 0, 0, r);
35314:         else if (condop == LIR_fle)
35314:             MOVFLEI(1, 0, 0, 0, r);
35314:         else if (condop == LIR_flt)
35314:             MOVFLI(1, 0, 0, 0, r);
35314:         else if (condop == LIR_fge)
35314:             MOVFGEI(1, 0, 0, 0, r);
35314:         else // if (condop == LIR_fgt)
35314:             MOVFGI(1, 0, 0, 0, r);
35314:         ORI(G0, 0, r);
35319:         asm_fcmp(ins);
25109:     }
25109: 
25109:     void Assembler::asm_cond(LInsp ins)
25109:     {
25109:         underrunProtect(8);
25109:         // only want certain regs
25109:         LOpcode op = ins->opcode();
25109:         Register r = prepResultReg(ins, AllowableFlagRegs);
25109: 
25109:         if (op == LIR_eq)
25109:             MOVEI(1, 1, 0, 0, r);
25109:         else if (op == LIR_ov)
25109:             MOVVSI(1, 1, 0, 0, r);
25109:         else if (op == LIR_lt)
25109:             MOVLI(1, 1, 0, 0, r);
25109:         else if (op == LIR_le)
25109:             MOVLEI(1, 1, 0, 0, r);
25109:         else if (op == LIR_gt)
25109:             MOVGI(1, 1, 0, 0, r);
25109:         else if (op == LIR_ge)
25109:             MOVGEI(1, 1, 0, 0, r);
25109:         else if (op == LIR_ult)
25109:             MOVEI(1, 1, 0, 0, r);
25109:         else if (op == LIR_ule)
25109:             MOVLEUI(1, 1, 0, 0, r);
25109:         else if (op == LIR_ugt)
25109:             MOVGUI(1, 1, 0, 0, r);
25109:         else // if (op == LIR_uge)
25109:             MOVCCI(1, 1, 0, 0, r);
25109:         ORI(G0, 0, r);
25109:         asm_cmp(ins);
25109:     }
25109: 
25109:     void Assembler::asm_arith(LInsp ins)
25109:     {
25109:         underrunProtect(28);
25109:         LOpcode op = ins->opcode();
25109:         LInsp lhs = ins->oprnd1();
25109:         LInsp rhs = ins->oprnd2();
25109: 
25109:         Register rb = UnknownReg;
25109:         RegisterMask allow = GpRegs;
25109:         bool forceReg = (op == LIR_mul || !rhs->isconst());
25109: 
25109:         if (lhs != rhs && forceReg)
25109:             {
25109:                 if ((rb = asm_binop_rhs_reg(ins)) == UnknownReg) {
25109:                     rb = findRegFor(rhs, allow);
25109:                 }
25109:                 allow &= ~rmask(rb);
25109:             }
32582:         else if ((op == LIR_add||op == LIR_iaddp) && lhs->isop(LIR_alloc) && rhs->isconst()) {
25109:             // add alloc+const, use lea
25109:             Register rr = prepResultReg(ins, allow);
28219:             int d = findMemFor(lhs) + rhs->imm32();
30048:             ADD(FP, L2, rr);
30048:             SET32(d, L2);
25109:         }
25109: 
25109:         Register rr = prepResultReg(ins, allow);
25109:         // if this is last use of lhs in reg, we can re-use result reg
32754:         // else, lhs already has a register assigned.
32754:         Register ra = ( lhs->isUnusedOrHasUnknownReg()
32754:                       ? findSpecificRegFor(lhs, rr)
32754:                       : lhs->getReg() );
25109: 
25109:         if (forceReg)
25109:             {
25109:                 if (lhs == rhs)
25109:                     rb = ra;
25109: 
30437:                 if (op == LIR_add || op == LIR_iaddp)
27640:                     ADDCC(rr, rb, rr);
25109:                 else if (op == LIR_sub)
27640:                     SUBCC(rr, rb, rr);
25109:                 else if (op == LIR_mul)
25109:                     MULX(rr, rb, rr);
25109:                 else if (op == LIR_and)
25109:                     AND(rr, rb, rr);
25109:                 else if (op == LIR_or)
25109:                     OR(rr, rb, rr);
25109:                 else if (op == LIR_xor)
25109:                     XOR(rr, rb, rr);
25109:                 else if (op == LIR_lsh)
25109:                     SLL(rr, rb, rr);
25109:                 else if (op == LIR_rsh)
25109:                     SRA(rr, rb, rr);
25109:                 else if (op == LIR_ush)
25109:                     SRL(rr, rb, rr);
25109:                 else
25109:                     NanoAssertMsg(0, "Unsupported");
25109:             }
25109:         else
25109:             {
28219:                 int c = rhs->imm32();
30437:                 if (op == LIR_add || op == LIR_iaddp) {
30048:                     ADDCC(rr, L2, rr);
25109:                 } else if (op == LIR_sub) {
30048:                     SUBCC(rr, L2, rr);
25109:                 } else if (op == LIR_and)
30048:                     AND(rr, L2, rr);
25109:                 else if (op == LIR_or)
30048:                     OR(rr, L2, rr);
25109:                 else if (op == LIR_xor)
30048:                     XOR(rr, L2, rr);
25109:                 else if (op == LIR_lsh)
30048:                     SLL(rr, L2, rr);
25109:                 else if (op == LIR_rsh)
30048:                     SRA(rr, L2, rr);
25109:                 else if (op == LIR_ush)
30048:                     SRL(rr, L2, rr);
25109:                 else
25109:                     NanoAssertMsg(0, "Unsupported");
30048:                 SET32(c, L2);
25109:             }
25109: 
25109:         if ( rr != ra )
25109:             ORI(ra, 0, rr);
25109:     }
25109: 
25109:     void Assembler::asm_neg_not(LInsp ins)
25109:     {
25109:         underrunProtect(8);
25109:         LOpcode op = ins->opcode();
25109:         Register rr = prepResultReg(ins, GpRegs);
25109: 
25109:         LIns* lhs = ins->oprnd1();
25109:         // if this is last use of lhs in reg, we can re-use result reg
32754:         // else, lhs already has a register assigned.
32754:         Register ra = ( lhs->isUnusedOrHasUnknownReg()
32754:                       ? findSpecificRegFor(lhs, rr)
32754:                       : lhs->getReg() );
25109: 
25109:         if (op == LIR_not)
25109:             ORN(G0, rr, rr);
25109:         else
25109:             SUB(G0, rr, rr);
25109: 
25109:         if ( rr != ra )
25109:             ORI(ra, 0, rr);
25109:     }
25109: 
36372:     void Assembler::asm_load32(LInsp ins)
25109:     {
25109:         underrunProtect(12);
25109:         LOpcode op = ins->opcode();
25109:         LIns* base = ins->oprnd1();
30238:         int d = ins->disp();
25109:         Register rr = prepResultReg(ins, GpRegs);
36552:         Register ra = getBaseReg(base, d, GpRegs);
36372:         switch(op) {
36372:             case LIR_ldzb:
36372:             case LIR_ldcb:
25587:                 LDUB32(ra, d, rr);
36372:                 break;
36372:             case LIR_ldzs:
36372:             case LIR_ldcs:
25587:                 LDUH32(ra, d, rr);
36372:                 break;
36372:             case LIR_ld:
36372:             case LIR_ldc:
25109:                 LDSW32(ra, d, rr);
36372:                 break;
36372:             case LIR_ldsb:
36372:             case LIR_ldss:
36372:             case LIR_ldcsb:
36372:             case LIR_ldcss:
36372:                 NanoAssertMsg(0, "NJ_EXPANDED_LOADSTORE_SUPPORTED not yet supported for this architecture");
36372:                 return;
36372:             default:
36372:                 NanoAssertMsg(0, "asm_load32 should never receive this LIR opcode");
36372:                 return;
25109:         }
25109:     }
25109: 
25109:     void Assembler::asm_cmov(LInsp ins)
25109:     {
25109:         underrunProtect(4);
25109:         LOpcode op = ins->opcode();
25109:         LIns* condval = ins->oprnd1();
30648:         LIns* iftrue  = ins->oprnd2();
30648:         LIns* iffalse = ins->oprnd3();
30648: 
30485:         NanoAssert(condval->isCmp());
25109:         NanoAssert(op == LIR_qcmov || (!iftrue->isQuad() && !iffalse->isQuad()));
25109: 
25109:         const Register rr = prepResultReg(ins, GpRegs);
25109: 
25109:         // this code assumes that neither LD nor MR nor MRcc set any of the condition flags.
25109:         // (This is true on Intel, is it true on all architectures?)
25109:         const Register iffalsereg = findRegFor(iffalse, GpRegs & ~rmask(rr));
25109:         if (op == LIR_cmov) {
25109:             switch (condval->opcode()) {
25109:                 // note that these are all opposites...
25109:             case LIR_eq:  MOVNE (iffalsereg, 1, 0, 0, rr); break;
25109:             case LIR_ov:  MOVVC (iffalsereg, 1, 0, 0, rr); break;
25109:             case LIR_lt:  MOVGE (iffalsereg, 1, 0, 0, rr); break;
25109:             case LIR_le:  MOVG  (iffalsereg, 1, 0, 0, rr); break;
25109:             case LIR_gt:  MOVLE (iffalsereg, 1, 0, 0, rr); break;
25109:             case LIR_ge:  MOVL  (iffalsereg, 1, 0, 0, rr); break;
25109:             case LIR_ult: MOVCC (iffalsereg, 1, 0, 0, rr); break;
25109:             case LIR_ule: MOVGU (iffalsereg, 1, 0, 0, rr); break;
25109:             case LIR_ugt: MOVLEU(iffalsereg, 1, 0, 0, rr); break;
25109:             case LIR_uge: MOVCS (iffalsereg, 1, 0, 0, rr); break;
25109:                 debug_only( default: NanoAssert(0); break; )
25109:                     }
25109:         } else if (op == LIR_qcmov) {
25109:             NanoAssert(0);
25109:         }
25109:         /*const Register iftruereg =*/ findSpecificRegFor(iftrue, rr);
25109:         asm_cmp(condval);
25109:     }
25109: 
25109:     void Assembler::asm_qhi(LInsp ins)
25109:     {
25109:         underrunProtect(12);
25109:         Register rr = prepResultReg(ins, GpRegs);
25109:         LIns *q = ins->oprnd1();
25109:         int d = findMemFor(q);
25109:         LDSW32(FP, d+4, rr);
25109:     }
25109: 
25109:     void Assembler::asm_param(LInsp ins)
25109:     {
30026:         uint32_t a = ins->paramArg();
30026:         uint32_t kind = ins->paramKind();
25109:         prepResultReg(ins, rmask(argRegs[a]));
25109:     }
25109: 
25109:     void Assembler::asm_int(LInsp ins)
25109:     {
25109:         underrunProtect(8);
25109:         Register rr = prepResultReg(ins, GpRegs);
25109:         int32_t val = ins->imm32();
25109:         if (val == 0)
25109:             XOR(rr, rr, rr);
25109:         else
25109:             SET32(val, rr);
25109:     }
25109: 
25109:     void Assembler::asm_quad(LInsp ins)
25109:     {
25109:         underrunProtect(64);
32754:         Register rr = ins->getReg();
25109:         if (rr != UnknownReg)
25109:             {
25109:                 // @todo -- add special-cases for 0 and 1
25109:                 _allocator.retire(rr);
32754:                 ins->setReg(UnknownReg);
25109:                 NanoAssert((rmask(rr) & FpRegs) != 0);
25109:                 findMemFor(ins);
32754:                 int d = disp(ins);
25109:                 LDDF32(FP, d, rr);
25109:             }
25109: 
25109:         // @todo, if we used xor, ldsd, fldz, etc above, we don't need mem here
32754:         int d = disp(ins);
25109:         freeRsrcOf(ins, false);
25109:         if (d)
25109:             {
30552:                 STW32(L2, d+4, FP);
30552:                 SET32(ins->imm64_0(), L2);
30552:                 STW32(L2, d, FP);
30552:                 SET32(ins->imm64_1(), L2);
25109:             }
25109:     }
25109: 
25109:     void Assembler::asm_qlo(LInsp ins)
25109:     {
25109:     }
25109: 
25109:     void Assembler::asm_fneg(LInsp ins)
25109:     {
25109:         underrunProtect(4);
25109:         Register rr = prepResultReg(ins, FpRegs);
25109:         LIns* lhs = ins->oprnd1();
25109: 
25109:         // lhs into reg, prefer same reg as result
25109:         // if this is last use of lhs in reg, we can re-use result reg
32754:         // else, lhs already has a different reg assigned
32754:         Register ra = ( lhs->isUnusedOrHasUnknownReg()
32754:                       ? findSpecificRegFor(lhs, rr)
32754:                       : findRegFor(lhs, FpRegs) );
25109: 
25109:         FNEGD(ra, rr);
25109:     }
25109: 
25109:     void Assembler::asm_fop(LInsp ins)
25109:     {
25109:         underrunProtect(4);
25109:         LOpcode op = ins->opcode();
25109:         LIns *lhs = ins->oprnd1();
25109:         LIns *rhs = ins->oprnd2();
25109: 
25109:         RegisterMask allow = FpRegs;
25109:         Register ra = findRegFor(lhs, FpRegs);
25109:         Register rb = (rhs == lhs) ? ra : findRegFor(rhs, FpRegs);
25109: 
25109:         Register rr = prepResultReg(ins, allow);
25109: 
25109:         if (op == LIR_fadd)
25109:             FADDD(ra, rb, rr);
25109:         else if (op == LIR_fsub)
25109:             FSUBD(ra, rb, rr);
25109:         else if (op == LIR_fmul)
25109:             FMULD(ra, rb, rr);
25109:         else //if (op == LIR_fdiv)
25109:             FDIVD(ra, rb, rr);
25109: 
25109:     }
25109: 
25109:     void Assembler::asm_i2f(LInsp ins)
25109:     {
25109:         underrunProtect(32);
25109:         // where our result goes
25109:         Register rr = prepResultReg(ins, FpRegs);
25109:         int d = findMemFor(ins->oprnd1());
25109:         FITOD(rr, rr);
25109:         LDDF32(FP, d, rr);
25109:     }
25109: 
25109:     void Assembler::asm_u2f(LInsp ins)
25109:     {
25109:         underrunProtect(72);
25109:         // where our result goes
25109:         Register rr = prepResultReg(ins, FpRegs);
35316:         Register rt = registerAllocTmp(FpRegs & ~(rmask(rr)));
25109:         Register gr = findRegFor(ins->oprnd1(), GpRegs);
25109:         int disp = -8;
25109: 
25109:         FABSS(rr, rr);
25109:         FSUBD(rt, rr, rr);
25109:         LDDF32(SP, disp, rr);
25109:         STWI(G0, disp+4, SP);
25109:         LDDF32(SP, disp, rt);
25109:         STWI(gr, disp+4, SP);
25109:         STWI(G1, disp, SP);
25109:         SETHI(0x43300000, G1);
25109:     }
25109: 
25109:     void Assembler::asm_nongp_copy(Register r, Register s)
25109:     {
25109:         underrunProtect(4);
25109:         NanoAssert((rmask(r) & FpRegs) && (rmask(s) & FpRegs));
25109:         FMOVD(s, r);
25109:     }
25109: 
35314:     NIns * Assembler::asm_fbranch(bool branchOnFalse, LIns *cond, NIns *targ)
25109:     {
25109:         NIns *at = 0;
25109:         LOpcode condop = cond->opcode();
25109:         NanoAssert(condop >= LIR_feq && condop <= LIR_fge);
25109:         underrunProtect(32);
25109:         intptr_t tt = ((intptr_t)targ - (intptr_t)_nIns + 8) >> 2;
25109:         // !targ means that it needs patch.
25109:         if( !(isIMM22((int32_t)tt)) || !targ ) {
25109:             JMP_long_nocheck((intptr_t)targ);
25109:             at = _nIns;
25109:             NOP();
25109:             BA(0, 5);
25109:             tt = 4;
25109:         }
25109:         NOP();
25109: 
25109:         // produce the branch
25109:         if (branchOnFalse)
25109:             {
25109:                 if (condop == LIR_feq)
25109:                     FBNE(0, tt);
25109:                 else if (condop == LIR_fle)
32624:                     FBUG(0, tt);
25109:                 else if (condop == LIR_flt)
32624:                     FBUGE(0, tt);
25109:                 else if (condop == LIR_fge)
32624:                     FBUL(0, tt);
25109:                 else //if (condop == LIR_fgt)
32624:                     FBULE(0, tt);
25109:             }
25109:         else // op == LIR_xt
25109:             {
25109:                 if (condop == LIR_feq)
25109:                     FBE(0, tt);
25109:                 else if (condop == LIR_fle)
25109:                     FBLE(0, tt);
25109:                 else if (condop == LIR_flt)
25109:                     FBL(0, tt);
25109:                 else if (condop == LIR_fge)
25109:                     FBGE(0, tt);
25109:                 else //if (condop == LIR_fgt)
25109:                     FBG(0, tt);
25109:             }
25109:         asm_fcmp(cond);
25109:         return at;
25109:     }
25109: 
25109:     void Assembler::asm_fcmp(LIns *cond)
25109:     {
25109:         underrunProtect(4);
25109:         LIns* lhs = cond->oprnd1();
25109:         LIns* rhs = cond->oprnd2();
25109: 
25109:         Register rLhs = findRegFor(lhs, FpRegs);
25109:         Register rRhs = findRegFor(rhs, FpRegs);
25109: 
25109:         FCMPD(rLhs, rRhs);
25109:     }
25109: 
25109:     void Assembler::nativePageReset()
25109:     {
25109:     }
25109: 
25109:     Register Assembler::asm_binop_rhs_reg(LInsp ins)
25109:     {
25109:         return UnknownReg;
25109:     }
25109: 
25109:     void Assembler::nativePageSetup()
25109:     {
35356:         NanoAssert(!_inExit);
33149:         if (!_nIns)
33149:             codeAlloc(codeStart, codeEnd, _nIns verbose_only(, codeBytes));
25109:     }
25109: 
33939:     // Increment the 32-bit profiling counter at pCtr, without
33939:     // changing any registers.
33939:     verbose_only(
33939:     void Assembler::asm_inc_m32(uint32_t*)
33939:     {
33939:         // todo: implement this
33939:     }
33939:     )
33939: 
25109:     void
32623:     Assembler::underrunProtect(int n)
25109:     {
32623:         NIns *eip = _nIns;
35356:         // This may be in a normal code chunk or an exit code chunk.
35356:         if (eip - n < codeStart) {
33149:             codeAlloc(codeStart, codeEnd, _nIns verbose_only(, codeBytes));
32623:             JMP_long_nocheck((intptr_t)eip);
25109:         }
25109:     }
25109: 
25109:     void Assembler::asm_ret(LInsp ins)
25109:     {
32634:         genEpilogue();
36672:         releaseRegisters();
25109:         assignSavedRegs();
25109:         LIns *val = ins->oprnd1();
25109:         if (ins->isop(LIR_ret)) {
25109:             findSpecificRegFor(val, retRegs[0]);
25109:         } else {
25109:             findSpecificRegFor(val, F0);
25109:         }
25109:     }
32556: 
32556:     void Assembler::asm_promote(LIns *) {
32556:         // i2q or u2q
33176:         TODO(asm_promote);
32556:     }
25109: 
35356:     void Assembler::swapCodeChunks() {
37698:         if (!_nExitIns)
37698:             codeAlloc(exitStart, exitEnd, _nExitIns verbose_only(, exitBytes));
35356:         SWAP(NIns*, _nIns, _nExitIns);
35356:         SWAP(NIns*, codeStart, exitStart);
35356:         SWAP(NIns*, codeEnd, exitEnd);
35356:         verbose_only( SWAP(size_t, codeBytes, exitBytes); )
35356:     }
35356: 
25109: #endif /* FEATURE_NANOJIT */
25109: }
